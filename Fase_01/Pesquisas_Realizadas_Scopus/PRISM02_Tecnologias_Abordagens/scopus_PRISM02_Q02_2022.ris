TY  - CONF
AU  - Licari, D.
AU  - Comandè, G.
TI  - ITALIAN-LEGAL-BERT: A Pre-trained Transformer Language Model for Italian Law
PY  - 2022
T2  - CEUR Workshop Proceedings
VL  - 3256
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142425721&partnerID=40&md5=13eb2add494a1be9f2058a69dddd5aab
AD  - EMbeDS, Sant'Anna School of Advanced Studies, Pisa, 56127, Italy
AB  - The state of the art in natural language processing is based on transformer models that are pre-trained on general knowledge and enable efficient transfer learning in a wide variety of downstream tasks even with limited data sets. However, these models significantly decrease performance when operating in specific and sectoral domains. This is problematic in the Italian legal context, as there are many discrepancies between the language found in generic open source corpora (e.g., Wikipedia and news articles) and legal language, which can be cryptic, Latin-based, and domain idiolectal formulas. In this paper, we introduce the ITALIAN-LEGAL-BERT model with additional pre-training of the Italian BERT model on Italian civil law corpora. It achieves better results than the 'general-purpose' Italian BERT in different domain-specific tasks. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0)
KW  - Italian Legal BERT
KW  - Legal artificial intelligence
KW  - Pre-trained language model
KW  - Natural language processing systems
KW  - Creative Commons
KW  - General knowledge
KW  - Italian legal BERT
KW  - Language model
KW  - Language processing
KW  - Legal artificial intelligence
KW  - Natural languages
KW  - Pre-trained language model
KW  - State of the art
KW  - Transformer modeling
KW  - Computational linguistics
A2  - Symeonidou D.
A2  - INRAE, Place Pierre Viala 2, Montpellier
A2  - Yu R.
A2  - University of Bonn, Friedrich-Hirzebruch-Allee 8, Bonn
A2  - Ceolin D.
A2  - CWI, Science Park 123, Amsterdam
A2  - Poveda-Villalon M.
A2  - Audrito D.
A2  - Universiy of Bologna, Via Zamboni 27/29, Bologna
A2  - Di Caro L.
A2  - University of Turin, Corso Svizzera 185, Torino
A2  - Grasso F.
A2  - University of Turin, Corso Svizzera 185, Torino
A2  - Nai R.
A2  - University of Turin, Corso Svizzera 185, Torino
A2  - Sulis E.
A2  - University of Turin, Corso Svizzera 185, Torino
A2  - Ekaputra F.J.
A2  - TU Wien, Favoritenstrasse 9-11, Vienna
A2  - Kutz O.
A2  - Free University of Bozen-Bolzano, Dominikanerplatz 3 - piazza Domenicani, Bolzano
A2  - Troquard N.
A2  - Free University of Bozen-Bolzano, Dominikanerplatz 3 - piazza Domenicani, Bolzano
PB  - CEUR-WS
SN  - 16130073 (ISSN)
LA  - English
J2  - CEUR Workshop Proc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 19; Correspondence Address: D. Licari; EMbeDS, Sant'Anna School of Advanced Studies, Pisa, 56127, Italy; email: d.licari@santannapisa.it; Conference name: 23rd International Conference on Knowledge Engineering and Knowledge Management, EKAW-C 2022; Conference date: 26 September 2022 through 29 September 2022; Conference code: 184046
ER  -

TY  - JOUR
AU  - Chen, G.
AU  - Fang, L.
AU  - Li, T.
AU  - Xiang, Y.
TI  - Ultralow-Loading Pt/Zn Hybrid Cluster in Zeolite HZSM-5 for Efficient Dehydroaromatization
PY  - 2022
T2  - Journal of the American Chemical Society
VL  - 144
IS  - 26
SP  - 11831
EP  - 11839
DO  - 10.1021/jacs.2c04278
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134360349&doi=10.1021%2fjacs.2c04278&partnerID=40&md5=80789726e4b31cc3d3b0227a66797e2e
AD  - Dave C. Swalm School of Chemical Engineering, Mississippi State University, Mississippi State, 39762, MS, United States
AD  - Department of Chemistry and Biochemistry, Northern Illinois University, DeKalb, 60115, IL, United States
AD  - X-ray Science Division, Argonne National Laboratory, Lemont, 60439, IL, United States
AB  - Minimizing Pt loading without sacrificing catalytic performance is critical, particularly for designing cost-efficient hydrocarbon transformation catalysts. Here, we show that ultralow-loading (0.001-0.05 wt %) Pt- and Zn-functionalized HZSM-5 catalysts, prepared through simple ion exchange and impregnation, are highly active and stable for light alkane dehydroaromatization (DHA). The specific activity of benzene, toluene, and xylene is up to 8.2 mol/gPt/min (or 1592 min-1) over the 0.001 wt % Pt-Zn2/HZSM-5 catalyst during ethane DHA at 550 °C under atmospheric pressure. Additionally, such bimetallic Ptx-Zny/HZSM-5 catalysts are highly stable in contrast to the monometallic Pt/HZSM-5 catalysts. The rate constant of deactivation (kdeactiv), according to the first-order generalized power law equation model, for the bimetallic catalysts is up to 120 times lower than that of the monometallic counterparts, depending on the Pt loading. This breakthrough is achieved through the formation of the [Pt1-Znn]δ+ hybrid cluster, instead of Pt0 cluster-proton adducts, in the micropores of the ZSM-5 zeolite.  © 2022 American Chemical Society.
KW  - Catalysts
KW  - Hydrocarbons
KW  - Ion exchange
KW  - Rate constants
KW  - Zeolites
KW  - benzene
KW  - ethane
KW  - platinum
KW  - proton
KW  - toluene
KW  - xylene
KW  - zeolite
KW  - zinc
KW  - Catalytic performance
KW  - Cost-efficient
KW  - Dehydroaromatization
KW  - Hybrid clusters
KW  - HZSM-5 catalyst
KW  - Monometallics
KW  - Pt loading
KW  - Ultralow loading
KW  - Zeolite HZSM-5
KW  - ]+ catalyst
KW  - acidity
KW  - aromatization
KW  - Article
KW  - catalysis
KW  - catalyst
KW  - chemical loading
KW  - chemical procedures
KW  - controlled study
KW  - dehydroaromatization
KW  - energy dispersive X ray spectroscopy
KW  - impregnation
KW  - ion exchange
KW  - scanning transmission electron microscopy
KW  - structure analysis
KW  - synthesis
KW  - X ray absorption spectroscopy
KW  - Atmospheric pressure
PB  - American Chemical Society
SN  - 00027863 (ISSN)
C2  - 35748573
LA  - English
J2  - J. Am. Chem. Soc.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 33; Correspondence Address: Y. Xiang; Dave C. Swalm School of Chemical Engineering, Mississippi State University, Mississippi State, 39762, United States; email: yzxiang@che.msstate.edu; CODEN: JACSA
ER  -

TY  - JOUR
AU  - Weijermars, R.
TI  - Production rate of multi-fractured wells modeled with Gaussian pressure transients
PY  - 2022
T2  - Journal of Petroleum Science and Engineering
VL  - 210
C7  - 110027
DO  - 10.1016/j.petrol.2021.110027
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121749243&doi=10.1016%2fj.petrol.2021.110027&partnerID=40&md5=b3710465c63617ddad4bda86b855d1ce
AD  - Department of Petroleum Engineering & Center for Integrative Petroleum Research (CIPR), College of Petroleum Engineering and Geosciences (CPG), King Fahd University of Petroleum & Minerals, KFUPM, Dhahran, 31261, Saudi Arabia
AB  - This study presents new pressure transient solutions, illustrated with some examples of the vast practical application potential. Gaussian pressure transients (GPT) are derived here to quantify the temporal and spatial propagation of instantaneous pressure changes in porous media, as initiated from cylindrical sources (vertical wells) and planar sources (hydraulic fractures). After solving the scalar pressure field in the reservoir space, and adequately accounting for the interference of the various pressure fronts by mathematical integration and superposition, the resulting pressure gradients solve for the velocity field in the reservoir space. Unique for GPT solutions is that the well rate, unlike in the traditional well-testing equations, does not appear as an input. Applying Darcy's Law, the fluid flux from the reservoir into the well and hydraulic fractures can be directly computed from the GPT solutions. The closed-form production-forecasting model can be implemented either in matrix-coded flow-visualizations of pressure depletion and flow paths for reservoir sections or in grid-less spreadsheet solutions to instantaneously generate production profiles for wells in any type of fluid injection/extraction project (water production, geothermal energy extraction, hydrocarbon production, and fluid disposal wells). Additionally, the Gaussian method also is suitable for physics-based decline curve analysis. The practical examples included in this study are for Eagle Ford shale oil and Marcellus dry gas wells. The hydraulic diffusivities are constrained by the field data, and range between 2.36 x10−10 and 3.48 x10−10 m2 s−1 for the Eagle Ford Formation; for the Marcellus the range is 3.64x10−9 to 5.67x10−8 m2 s−1. The breakthrough solution method of Gaussian pressure transients is placed in the context of past and present modeling approaches for shale plays developed with multi-fractured wells. © 2021 Elsevier B.V.
KW  - Decline curve analysis
KW  - Gaussian pressure transients
KW  - Hydraulic diffusivity
KW  - Multi-fractured wells
KW  - Production forecasting
KW  - Reservoir modeling
KW  - Shale well performance
KW  - Flow of fluids
KW  - Fracture
KW  - Geothermal energy
KW  - Horizontal wells
KW  - Hydraulic fracturing
KW  - Oil wells
KW  - Porous materials
KW  - Shale
KW  - Transient analysis
KW  - Velocity
KW  - Well testing
KW  - Decline curve analysis
KW  - Gaussian pressure transient
KW  - Gaussians
KW  - Hydraulic diffusivity
KW  - Multi-fractured well
KW  - Pressure transient
KW  - Production forecasting
KW  - Reservoir models
KW  - Shale well performance
KW  - Well performance
KW  - diffusivity
KW  - fluid injection
KW  - gas well
KW  - hydraulic fracturing
KW  - hydrocarbon reservoir
KW  - oil shale
KW  - porous medium
KW  - pressure field
KW  - Gaussian distribution
PB  - Elsevier B.V.
SN  - 09204105 (ISSN)
LA  - English
J2  - J. Pet. Sci. Eng.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 19
ER  -

TY  - CONF
AU  - Lin, S.
AU  - Hilton, J.
AU  - Evans, O.
TI  - TruthfulQA: Measuring How Models Mimic Human Falsehoods
PY  - 2022
T2  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
VL  - 1
SP  - 3214
EP  - 3252
DO  - 10.18653/v1/2022.acl-long.229
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139104299&doi=10.18653%2fv1%2f2022.acl-long.229&partnerID=40&md5=e72e7fa18ab7e569520935ef25d8423d
AD  - University of Oxford, United Kingdom
AD  - OpenAI
AB  - We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than finetuning using training objectives other than imitation of text from the web. © 2022 Association for Computational Linguistics.
KW  - Best model
KW  - False beliefs
KW  - Human performance
KW  - Language model
KW  - Large models
KW  - Model size
KW  - Performance
KW  - Scaling-up
KW  - Well model
A2  - Muresan S.
A2  - Nakov P.
A2  - Villavicencio A.
PB  - Association for Computational Linguistics (ACL)
SN  - 0736587X (ISSN); 978-195591721-6 (ISBN)
LA  - English
J2  - Proc. Annu. Meet. Assoc. Comput Linguist.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 251; Conference name: 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022; Conference date: 22 May 2022 through 27 May 2022; Conference code: 181737
ER  -

TY  - JOUR
AU  - de Menezes-Neto, E.J.
AU  - Clementino, M.B.M.
TI  - Using deep learning to predict outcomes of legal appeals better than human experts: A study with data from Brazilian federal courts
PY  - 2022
T2  - PLoS ONE
VL  - 17
IS  - 7 July
C7  - e0272287
DO  - 10.1371/journal.pone.0272287
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135026611&doi=10.1371%2fjournal.pone.0272287&partnerID=40&md5=a317d663629fa3085bd0bef1fca2fe69
AD  - Department of Law, Universidade Federal do Rio Grande do Norte, RN, Caicó, Brazil
AD  - Department of Private Law, Universidade Federal do Rio Grande do Norte, RN, Natal, Brazil
AD  - Tribunal Regional Federal da 5a Região, 6th Federal Court, RN, Natal, Brazil
AB  - Legal scholars have been trying to predict the outcomes of trials for a long time. In recent years, researchers have been harnessing advancements in machine learning to predict the behavior of natural and social processes. At the same time, the Brazilian judiciary faces a challenging number of new cases every year, which generates the need to improve the throughput of the justice system. Based on those premises, we trained three deep learning architectures, ULMFiT, BERT, and Big Bird, on 612,961 Federal Small Claims Courts appeals within the Brazilian 5th Regional Federal Court to predict their outcomes. We compare the predictive performance of the models to the predictions of 22 highly skilled experts. All models outperform human experts, with the best one achieving a Matthews Correlation Coefficient of 0.3688 compared to 0.1253 from the human experts. Our results demonstrate that natural language processing and machine learning techniques provide a promising approach for predicting legal outcomes. We also release the Brazilian Courts Appeal Dataset for the 5th Regional Federal Court (BrCAD-5), containing data from 765,602 appeals to promote further developments in this area.  © 2022 Jacob de Menezes-Neto, Clementino. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
KW  - Brazil
KW  - Deep Learning
KW  - Humans
KW  - Law Enforcement
KW  - Natural Language Processing
KW  - article
KW  - bird
KW  - correlation coefficient
KW  - court
KW  - deep learning
KW  - human
KW  - human experiment
KW  - machine learning
KW  - natural language processing
KW  - nonhuman
KW  - prediction
KW  - Brazil
KW  - law enforcement
PB  - Public Library of Science
SN  - 19326203 (ISSN)
C2  - 35901178
LA  - English
J2  - PLoS ONE
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 16; Correspondence Address: E.J. de Menezes-Neto; Department of Law, Universidade Federal do Rio Grande do Norte, Caicó, RN, Brazil; email: elias.jacob@ufrn.br; CODEN: POLNC
ER  -

TY  - JOUR
AU  - Tagarelli, A.
AU  - Simeri, A.
TI  - Unsupervised law article mining based on deep pre-trained language representation models with application to the Italian civil code
PY  - 2022
T2  - Artificial Intelligence and Law
VL  - 30
IS  - 3
SP  - 417
EP  - 473
DO  - 10.1007/s10506-021-09301-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114943306&doi=10.1007%2fs10506-021-09301-8&partnerID=40&md5=5553a33c279e9f0fa0ab35d2e8d8e6de
AD  - Department of Computer Engineering, Modeling, Electronics, and Systems Engineering (DIMES), University of Calabria, CS, Rende, 87036, Italy
AB  - Modeling law search and retrieval as prediction problems has recently emerged as a predominant approach in law intelligence. Focusing on the law article retrieval task, we present a deep learning framework named LamBERTa, which is designed for civil-law codes, and specifically trained on the Italian civil code. To our knowledge, this is the first study proposing an advanced approach to law article prediction for the Italian legal system based on a BERT (Bidirectional Encoder Representations from Transformers) learning framework, which has recently attracted increased attention among deep learning approaches, showing outstanding effectiveness in several natural language processing and learning tasks. We define LamBERTa models by fine-tuning an Italian pre-trained BERT on the Italian civil code or its portions, for law article retrieval as a classification task. One key aspect of our LamBERTa framework is that we conceived it to address an extreme classification scenario, which is characterized by a high number of classes, the few-shot learning problem, and the lack of test query benchmarks for Italian legal prediction tasks. To solve such issues, we define different methods for the unsupervised labeling of the law articles, which can in principle be applied to any law article code system. We provide insights into the explainability and interpretability of our LamBERTa models, and we present an extensive experimental analysis over query sets of different type, for single-label as well as multi-label evaluation tasks. Empirical evidence has shown the effectiveness of LamBERTa, and also its superiority against widely used deep-learning text classifiers and a few-shot learner conceived for an attribute-aware prediction task. © 2021, The Author(s).
KW  - BERT
KW  - Deep learning
KW  - Deep pre-trained language models
KW  - Italian civil code
KW  - Law article retrieval
KW  - Text classification
KW  - Computational linguistics
KW  - Deep learning
KW  - Forecasting
KW  - Learning algorithms
KW  - Learning systems
KW  - Natural language processing systems
KW  - Text processing
KW  - Bidirectional encoder representation from transformer
KW  - Deep learning
KW  - Deep pre-trained language model
KW  - Italian civil code
KW  - Language model
KW  - Law article retrieval
KW  - Learning frameworks
KW  - Prediction tasks
KW  - Representation model
KW  - Text classification
KW  - Classification (of information)
PB  - Institute for Ionics
SN  - 09248463 (ISSN)
LA  - English
J2  - Artif Intell Law
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 30; Correspondence Address: A. Tagarelli; Department of Computer Engineering, Modeling, Electronics, and Systems Engineering (DIMES), University of Calabria, Rende, CS, 87036, Italy; email: tagarelli@dimes.unical.it; CODEN: AINLE
ER  -

TY  - JOUR
AU  - Wan, C.-X.
AU  - Li, B.
TI  - Financial causal sentence recognition based on BERT-CNN text classification
PY  - 2022
T2  - Journal of Supercomputing
VL  - 78
IS  - 5
SP  - 6503
EP  - 6527
DO  - 10.1007/s11227-021-04097-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117364329&doi=10.1007%2fs11227-021-04097-5&partnerID=40&md5=69ec1e9813b5cc2a2339c11d8c4f1c44
AD  - School of Information and Technology, Jiangxi University of Finance and Economics, Nanchang, 330013, China
AD  - Jiangxi University of Science and Technology, Ganzhou, 340000, China
AD  - Jiangxi Key Laboratory of Data and Knowledge Engineering, Jiangxi University of Finance and Economics, Nanchang, 330013, China
AB  - By studying the causality contained in financial texts, we can further reveal more potential laws of economic activities, such as “factors promoting stable and healthy economic development,” “The central bank's use of the loan window to issue money will increase the probability of inflation,” “The consequence of overcapacity is a decline in product prices,” and so on. Causal sentence recognition usually includes two sub-tasks: one is to design rules or templates to find candidate causal sentences; the other is to design a classifier to sort candidate causal sentences to finally identify the causal sentence. This article first focuses on the characteristics of complex sentence patterns of multiple causes and one effect, multiple effects and one cause, and multiple causes and multiple effects in financial review texts, and provides a relatively complete candidate causal sentence identification rules, which can identify both simple causal sentences and complex causal sentences. A BERT-CNN (Bidirectional Encoder Representations from Transformers-Convolutional Neural Networks) combination model is proposed for the classification of candidate causal sentences. On the one hand, by adding a CNN (Convolutional Neural Networks) structure to the specific task layer of the BERT (Bidirectional Encoder Representations from Transformers) model to capture important local information in the text. On the other hand, in order to make better use of the self-attention mechanism, the local text representation and the output of the BERT are input together in the multi-layer transformer encoder. A complete representation of the text is finally obtained through a single-layer transformer encoder. Experimental results show that our model is significantly better than the most advanced baseline model, with a 5.31 pts improvement in F1 over previous analyzers. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
KW  - BERT model
KW  - Recognition of causality
KW  - Text classification
KW  - Character recognition
KW  - Complex networks
KW  - Convolution
KW  - Convolutional neural networks
KW  - Economics
KW  - Finance
KW  - Multilayer neural networks
KW  - Signal encoding
KW  - Text processing
KW  - Bidirectional encoder representation from transformer model
KW  - Convolutional neural network
KW  - Economic activities
KW  - Economic development
KW  - Multiple effect
KW  - Potential law
KW  - Recognition of causality
KW  - Sentence recognition
KW  - Text classification
KW  - Transformer modeling
KW  - Classification (of information)
PB  - Springer
SN  - 09208542 (ISSN)
LA  - English
J2  - J Supercomput
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 44; Correspondence Address: C.-X. Wan; School of Information and Technology, Jiangxi University of Finance and Economics, Nanchang, 330013, China; email: wanchangxuan@263.net; CODEN: JOSUE
ER  -

TY  - CONF
AU  - Abualhaija, S.
AU  - Arora, C.
AU  - Sleimi, A.
AU  - Briand, L.C.
TI  - Automated Question Answering for Improved Understanding of Compliance Requirements: A Multi-Document Study
PY  - 2022
T2  - Proceedings of the IEEE International Conference on Requirements Engineering
VL  - 2022-August
SP  - 39
EP  - 50
DO  - 10.1109/RE54965.2022.00011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138923987&doi=10.1109%2fRE54965.2022.00011&partnerID=40&md5=7cf2c7cf23621ca83b2f0cb20317838e
AD  - University of Luxembourg, SnT Centre for Security, Reliability and Trust, Luxembourg
AD  - Deakin University, Geelong, Australia
AD  - University of Ottawa, School of Electrical Engineering and Computer Science, Canada
AB  - Software systems are increasingly subject to regulatory compliance. Extracting compliance requirements from regulations is challenging. Ideally, locating compliance-related information in a regulation requires a joint effort from requirements engineers and legal experts, whose availability is limited. However, regulations are typically long documents spanning hundreds of pages, containing legal jargon, applying complicated natural language structures, and including cross-references, thus making their analysis effort-intensive. In this paper, we propose an automated question-answering (QA) approach that assists requirements engineers in finding the legal text passages relevant to compliance requirements. Our approach utilizes large-scale language models fine-tuned for QA, including BERT and three variants. We evaluate our approach on 107 question-answer pairs, manually curated by subject-matter experts, for four different European regulatory documents. Among these documents is the general data protection regulation (GDPR) - a major source for privacy-related requirements. Our empirical results show that, in $\approx 94$% of the cases, our approach finds the text passage containing the answer to a given question among the top five passages that our approach marks as most relevant. Further, our approach successfully demarcates, in the selected passage, the right answer with an average accuracy of $\approx$91%.  © 2022 IEEE.
KW  - BERT
KW  - Language Models (LMs)
KW  - Natural Language Processing (NLP)
KW  - Question Answering
KW  - Regulatory Compliance
KW  - Requirements Engineering
KW  - Natural language processing systems
KW  - Requirements engineering
KW  - BERT
KW  - Language model
KW  - Language processing
KW  - Multidocuments
KW  - Natural language processing
KW  - Natural languages
KW  - Question Answering
KW  - Requirement engineering
KW  - Software-systems
KW  - Regulatory compliance
A2  - Knauss E.
A2  - Mussbacher G.
A2  - Arora C.
A2  - Bano M.
A2  - Schneider J.-G.
PB  - IEEE Computer Society
SN  - 1090705X (ISSN); 978-166547000-1 (ISBN)
LA  - English
J2  - Proc. Int. Conf. Requir. Eng.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 23; Conference name: 30th IEEE International Requirements Engineering Conference, RE 2022; Conference date: 15 August 2022 through 19 August 2022; Conference code: 183667
ER  -

TY  - CONF
AU  - Ganguli, D.
AU  - Hernandez, D.
AU  - Lovitt, L.
AU  - Askell, A.
AU  - Bai, Y.
AU  - Chen, A.
AU  - Conerly, T.
AU  - Dassarma, N.
AU  - Drain, D.
AU  - Elhage, N.
AU  - El Showk, S.
AU  - Fort, S.
AU  - Hatfield-Dodds, Z.
AU  - Henighan, T.
AU  - Johnston, S.
AU  - Jones, A.
AU  - Joseph, N.
AU  - Kernian, J.
AU  - Kravec, S.
AU  - Mann, B.
AU  - Nanda, N.
AU  - Ndousse, K.
AU  - Olsson, C.
AU  - Amodei, D.
AU  - Brown, T.
AU  - Kaplan, J.
AU  - McCandlish, S.
AU  - Olah, C.
AU  - Amodei, D.
AU  - Clark, J.
TI  - Predictability and Surprise in Large Generative Models
PY  - 2022
T2  - ACM International Conference Proceeding Series
SP  - 1747
EP  - 1764
DO  - 10.1145/3531146.3533229
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132979548&doi=10.1145%2f3531146.3533229&partnerID=40&md5=45263fe1306b40b1e4a5502b03332f04
AD  - Anthropic, United States
AB  - Large-scale pre-training has recently emerged as a technique for creating capable, general-purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have a paradoxical combination of predictable loss on a broad training distribution (as embodied in their "scaling laws"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend for this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, funders who want to support work addressing these challenges, and academics who want to analyze, critique, and potentially develop large generative models. © 2022 Owner/Author.
KW  - Artificial intelligence
KW  - AI systems
KW  - Generative model
KW  - Input and outputs
KW  - Large-scales
KW  - Policy implications
KW  - Policy makers
KW  - Pre-training
KW  - Property
KW  - Real-world
KW  - Public policy
PB  - Association for Computing Machinery
SN  - 978-145039352-2 (ISBN)
LA  - English
J2  - ACM Int. Conf. Proc. Ser.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 91; Conference name: 5th ACM Conference on Fairness, Accountability, and Transparency, FAccT 2022; Conference date: 21 June 2022 through 24 June 2022; Conference code: 180210
ER  -

TY  - JOUR
AU  - Alcántara Francia, O.A.
AU  - Nunez-del-Prado, M.
AU  - Alatrista-Salas, H.
TI  - Survey of Text Mining Techniques Applied to Judicial Decisions Prediction
PY  - 2022
T2  - Applied Sciences (Switzerland)
VL  - 12
IS  - 20
C7  - 10200
DO  - 10.3390/app122010200
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140488281&doi=10.3390%2fapp122010200&partnerID=40&md5=9f70b7efa1f63e03c9ee7961316edb7b
AD  - Faculty of Law, Universidad de Lima, Lima, 15023, Peru
AD  - Peru Research, Development and Innovation Center (Peru IDI), Lima, 15076, Peru
AD  - Instituto de investigación de la Universidad de Andina del Cusco, Cusco, 080104, Peru
AD  - Escuela de Posgrado Newman, Tacna, 23001, Peru
AD  - Facultad de Ciencias e Ingeniería, Pontificia Universidad Católica del Perú, Lima, 15088, Peru
AB  - This paper reviews the most recent literature on experiments with different Machine Learning, Deep Learning and Natural Language Processing techniques applied to predict judicial and administrative decisions. Among the most outstanding findings, we have that the most used data mining techniques are Support Vector Machine (SVM), K Nearest Neighbours (K-NN) and Random Forest (RF), and in terms of the most used deep learning techniques, we found Long-Term Memory (LSTM) and transformers such as BERT. An important finding in the papers reviewed was that the use of machine learning techniques has prevailed over those of deep learning. Regarding the place of origin of the research carried out, we found that 64% of the works belong to studies carried out in English-speaking countries, 8% in Portuguese and 28% in other languages (such as German, Chinese, Turkish, Spanish, etc.). Very few works of this type have been carried out in Spanish-speaking countries. The classification criteria of the works have been based, on the one hand, on the identification of the classifiers used to predict situations (or events with legal interference) or judicial decisions and, on the other hand, on the application of classifiers to the phenomena regulated by the different branches of law: criminal, constitutional, human rights, administrative, intellectual property, family law, tax law and others. The corpus size analyzed in the reviewed works reached 100,000 documents in 2020. Finally, another important finding lies in the accuracy of these predictive techniques, reaching predictions of over 60% in different branches of law. © 2022 by the authors.
KW  - deep learning
KW  - judicial prediction
KW  - legal prediction
KW  - legal tech
KW  - machine learning
KW  - natural language processing
PB  - MDPI
SN  - 20763417 (ISSN)
LA  - English
J2  - Appl. Sci.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 13; Correspondence Address: O.A. Alcántara Francia; Faculty of Law, Universidad de Lima, Lima, 15023, Peru; email: oalcanta@ulima.edu.pe; H. Alatrista-Salas; Escuela de Posgrado Newman, Tacna, 23001, Peru; email: halatrista@pucp.edu.pe
ER  -

