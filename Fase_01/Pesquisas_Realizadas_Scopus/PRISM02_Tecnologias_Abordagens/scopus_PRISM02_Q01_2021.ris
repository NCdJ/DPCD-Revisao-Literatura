TY  - JOUR
AU  - Ofer, D.
AU  - Brandes, N.
AU  - Linial, M.
TI  - The language of proteins: NLP, machine learning & protein sequences
PY  - 2021
T2  - Computational and Structural Biotechnology Journal
VL  - 19
SP  - 1750
EP  - 1758
DO  - 10.1016/j.csbj.2021.03.022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103692698&doi=10.1016%2fj.csbj.2021.03.022&partnerID=40&md5=0c846d5dfe80765b2cc01f4a16474c5a
AD  - Medtronic, Inc, Israel
AD  - The Rachel and Selim Benin School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem, Israel
AD  - Department of Biological Chemistry, Institute of Life Sciences, The Hebrew University of Jerusalem, Jerusalem, Israel
AB  - Natural language processing (NLP) is a field of computer science concerned with automated text and language analysis. In recent years, following a series of breakthroughs in deep and machine learning, NLP methods have shown overwhelming progress. Here, we review the success, promise and pitfalls of applying NLP algorithms to the study of proteins. Proteins, which can be represented as strings of amino-acid letters, are a natural fit to many NLP methods. We explore the conceptual similarities and differences between proteins and language, and review a range of protein-related tasks amenable to machine learning. We present methods for encoding the information of proteins as text and analyzing it with NLP methods, reviewing classic concepts such as bag-of-words, k-mers/n-grams and text search, as well as modern techniques such as word embedding, contextualized embedding, deep learning and neural language models. In particular, we focus on recent innovations such as masked language modeling, self-supervised learning and attention-based models. Finally, we discuss trends and challenges in the intersection of NLP and protein research. © 2021 The Author(s)
KW  - Artificial neural networks
KW  - Bag of words
KW  - BERT
KW  - Bioinformatics
KW  - Contextualized embedding
KW  - Deep learning
KW  - Language models
KW  - Natural language processing
KW  - Tokenization
KW  - Transformer
KW  - Word embedding
KW  - Word2vec
KW  - Computational linguistics
KW  - Deep learning
KW  - Embeddings
KW  - Modeling languages
KW  - Natural language processing systems
KW  - Neural networks
KW  - protein
KW  - Bag of words
KW  - BERT
KW  - Contextualized embedding
KW  - Deep learning
KW  - Embeddings
KW  - Language model
KW  - Language processing
KW  - Natural language processing
KW  - Natural languages
KW  - Tokenization
KW  - Transformer
KW  - Word embedding
KW  - Word2vec
KW  - algorithm
KW  - amino acid sequence
KW  - artificial neural network
KW  - atom
KW  - contextualized embedding
KW  - deep learning
KW  - human
KW  - machine learning
KW  - natural language processing
KW  - nonhuman
KW  - prediction
KW  - priority journal
KW  - protein analysis
KW  - proteomics
KW  - Review
KW  - sequence analysis
KW  - word embedding
KW  - Proteins
PB  - Elsevier B.V.
SN  - 20010370 (ISSN)
LA  - English
J2  - Comput. Struct. Biotechnol. J.
M3  - Review
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 186; Correspondence Address: N. Brandes; The Rachel and Selim Benin School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem, Israel; email: nadav.brandes@mail.huji.ac.il
ER  -

TY  - JOUR
AU  - Schwaller, P.
AU  - Vaucher, A.C.
AU  - Laino, T.
AU  - Reymond, J.-L.
TI  - Prediction of chemical reaction yields using deep learning
PY  - 2021
T2  - Machine Learning: Science and Technology
VL  - 2
IS  - 1
C7  - 015016
DO  - 10.1088/2632-2153/abc81d
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104757563&doi=10.1088%2f2632-2153%2fabc81d&partnerID=40&md5=bf47e8de0b59a22eba66365e9fa69ee3
AD  - IBM Research—Europe, Säumerstrasse 4, Rüschlikon, 8803, Switzerland
AD  - Department of Chemistry and Biochemistry, University of Bern, Freiestrasse 3, Bern, 3012, Switzerland
AB  - Artificial intelligence is driving one of the most important revolutions in organic chemistry. Multiple platforms, including tools for reaction prediction and synthesis planning based on machine learning, have successfully become part of the organic chemists’ daily laboratory, assisting in domain-specific synthetic problems. Unlike reaction prediction and retrosynthetic models, the prediction of reaction yields has received less attention in spite of the enormous potential of accurately predicting reaction conversion rates. Reaction yields models, describing the percentage of the reactants converted to the desired products, could guide chemists and help them select high-yielding reactions and score synthesis routes, reducing the number of attempts. So far, yield predictions have been predominantly performed for high-throughput experiments using a categorical (one-hot) encoding of reactants, concatenated molecular fingerprints, or computed chemical descriptors. Here, we extend the application of natural language processing architectures to predict reaction properties given a text-based representation of the reaction, using an encoder transformer model combined with a regression layer. We demonstrate outstanding prediction performance on two high-throughput experiment reactions sets. An analysis of the yields reported in the open-source USPTO data set shows that their distribution differs depending on the mass scale, limiting the data set applicability in reaction yields predictions. © 2021 The Author(s). Published by IOP Publishing Ltd
KW  - Chemical reactions
KW  - Deep learning
KW  - Transformer
KW  - Yield prediction
KW  - Forecasting
KW  - Natural language processing systems
KW  - Predictive analytics
KW  - Reaction rates
KW  - Signal encoding
KW  - Synthesis (chemical)
KW  - Chemical descriptors
KW  - High throughput experiments
KW  - Molecular fingerprint
KW  - NAtural language processing
KW  - Prediction performance
KW  - Reaction conversion
KW  - Reaction prediction
KW  - Transformer modeling
KW  - Deep learning
PB  - IOP Publishing Ltd
SN  - 26322153 (ISSN)
LA  - English
J2  - Mach. Learn.: Sci. Technol.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 187; Correspondence Address: P. Schwaller; IBM Research—Europe, Rüschlikon, Säumerstrasse 4, 8803, Switzerland; email: phs@zurich.ibm.com
ER  -

TY  - JOUR
AU  - Cui, Y.
AU  - Che, W.
AU  - Liu, T.
AU  - Qin, B.
AU  - Yang, Z.
TI  - Pre-Training with Whole Word Masking for Chinese BERT
PY  - 2021
T2  - IEEE/ACM Transactions on Audio Speech and Language Processing
VL  - 29
SP  - 3504
EP  - 3514
DO  - 10.1109/TASLP.2021.3124365
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118623180&doi=10.1109%2fTASLP.2021.3124365&partnerID=40&md5=e16f106b076431ca01cef0afc415db10
AD  - Harbin Institute of Technology, Harbin, 150001, China
AD  - State Key Laboratory of Cognitive Intelligence, IFLYTEK Research, Beijing, 100010, China
AB  - Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and its consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we aim to first introduce the whole word masking (wwm) strategy for Chinese BERT, along with a series of Chinese pre-trained language models. Then we also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways. Especially, we propose a new masking strategy called MLM as correction (Mac). To demonstrate the effectiveness of these models, we create a series of Chinese pre-trained language models as our baselines, including BERT, RoBERTa, ELECTRA, RBT, etc. We carried out extensive experiments on ten Chinese NLP tasks to evaluate the created Chinese pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research. We open-source our pre-trained language models for further facilitating our research community.1  © 2014 IEEE.
KW  - Natural language processing
KW  - Pre-trained language model
KW  - Representation learning
KW  - Bit error rate
KW  - Computational linguistics
KW  - Job analysis
KW  - Modeling languages
KW  - Adaptation models
KW  - Bit-error rate
KW  - Computational modelling
KW  - Language model
KW  - Performance
KW  - Pre-trained language model
KW  - Pre-training
KW  - Predictive models
KW  - Representation learning
KW  - Task analysis
KW  - Natural language processing systems
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 23299290 (ISSN)
LA  - English
J2  - IEEE ACM Trans. Audio Speech Lang. Process.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 798; Correspondence Address: T. Liu; Harbin Institute of Technology, Harbin, 150001, China; email: tliu@hit.edu.cn
ER  -

TY  - CONF
AU  - Wang, Y.
AU  - Wang, W.
AU  - Joty, S.
AU  - Hoi, S.C.H.
TI  - CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation
PY  - 2021
T2  - EMNLP 2021 - 2021 Conference on Empirical Methods in Natural Language Processing, Proceedings
SP  - 8696
EP  - 8708
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125780341&partnerID=40&md5=e737f03414b7cc364c8ce3da26c33ea4
AD  - Salesforce Research Asia
AD  - Nanyang Technological University, Singapore
AB  - Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5. © 2021 Association for Computational Linguistics
KW  - Computational linguistics
KW  - Decoding
KW  - Natural language processing systems
KW  - Semantics
KW  - Signal encoding
KW  - 'current
KW  - Code defects
KW  - Code semantics
KW  - Code understanding
KW  - Codegeneration
KW  - Encoder-decoder
KW  - Natural languages
KW  - Pre-training
KW  - Transformer modeling
KW  - Unified framework
KW  - Learning systems
PB  - Association for Computational Linguistics (ACL)
SN  - 978-195591709-4 (ISBN)
LA  - English
J2  - EMNLP - Conf. Empir. Methods Nat. Lang. Process., Proc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 467; Conference name: 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021; Conference date: 7 November 2021 through 11 November 2021; Conference code: 177530
ER  -

TY  - CONF
AU  - Yuan, K.
AU  - Guo, S.
AU  - Liu, Z.
AU  - Zhou, A.
AU  - Yu, F.
AU  - Wu, W.
TI  - Incorporating Convolution Designs into Visual Transformers
PY  - 2021
T2  - Proceedings of the IEEE International Conference on Computer Vision
SP  - 559
EP  - 568
DO  - 10.1109/ICCV48922.2021.00062
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119124402&doi=10.1109%2fICCV48922.2021.00062&partnerID=40&md5=03f5fa274fa808bd17f6316d7e963102
AD  - SenseTime Research
AD  - HKUST, Hong Kong
AD  - S-Lab, Nanyang Technological University, China
AB  - Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new Convolution-enhanced image Transformer (CeiT) which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: 1) instead of the straightforward tokenization from raw input images, we design an Image-to-Tokens (I2T) module that extracts patches from generated low-level features; 2) the feed-froward network in each encoder block is replaced with a Locally-enhanced Feed-Forward (LeFF) layer that promotes the correlation among neighboring tokens in the spatial dimension; 3) a Layer-wise Class token Attention (LCA) is attached at the top of the Transformer that utilizes the multi-level representations. Experimental results on ImageNet and seven downstream tasks show the effectiveness and generalization ability of CeiT compared with previous Transformers and state-of-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models demonstrate better convergence with 3× fewer training iterations, which can reduce the training cost significantly. © 2021 IEEE
KW  - Computer vision
KW  - Convolutional neural networks
KW  - Image enhancement
KW  - Natural language processing systems
KW  - Network architecture
KW  - Personnel training
KW  - Convolutional neural network
KW  - Feed forward
KW  - Image transformers
KW  - Input image
KW  - Large amounts
KW  - Long-range dependencies
KW  - Low-level features
KW  - Performance
KW  - Tokenization
KW  - Training data
KW  - Convolution
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 15505499 (ISSN); 978-166542812-5 (ISBN)
LA  - English
J2  - Proc IEEE Int Conf Comput Vision
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 413; Conference name: 18th IEEE/CVF International Conference on Computer Vision, ICCV 2021; Conference date: 11 October 2021 through 17 October 2021; Conference code: 177652; CODEN: PICVE
ER  -

TY  - CONF
AU  - Giorgi, J.
AU  - Nitski, O.
AU  - Wang, B.
AU  - Bader, G.
TI  - DeCLUTR: Deep contrastive learning for unsupervised textual representations
PY  - 2021
T2  - ACL-IJCNLP 2021 - 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the Conference
SP  - 879
EP  - 895
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115425635&partnerID=40&md5=61c5189e9557b22ce789930c2e33e8fa
AD  - Department of Computer Science, University of Toronto, Canada
AD  - Faculty of Applied Science and Engineering, University of Toronto, Canada
AD  - Department of Molecular Genetics, University of Toronto, Canada
AD  - Department of Laboratory Medicine and Pathobiology, University of Toronto, Canada
AD  - Terrence Donnelly Centre for Cellular and Biomolecular Research
AD  - Vector Institute for Artificial Intelligence
AD  - Peter Munk Cardiac Center, University Health Network
AB  - Sentence embeddings are an important component of many natural language processing (NLP) systems. Like word embeddings, sentence embeddings are typically learned on large text corpora and then transferred to various downstream tasks, such as clustering and retrieval. Unlike word embeddings, the highest performing solutions for learning sentence embeddings require labelled data, limiting their usefulness to languages and domains where labelled data is abundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. Inspired by recent advances in deep metric learning (DML), we carefully design a self-supervised objective for learning universal sentence embeddings that does not require labelled training data. When used to extend the pretraining of transformer-based language models, our approach closes the performance gap between unsupervised and supervised pretraining for universal sentence encoders. Importantly, our experiments suggest that the quality of the learned embeddings scale with both the number of trainable parameters and the amount of unlabelled training data. Our code and pretrained models are publicly available and can be easily adapted to new domains or used to embed unseen text. © 2021 Association for Computational Linguistics
KW  - Computational linguistics
KW  - Deep learning
KW  - Natural language processing systems
KW  - Clusterings
KW  - Down-stream
KW  - Embeddings
KW  - Labeled data
KW  - Labeled training data
KW  - Language model
KW  - Metric learning
KW  - Pre-training
KW  - Text corpora
KW  - Textual representation
KW  - Embeddings
PB  - Association for Computational Linguistics (ACL)
SN  - 978-195408552-7 (ISBN)
LA  - English
J2  - ACL-IJCNLP - Annu. Meet. Assoc. Comput. Linguist. Int. Jt. Conf. Nat. Lang. Process., Proc. Conf.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 254; Correspondence Address: B. Wang; Department of Computer Science, University of Toronto, Canada; email: bowang@vectorinstitute.ai; G. Bader; Department of Computer Science, University of Toronto, Canada; email: gary.bader@mail.utoronto.ca; Conference name: Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL-IJCNLP 2021; Conference date: 1 August 2021 through 6 August 2021; Conference code: 173030
ER  -

TY  - CONF
AU  - Mastropaolo, A.
AU  - Scalabrino, S.
AU  - Cooper, N.
AU  - Nader Palacio, D.
AU  - Poshyvanyk, D.
AU  - Oliveto, R.
AU  - Bavota, G.
TI  - Studying the usage of text-to-text transfer transformer to support code-related tasks
PY  - 2021
T2  - Proceedings - International Conference on Software Engineering
SP  - 336
EP  - 347
DO  - 10.1109/ICSE43902.2021.00041
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113421369&doi=10.1109%2fICSE43902.2021.00041&partnerID=40&md5=8a53279ede6493be3e9eb948dc66a86d
AD  - SEART @ Software Institute, Università della Svizzera Italiana (USI), Switzerland
AD  - University of Molise, Italy
AD  - SEMERU @ Computer Science Department, William and Mary, United States
AB  - Deep learning (DL) techniques are gaining more and more attention in the software engineering community. They have been used to support several code-related tasks, such as automatic bug fixing and code comments generation. Recent studies in the Natural Language Processing (NLP) field have shown that the Text-To-Text Transfer Transformer (T5) architecture can achieve state-of-the-art performance for a variety of NLP tasks. The basic idea behind T5 is to first pre-train a model on a large and generic dataset using a self-supervised task (e.g., filling masked words in sentences). Once the model is pre-trained, it is fine-tuned on smaller and specialized datasets, each one related to a specific task (e.g., language translation, sentence classification). In this paper, we empirically investigate how the T5 model performs when pre-trained and fine-tuned to support code-related tasks. We pre-train a T5 model on a dataset composed of natural language English text and source code. Then, we fine-tune such a model by reusing datasets used in four previous works that used DL techniques to: (i) fix bugs, (ii) inject code mutants, (iii) generate assert statements, and (iv) generate code comments. We compared the performance of this single model with the results reported in the four original papers proposing DL-based solutions for those four tasks. We show that our T5 model, exploiting additional data for the self-supervised pre-training phase, can achieve performance improvements over the four baselines.  © 2021 IEEE.
KW  - Deep Learning
KW  - Empirical software engineering
KW  - Classification (of information)
KW  - Deep learning
KW  - Large dataset
KW  - Bug-fixing
KW  - Deep learning
KW  - Empirical Software Engineering
KW  - Engineering community
KW  - Language translation
KW  - Learning techniques
KW  - Performance
KW  - Sentence classifications
KW  - Specific tasks
KW  - State-of-the-art performance
KW  - Natural language processing systems
PB  - IEEE Computer Society
SN  - 02705257 (ISSN); 978-073811319-7 (ISBN)
LA  - English
J2  - Proc Int Conf Software Eng
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 151; Conference name: 43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021; Conference date: 22 May 2021 through 30 May 2021; Conference code: 170992; CODEN: PCSED
ER  -

TY  - CONF
AU  - Liu, H.
AU  - Dai, Z.
AU  - So, D.R.
AU  - Le, Q.V.
TI  - Pay Attention to MLPs
PY  - 2021
T2  - Advances in Neural Information Processing Systems
VL  - 11
SP  - 9204
EP  - 9215
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131898296&partnerID=40&md5=4cb97c8e30cc45196420b09ac44fccf9
AD  - Google Research, Brain Team, United States
AB  - Transformers [1] have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple network architecture, gMLP, based on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream NLP tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute. © 2021 Neural information processing systems foundation. All rights reserved.
KW  - Deep learning
KW  - Architectural innovation
KW  - Down-stream
KW  - Pre-training
KW  - Simple networks
KW  - Vision applications
KW  - Network architecture
A2  - Ranzato M.
A2  - Beygelzimer A.
A2  - Dauphin Y.
A2  - Liang P.S.
A2  - Wortman Vaughan J.
PB  - Neural information processing systems foundation
SN  - 10495258 (ISSN); 978-171384539-3 (ISBN)
LA  - English
J2  - Adv. neural inf. proces. syst.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 357; Conference name: 35th Conference on Neural Information Processing Systems, NeurIPS 2021; Conference date: 6 December 2021 through 14 December 2021; Conference code: 179642
ER  -

TY  - JOUR
AU  - Acheampong, F.A.
AU  - Nunoo-Mensah, H.
AU  - Chen, W.
TI  - Transformer models for text-based emotion detection: a review of BERT-based approaches
PY  - 2021
T2  - Artificial Intelligence Review
VL  - 54
IS  - 8
SP  - 5789
EP  - 5829
DO  - 10.1007/s10462-021-09958-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100710769&doi=10.1007%2fs10462-021-09958-2&partnerID=40&md5=05ab3c384057f2825868bae6e14e843b
AD  - Computational Intelligence Lab, School of Computer Science and Technology, University of Electronic Science and Technology of China, Chengdu, China
AD  - Connected Devices Lab, Department of Computer Engineering, Kwame Nkrumah University of Science and Technology, Kumasi, Ghana
AB  - We cannot overemphasize the essence of contextual information in most natural language processing (NLP) applications. The extraction of context yields significant improvements in many NLP tasks, including emotion recognition from texts. The paper discusses transformer-based models for NLP tasks. It highlights the pros and cons of the identified models. The models discussed include the Generative Pre-training (GPT) and its variants, Transformer-XL, Cross-lingual Language Models (XLM), and the Bidirectional Encoder Representations from Transformers (BERT). Considering BERT’s strength and popularity in text-based emotion detection, the paper discusses recent works in which researchers proposed various BERT-based models. The survey presents its contributions, results, limitations, and datasets used. We have also provided future research directions to encourage research in text-based emotion detection using these models. © 2021, The Author(s), under exclusive licence to Springer Nature B.V. part of Springer Nature.
KW  - Natural language processing
KW  - Sentiment analysis
KW  - Text-based emotion detection
KW  - Transformers
KW  - Artificial intelligence
KW  - Contextual information
KW  - Cross-lingual
KW  - Emotion detection
KW  - Emotion recognition
KW  - Future research directions
KW  - Language model
KW  - NAtural language processing
KW  - Transformer models
KW  - Natural language processing systems
PB  - Springer Science and Business Media B.V.
SN  - 02692821 (ISSN)
LA  - English
J2  - Artif Intell Rev
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 304; Correspondence Address: H. Nunoo-Mensah; Connected Devices Lab, Department of Computer Engineering, Kwame Nkrumah University of Science and Technology, Kumasi, Ghana; email: hnunoo-mensah@knust.edu.gh; CODEN: AIRVE
ER  -

TY  - CONF
AU  - Xue, L.
AU  - Constant, N.
AU  - Roberts, A.
AU  - Kale, M.
AU  - Al-Rfou, R.
AU  - Siddhant, A.
AU  - Barua, A.
AU  - Raffel, C.
TI  - mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer
PY  - 2021
T2  - NAACL-HLT 2021 - 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference
SP  - 483
EP  - 498
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121486920&partnerID=40&md5=690890eb359a684d518607934c0a7da5
AD  - Google Research, United States
AB  - The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent “accidental translation” in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available. © 2021 Association for Computational Linguistics.
KW  - Benchmarking
KW  - Computational linguistics
KW  - Natural language processing systems
KW  - Zero-shot learning
KW  - English languages
KW  - Generative model
KW  - Simple++
KW  - State of the art
KW  - State-of-the-art performance
KW  - Text format
KW  - Translation (languages)
PB  - Association for Computational Linguistics (ACL)
SN  - 978-195408546-6 (ISBN)
LA  - English
J2  - NAACL-HLT - Conf. N. Am. Chapter Assoc. Comput. Linguist.: Hum. Lang. Technol., Proc. Conf.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 1070; Correspondence Address: L. Xue; Google Research, United States; email: lintingx@google.com; N. Constant; Google Research, United States; email: nconstant@google.com; A. Roberts; Google Research, United States; email: adarob@google.com; C. Raffel; Google Research, United States; email: craffel@google.com; Conference name: 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021; Conference date: 6 June 2021 through 11 June 2021; Conference code: 182055
ER  -

