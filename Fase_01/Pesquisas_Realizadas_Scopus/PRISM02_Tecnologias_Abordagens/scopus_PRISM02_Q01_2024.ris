TY  - JOUR
AU  - Raiaan, M.A.K.
AU  - Mukta, M.S.H.
AU  - Fatema, K.
AU  - Fahad, N.M.
AU  - Sakib, S.
AU  - Mim, M.M.J.
AU  - Ahmad, J.
AU  - Ali, M.E.
AU  - Azam, S.
TI  - A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges
PY  - 2024
T2  - IEEE Access
VL  - 12
SP  - 26839
EP  - 26874
DO  - 10.1109/ACCESS.2024.3365742
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185128981&doi=10.1109%2fACCESS.2024.3365742&partnerID=40&md5=2b8272d893e7d9bb9ee3ee8fc17356ed
AD  - United International University, Department of Computer Science and Engineering, Dhaka, 1212, Bangladesh
AD  - Lappeenranta-Lahti University of Technology, LUT School of Engineering Sciences, Lappeenranta, 53850, Finland
AD  - Charles Darwin University, Faculty of Science and Technology, Casuarina, 0909, NT, Australia
AD  - Bangladesh University of Engineering and Technology (BUET), Department of CSE, Dhaka, 1000, Bangladesh
AB  - Large Language Models (LLMs) recently demonstrated extraordinary capability in various natural language processing (NLP) tasks including language translation, text generation, question answering, etc. Moreover, LLMs are new and essential part of computerized language processing, having the ability to understand complex verbal patterns and generate coherent and appropriate replies in a given context. Though this success of LLMs has prompted a substantial increase in research contributions, rapid growth has made it difficult to understand the overall impact of these improvements. Since a plethora of research on LLMs have been appeared within a short time, it is quite impossible to track all of these and get an overview of the current state of research in this area. Consequently, the research community would benefit from a short but thorough review of the recent changes in this area. This article thoroughly overviews LLMs, including their history, architectures, transformers, resources, training methods, applications, impacts, challenges, etc. This paper begins by discussing the fundamental concepts of LLMs with its traditional pipeline of the LLMs training phase. Then the paper provides an overview of the existing works, the history of LLMs, their evolution over time, the architecture of transformers in LLMs, the different resources of LLMs, and the different training methods that have been used to train them. The paper also demonstrates the datasets utilized in the studies. After that, the paper discusses the wide range of applications of LLMs, including biomedical and healthcare, education, social, business, and agriculture. The study also illustrates how LLMs create an impact on society and shape the future of AI and how they can be used to solve real-world problems. Finally, the paper also explores open issues and challenges to deploy LLMs in real-world scenario. Our review paper aims to help practitioners, researchers, and experts thoroughly understand the evolution of LLMs, pre-trained architectures, applications, challenges, and future goals.  © 2013 IEEE.
KW  - application
KW  - artificial intelligence
KW  - Large language models (LLM)
KW  - natural language processing (NLP)
KW  - pre-trained models
KW  - taxonomy
KW  - transformer
KW  - Artificial intelligence
KW  - Computational linguistics
KW  - Job analysis
KW  - Natural language processing systems
KW  - Cognition
KW  - Evolution
KW  - Language model
KW  - Language processing
KW  - Large language model
KW  - Natural language processing
KW  - Natural languages
KW  - Pretrained model
KW  - Question Answering
KW  - Question answering (information retrieval)
KW  - Task analysis
KW  - Transformer
KW  - Taxonomies
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 21693536 (ISSN)
LA  - English
J2  - IEEE Access
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 67; Correspondence Address: M.S.H. Mukta; Lappeenranta-Lahti University of Technology, LUT School of Engineering Sciences, Lappeenranta, 53850, Finland; email: Saddam.Mukta@lut.fi
ER  -

TY  - JOUR
AU  - Gupta, R.
AU  - Nair, K.
AU  - Mishra, M.
AU  - Ibrahim, B.
AU  - Bhardwaj, S.
TI  - Adoption and impacts of generative artificial intelligence: Theoretical underpinnings and research agenda
PY  - 2024
T2  - International Journal of Information Management Data Insights
VL  - 4
IS  - 1
C7  - 100232
DO  - 10.1016/j.jjimei.2024.100232
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189075847&doi=10.1016%2fj.jjimei.2024.100232&partnerID=40&md5=09bc7bfe6af34a1a69f57acaef237b53
AD  - Department of Commerce, Shaheed Bhagat Singh College, University of Delhi, India
AD  - College of Business, Abu Dhabi University, Abu Dhabi, United Arab Emirates
AD  - Abu Dhabi School of Management, Abu Dhabi, United Arab Emirates
AD  - Department of Business, Faculty of Business, Istanbul Ticaret University, Istanbul, Turkey
AD  - Symbiosis International (Deemed) University, Pune, India
AD  - Maldives Business School, Male, Maldives
AB  - Large language models (LLMs) have received considerable interest in the field of natural language processing (NLP) owing to their remarkable ability to generate clear, consistent, and contextually relevant materials. Among the numerous LLMs, ChatGPT (Generative Pre-trained Transformer for Chatbots) is emerging as a prominent prospective tool for developing conversational agents such as chatbots. However, there is a need for a clear conceptual understanding of ChatGPT's potential implications for the industry and its role in marketing. This study explores the adoption of ChatGPT in marketing and examines theories that may influence its adoption by marketers and consumers, as well as its implications for marketers. This study discusses how ChatGPT may allow for more personalized and engaging content, better customer experience, and improved ROI. However, adoption also brings challenges, including ethical considerations and the need for new skill development. This study also discusses future research opportunities for the adoption of ChatGPT and other generative artificial intelligence technologies in marketing. The goal is to provide insights for organizations that consider implementing these technologies, and to contribute to the literature on the adoption of Artificial Intelligence (AI) and the use of Generative AI in marketing. © 2024
KW  - Adoption
KW  - Chatbots
KW  - ChatGPT
KW  - Generative AI
PB  - Elsevier B.V.
SN  - 26670968 (ISSN)
LA  - English
J2  - Int. J. Inf. Manag. Data Insights
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 27; Correspondence Address: K. Nair; College of Business, Abu Dhabi University, Abu Dhabi, United Arab Emirates; email: kiran.nair@adu.ac.ae
ER  -

TY  - JOUR
AU  - Ferrag, M.A.
AU  - Ndhlovu, M.
AU  - Tihanyi, N.
AU  - Cordeiro, L.C.
AU  - Debbah, M.
AU  - Lestable, T.
AU  - Thandi, N.S.
TI  - Revolutionizing Cyber Threat Detection with Large Language Models: A Privacy-Preserving BERT-Based Lightweight Model for IoT/IIoT Devices
PY  - 2024
T2  - IEEE Access
VL  - 12
SP  - 23733
EP  - 23750
DO  - 10.1109/ACCESS.2024.3363469
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184808429&doi=10.1109%2fACCESS.2024.3363469&partnerID=40&md5=2e93755707d1b8ec5af756a0d22ad2b7
AD  - Technology Innovation Institute, Abu Dhabi, United Arab Emirates
AD  - The University of Manchester, Department of Computer Science, Manchester, M13 9PL, United Kingdom
AD  - Federal University of Amazonas, Department of Electronics and Computing, Manaus, 69067, Brazil
AD  - Khalifa University of Science and Technology, KU 6G Research Center, Abu Dhabi, United Arab Emirates
AB  - The field of Natural Language Processing (NLP) is currently undergoing a revolutionary transformation driven by the power of pre-trained Large Language Models (LLMs) based on groundbreaking Transformer architectures. As the frequency and diversity of cybersecurity attacks continue to rise, the importance of incident detection has significantly increased. IoT devices are expanding rapidly, resulting in a growing need for efficient techniques to autonomously identify network-based attacks in IoT networks with both high precision and minimal computational requirements. This paper presents SecurityBERT, a novel architecture that leverages the Bidirectional Encoder Representations from Transformers (BERT) model for cyber threat detection in IoT networks. During the training of SecurityBERT, we incorporated a novel privacy-preserving encoding technique called Privacy-Preserving Fixed-Length Encoding (PPFLE). We effectively represented network traffic data in a structured format by combining PPFLE with the Byte-level Byte-Pair Encoder (BBPE) Tokenizer. Our research demonstrates that SecurityBERT outperforms traditional Machine Learning (ML) and Deep Learning (DL) methods, such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), in cyber threat detection. Employing the Edge-IIoTset cybersecurity dataset, our experimental analysis shows that SecurityBERT achieved an impressive 98.2% overall accuracy in identifying fourteen distinct attack types, surpassing previous records set by hybrid solutions such as GAN-Transformer-based architectures and CNN-LSTM models. With an inference time of less than 0.15 seconds on an average CPU and a compact model size of just 16.7MB, SecurityBERT is ideally suited for real-life traffic analysis and a suitable choice for deployment on resource-constrained IoT devices.  © 2013 IEEE.
KW  - BERT
KW  - Cyber threat detection
KW  - generative AI
KW  - IoT networks
KW  - large language models
KW  - Computer architecture
KW  - Cybersecurity
KW  - Data privacy
KW  - Encoding (symbols)
KW  - Long short-term memory
KW  - Natural language processing systems
KW  - Network architecture
KW  - Network coding
KW  - Network security
KW  - Bidirectional encoder representation from transformer
KW  - Cybe threat detection
KW  - Cyber threats
KW  - Encodings
KW  - Generative AI
KW  - IoT network
KW  - Language model
KW  - Language processing
KW  - Large language model
KW  - Natural language processing
KW  - Natural languages
KW  - Threat assessment
KW  - Threat detection
KW  - Transformer
KW  - Internet of things
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 21693536 (ISSN)
LA  - English
J2  - IEEE Access
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 22; Correspondence Address: M.A. Ferrag; Technology Innovation Institute, Abu Dhabi, United Arab Emirates; email: Mohamed.Ferrag@tii.ae
ER  -

TY  - JOUR
AU  - Guo, E.
AU  - Gupta, M.
AU  - Deng, J.
AU  - Park, Y.-J.
AU  - Paget, M.
AU  - Naugler, C.
TI  - Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study
PY  - 2024
T2  - Journal of Medical Internet Research
VL  - 26
IS  - 1
C7  - e48996
DO  - 10.2196/48996
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182288843&doi=10.2196%2f48996&partnerID=40&md5=bcefbc7b4792530573f06a8868a90229
AD  - Cumming School of Medicine, University of Calgary, Calgary, AB, Canada
AD  - Temerty Faculty of Medicine, University of Toronto, Toronto, AB, Canada
AB  - Background: The systematic review of clinical research papers is a labor-intensive and time-consuming process that often involves the screening of thousands of titles and abstracts. The accuracy and efficiency of this process are critical for the quality of the review and subsequent health care decisions. Traditional methods rely heavily on human reviewers, often requiring a significant investment of time and resources. Objective: This study aims to assess the performance of the OpenAI generative pretrained transformer (GPT) and GPT-4 application programming interfaces (APIs) in accurately and efficiently identifying relevant titles and abstracts from real-world clinical review data sets and comparing their performance against ground truth labeling by 2 independent human reviewers. Methods: We introduce a novel workflow using the Chat GPT and GPT-4 APIs for screening titles and abstracts in clinical reviews. A Python script was created to make calls to the API with the screening criteria in natural language and a corpus of title and abstract data sets filtered by a minimum of 2 human reviewers. We compared the performance of our model against human-reviewed papers across 6 review papers, screening over 24,000 titles and abstracts. Results: Our results show an accuracy of 0.91, a macro F1-score of 0.60, a sensitivity of excluded papers of 0.91, and a sensitivity of included papers of 0.76. The interrater variability between 2 independent human screeners was κ=0.46, and the prevalence and bias-adjusted κ between our proposed methods and the consensus-based human decisions was κ=0.96. On a randomly selected subset of papers, the GPT models demonstrated the ability to provide reasoning for their decisions and corrected their initial decisions upon being asked to explain their reasoning for incorrect classifications. Conclusions: Large language models have the potential to streamline the clinical review process, save valuable time and effort for researchers, and contribute to the overall quality of clinical reviews. By prioritizing the workflow and acting as an aid rather than a replacement for researchers and reviewers, models such as GPT-4 can enhance efficiency and lead to more accurate and reliable conclusions in medical research. © 2024 Journal of Medical Internet Research. All rights reserved.
KW  - abstract screening
KW  - Chat GPT
KW  - classification
KW  - extract
KW  - extraction
KW  - free text
KW  - GPT
KW  - GPT-4
KW  - language model
KW  - large language models
KW  - LLM
KW  - natural language processing
KW  - NLP
KW  - nonopiod analgesia
KW  - review methodology
KW  - review methods
KW  - screening
KW  - systematic
KW  - systematic review
KW  - unstructured data
KW  - Biomedical Research
KW  - Consensus
KW  - Data Analysis
KW  - Humans
KW  - Language
KW  - Problem Solving
KW  - Article
KW  - artificial intelligence
KW  - ChatGPT
KW  - consensus
KW  - data analysis
KW  - human
KW  - large language model
KW  - prevalence
KW  - sensitivity analysis
KW  - workflow
KW  - data analysis
KW  - language
KW  - medical research
KW  - problem solving
PB  - JMIR Publications Inc.
SN  - 14388871 (ISSN)
C2  - 38214966
LA  - English
J2  - J. Med. Internet Res.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 21; Correspondence Address: E. Guo; Cumming School of Medicine, University of Calgary, Calgary, 3330 University Dr NW, T2N 1N4, Canada; email: eddie.guo@ucalgary.ca
ER  -

TY  - JOUR
AU  - Li, J.
AU  - Dada, A.
AU  - Puladi, B.
AU  - Kleesiek, J.
AU  - Egger, J.
TI  - ChatGPT in healthcare: A taxonomy and systematic review
PY  - 2024
T2  - Computer Methods and Programs in Biomedicine
VL  - 245
C7  - 108013
DO  - 10.1016/j.cmpb.2024.108013
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183173863&doi=10.1016%2fj.cmpb.2024.108013&partnerID=40&md5=1e0ec823297dd19c5125c234f2ec16a4
AD  - Institute for Artificial Intelligence in Medicine, University Hospital Essen (AöR), Girardetstraße 2, Essen, 45131, Germany
AD  - Institute of Medical Informatics, University Hospital RWTH Aachen, Pauwelsstraße 30, Aachen, 52074, Germany
AD  - Department of Oral and Maxillofacial Surgery, University Hospital RWTH Aachen, Pauwelsstraße 30, Aachen, 52074, Germany
AD  - Center for Virtual and Extended Reality in Medicine (ZvRM), University Hospital Essen, University Medicine Essen, Hufelandstraße 55, Essen, 45147, Germany
AD  - TU Dortmund University, Department of Physics, Otto-Hahn-Straße 4, Dortmund, 44227, Germany
AB  - The recent release of ChatGPT, a chat bot research project/product of natural language processing (NLP) by OpenAI, stirs up a sensation among both the general public and medical professionals, amassing a phenomenally large user base in a short time. This is a typical example of the ‘productization’ of cutting-edge technologies, which allows the general public without a technical background to gain firsthand experience in artificial intelligence (AI), similar to the AI hype created by AlphaGo (DeepMind Technologies, UK) and self-driving cars (Google, Tesla, etc.). However, it is crucial, especially for healthcare researchers, to remain prudent amidst the hype. This work provides a systematic review of existing publications on the use of ChatGPT in healthcare, elucidating the ‘status quo’ of ChatGPT in medical applications, for general readers, healthcare professionals as well as NLP scientists. The large biomedical literature database PubMed is used to retrieve published works on this topic using the keyword ‘ChatGPT’. An inclusion criterion and a taxonomy are further proposed to filter the search results and categorize the selected publications, respectively. It is found through the review that the current release of ChatGPT has achieved only moderate or ‘passing’ performance in a variety of tests, and is unreliable for actual clinical deployment, since it is not intended for clinical applications by design. We conclude that specialized NLP models trained on (bio)medical datasets still represent the right direction to pursue for critical clinical applications. © 2024 The Author(s)
KW  - Bard
KW  - BERT
KW  - ChatGPT
KW  - Healthcare
KW  - LLaMA
KW  - LLM
KW  - NLP
KW  - OpenAI
KW  - Taxonomy
KW  - Transformer
KW  - Artificial Intelligence
KW  - Databases, Factual
KW  - Humans
KW  - Natural Language Processing
KW  - Physicians
KW  - PubMed
KW  - Binary alloys
KW  - Health care
KW  - Medical applications
KW  - Natural language processing systems
KW  - Potassium alloys
KW  - Uranium alloys
KW  - Bard
KW  - BERT
KW  - ChatGPT
KW  - Healthcare
KW  - Language processing
KW  - LLaMA
KW  - LLM
KW  - Natural language processing
KW  - Natural languages
KW  - Openai
KW  - Transformer
KW  - ChatGPT
KW  - classification
KW  - clinical decision making
KW  - consultation
KW  - health care personnel
KW  - large language model
KW  - long short term memory network
KW  - medical education
KW  - medical research
KW  - medicine
KW  - natural language processing
KW  - publication
KW  - recurrent neural network
KW  - Review
KW  - systematic review
KW  - taxonomy
KW  - workflow
KW  - artificial intelligence
KW  - factual database
KW  - human
KW  - Medline
KW  - physician
KW  - Taxonomies
PB  - Elsevier Ireland Ltd
SN  - 01692607 (ISSN)
C2  - 38262126
LA  - English
J2  - Comput. Methods Programs Biomed.
M3  - Review
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 77; Correspondence Address: J. Egger; Institute for Artificial Intelligence in Medicine, University Hospital Essen (AöR), Essen, Girardetstraße, 45131, Germany; email: jan.egger@uk-essen.de; CODEN: CMPBE
ER  -

TY  - JOUR
AU  - Liu, Y.
AU  - Zhang, Y.
AU  - Wang, Y.
AU  - Hou, F.
AU  - Yuan, J.
AU  - Tian, J.
AU  - Zhang, Y.
AU  - Shi, Z.
AU  - Fan, J.
AU  - He, Z.
TI  - A Survey of Visual Transformers
PY  - 2024
T2  - IEEE Transactions on Neural Networks and Learning Systems
VL  - 35
IS  - 6
SP  - 7478
EP  - 7498
DO  - 10.1109/TNNLS.2022.3227717
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153368140&doi=10.1109%2fTNNLS.2022.3227717&partnerID=40&md5=324ae9b15179fd9baa63c3cc4a1146da
AD  - University of Chinese Academy of Sciences, School of Computer Science and Technology, Beijing, 100000, China
AD  - Stanford University, School of Engineering, Palo Alto, 94305, United States
AD  - Southeast University, School of Computer Science and Engineering, Nanjing, 214135, China
AD  - Lenovo Research, AI Lab, Beijing, 100000, China
AD  - Chinese Academy of Sciences, Institute of Computing Technology, Beijing, 100000, China
AD  - University of Chinese Academy of Sciences, Beijing, 100000, China
AD  - Lenovo Ltd., Beijing, 100000, China
AB  - Transformer, an attention-based encoder-decoder model, has already revolutionized the field of natural language processing (NLP). Inspired by such significant achievements, some pioneering works have recently been done on employing Transformer-liked architectures in the computer vision (CV) field, which have demonstrated their effectiveness on three fundamental CV tasks (classification, detection, and segmentation) as well as multiple sensory data stream (images, point clouds, and vision-language data). Because of their competitive modeling capabilities, the visual Transformers have achieved impressive performance improvements over multiple benchmarks as compared with modern convolution neural networks (CNNs). In this survey, we have reviewed over 100 of different visual Transformers comprehensively according to three fundamental CV tasks and different data stream types, where taxonomy is proposed to organize the representative methods according to their motivations, structures, and application scenarios. Because of their differences on training settings and dedicated vision tasks, we have also evaluated and compared all these existing visual Transformers under different configurations. Furthermore, we have revealed a series of essential but unexploited aspects that may empower such visual Transformers to stand out from numerous architectures, e.g., slack high-level semantic embeddings to bridge the gap between the visual Transformers and the sequential ones. Finally, two promising research directions are suggested for future investment. We will continue to update the latest articles and their released source codes at https://github.com/liuyang-ict/awesome-visual-transformers.  © 2012 IEEE.
KW  - Classification
KW  - computer vision (CV)
KW  - detection
KW  - point clouds
KW  - segmentation
KW  - self-supervision
KW  - visual Transformer
KW  - visual-linguistic pretraining
KW  - Benchmarking
KW  - Computer vision
KW  - Decoding
KW  - Deep neural networks
KW  - Image segmentation
KW  - Natural language processing systems
KW  - Network architecture
KW  - Semantics
KW  - Taxonomies
KW  - Visual languages
KW  - Computational modelling
KW  - Computer vision
KW  - Decoding
KW  - Detection
KW  - Point-clouds
KW  - Pre-training
KW  - Segmentation
KW  - Self-supervision
KW  - Task analysis
KW  - Transformer
KW  - Visual transformer
KW  - Visual-linguistic pretraining
KW  - article
KW  - computer vision
KW  - embedding
KW  - human
KW  - human experiment
KW  - investment
KW  - motivation
KW  - taxonomy
KW  - vision
KW  - Convolution
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 2162237X (ISSN)
C2  - 37015131
LA  - English
J2  - IEEE Trans. Neural Networks Learn. Sys.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 140; Correspondence Address: Y. Zhang; Lenovo Research, AI Lab, Beijing, 100000, China; email: zhangyang20@lenovo.com; Z. Shi; Lenovo Research, AI Lab, Beijing, 100000, China; email: shizc2@lenovo.com
ER  -

TY  - JOUR
AU  - Godi, R.K.
AU  - Basvant, M.S.
AU  - Deepak, A.
AU  - Srivastava, A.P.
AU  - Kumar, T.M.
AU  - Sankhyan, A.
AU  - Shrivastava, A.
TI  - Sentiment Analysis on Omicron Tweets Using Hybrid Classifiers with Multiple Feature ExtractionTechniques and Transformer Based Models
PY  - 2024
T2  - International Journal of Intelligent Systems and Applications in Engineering
VL  - 12
IS  - 15s
SP  - 257
EP  - 275
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187454083&partnerID=40&md5=c8d81687d4e80f7949f44e46271a2a0f
AD  - Department of Computer Science and Engineering, Manipal Institute of Technology, Manipal Academy of Higher Education (MAHE), Bengaluru, India
AD  - Department of Electronics & Telecommunication Engineering, Sinhgad College of Engineering, Pune, 41, India
AD  - Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Saveetha University, Tamilnadu, Chennai, India
AD  - Lloyd Institute of Engineering & Technology, Greater Noida, India
AD  - St Thomas College of Engineering and Technology, Kerala, Chengannur, 689521, India
AD  - Lloyd Law College, Greater Noida, India
AD  - Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Tamilnadu, Chennai, India
AB  - Since the beginning of Covid-19, the world has been in a dilemma to cope up with its effects. With time the coronavirus has evolved into variants that caused a lot of destruction to human race. One such variant is “Omicron”. This variant made its presence in many countries throughout the world. The government is left in a straining situation to curb the spread of this variant and to stop the evolution of coronavirus. Though the strict precautions were exercised, the evolution was unstoppable. To understand the thoughts and feelings of the public, twitter can be considered as one of the best platforms for sentiment analysis. Analyzing the sentiments of people across the continents is horridly difficult but with the way technology has been making advancement in the world, analyzing has become a quiet easy job. In the existing studies on Covid-19, various word embedding techniques with machine learning and deeplearning classifiers has been used for the analysis. Language based models have proven to achieve higher accuracy forsentiment analysis. Amidst these hybrid classifiers, have performed tremendously good. In the proposed work, seven Machine Learning hybrid classifiers are compared with four single classifiers using TF-IDF and Word2Vec. A proposedDeep Learning hybrid classifier is compared with two single classifiers using GloVe and FastText. Furthermore, language models like BERT and RoBERTa are employed in an effort to boost validation outcomes upto 93.39% and 93.47%. © 2024, Auricle Global Society of Education and Research. All rights reserved.
KW  - BERT
KW  - Big Data
KW  - Deep learning
KW  - FastText
KW  - GloVe
KW  - hybridclassifiers
KW  - Machine learning
KW  - NLP
KW  - Omicron
KW  - RoBERTa
KW  - Sentiment analysis
KW  - TextBlob
KW  - TF-IDF
KW  - Twitter analysis
KW  - Word2Vec
PB  - Auricle Global Society of Education and Research
SN  - 21476799 (ISSN)
LA  - English
J2  - Internat. J. Intel. Syst. Appl. Eng.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 25
ER  -

TY  - JOUR
AU  - Su, J.
AU  - Ahmed, M.
AU  - Lu, Y.
AU  - Pan, S.
AU  - Bo, W.
AU  - Liu, Y.
TI  - RoFormer: Enhanced transformer with Rotary Position Embedding
PY  - 2024
T2  - Neurocomputing
VL  - 568
C7  - 127063
DO  - 10.1016/j.neucom.2023.127063
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178660816&doi=10.1016%2fj.neucom.2023.127063&partnerID=40&md5=ced44c7c3eb82b767e856bdac9110241
AD  - Zhuiyi Technology Co., Ltd. Shenzhen, China
AB  - Position encoding has recently been shown to be effective in transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding (RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in the self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: https://huggingface.co/docs/transformers/model_doc/roformer. © 2023 Elsevier B.V.
KW  - Natural language processing
KW  - Position information encoding
KW  - Pre-trained language models
KW  - Pre-training
KW  - Classification (of information)
KW  - Computational linguistics
KW  - Encoding (symbols)
KW  - Natural language processing systems
KW  - Rope
KW  - Signal encoding
KW  - Text processing
KW  - Embeddings
KW  - Information encoding
KW  - Language model
KW  - Language processing
KW  - Natural language processing
KW  - Natural languages
KW  - Position information
KW  - Position information encoding
KW  - Pre-trained language model
KW  - Pre-training
KW  - article
KW  - benchmarking
KW  - female
KW  - human
KW  - language model
KW  - learning
KW  - male
KW  - natural language processing
KW  - rotation
KW  - training
KW  - Embeddings
PB  - Elsevier B.V.
SN  - 09252312 (ISSN)
LA  - English
J2  - Neurocomputing
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 203; Correspondence Address: M. Ahmed; Zhuiyi Technology Co., Ltd. Shenzhen, China; email: murtadha.alrahbi@gmail.com; CODEN: NRCGE
ER  -

TY  - JOUR
AU  - Islam, S.
AU  - Elmekki, H.
AU  - Elsebai, A.
AU  - Bentahar, J.
AU  - Drawel, N.
AU  - Rjoub, G.
AU  - Pedrycz, W.
TI  - A comprehensive survey on applications of transformers for deep learning tasks
PY  - 2024
T2  - Expert Systems with Applications
VL  - 241
C7  - 122666
DO  - 10.1016/j.eswa.2023.122666
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183596144&doi=10.1016%2fj.eswa.2023.122666&partnerID=40&md5=69e705fa3ff7e537e478ae12ba9e96d6
AD  - Concordia Institute for Information Systems Engineering, Concordia University, Montreal, Canada
AD  - Department of Electrical Engineering and Computer Science, Khalifa University, Abu Dhabi, United Arab Emirates
AD  - Faculty of Information Technology, Aqaba University of Technology, Jordan
AD  - Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Canada
AD  - Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland
AD  - Department of Computer Engineering, Istinye University, Sariyer/Istanbul, Turkey
AB  - Transformers are Deep Neural Networks (DNN) that utilize a self-attention mechanism to capture contextual relationships within sequential data. Unlike traditional neural networks and variants of Recurrent Neural Networks (RNNs), such as Long Short-Term Memory (LSTM), Transformer models excel at managing long dependencies among input sequence elements and facilitate parallel processing. Consequently, Transformer-based models have garnered significant attention from researchers in the field of artificial intelligence. This is due to their tremendous potential and impressive accomplishments, which extend beyond Natural Language Processing (NLP) tasks to encompass various domains, including Computer Vision (CV), audio and speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have been published, spotlighting the Transformer's contributions in specific fields, architectural disparities, or performance assessments, there remains a notable absence of a comprehensive survey paper that encompasses its major applications across diverse domains. Therefore, this paper addresses this gap by conducting an extensive survey of proposed Transformer models spanning from 2017 to 2022. Our survey encompasses the identification of the top five application domains for Transformer-based models, namely: NLP, CV, multi-modality, audio and speech processing, and signal processing. We analyze the influence of highly impactful Transformer-based models within these domains and subsequently categorize them according to their respective tasks, employing a novel taxonomy. Our primary objective is to illuminate the existing potential and future prospects of Transformers for researchers who are passionate about this area, thereby contributing to a more comprehensive understanding of this groundbreaking technology. © 2023 Elsevier Ltd
KW  - Computer vision (CV)
KW  - Deep learning
KW  - Multi-modality
KW  - Natural language processing (NLP)
KW  - Self-attention
KW  - Transformer
KW  - Computer vision
KW  - Deep neural networks
KW  - Internet of things
KW  - Natural language processing systems
KW  - Speech processing
KW  - Computer vision
KW  - Deep learning
KW  - Language processing
KW  - Learning tasks
KW  - Multi-modality
KW  - Natural language processing
KW  - Natural languages
KW  - Self-attention
KW  - Transformer
KW  - Transformer modeling
KW  - Long short-term memory
PB  - Elsevier Ltd
SN  - 09574174 (ISSN)
LA  - English
J2  - Expert Sys Appl
M3  - Review
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 61; Correspondence Address: J. Bentahar; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, Canada; email: jamal.bentahar@concordia.ca; CODEN: ESAPE
ER  -

TY  - JOUR
AU  - Cascella, M.
AU  - Semeraro, F.
AU  - Montomoli, J.
AU  - Bellini, V.
AU  - Piazza, O.
AU  - Bignami, E.
TI  - The Breakthrough of Large Language Models Release for Medical Applications: 1-Year Timeline and Perspectives
PY  - 2024
T2  - Journal of Medical Systems
VL  - 48
IS  - 1
C7  - 22
DO  - 10.1007/s10916-024-02045-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185390721&doi=10.1007%2fs10916-024-02045-3&partnerID=40&md5=73be66b342b8dd2c6648e3b7efefb9c9
AD  - Anesthesia and Pain Medicine, Department of Medicine, Surgery and Dentistry “Scuola Medica Salernitana”, University of Salerno, Via S. Allende, Baronissi, 84081, Italy
AD  - Department of Anesthesia, Intensive Care and Prehospital Emergency, Maggiore Hospital Carlo Alberto Pizzardi, Bologna, Italy
AD  - Department of Anesthesia and Intensive Care, Infermi Hospital, AUSL Romagna, Viale Settembrini 2, Rimini, 47923, Italy
AD  - Anesthesiology, Critical Care and Pain Medicine Division, Department of Medicine and Surgery, University of Parma, Viale Gramsci 14, Parma, 43126, Italy
AB  - Within the domain of Natural Language Processing (NLP), Large Language Models (LLMs) represent sophisticated models engineered to comprehend, generate, and manipulate text resembling human language on an extensive scale. They are transformer-based deep learning architectures, obtained through the scaling of model size, pretraining of corpora, and computational resources. The potential healthcare applications of these models primarily involve chatbots and interaction systems for clinical documentation management, and medical literature summarization (Biomedical NLP). The challenge in this field lies in the research for applications in diagnostic and clinical decision support, as well as patient triage. Therefore, LLMs can be used for multiple tasks within patient care, research, and education. Throughout 2023, there has been an escalation in the release of LLMs, some of which are applicable in the healthcare domain. This remarkable output is largely the effect of the customization of pre-trained models for applications like chatbots, virtual assistants, or any system requiring human-like conversational engagement. As healthcare professionals, we recognize the imperative to stay at the forefront of knowledge. However, keeping abreast of the rapid evolution of this technology is practically unattainable, and, above all, understanding its potential applications and limitations remains a subject of ongoing debate. Consequently, this article aims to provide a succinct overview of the recently released LLMs, emphasizing their potential use in the field of medicine. Perspectives for a more extensive range of safe and effective applications are also discussed. The upcoming evolutionary leap involves the transition from an AI-powered model primarily designed for answering medical questions to a more versatile and practical tool for healthcare providers such as generalist biomedical AI systems for multimodal-based calibrated decision-making processes. On the other hand, the development of more accurate virtual clinical partners could enhance patient engagement, offering personalized support, and improving chronic disease management. © The Author(s) 2024.
KW  - Artificial Intelligence
KW  - Chatbot
KW  - ChatGPT
KW  - Clinical Decision Support
KW  - Generative AI
KW  - Large Language Models
KW  - Natural Language Processing
KW  - Communication
KW  - Documentation
KW  - Educational Status
KW  - Electric Power Supplies
KW  - Humans
KW  - Language
KW  - artificial intelligence
KW  - biomedicine
KW  - ChatGPT
KW  - clinical decision support system
KW  - deep learning
KW  - health care
KW  - health care personnel
KW  - human
KW  - large language model
KW  - medical literature
KW  - natural language processing
KW  - Review
KW  - documentation
KW  - educational status
KW  - interpersonal communication
KW  - language
KW  - power supply
PB  - Springer
SN  - 01485598 (ISSN)
C2  - 38366043
LA  - English
J2  - J. Med. Syst.
M3  - Review
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 27; Correspondence Address: V. Bellini; Anesthesiology, Critical Care and Pain Medicine Division, Department of Medicine and Surgery, University of Parma, Parma, Viale Gramsci 14, 43126, Italy; email: valentina.bellini@unipr.it; CODEN: JMSYD
ER  -

