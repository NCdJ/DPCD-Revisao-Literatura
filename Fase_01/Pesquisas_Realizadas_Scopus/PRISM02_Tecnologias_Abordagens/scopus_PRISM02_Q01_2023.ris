TY  - JOUR
AU  - Han, K.
AU  - Wang, Y.
AU  - Chen, H.
AU  - Chen, X.
AU  - Guo, J.
AU  - Liu, Z.
AU  - Tang, Y.
AU  - Xiao, A.
AU  - Xu, C.
AU  - Xu, Y.
AU  - Yang, Z.
AU  - Zhang, Y.
AU  - Tao, D.
TI  - A Survey on Vision Transformer
PY  - 2023
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
VL  - 45
IS  - 1
SP  - 87
EP  - 110
DO  - 10.1109/TPAMI.2022.3152247
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125362171&doi=10.1109%2fTPAMI.2022.3152247&partnerID=40&md5=e6f58af2558fe73908eed19f4a759bff
AD  - Huawei Noah's Ark Lab, Beijing, 100084, China
AD  - Peking University, School of EECS, Beijing, 100871, China
AD  - University of Sydney, School of Computer Science, Faculty of Engineering, Darlington, 2008, NSW, Australia
AB  - Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-Attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-Attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.  © 1979-2012 IEEE.
KW  - Computer vision
KW  - high-level vision
KW  - low-level vision
KW  - self-Attention
KW  - transformer
KW  - video
KW  - Deep neural networks
KW  - Natural language processing systems
KW  - Object detection
KW  - Recurrent neural networks
KW  - Signal encoding
KW  - Video signal processing
KW  - Computational modelling
KW  - Encodings
KW  - High-level visions
KW  - Low-level vision
KW  - Objects detection
KW  - Self-attention
KW  - Task analysis
KW  - Transformer
KW  - Video
KW  - Computer vision
PB  - IEEE Computer Society
SN  - 01628828 (ISSN)
C2  - 35180075
LA  - English
J2  - IEEE Trans Pattern Anal Mach Intell
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 1557; Correspondence Address: Y. Wang; Huawei Noah's Ark Lab, Beijing, 100084, China; email: wangyunhe@pku.edu.cn; D. Tao; University of Sydney, School of Computer Science, Faculty of Engineering, Darlington, 2008, Australia; email: dacheng.tao@sydney.edu.au; CODEN: ITPID
ER  -

TY  - JOUR
AU  - Khetani, V.
AU  - Gandhi, Y.
AU  - Bhattacharya, S.
AU  - Ajani, S.N.
AU  - Limkar, S.
TI  - Cross-Domain Analysis of ML and DL: Evaluating their Impact in Diverse Domains
PY  - 2023
T2  - International Journal of Intelligent Systems and Applications in Engineering
VL  - 11
IS  - 7s
SP  - 253
EP  - 262
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164010801&partnerID=40&md5=05bb5fc2067af827d1772808b404290b
AD  - Cybrix Technologies, Maharashtra, Nagpur, India
AD  - Competent Software, Maharashtra, Pune, India
AD  - Department of Computer Applications, National Institue of Technology, Raipur, India
AD  - Department of Computer Engineering, St. Vincent Pallotti College of Engineering and Technology, Maharashtra, Nagpur, India
AD  - Department of Artificial Intelligence & Data Science, AISSMS Institute of Information Technology, Pune, India
AB  - Deep Learning (DL) and Machine Learning (ML) techniques have been widely used in recent years to develop new and innovative products and services in various industries. These techniques have the potential to transform the way people think about and use technology. They have the capability to perform complex tasks and make accurate predictions. The efficiency of DL and ML algorithms has been studied in various domains, leading to significant progress in several applications. As technology and the domains become more interconnected, it is important to explore their effects on different sectors. One of the most important factors that can be considered when it comes to analyzing the generalizability and transferability of these techniques is cross-domain analysis. This allows us to identify the potential of these techniques to solve various problems. Cross-domain analysis is beneficial for several reasons. It allows us to identify ML and DL algorithms' limitations and strengths and transfer knowledge between them, which can help speed up the development of new solutions and decrease the time and effort involved in the process. If ML algorithm is able to perform high-accuracy in healthcare, it can provide valuable insights for the detection of financial fraud. For several reasons, cross-domain analysis is essential for the design and implementation of DL and ML algorithms. It helps in identifying the specific requirements and challenges of the given domain, and it enables the optimization of existing frameworks. The objectives and characteristics of each domain dictate the need for specific modifications or upgrades. This study aims to analyze the effects of DL and ML algorithms on different sectors, such as healthcare, financial services, and network security. It will examine the suitability and performance of different ML and DL algorithms in these domains. The findings of this research will allow us to gain a deeper understanding of their potential to address specific applications. The study covers the effects of DL and ML algorithms on different sectors, such as healthcare, NLP, financial services, and network security. It performs a comprehensive analysis of the different algorithms in these areas, including Gradient Boosting Machines (GBM), Logistic Regression (LR), Random Forest (RF), and Support Vector Machine (SVM). DL algorithms, such as Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU) and Transformer are also evaluated for their suitability and performance. This research offers actionable insights to practitioners and researchers, guiding them in picking suitable algorithms for specific applications, ultimately serving the goals of network security, healthcare, financial services, and NLP. The findings of this study will contribute to the increasing number of people who know about the applications of DL and DL algorithms. It will also help practitioners and researchers use these tools effectively in various fields. The study's cross-domain analysis also provides opportunities to enhance and transfer knowledge. © 2023, Ismail Saritas. All rights reserved.
KW  - interconnected
KW  - opportunities
KW  - potential
KW  - provides
KW  - transferability
PB  - Ismail Saritas
SN  - 21476799 (ISSN)
LA  - English
J2  - Internat. J. Intel. Syst. Appl. Eng.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 163
ER  -

TY  - JOUR
AU  - Tay, Y.
AU  - Dehghani, M.
AU  - Bahri, D.
AU  - Metzler, D.
TI  - Efficient Transformers: A Survey
PY  - 2023
T2  - ACM Computing Surveys
VL  - 55
IS  - 6
C7  - 3530811
DO  - 10.1145/3530811
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146493821&doi=10.1145%2f3530811&partnerID=40&md5=7656b9c032dfd94cccd4de8d7714907a
AD  - Google Research, United States
AD  - Google Research, Amsterdam, Netherlands
AB  - Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former"models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored "X-former"models, providing an organized and comprehensive overview of existing work and models across multiple domains.  © 2022 Copyright held by the owner/author(s).
KW  - attention
KW  - deep learning
KW  - neural networks
KW  - Transformers
KW  - Computational efficiency
KW  - Deep learning
KW  - Learning algorithms
KW  - Natural language processing systems
KW  - Network architecture
KW  - Attention
KW  - Deep learning
KW  - Language learning
KW  - Modeling architecture
KW  - Natural languages
KW  - Neural-networks
KW  - Reinforcement learnings
KW  - Transformer
KW  - Transformer modeling
KW  - Vision learning
KW  - Reinforcement learning
PB  - Association for Computing Machinery
SN  - 03600300 (ISSN)
LA  - English
J2  - ACM Comput Surv
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 332; CODEN: ACSUE
ER  -

TY  - JOUR
AU  - Roumeliotis, K.I.
AU  - Tselikas, N.D.
TI  - ChatGPT and Open-AI Models: A Preliminary Review
PY  - 2023
T2  - Future Internet
VL  - 15
IS  - 6
C7  - 192
DO  - 10.3390/fi15060192
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163727625&doi=10.3390%2ffi15060192&partnerID=40&md5=983680a889ad15454028f841da828a9f
AD  - Department of Informatics and Telecommunications, University of Peloponnese, Tripoli, 221 00, Greece
AB  - According to numerous reports, ChatGPT represents a significant breakthrough in the field of artificial intelligence. ChatGPT is a pre-trained AI model designed to engage in natural language conversations, utilizing sophisticated techniques from Natural Language Processing (NLP), Supervised Learning, and Reinforcement Learning to comprehend and generate text comparable to human-generated text. This article provides an overview of the training process and fundamental functionality of ChatGPT, accompanied by a preliminary review of the relevant literature. Notably, this article presents the first comprehensive literature review of this technology at the time of publication, aiming to aggregate all the available pertinent articles to facilitate further developments in the field. Ultimately, the authors aim to offer an appraisal of the technology’s potential implications on existing knowledge and technology, along with potential challenges that must be addressed. © 2023 by the authors.
KW  - ChatGPT
KW  - ChatGPT review
KW  - generative pre-trained transformer
KW  - GPT-4
KW  - natural language processing
KW  - Learning algorithms
KW  - Natural language processing systems
KW  - ChatGPT
KW  - ChatGPT review
KW  - Generative pre-trained transformer
KW  - GPT-4
KW  - Language processing
KW  - Literature reviews
KW  - Natural language processing
KW  - Natural languages
KW  - Reinforcement learnings
KW  - Training process
KW  - Reinforcement learning
PB  - MDPI
SN  - 19995903 (ISSN)
LA  - English
J2  - Future Internet
M3  - Review
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 197; Correspondence Address: N.D. Tselikas; Department of Informatics and Telecommunications, University of Peloponnese, Tripoli, 221 00, Greece; email: ntsel@uop.gr
ER  -

TY  - JOUR
AU  - Gilson, A.
AU  - Safranek, C.W.
AU  - Huang, T.
AU  - Socrates, V.
AU  - Chi, L.
AU  - Taylor, R.A.
AU  - Chartash, D.
TI  - How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment
PY  - 2023
T2  - JMIR Medical Education
VL  - 9
C7  - e45312
DO  - 10.2196/45312
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148992575&doi=10.2196%2f45312&partnerID=40&md5=a70a9b11b70f9182a6b1a9ed24dc5122
AD  - Section for Biomedical Informatics and Data Science, Yale University School of Medicine, New Haven, CT, United States
AD  - Department of Emergency Medicine, Yale University School of Medicine, New Haven, CT, United States
AD  - Program of Computational Biology and Bioinformatics, Yale University, New Haven, CT, United States
AD  - School of Medicine, University College Dublin, National University of Ireland, Dublin, Ireland
AB  - Background: Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input. Objective: This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination Step 1 and Step 2 exams, as well as to analyze responses for user interpretability. Methods: We used 2 sets of multiple-choice questions to evaluate ChatGPT’s performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT’s performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results: Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT’s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions: ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT’s capacity to provide logic and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as an interactive medical education tool to support learning. ©Aidan Gilson, Conrad W Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Andrew Taylor, David Chartash.
KW  - artificial intelligence
KW  - chatbot
KW  - ChatGPT
KW  - conversational agent
KW  - education technology
KW  - generative pre-trained transformer
KW  - GPT
KW  - machine learning
KW  - medical education
KW  - MedQA
KW  - natural language processing
KW  - NLP
PB  - JMIR Publications Inc.
SN  - 23693762 (ISSN)
LA  - English
J2  - JMIR Med. Educ.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 872; Correspondence Address: D. Chartash; Section for Biomedical Informatics and Data Science Yale University School of Medicine, New Haven, 300 George Street Suite 501, 06511, United States; email: david.chartash@yale.edu
ER  -

TY  - JOUR
AU  - Javaid, M.
AU  - Haleem, A.
AU  - Singh, R.P.
TI  - ChatGPT for healthcare services: An emerging stage for an innovative perspective
PY  - 2023
T2  - BenchCouncil Transactions on Benchmarks, Standards and Evaluations
VL  - 3
IS  - 1
C7  - 100105
DO  - 10.1016/j.tbench.2023.100105
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160864044&doi=10.1016%2fj.tbench.2023.100105&partnerID=40&md5=7ea96df397ecb98fe442739d9c87827e
AD  - Department of Mechanical Engineering, Jamia Millia Islamia, New Delhi, India
AD  - Department of Mechanical Engineering, National Institute of Technology, Haryana, Kurukshetra, India
AB  - Generative Pretrained Transformer, often known as GPT, is an innovative kind of Artificial Intelligence (AI) which can produce writing that seems to have been written by a person. OpenAI created this AI language model called ChatGPT. It is built using the GPT architecture and is trained on a large corpus of text data to respond to natural language inquiries that resemble a person's requirements. This technology has lots of applications in healthcare. The need for accurate and current data is one of the major obstacles to adopting ChatGPT in healthcare. GPT must have access to precise and up-to-date medical data to provide trustworthy suggestions and treatment options. It might be accomplished by ensuring that the data used by GPT is received from reliable sources and that the data is updated regularly. Since sensitive medical information would be involved, it will also be crucial to consider privacy and security issues while utilising GPT in the healthcare industry. This paper briefs about ChatGPT and its need for healthcare, its significant Work Flow Dimensions and typical features of ChatGPT for the Healthcare domain. Finally, it identified and discussed significant applications of ChatGPT for healthcare. ChatGPT can comprehend the conversational context and provide contextually appropriate replies. Its effectiveness as a conversational AI tool makes it useful for chatbots, virtual assistants, and other applications. However, we see many limitations in medical ethics, data interpretation, accountability and other issues related to the privacy. Regarding specialised tasks like text creation, language translation, text categorisation, text summarisation, and creating conversation systems, ChatGPT has been pre-trained on a large corpus of text data, and somewhat satisfactory results can be expected. Moreover, it can also be utilised for various Natural Language Processing (NLP) activities, including sentiment analysis, part-of-speech tagging, and named entity identification. © 2023 The Authors
KW  - Applications
KW  - ChatGPT
KW  - Education
KW  - Healthcare
KW  - Learning
KW  - Limitations
KW  - Treatment
KW  - Computational linguistics
KW  - Health care
KW  - ChatGPT
KW  - Emerging stages
KW  - Healthcare
KW  - Healthcare services
KW  - Large corpora
KW  - Learning
KW  - Limitation
KW  - Natural languages
KW  - Text data
KW  - Treatment
KW  - Sentiment analysis
PB  - Elsevier B.V.
SN  - 27724859 (ISSN)
LA  - English
J2  - BenchCounc. Trans. Benchmarks, Stand. Evaluation
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 196; Correspondence Address: M. Javaid; Department of Mechanical Engineering, Jamia Millia Islamia, New Delhi, India; email: mjavaid@jmi.ac.in
ER  -

TY  - JOUR
AU  - Touvron, H.
AU  - Bojanowski, P.
AU  - Caron, M.
AU  - Cord, M.
AU  - El-Nouby, A.
AU  - Grave, E.
AU  - Izacard, G.
AU  - Joulin, A.
AU  - Synnaeve, G.
AU  - Verbeek, J.
AU  - Jegou, H.
TI  - ResMLP: Feedforward Networks for Image Classification with Data-Efficient Training
PY  - 2023
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
VL  - 45
IS  - 4
SP  - 5314
EP  - 5321
DO  - 10.1109/TPAMI.2022.3206148
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139426381&doi=10.1109%2fTPAMI.2022.3206148&partnerID=40&md5=390425173748f3cd12ddf88f7daabf2b
AD  - Facebook AI Research, Paris, 75004, France
AD  - Sorbonne University, Paris, 75006, France
AB  - We present ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity trade-offs on ImageNet. We also train ResMLP models in a self-supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre-trained models and our code based on the Timm library.  © 1979-2012 IEEE.
KW  - computer-vision
KW  - Multi-layer perceptron
KW  - NLP
KW  - Benchmarking
KW  - Classification (of information)
KW  - Computational linguistics
KW  - Computer aided language translation
KW  - Computer architecture
KW  - Decoding
KW  - Distillation
KW  - Economic and social effects
KW  - Image classification
KW  - Knowledge engineering
KW  - Machine translation
KW  - Multilayer neural networks
KW  - Network architecture
KW  - Network layers
KW  - Semantics
KW  - Signal encoding
KW  - Decoding
KW  - Feed-forward network
KW  - Image patches
KW  - Images classification
KW  - Machine translations
KW  - Multilayers perceptrons
KW  - Simple++
KW  - Task analysis
KW  - Transformer
KW  - Two-layer
KW  - article
KW  - Computer vision
PB  - IEEE Computer Society
SN  - 01628828 (ISSN)
C2  - 36094972
LA  - English
J2  - IEEE Trans Pattern Anal Mach Intell
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 246; Correspondence Address: H. Touvron; Facebook AI Research, Paris, 75004, France; email: htouvron@fb.com; CODEN: ITPID
ER  -

TY  - JOUR
AU  - Rudolph, J.
AU  - Tan, S.
AU  - Tan, S.
TI  - ChatGPT: Bullshit spewer or the end of traditional assessments in higher education?
PY  - 2023
T2  - Journal of Applied Learning and Teaching
VL  - 6
IS  - 1
SP  - 342
EP  - 363
DO  - 10.37074/jalt.2023.6.1.9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148608599&doi=10.37074%2fjalt.2023.6.1.9&partnerID=40&md5=5aed8395fdc3cde275b039226634df1f
AD  - Regional Strategy & Operations, Singapore
AB  - ChatGPT is the world’s most advanced chatbot thus far. Unlike other chatbots, it can create impressive prose within seconds, and it has created much hype and doomsday predictions when it comes to student assessment in higher education and a host of other matters. ChatGPT is a state-of-the-art language model (a variant of OpenAI’s Generative Pretrained Transformer (GPT) language model) designed to generate text that can be indistinguishable from text written by humans. It can engage in conversation with users in a seemingly natural and intuitive way. In this article, we briefly tell the story of OpenAI, the organisation behind ChatGPT. We highlight the fundamental change from a not-for-profit organisation to a commercial business model. In terms of our methods, we conducted an extensive literature review and experimented with this artificial intelligence (AI) software. Our literature review shows our review to be amongst the first peer-reviewed academic journal articles to explore ChatGPT and its relevance for higher education (especially assessment, learning and teaching). After a description of ChatGPT’s functionality and a summary of its strengths and limitations, we focus on the technology’s implications for higher education and discuss what is the future of learning, teaching and assessment in higher education in the context of AI chatbots such as ChatGPT. We position ChatGPT in the context of current Artificial Intelligence in Education (AIEd) research, discuss student-facing, teacher-facing and system-facing applications, and analyse opportunities and threats. We conclude the article with recommendations for students, teachers and higher education institutions. Many of them focus on assessment. © 2023. Jürgen Rudolph, Samson Tan and Shannon Tan.
KW  - Artificial Intelligence (AI)
KW  - Artificial Intelligence in Education (AIEd)
KW  - assessment
KW  - ChatGPT
KW  - Generative Pre-trained Transformer 3 (GPT-3)
KW  - higher education
KW  - learning & teaching
KW  - natural language processing (NLP)
PB  - Kaplan Singapore
SN  - 2591801X (ISSN)
LA  - English
J2  - J. Appl. Learn. Teach.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 646
ER  -

TY  - JOUR
AU  - Kocoń, J.
AU  - Cichecki, I.
AU  - Kaszyca, O.
AU  - Kochanek, M.
AU  - Szydło, D.
AU  - Baran, J.
AU  - Bielaniewicz, J.
AU  - Gruza, M.
AU  - Janz, A.
AU  - Kanclerz, K.
AU  - Kocoń, A.
AU  - Koptyra, B.
AU  - Mieleszczenko-Kowszewicz, W.
AU  - Miłkowski, P.
AU  - Oleksy, M.
AU  - Piasecki, M.
AU  - Radliński, Ł.
AU  - Wojtasik, K.
AU  - Woźniak, S.
AU  - Kazienko, P.
TI  - ChatGPT: Jack of all trades, master of none
PY  - 2023
T2  - Information Fusion
VL  - 99
C7  - 101861
DO  - 10.1016/j.inffus.2023.101861
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162121706&doi=10.1016%2fj.inffus.2023.101861&partnerID=40&md5=50aafcd73742a897267f9e0ea3796df5
AD  - Department of Artificial Intelligence, Wrocław University of Science and Technology, Wyb. Wyspiańskiego 27, Wrocław, 50-370, Poland
AB  - OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. The first contact with the chatbot reveals its ability to provide detailed and precise answers in various areas. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quality of the ChatGPT model was about 25% for zero-shot and few-shot evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed that the more difficult the task (lower SOTA performance), the higher the ChatGPT loss. It especially refers to pragmatic NLP problems like emotion recognition. We also tested the ability to personalize ChatGPT responses for selected subjective tasks via Random Contextual Few-Shot Personalization, and we obtained significantly better user-based predictions. Additional qualitative analysis revealed a ChatGPT bias, most likely due to the rules imposed on human trainers by OpenAI. Our results provide the basis for a fundamental discussion of whether the high quality of recent predictive NLP models can indicate a tool's usefulness to society and how the learning and validation procedures for such systems should be established. © 2023 The Author(s)
KW  - ChatGPT
KW  - Emotion recognition
KW  - GPT-4
KW  - Humor detection
KW  - Large language model
KW  - Model personalization
KW  - Natural language inference (NLI)
KW  - Natural language processing (NLP)
KW  - Offensive content
KW  - Pragmatic NLP tasks
KW  - Prompting
KW  - Question answering (QA)
KW  - Semantic NLP tasks
KW  - Sentiment analysis
KW  - SOTA analysis
KW  - Stance detection
KW  - Subjective NLP tasks
KW  - Text classification
KW  - Word sense disambiguation (WSD)
KW  - Classification (of information)
KW  - Emotion Recognition
KW  - Petroleum reservoir evaluation
KW  - Quality control
KW  - Speech recognition
KW  - Zero-shot learning
KW  - Art analysis
KW  - Chat generative pre-trained transformer
KW  - Emotion recognition
KW  - GPT-4
KW  - Humor detection
KW  - Language inference
KW  - Language model
KW  - Language processing
KW  - Large language model
KW  - Model personalization
KW  - Natural language inference
KW  - Natural language processing
KW  - Natural languages
KW  - Offensive content
KW  - Personalizations
KW  - Pragmatic natural language processing task
KW  - Prompting
KW  - Question Answering
KW  - Semantic natural language processing task
KW  - Sentiment analysis
KW  - Stance detection
KW  - State of the art
KW  - State-of-the-art analyse
KW  - Subjective natural language processing task
KW  - Text classification
KW  - Word Sense Disambiguation
KW  - Semantics
PB  - Elsevier B.V.
SN  - 15662535 (ISSN)
LA  - English
J2  - Inf. Fusion
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 260; Correspondence Address: J. Kocoń; Department of Artificial Intelligence, Wrocław University of Science and Technology, Wrocław, Wyb. Wyspiańskiego 27, 50-370, Poland; email: jan.kocon@pwr.edu.pl
ER  -

TY  - JOUR
AU  - Dergaa, I.
AU  - Chamari, K.
AU  - Zmijewski, P.
AU  - Saad, H.B.
TI  - From human writing to artificial intelligence generated text: examining the prospects and potential threats of ChatGPT in academic writing
PY  - 2023
T2  - Biology of Sport
VL  - 40
IS  - 2
SP  - 615
EP  - 622
DO  - 10.5114/BIOLSPORT.2023.125623
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153204653&doi=10.5114%2fBIOLSPORT.2023.125623&partnerID=40&md5=72f951f36288364372691a144dec3114
AD  - Primary Health Care Corporation (PHCC), Doha, Qatar
AD  - Research Unit Physical Activity, Sport, and Health, UR18JS01, National Observatory of Sport, Tunis, 1003, Tunisia
AD  - High Institute of Sport and Physical Education, University of Sfax, Sfax, Tunisia
AD  - Aspetar, Orthopaedic and Sports Medicine Hospital, FIFA Medical Centre of Excellence, Doha, Qatar
AD  - Jozef Pilsudski University of Physical Education in Warsaw, Warsaw, Poland
AD  - University of Sousse, Farhat HACHED hospital, Service of Physiology and Functional Explorations, Sousse, Tunisia
AD  - University of Sousse, Farhat HACHED Hospital, Research Laboratory LR12SP09 «Heart Failure», Sousse, Tunisia
AD  - University of Sousse, Faculty of Medicine of Sousse, Laboratory of Physiology, Sousse, Tunisia
AB  - Natural language processing (NLP) has been studied in computing for decades. Recent technological advancements have led to the development of sophisticated artificial intelligence (AI) models, such as Chat Generative Pre-trained Transformer (ChatGPT). These models can perform a range of language tasks and generate human-like responses, which offers exciting prospects for academic efficiency. This manuscript aims at (i) exploring the potential benefits and threats of ChatGPT and other NLP technologies in academic writing and research publications; (ii) highlights the ethical considerations involved in using these tools, and (iii) consider the impact they may have on the authenticity and credibility of academic work. This study involved a literature review of relevant scholarly articles published in peer-reviewed journals indexed in Scopus as quartile 1. The search used keywords such as “ChatGPT,” “AI-generated text,” “academic writing,” and “natural language processing.” The analysis was carried out using a quasi-qualitative approach, which involved reading and critically evaluating the sources and identifying relevant data to support the research questions. The study found that ChatGPT and other NLP technologies have the potential to enhance academic writing and research efficiency. However, their use also raises concerns about the impact on the authenticity and credibility of academic work. The study highlights the need for comprehensive discussions on the potential use, threats, and limitations of these tools, emphasizing the importance of ethical and academic principles, with human intelligence and critical thinking at the forefront of the research process. This study highlights the need for comprehensive debates and ethical considerations involved in their use. The study also recommends that academics exercise caution when using these tools and ensure transparency in their use, emphasizing the importance of human intelligence and critical thinking in academic work. © 2023 Institute of Sport. All rights reserved.
KW  - Artificial Intelligence
KW  - Chatbot
KW  - Deep Learning
KW  - Google Bard
KW  - Higher Education
KW  - LLaMA
KW  - LLM
KW  - Machine Learning
KW  - Natural Language Processing
KW  - NLM
KW  - NLP
KW  - Paperpal
KW  - Peer Review
KW  - QuillBot
KW  - Rayyan
KW  - Research
KW  - Sports Medicine
PB  - Institute of Sport
SN  - 0860021X (ISSN)
LA  - English
J2  - Biol. Sport
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 217; Correspondence Address: H.B. Saad; University of Sousse, Farhat HACHED Hospital, Research Laboratory LR12SP09 «Heart Failure», Sousse, Tunisia; email: helmi.bensaad@rns.tn
ER  -

