TY  - JOUR
AU  - Lee, J
AU  - Yoon, W
AU  - Kim, S
AU  - Kim, D
AU  - Kim, S
AU  - So, CH
AU  - Kang, J
TI  - BioBERT: a pre-trained biomedical language representation model for biomedical text mining
T2  - BIOINFORMATICS
LA  - English
KW  - RECOGNITION
KW  - CORPUS
AB  - Motivation: Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.
   Results: We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.
AD  - Korea Univ, Dept Comp Sci & Engn, Seoul 02841, South KoreaAD  - Naver Corp, Clova Res, Seongnam 13561, South KoreaAD  - Korea Univ, Interdisciplinary Grad Program Bioinformat, Seoul 02841, South KoreaC3  - Korea UniversityC3  - NaverC3  - Korea UniversityFU  - National Research Foundation of Korea(NRF) - Korea government [NRF-2017R1A2A1A17069645, NRF-2017M3C4A7065887, NRF-2014M3C9A3063541]
FX  - This research was supported by the National Research Foundation of Korea(NRF) funded by the Korea government (NRF-2017R1A2A1A17069645, NRF-2017M3C4A7065887, NRF-2014M3C9A3063541).
PU  - OXFORD UNIV PRESS
PI  - OXFORD
PA  - GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND
SN  - 1367-4803
SN  - 1367-4811
J9  - BIOINFORMATICS
JI  - Bioinformatics
DA  - FEB 15
PY  - 2020
VL  - 36
IS  - 4
SP  - 1234
EP  - 1240
DO  - 10.1093/bioinformatics/btz682
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000518528800035
N1  - Times Cited in Web of Science Core Collection:  2403
Total Times Cited:  2630
Cited Reference Count:  38
ER  -

TY  - JOUR
AU  - Joshi, M
AU  - Chen, DQ
AU  - Liu, YH
AU  - Weld, DS
AU  - Zettlemoyer, L
AU  - Levy, O
TI  - SpanBERT: Improving Pre-training by Representing and Predicting Spans
T2  - TRANSACTIONS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS
LA  - English
AB  - We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.(1)
AD  - Univ Washington, Allen Sch Comp Sci & Engn, Seattle, WA 98195 USAAD  - Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USAAD  - Allen Inst Artificial Intelligence, Seattle, WA USAAD  - Facebook AI Res, Seattle, WA USAC3  - University of WashingtonC3  - University of Washington SeattleC3  - Princeton UniversityC3  - Facebook IncPU  - MIT PRESS
PI  - CAMBRIDGE
PA  - ONE ROGERS ST, CAMBRIDGE, MA 02142-1209 USA
SN  - 2307-387X
J9  - T ASSOC COMPUT LING
JI  - Trans. Assoc. Comput. Linguist.
PY  - 2020
VL  - 8
SP  - 64
EP  - 77
DO  - 10.1162/tacl_a_00300
WE  - Science Citation Index Expanded (SCI-EXPANDED)WE  - Social Science Citation Index (SSCI)WE  - Arts &amp; Humanities Citation Index (A&amp;HCI)AN  - WOS:000736531900005
N1  - Times Cited in Web of Science Core Collection:  857
Total Times Cited:  991
Cited Reference Count:  55
ER  -

TY  - JOUR
AU  - Clark, K
AU  - Luong, MT
AU  - Le, QV
AU  - Manning, CD
TI  - ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS
T2  - INFORMATION SYSTEMS RESEARCH
LA  - English
AB  - Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.
AD  - Stanford Univ, Stanford, CA 94305 USAAD  - Google Brain, Mountain View, CA USAAD  - CIFAR Fellow, Toronto, ON, CanadaC3  - Stanford UniversityC3  - Google IncorporatedFU  - Google PhD Fellowship
FX  - We thank Allen Nie, Prajit Ramachandran, audiences at the CIFAR LMB meeting and U. de Montre'al, and the anonymous reviewers for their thoughtful comments and suggestions. We thank Matt Peters for answering our questions about ELMo, Alec Radford for answers about GPT, Naman Goyal and Myle Ott for answers about RoBERTa, Zihang Dai for answers about XLNet, Zhenzhong Lan for answers about ALBERT, and Danqi Chen and Mandar Joshi for answers about SpanBERT. Kevin is supported by a Google PhD Fellowship.
PU  - INFORMS
PI  - CATONSVILLE
PA  - 5521 RESEARCH PARK DR, SUITE 200, CATONSVILLE, MD 21228 USA
SN  - 1047-7047
SN  - 1526-5536
J9  - INFORM SYST RES
JI  - Inf. Syst. Res.
DA  - MAR 23
PY  - 2020
DO  - 10.48550/arXiv.2003.10555
WE  - Social Science Citation Index (SSCI)AN  - WOS:001046971300001
N1  - Times Cited in Web of Science Core Collection:  815
Total Times Cited:  840
Cited Reference Count:  44
ER  -

TY  - JOUR
AU  - Qiu, XP
AU  - Sun, TX
AU  - Xu, YG
AU  - Shao, YF
AU  - Dai, N
AU  - Huang, XJ
TI  - Pre-trained models for natural language processing: A survey
T2  - SCIENCE CHINA-TECHNOLOGICAL SCIENCES
LA  - English
KW  - deep learning
KW  - neural network
KW  - natural language processing
KW  - pre-trained model
KW  - distributed representation
KW  - word embedding
KW  - self-supervised learning
KW  - language modelling
KW  - ANSWER
KW  - BERT
AB  - Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy from four different perspectives. Next, we describe how to adapt the knowledge of PTMs to downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.
AD  - Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R ChinaAD  - Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R ChinaC3  - Fudan UniversityFU  - National Natural Science Foundation of China [61751201, 61672162]; Shanghai Municipal Science and Technology Major Project [2018SHZDZX01]; ZJLab
FX  - This work was supported by the National Natural Science Foundation of China (Grant Nos. 61751201 and 61672162), the Shanghai Municipal Science and Technology Major Project (Grant No. 2018SHZDZX01) and ZJLab. We thank Zhiyuan Liu, Wanxiang Che, Minlie Huang, Danqing Wang and Luyao Huang for their valuable feedback on this manuscript.
PU  - SCIENCE PRESS
PI  - BEIJING
PA  - 16 DONGHUANGCHENGGEN NORTH ST, BEIJING 100717, PEOPLES R CHINA
SN  - 1674-7321
SN  - 1869-1900
J9  - SCI CHINA TECHNOL SC
JI  - Sci. China-Technol. Sci.
DA  - OCT
PY  - 2020
VL  - 63
IS  - 10
SP  - 1872
EP  - 1897
DO  - 10.1007/s11431-020-1647-3
C6  - SEP 2020
WE  - Science Citation Index Expanded (SCI-EXPANDED)AN  - WOS:000571751900005
N1  - Times Cited in Web of Science Core Collection:  710
Total Times Cited:  841
Cited Reference Count:  223
ER  -

TY  - JOUR
AU  - Rogers, A
AU  - Kovaleva, O
AU  - Rumshisky, A
TI  - A Primer in BERTology: What We Know About How BERT Works
T2  - TRANSACTIONS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS
LA  - English
AB  - Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.
AD  - Univ Copenhagen, Ctr Social Data Sci, Copenhagen, DenmarkAD  - Univ Massachusetts, Dept Comp Sci, Lowell, MA USAC3  - University of CopenhagenC3  - University of Massachusetts SystemC3  - University of Massachusetts LowellFU  - NSF [IIS-1844740]
FX  - We thank the anonymous reviewers for their valuable feedback. This work is funded in part by NSF award number IIS-1844740 to Anna Rumshisky.
PU  - MIT PRESS
PI  - CAMBRIDGE
PA  - ONE ROGERS ST, CAMBRIDGE, MA 02142-1209 USA
SN  - 2307-387X
J9  - T ASSOC COMPUT LING
JI  - Trans. Assoc. Comput. Linguist.
PY  - 2020
VL  - 8
SP  - 842
EP  - 866
DO  - 10.1162/tacl_a_00349
WE  - Science Citation Index Expanded (SCI-EXPANDED)WE  - Social Science Citation Index (SSCI)WE  - Arts &amp; Humanities Citation Index (A&amp;HCI)AN  - WOS:000736531900054
N1  - Times Cited in Web of Science Core Collection:  552
Total Times Cited:  622
Cited Reference Count:  180
ER  -

TY  - CPAPER
AU  - Khattab, O
AU  - Zaharia, M
A1  - ACM
TI  - ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT
T2  - PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20)
LA  - English
CP  - 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)
KW  - Neural IR
KW  - Efficiency
KW  - Deep Language Models
KW  - BERT
AB  - Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.
AD  - Stanford Univ, Stanford, CA 94305 USAC3  - Stanford UniversityFU  - Eltoukhy Family Graduate Fellowship at the Stanford School of Engineering; Facebook; Google; Infosys; NEC; VMware; Cisco; SAP; NSF [CNS-1651570]
FX  - OK was supported by the Eltoukhy Family Graduate Fellowship at the Stanford School of Engineering. This research was supported in part by affiliate members and other supporters of the Stanford DAWN project-Ant Financial, Facebook, Google, Infosys, NEC, and VMware-as well as Cisco, SAP, and the NSF under CAREER grant CNS-1651570. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-8016-4
PY  - 2020
SP  - 39
EP  - 48
DO  - 10.1145/3397271.3401075
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000722377700012
N1  - Times Cited in Web of Science Core Collection:  517
Total Times Cited:  549
Cited Reference Count:  42
ER  -

TY  - CPAPER
AU  - Jiao, XQ
AU  - Yin, YC
AU  - Shang, LF
AU  - Jiang, X
AU  - Chen, X
AU  - Li, LL
AU  - Wang, F
AU  - Liu, Q
ED  - Cohn, T
ED  - He, Y
ED  - Liu, Y
TI  - TinyBERT: Distilling BERT for Natural Language Understanding
T2  - FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020
LA  - English
CP  - Meeting of the Association-for-Computational-Linguistics (ACL-EMNLP)
AB  - Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resourcerestricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large '' teacher '' BERT can be effectively transferred to a small '' student '' TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT.
   TinyBERT 41 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT BASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT (4) is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only similar to 28% parameters and similar to 31% inference time of them. Moreover, TinyBERT 6 with 6 layers performs on-par with its teacher BERT BASE
AD  - Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Key Lab Informat Storage Syst, Wuhan, Peoples R ChinaAD  - Huawei Noahs Ark Lab, Montreal, PQ, CanadaAD  - Huawei Technol Co Ltd, Shenzhen, Peoples R ChinaC3  - Huazhong University of Science & TechnologyC3  - Huawei TechnologiesFU  - NSFC [61832020, 61821003, 61772216]; National Science and Technology Major Project [2017ZX01032-101]; Fundamental Research Funds for the Central Universities
FX  - This work is supported in part by NSFC NO.61832020, No.61821003, 61772216, National Science and Technology Major Project No.2017ZX01032-101, Fundamental Research Funds for the Central Universities.
PU  - ASSOC COMPUTATIONAL LINGUISTICS-ACL
PI  - STROUDSBURG
PA  - 209 N EIGHTH STREET, STROUDSBURG, PA 18360 USA
SN  - 978-1-952148-90-3
PY  - 2020
SP  - 4163
EP  - 4174
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:001181866503040
N1  - Times Cited in Web of Science Core Collection:  482
Total Times Cited:  527
Cited Reference Count:  51
ER  -

TY  - CPAPER
AU  - Jin, D
AU  - Jin, ZJ
AU  - Zhou, JTY
AU  - Szolovits, P
A1  - Assoc Advancement Artificial Intelligence
TI  - Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment
T2  - THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
LA  - English
CP  - 34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence
AB  - Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TEXTFOOLER, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective-it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving-it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient-it generates adversarial text with computational complexity linear to the text length.(1)
AD  - MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USAAD  - Univ Hong Kong, Hong Kong, Peoples R ChinaAD  - ASTAR, Singapore, SingaporeC3  - Massachusetts Institute of Technology (MIT)C3  - University of Hong KongC3  - Agency for Science Technology & Research (A*STAR)PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-835-0
J9  - AAAI CONF ARTIF INTE
PY  - 2020
VL  - 34
SP  - 8018
EP  - 8025
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000668126800048
N1  - Times Cited in Web of Science Core Collection:  355
Total Times Cited:  400
Cited Reference Count:  32
ER  -

TY  - CPAPER
AU  - Liu, WJ
AU  - Zhou, P
AU  - Zhao, Z
AU  - Wang, ZR
AU  - Ju, Q
AU  - Deng, HT
AU  - Wang, P
A1  - Assoc Advancement Artificial Intelligence
TI  - K-BERT: Enabling Language Representation with Knowledge Graph
T2  - THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
LA  - English
CP  - 34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence
AB  - Pre-trained language representation models, such as BERT, capture a general language representation from large-scale corpora, but lack domain-specific knowledge. When reading a domain text, experts make inferences with relevant knowledge. For machines to achieve this capability, we propose a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge. However, too much knowledge incorporation may divert the sentence from its correct meaning, which is called knowledge noise (KN) issue. To overcome KN, K-BERT introduces soft-position and visible matrix to limit the impact of knowledge. K-BERT can easily inject domain knowledge into the models by being equipped with a KG without pre-training by itself because it is capable of loading model parameters from the pre-trained BERT. Our investigation reveals promising results in twelve NLP tasks. Especially in domain-specific tasks (including finance, law, and medicine), K-BERT significantly outperforms BERT, which demonstrates that K-BERT is an excellent choice for solving the knowledge-driven problems that require experts.
AD  - Peking Univ, Beijing, Peoples R ChinaAD  - Tencent Res, Beijing, Peoples R ChinaAD  - Beijing Normal Univ, Beijing, Peoples R ChinaC3  - Peking UniversityC3  - TencentC3  - Beijing Normal UniversityFU  - National Key R&D Program of China [2017YFB1200700]; Tencent Rhino-Bird Elite Training Program
FX  - This work is funded by the National Key R&D Program of China (2017YFB1200700), and 2019 Tencent Rhino-Bird Elite Training Program.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-835-0
J9  - AAAI CONF ARTIF INTE
PY  - 2020
VL  - 34
SP  - 2901
EP  - 2908
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000667722802119
N1  - Times Cited in Web of Science Core Collection:  342
Total Times Cited:  400
Cited Reference Count:  25
ER  -

TY  - CPAPER
AU  - Sun, Y
AU  - Wang, SH
AU  - Li, YK
AU  - Feng, SK
AU  - Tian, H
AU  - Wu, H
AU  - Wang, HF
A1  - Assoc Advancement Artificial Intelligence
TI  - ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding
T2  - THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
LA  - English
CP  - 34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence
AB  - Recently pre-trained models have achieved state-of-the-art results in various language understanding tasks. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring information, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entities, semantic closeness and discourse relations. In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning. Based on this framework, we construct several tasks and train the ERNIE 2.0 model to capture lexical, syntactic and semantic aspects of information in the training data. Experimental results demonstrate that ERNIE 2.0 model outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several similar tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.
AD  - Baidu Inc, Beijing, Peoples R ChinaC3  - BaiduFU  - National Key Research and Development Project of China [2018AAA0101900]
FX  - This work is supported by the National Key Research and Development Project of China (No. 2018AAA0101900).
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-835-0
J9  - AAAI CONF ARTIF INTE
PY  - 2020
VL  - 34
SP  - 8968
EP  - 8975
WE  - Conference Proceedings Citation Index - Science (CPCI-S)AN  - WOS:000668126801050
N1  - Times Cited in Web of Science Core Collection:  309
Total Times Cited:  366
Cited Reference Count:  25
ER  -

