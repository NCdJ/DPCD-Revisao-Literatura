"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"HTKAUHJG","preprint","2024","OpenAI; Achiam, Josh; Adler, Steven; Agarwal, Sandhini; Ahmad, Lama; Akkaya, Ilge; Aleman, Florencia Leoni; Almeida, Diogo; Altenschmidt, Janko; Altman, Sam; Anadkat, Shyamal; Avila, Red; Babuschkin, Igor; Balaji, Suchir; Balcom, Valerie; Baltescu, Paul; Bao, Haiming; Bavarian, Mohammad; Belgum, Jeff; Bello, Irwan; Berdine, Jake; Bernadett-Shapiro, Gabriel; Berner, Christopher; Bogdonoff, Lenny; Boiko, Oleg; Boyd, Madelaine; Brakman, Anna-Luisa; Brockman, Greg; Brooks, Tim; Brundage, Miles; Button, Kevin; Cai, Trevor; Campbell, Rosie; Cann, Andrew; Carey, Brittany; Carlson, Chelsea; Carmichael, Rory; Chan, Brooke; Chang, Che; Chantzis, Fotis; Chen, Derek; Chen, Sully; Chen, Ruby; Chen, Jason; Chen, Mark; Chess, Ben; Cho, Chester; Chu, Casey; Chung, Hyung Won; Cummings, Dave; Currier, Jeremiah; Dai, Yunxing; Decareaux, Cory; Degry, Thomas; Deutsch, Noah; Deville, Damien; Dhar, Arka; Dohan, David; Dowling, Steve; Dunning, Sheila; Ecoffet, Adrien; Eleti, Atty; Eloundou, Tyna; Farhi, David; Fedus, Liam; Felix, Niko; Fishman, Simón Posada; Forte, Juston; Fulford, Isabella; Gao, Leo; Georges, Elie; Gibson, Christian; Goel, Vik; Gogineni, Tarun; Goh, Gabriel; Gontijo-Lopes, Rapha; Gordon, Jonathan; Grafstein, Morgan; Gray, Scott; Greene, Ryan; Gross, Joshua; Gu, Shixiang Shane; Guo, Yufei; Hallacy, Chris; Han, Jesse; Harris, Jeff; He, Yuchen; Heaton, Mike; Heidecke, Johannes; Hesse, Chris; Hickey, Alan; Hickey, Wade; Hoeschele, Peter; Houghton, Brandon; Hsu, Kenny; Hu, Shengli; Hu, Xin; Huizinga, Joost; Jain, Shantanu; Jain, Shawn; Jang, Joanne; Jiang, Angela; Jiang, Roger; Jin, Haozhun; Jin, Denny; Jomoto, Shino; Jonn, Billie; Jun, Heewoo; Kaftan, Tomer; Kaiser, Łukasz; Kamali, Ali; Kanitscheider, Ingmar; Keskar, Nitish Shirish; Khan, Tabarak; Kilpatrick, Logan; Kim, Jong Wook; Kim, Christina; Kim, Yongjik; Kirchner, Jan Hendrik; Kiros, Jamie; Knight, Matt; Kokotajlo, Daniel; Kondraciuk, Łukasz; Kondrich, Andrew; Konstantinidis, Aris; Kosic, Kyle; Krueger, Gretchen; Kuo, Vishal; Lampe, Michael; Lan, Ikai; Lee, Teddy; Leike, Jan; Leung, Jade; Levy, Daniel; Li, Chak Ming; Lim, Rachel; Lin, Molly; Lin, Stephanie; Litwin, Mateusz; Lopez, Theresa; Lowe, Ryan; Lue, Patricia; Makanju, Anna; Malfacini, Kim; Manning, Sam; Markov, Todor; Markovski, Yaniv; Martin, Bianca; Mayer, Katie; Mayne, Andrew; McGrew, Bob; McKinney, Scott Mayer; McLeavey, Christine; McMillan, Paul; McNeil, Jake; Medina, David; Mehta, Aalok; Menick, Jacob; Metz, Luke; Mishchenko, Andrey; Mishkin, Pamela; Monaco, Vinnie; Morikawa, Evan; Mossing, Daniel; Mu, Tong; Murati, Mira; Murk, Oleg; Mély, David; Nair, Ashvin; Nakano, Reiichiro; Nayak, Rajeev; Neelakantan, Arvind; Ngo, Richard; Noh, Hyeonwoo; Ouyang, Long; O'Keefe, Cullen; Pachocki, Jakub; Paino, Alex; Palermo, Joe; Pantuliano, Ashley; Parascandolo, Giambattista; Parish, Joel; Parparita, Emy; Passos, Alex; Pavlov, Mikhail; Peng, Andrew; Perelman, Adam; Peres, Filipe de Avila Belbute; Petrov, Michael; Pinto, Henrique Ponde de Oliveira; Michael; Pokorny; Pokrass, Michelle; Pong, Vitchyr H.; Powell, Tolly; Power, Alethea; Power, Boris; Proehl, Elizabeth; Puri, Raul; Radford, Alec; Rae, Jack; Ramesh, Aditya; Raymond, Cameron; Real, Francis; Rimbach, Kendra; Ross, Carl; Rotsted, Bob; Roussez, Henri; Ryder, Nick; Saltarelli, Mario; Sanders, Ted; Santurkar, Shibani; Sastry, Girish; Schmidt, Heather; Schnurr, David; Schulman, John; Selsam, Daniel; Sheppard, Kyla; Sherbakov, Toki; Shieh, Jessica; Shoker, Sarah; Shyam, Pranav; Sidor, Szymon; Sigler, Eric; Simens, Maddie; Sitkin, Jordan; Slama, Katarina; Sohl, Ian; Sokolowsky, Benjamin; Song, Yang; Staudacher, Natalie; Such, Felipe Petroski; Summers, Natalie; Sutskever, Ilya; Tang, Jie; Tezak, Nikolas; Thompson, Madeleine B.; Tillet, Phil; Tootoonchian, Amin; Tseng, Elizabeth; Tuggle, Preston; Turley, Nick; Tworek, Jerry; Uribe, Juan Felipe Cerón; Vallone, Andrea; Vijayvergiya, Arun; Voss, Chelsea; Wainwright, Carroll; Wang, Justin Jay; Wang, Alvin; Wang, Ben; Ward, Jonathan; Wei, Jason; Weinmann, C. J.; Welihinda, Akila; Welinder, Peter; Weng, Jiayi; Weng, Lilian; Wiethoff, Matt; Willner, Dave; Winter, Clemens; Wolrich, Samuel; Wong, Hannah; Workman, Lauren; Wu, Sherwin; Wu, Jeff; Wu, Michael; Xiao, Kai; Xu, Tao; Yoo, Sarah; Yu, Kevin; Yuan, Qiming; Zaremba, Wojciech; Zellers, Rowan; Zhang, Chong; Zhang, Marvin; Zhao, Shengjia; Zheng, Tianhao; Zhuang, Juntang; Zhuk, William; Zoph, Barret","GPT-4 Technical Report","","","","","http://arxiv.org/abs/2303.08774","We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.","2024-03-04","2024-11-02 19:58:40","2024-11-02 19:58:40","2024-11-02 19:58:40","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2303.08774 [cs]","","C:\Users\jesus\Zotero\storage\5HRQYSET\OpenAI et al. - 2024 - GPT-4 Technical Report.pdf; C:\Users\jesus\Zotero\storage\DEA3IVSJ\2303.html","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2303.08774","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DKSKR59L","preprint","2019","Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina","BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","","","","","http://arxiv.org/abs/1810.04805","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","2019-05-24","2024-11-02 19:59:20","2024-11-02 19:59:20","2024-11-02 19:59:20","","","","","","","BERT","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1810.04805 [cs]","","C:\Users\jesus\Zotero\storage\ETI4G69E\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf; C:\Users\jesus\Zotero\storage\4J4TR439\1810.html","","","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:1810.04805","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZG5JIGE7","preprint","2024","Lopes, Ricardo; Magalhães, João; Semedo, David","GlórIA -- A Generative and Open Large Language Model for Portuguese","","","","","http://arxiv.org/abs/2402.12969","Significant strides have been made in natural language tasks, largely attributed to the emergence of powerful large language models (LLMs). These models, pre-trained on extensive and diverse corpora, have become increasingly capable of comprehending the intricacies of language. Despite the abundance of LLMs for many high-resource languages, the availability of such models remains limited for European Portuguese. We introduce Gl\'orIA, a robust European Portuguese decoder LLM. To pre-train Gl\'orIA, we assembled a comprehensive PT-PT text corpus comprising 35 billion tokens from various sources. We present our pre-training methodology, followed by an assessment of the model's effectiveness on multiple downstream tasks. Additionally, to evaluate our models' language modeling capabilities, we introduce CALAME-PT (Context-Aware LAnguage Modeling Evaluation for Portuguese), the first Portuguese zero-shot language-modeling benchmark. Evaluation shows that Gl\'orIA significantly outperforms existing open PT decoder models in language modeling and that it can generate sound, knowledge-rich, and coherent PT-PT text. The model also exhibits strong potential for various downstream tasks.","2024-02-20","2024-11-02 20:01:36","2024-11-02 20:01:36","2024-11-02 20:01:36","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2402.12969 [cs]","","C:\Users\jesus\Zotero\storage\E4DU8ZGN\Lopes et al. - 2024 - GlórIA -- A Generative and Open Large Language Model for Portuguese.pdf; C:\Users\jesus\Zotero\storage\4LWVCSVF\2402.html","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2402.12969","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MQPX8PP6","preprint","2021","Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; Goyal, Naman; Küttler, Heinrich; Lewis, Mike; Yih, Wen-tau; Rocktäschel, Tim; Riedel, Sebastian; Kiela, Douwe","Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks","","","","","http://arxiv.org/abs/2005.11401","Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.","2021-04-12","2024-11-02 20:02:35","2024-11-02 20:02:35","2024-11-02 20:02:35","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2005.11401 [cs]","","C:\Users\jesus\Zotero\storage\NWWJ5GDA\Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf; C:\Users\jesus\Zotero\storage\NJE2Y8CA\2005.html","","","Computer Science - Computation and Language; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2005.11401","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L85ISNQC","preprint","2024","Zhang, Tianjun; Patil, Shishir G.; Jain, Naman; Shen, Sheng; Zaharia, Matei; Stoica, Ion; Gonzalez, Joseph E.","RAFT: Adapting Language Model to Domain Specific RAG","","","","","http://arxiv.org/abs/2403.10131","Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a ""open-book"" in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG. RAFT's code and demo are open-sourced at github.com/ShishirPatil/gorilla.","2024-06-05","2024-11-02 20:03:00","2024-11-02 20:03:00","2024-11-02 20:03:00","","","","","","","RAFT","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2403.10131 [cs]","","C:\Users\jesus\Zotero\storage\UQIC4NPH\Zhang et al. - 2024 - RAFT Adapting Language Model to Domain Specific RAG.pdf; C:\Users\jesus\Zotero\storage\35KET35W\2403.html","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2403.10131","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"464IXV34","journalArticle","2024","Ramos, Gil; Batista, Fernando; Ribeiro, Ricardo; Fialho, Pedro; Moro, Sérgio; Fonseca, António; Guerra, Rita; Carvalho, Paula; Marques, Catarina; Silva, Cláudia","A comprehensive review on automatic hate speech detection in the age of the transformer","Social Network Analysis and Mining","","1869-5469","10.1007/s13278-024-01361-3","https://link.springer.com/10.1007/s13278-024-01361-3","Abstract             The rapid proliferation of hate speech on social media poses significant challenges to maintaining a safe and inclusive digital environment. This paper presents a comprehensive review of automatic hate speech detection methods, with a particular focus on the evolution of approaches from traditional machine learning and deep learning models to the more advanced Transformer-based architectures. We systematically analyze over 100 studies, comparing the effectiveness, computational requirements, and applicability of various techniques, including Support Vector Machines, Long Short-Term Memory networks, Convolutional Neural Networks, and Transformer models like BERT and its multilingual variants. The review also explores the datasets, languages, and sources used for hate speech detection, noting the predominance of English-focused research while highlighting emerging efforts in low-resource languages and cross-lingual detection using multilingual Transformers. Additionally, we discuss the role of generative and multi-task learning models as promising avenues for future development. While Transformer-based models consistently achieve state-of-the-art performance, this review underscores the trade-offs between performance and computational cost, emphasizing the need for context-specific solutions. Key challenges such as algorithmic bias, data scarcity, and the need for more standardized benchmarks are also identified. This review provides crucial insights for advancing the field of hate speech detection and shaping future research directions.","2024-10-09","2024-11-02 22:43:11","2024-11-03 11:14:39","2024-11-02 22:43:11","204","","1","14","","Soc. Netw. Anal. Min.","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\jesus\Zotero\storage\FFMPHHLB\Ramos et al. - 2024 - A comprehensive review on automatic hate speech detection in the age of the transformer.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""