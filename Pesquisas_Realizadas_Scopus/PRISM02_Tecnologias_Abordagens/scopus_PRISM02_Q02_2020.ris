TY  - CONF
AU  - Li, Q.
AU  - Zhang, Q.
AU  - Yao, J.
AU  - Zhang, Y.
TI  - Event extraction for criminal legal text
PY  - 2020
T2  - Proceedings - 11th IEEE International Conference on Knowledge Graph, ICKG 2020
C7  - 9194466
SP  - 573
EP  - 580
DO  - 10.1109/ICBK50248.2020.00086
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092526577&doi=10.1109%2fICBK50248.2020.00086&partnerID=40&md5=57f32f4d93771c9310e9b10e1cb98e8a
AD  - East China Normal University, Shanghai, China
AD  - Shanghai Electric Vehicle Public Data Collecting, Monitoring and Research Center, Shanghai, China
AB  - This paper concerns with the actual problems in the legal work. We apply event extraction technology to the case description part in the Chinese legal text. We define the event type, event argument and event argument role of the larceny case, and construct a larceny case event extraction dataset through data annotation. We divide event extraction into two steps: event trigger word and argument joint extraction and event argument role assignment. We use BERT to obtain Chinese character vectors, use the BiLSTM-CRF model for extraction at the first step, and combine additional features with the extraction results of the first step, then input them to the CRF model of the second step to obtain an improvement in extraction result. We display the extracted event information in time series to realize the litigation visualization. We format Chinese time expressions, sorts the event information in tine series, and develops a Web application to display the timeline of event information. © 2020 IEEE.
KW  - Chinese legal text
KW  - Event dataset construction
KW  - Event extraction
KW  - Litigation visualization
KW  - Information analysis
KW  - Knowledge representation
KW  - Laws and legislation
KW  - Case description
KW  - Chinese characters
KW  - Data annotation
KW  - Event extraction
KW  - Event trigger
KW  - Legal texts
KW  - Role assignment
KW  - WEB application
KW  - Extraction
A2  - Chen E.
A2  - Antoniou G.
A2  - Wu X.
A2  - Kumar V.
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-172818156-1 (ISBN)
LA  - English
J2  - Proc. - IEEE Int. Conf. Knowl. Graph, ICKG
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 11; Correspondence Address: J. Yao; East China Normal University, Shanghai, China; email: junjie.yao@cs.ecnu.edu.cn; Conference name: 11th IEEE International Conference on Knowledge Graph, ICKG 2020; Conference date: 9 August 2020 through 11 August 2020; Conference code: 163030
ER  -

TY  - CONF
AU  - Shao, Y.
AU  - Mao, J.
AU  - Liu, Y.
AU  - Ma, W.
AU  - Satoh, K.
AU  - Zhang, M.
AU  - Ma, S.
TI  - BERT-PLI: Modeling paragraph-level interactions for legal case retrieval
PY  - 2020
T2  - IJCAI International Joint Conference on Artificial Intelligence
VL  - 2021-January
SP  - 3501
EP  - 3507
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097347564&partnerID=40&md5=e81b06717bf65b9adfa866b758c2f4e0
AD  - BNRist, DCST, Tsinghua University, Beijing, China
AD  - National Institute of Informatics, Tokyo, Japan
AB  - Legal case retrieval is a specialized IR task that involves retrieving supporting cases given a query case. Compared with traditional ad-hoc text retrieval, the legal case retrieval task is more challenging since the query case is much longer and more complex than common keyword queries. Besides that, the definition of relevance between a query case and a supporting case is beyond general topical relevance and it is therefore difficult to construct a large-scale case retrieval dataset, especially one with accurate relevance judgments. To address these challenges, we propose BERT-PLI, a novel model that utilizes BERT to capture the semantic relationships at the paragraph-level and then infers the relevance between two cases by aggregating paragraph-level interactions. We fine-tune the BERT model with a relatively small-scale case law entailment dataset to adapt it to the legal scenario and employ a cascade framework to reduce the computational cost. We conduct extensive experiments on the benchmark of the relevant case retrieval task in COLIEE 2019. Experimental results demonstrate that our proposed method outperforms existing solutions. © 2020 Inst. Sci. inf., Univ. Defence in Belgrade. All rights reserved.
KW  - Large dataset
KW  - Query processing
KW  - Semantics
KW  - Case retrieval
KW  - Computational costs
KW  - Keyword queries
KW  - Legal case
KW  - Relevance judgment
KW  - Semantic relationships
KW  - Small scale
KW  - Text retrieval
KW  - Artificial intelligence
A2  - Bessiere C.
PB  - International Joint Conferences on Artificial Intelligence
SN  - 10450823 (ISSN); 978-099924116-5 (ISBN)
LA  - English
J2  - IJCAI Int. Joint Conf. Artif. Intell.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 117; Correspondence Address: Y. Liu; BNRist, DCST, Tsinghua University, Beijing, China; email: yiqunliu@tsinghua.edu.cn; Conference name: 29th International Joint Conference on Artificial Intelligence, IJCAI 2020; Conference code: 165342
ER  -

TY  - CONF
AU  - Akay, H.
AU  - Kim, S.-G.
TI  - Measuring functional independence in design with deep-learning language representation models
PY  - 2020
T2  - Procedia CIRP
VL  - 91
SP  - 528
EP  - 533
DO  - 10.1016/j.procir.2020.02.210
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091699704&doi=10.1016%2fj.procir.2020.02.210&partnerID=40&md5=845e7642a8711cbb731c7390b7db795b
AD  - Massachusetts Institute of Technology, Cambridge, MA, United States
AB  - Measuring functional coupling in complex systems is an important task for good design practice, though historically it has been an art of subjective judgement. With the recent advancements in Deep Learning and Natural Language Processing, functional requirements (FRs) and design parameters (DPs), which are expressed as words and sentences, can be represented in a vector space. The sentence embedding model, BERT, was used in this paper to vectorize FRs and DPs, to calculate functional independence and to study how metrics for functional coupling measurement can be enhanced. It was found that semantic similarity among FRs and DPs, represented in vector space, could be used to compute quantitative values for metrics of functional independence. It was also found that design cases where coupling was unambiguous yielded the best results, while cases where laws of physics needed to define the FR-DP relationship did not transliterate well to the natural language used to express the FR-DP highlighted the limitations of the model in its current state. This study, however, demonstrates a great opportunity to develop a robust, fine-tuned design language representation model for accurately measuring functional independence as a part of our effort to enhance design intelligence. © 2017 The Authors. Published by Elsevier B.V.
KW  - Axiomatic design
KW  - deep learning
KW  - functional independence
KW  - language representation
KW  - Natural language processing systems
KW  - Semantics
KW  - Vector spaces
KW  - Functional requirement
KW  - Learning languages
KW  - NAtural language processing
KW  - Natural languages
KW  - Quantitative values
KW  - Representation model
KW  - Semantic similarity
KW  - Subjective judgement
KW  - Deep learning
A2  - Mpofu K.
A2  - Butala P.
PB  - Elsevier B.V.
SN  - 22128271 (ISSN)
LA  - English
J2  - Procedia CIRP
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 7; Correspondence Address: S.-G. Kim; Massachusetts Institute of Technology, Cambridge, United States; email: sangkim@mit.edu; Conference name: 30th CIRP Design on Design, CIRP Design 2020; Conference date: 5 May 2020 through 8 May 2020; Conference code: 162574
ER  -

TY  - CONF
AU  - Hong, Z.
AU  - Zhou, Q.
AU  - Zhang, R.
AU  - Li, W.
AU  - Mo, T.
TI  - Legal Feature Enhanced Semantic Matching Network for Similar Case Matching
PY  - 2020
T2  - Proceedings of the International Joint Conference on Neural Networks
C7  - 9207528
DO  - 10.1109/IJCNN48605.2020.9207528
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093864460&doi=10.1109%2fIJCNN48605.2020.9207528&partnerID=40&md5=e6ad15744d5ebe9321c6415398f56c7f
AD  - Peking University, School of Software and Microelectronics, Beijing, China
AB  - Similar case matching (SCM) aims to determine whether legal case documents are similar or not. In fact, SCM is an extension of the semantic text matching. Various deep learning models are proposed to solve the semantic text matching problems. However, the main difference between the case documents may be subtle, and the length of documents can be quite long. Moreover, the case documents are written in structural format and contain plenty of legal terms. To address these challenges, we propose a novel model in this paper. Accordingly, the legal feature vector is introduced into the semantic text matching model, and BERT is adopted as the encoding layer to capture long-range dependencies in the case documents. We conduct several experiments to evaluate the performance of our proposed model. The results show that our model outperforms other existing methods on the public dataset CAIL2019-SCM. © 2020 IEEE.
KW  - Attention mechanism
KW  - BERT
KW  - Law intelligence
KW  - Similar case matching
KW  - Deep learning
KW  - Semantic Web
KW  - Semantics
KW  - Enhanced semantics
KW  - Feature vectors
KW  - Learning models
KW  - Legal case
KW  - Long-range dependencies
KW  - Public dataset
KW  - Similar case
KW  - Text-matching
KW  - Neural networks
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-172816926-2 (ISBN)
LA  - English
J2  - Proc Int Jt Conf Neural Networks
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 26; Conference name: 2020 International Joint Conference on Neural Networks, IJCNN 2020; Conference date: 19 July 2020 through 24 July 2020; Conference code: 163566; CODEN: 85OFA
ER  -

TY  - CONF
AU  - Chalkidis, I.
AU  - Fergadiotis, M.
AU  - Malakasiotis, P.
AU  - Androutsopoulos, I.
TI  - Large-scale multi-label text classification on EU legislation
PY  - 2020
T2  - ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference
SP  - 6314
EP  - 6322
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084087887&partnerID=40&md5=592be7232ecc0dcd1836d9b359a63daf
AD  - Department of Informatics, Athens University of Economics and Business, Greece
AB  - We consider Large-Scale Multi-Label Text Classification (LMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR-LEX, annotated with ~4.3k EUROVOC labels, which is suitable for LMTC, few- and zero-shot learning. Experimenting with several neural classifiers, we show that BIGRUs with label-wise attention perform better than other current state of the art methods. Domain-specific WORD2VEC and context-sensitive ELMO embeddings further improve performance. We also find that considering only particular zones of the documents is sufficient. This allows us to bypass BERT's maximum text length limit and fine-tune BERT, obtaining the best results in all but zero-shot learning cases. © 2019 Association for Computational Linguistics
KW  - Classification (of information)
KW  - Computational linguistics
KW  - Laws and legislation
KW  - Context sensitive
KW  - Domain specific
KW  - EU legislation
KW  - Improve performance
KW  - Legal domains
KW  - Multi-label text classification
KW  - Neural classifiers
KW  - State-of-the-art methods
KW  - Text processing
PB  - Association for Computational Linguistics (ACL)
SN  - 978-195073748-2 (ISBN)
LA  - English
J2  - ACL - Annu. Meet. Assoc. Comput. Linguist., Proc. Conf.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 114; Conference name: 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019; Conference date: 28 July 2019 through 2 August 2019; Conference code: 159206
ER  -

TY  - CONF
AU  - Liu, W.
AU  - Zhou, P.
AU  - Zhao, Z.
AU  - Wang, Z.
AU  - Ju, Q.
AU  - Deng, H.
AU  - Wang, P.
TI  - K-BERT: Enabling language representation with knowledge graph
PY  - 2020
T2  - AAAI 2020 - 34th AAAI Conference on Artificial Intelligence
SP  - 2901
EP  - 2908
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106402604&partnerID=40&md5=f871a87e7bb104c4921c1497e39c8366
AD  - Peking University, Beijing, China
AD  - Tencent Research, Beijing, China
AD  - Beijing Normal University, Beijing, China
AB  - Pre-trained language representation models, such as BERT, capture a general language representation from large-scale corpora, but lack domain-specific knowledge. When reading a domain text, experts make inferences with relevant knowledge. For machines to achieve this capability, we propose a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge. However, too much knowledge incorporation may divert the sentence from its correct meaning, which is called knowledge noise (KN) issue. To overcome KN, K-BERT introduces soft-position and visible matrix to limit the impact of knowledge. K-BERT can easily inject domain knowledge into the models by being equipped with a KG without pre-training by itself because it is capable of loading model parameters from the pre-trained BERT. Our investigation reveals promising results in twelve NLP tasks. Especially in domain-specific tasks (including finance, law, and medicine), K-BERT significantly outperforms BERT, which demonstrates that K-BERT is an excellent choice for solving the knowledge-driven problems that require experts. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
KW  - Artificial intelligence
KW  - Domain knowledge
KW  - Domain specific
KW  - Domain-specific knowledge
KW  - Knowledge graphs
KW  - Knowledge incorporation
KW  - Loading models
KW  - Pre-training
KW  - Representation model
KW  - Knowledge representation
PB  - AAAI press
SN  - 978-157735835-0 (ISBN)
LA  - English
J2  - AAAI - AAAI Conf. Artif. Intell.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 546; Correspondence Address: Q. Ju; Tencent Research, Beijing, China; email: damonju@tencent.com; P. Wang; Peking University, Beijing, China; email: pwang@pku.edu.cn; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426
ER  -

TY  - CONF
AU  - D'Sa, A.G.
AU  - Illina, I.
AU  - Fohr, D.
TI  - BERT and fastText Embeddings for Automatic Detection of Toxic Speech
PY  - 2020
T2  - Proceedings of 2020 International Multi-Conference on: Organization of Knowledge and Advanced Technologies, OCTA 2020
C7  - 9151853
DO  - 10.1109/OCTA49274.2020.9151853
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091178128&doi=10.1109%2fOCTA49274.2020.9151853&partnerID=40&md5=d1720178812470b13ee9e8838bd2d3a8
AD  - Université de Lorraine, CNRS, Inria, LORIA, Nancy, F-54000, France
AB  - With the expansion of Internet usage, catering to the dissemination of thoughts and expressions of an individual, there has been an immense increase in the spread of online hate speech. Social media, community forums, discussion platforms are few examples of common playground of online discussions where people are freely allowed to communicate. However, the freedom of speech may be misused by some people by arguing aggressively, offending others and spreading verbal violence. As there is no clear distinction between the terms offensive, abusive, hate and toxic speech, in this paper we consider the above mentioned terms as toxic speech. In many countries, online toxic speech is punishable by the law. Thus, it is important to automatically detect and remove toxic speech from online medias. Through this work, we propose automatic classification of toxic speech using embedding representations of words and deep-learning techniques. We perform binary and multi-class classification using a Twitter corpus and study two approaches: (a) a method which consists in extracting of word embeddings and then using a DNN classifier; (b) fine-tuning the pre-trained BERT model. We observed that BERT fine-tuning performed much better. Proposed methodology can be used for any other type of social media comments.  © 2020 IEEE.
KW  - Classification
KW  - Deep neural network
KW  - Hate speech
KW  - Natural language processing
KW  - Deep learning
KW  - Embeddings
KW  - Knowledge management
KW  - Learning systems
KW  - Social networking (online)
KW  - Speech
KW  - Automatic classification
KW  - Automatic Detection
KW  - Freedom of speech
KW  - Internet usage
KW  - Learning techniques
KW  - Multi-class classification
KW  - Online discussions
KW  - Online media
KW  - Speech recognition
A2  - Krichen S.
A2  - Ben-Romdhane H.
A2  - Sidhom S.
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 978-172816403-8 (ISBN)
LA  - English
J2  - Proc. Int. Multi-Conf.: Organ. Knowl. Adv. Technol., OCTA
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 59; Conference name: 2020 International Multi-Conference on: Organization of Knowledge and Advanced Technologies�, OCTA 2020; Conference date: 6 February 2020 through 8 February 2020; Conference code: 162064
ER  -

TY  - CONF
AU  - Esfahani, S.S.
AU  - Cafarella, M.J.
AU  - Pouyan, M.B.
AU  - DeAngelo, G.J.
AU  - Eneva, E.
AU  - Fano, A.E.
TI  - Context-specific language modeling for human trafficking detection from online advertisements
PY  - 2020
T2  - ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference
SP  - 1180
EP  - 1184
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084041788&partnerID=40&md5=9842addd1346caf3cfbcb42bc5c091d7
AD  - Accenture Technology Labs, San Francisco, CA, United States
AD  - Department of Computer Science, University of Michigan, United States
AD  - Department of Economics, Claremont Graduate University, United States
AB  - Human trafficking is a worldwide crisis. Traffickers exploit their victims by anonymously offering sexual services through online advertisements. These ads often contain clues that law enforcement can use to separate out potential trafficking cases from volunteer sex advertisements. The problem is that the sheer volume of ads is too overwhelming for manual processing. Ideally, a centralized semi-automated tool can be used to assist law enforcement agencies with this task. Here, we present an approach using natural language processing to identify trafficking ads on these websites. We propose a classifier by integrating multiple text feature sets, including the publicly available pre-trained textual language model Bi-directional Encoder Representation from transformers (BERT). In this paper, we demonstrate that a classifier using this composite feature set has significantly better performance compared to any single feature set alone. © 2019 Association for Computational Linguistics
KW  - Classification (of information)
KW  - Computational linguistics
KW  - Crime
KW  - Natural language processing systems
KW  - Composite features
KW  - Human trafficking
KW  - Law-enforcement agencies
KW  - Manual processing
KW  - NAtural language processing
KW  - Online advertisements
KW  - Specific languages
KW  - Textual language
KW  - Modeling languages
PB  - Association for Computational Linguistics (ACL)
SN  - 978-195073748-2 (ISBN)
LA  - English
J2  - ACL - Annu. Meet. Assoc. Comput. Linguist., Proc. Conf.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 12; Conference name: 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019; Conference date: 28 July 2019 through 2 August 2019; Conference code: 159206
ER  -

TY  - JOUR
AU  - Fang, Y.
AU  - Tian, X.
AU  - Wu, H.
AU  - Gu, S.
AU  - Wang, Z.
AU  - Wang, F.
AU  - Li, J.
AU  - Weng, Y.
TI  - Few-Shot Learning for Chinese Legal Controversial Issues Classification
PY  - 2020
T2  - IEEE Access
VL  - 8
C7  - 9069958
SP  - 75022
EP  - 75034
DO  - 10.1109/ACCESS.2020.2988493
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084378439&doi=10.1109%2fACCESS.2020.2988493&partnerID=40&md5=334a11249f23f953592379f6a653cc52
AD  - College of Mathematics, Sichuan University, Chengdu, 610065, China
AD  - Law School, Sichuan University, Chengdu, 610207, China
AD  - Union Big Data Technology, Chengdu, 610041, China
AB  - Chinese courts organize debates surrounding controversial issues along with the gradual formation of the new procedural system. With the progress of China's judicial reform, more than 80 million judgement documents have been made public online. Similar controversial issues identified in and among the massive public judgment documents are of significant value for judges in their trial work. Hence, homogeneous controversial issues classification becomes the basis for similar cases retrieval. However, controversial issues follow the power-law distribution, not all of them are within the labels provided by manual annotation and their categories cannot be exhausted. In order to generalize those unfamiliar categories without necessitating extensive retraining, we propose a controversial issues classification algorithm based on few-shot learning. Two few-shot learning algorithms are proposed for our controversial issues problem, Relation Network and Induction Network, respectively. With only a handful of given instances, both of them have shown excellent results on the two datasets, which proves their effectiveness in adapting to accommodating new categories not seen in training. The proposed method provides trial assistance for judges, promotes the dissemination of experience and improves fairness of adjudication. © 2013 IEEE.
KW  - BERT
KW  - Controversial issues
KW  - few-shot learning
KW  - power-law
KW  - text classification
KW  - Classification algorithm
KW  - Manual annotation
KW  - Power law distribution
KW  - Procedural systems
KW  - Similar case
KW  - Learning algorithms
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 21693536 (ISSN)
LA  - English
J2  - IEEE Access
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 11; Correspondence Address: Y. Weng; College of Mathematics, Sichuan University, Chengdu, 610065, China; email: wengyang@scu.edu.cn
ER  -

TY  - CONF
AU  - Chalkidis, I.
AU  - Fergadiotis, M.
AU  - Malakasiotis, P.
AU  - Aletras, N.
AU  - Androutsopoulos, I.
TI  - LEGAL-BERT: The muppets straight out of law school
PY  - 2020
T2  - Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020
SP  - 2898
EP  - 2904
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114906399&partnerID=40&md5=3ab52c2cc8d51c7b6fbff5a2122eed57
AD  - Department of Informatics, Athens University of Economics and Business, Greece
AD  - Institute of Informatics and Telecommunications, NCSR “Demokritos”, Greece
AD  - Computer Science Department, University of Sheffield, United Kingdom
AB  - BERT has achieved impressive performance in several NLP tasks. However, there has been limited investigation on its adaptation guidelines in specialised domains. Here we focus on the legal domain, where we explore several approaches for applying BERT models to downstream legal tasks, evaluating on multiple datasets. Our findings indicate that the previous guidelines for pre-training and fine-tuning, often blindly followed, do not always generalize well in the legal domain. Thus we propose a systematic investigation of the available strategies when applying BERT in specialised domains. These are: (a) use the original BERT out of the box, (b) adapt BERT by additional pre-training on domain-specific corpora, and (c) pre-train BERT from scratch on domain-specific corpora. We also propose a broader hyper-parameter search space when fine-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT models intended to assist legal NLP research, computational law, and legal technology applications. © 2020 Association for Computational Linguistics
KW  - Computational linguistics
KW  - Domain specific
KW  - Down-stream
KW  - Fine tuning
KW  - Hyper-parameter
KW  - Law schools
KW  - Legal domains
KW  - Multiple data sets
KW  - Parameter search space
KW  - Performance
KW  - Pre-training
KW  - Natural language processing systems
PB  - Association for Computational Linguistics (ACL)
SN  - 978-195214890-3 (ISBN)
LA  - English
J2  - Findings Assoc. Comp. Linguist. Findings ACL: EMNLP
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 342; Conference name: Findings of the Association for Computational Linguistics, ACL 2020: EMNLP 2020; Conference date: 16 November 2020 through 20 November 2020; Conference code: 172733
ER  -

