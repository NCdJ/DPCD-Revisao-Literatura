TY  - CONF
AU  - Morris, J.X.
AU  - Lifland, E.
AU  - Yoo, J.Y.
AU  - Grigsby, J.
AU  - Jin, D.
AU  - Qi, Y.
TI  - TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP
PY  - 2020
T2  - EMNLP 2020 - Conference on Empirical Methods in Natural Language Processing, Proceedings of Systems Demonstrations
SP  - 119
EP  - 126
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150452032&partnerID=40&md5=c32bd9e1db2c93918c6167e9a7cb5e0f
AD  - Department of Computer Science, University of Virginia, United States
AD  - Computer Science and Artificial Intelligence Laboratory, MIT, United States
AB  - While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack’s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness. TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack. © 2020 Association for Computational Linguistics.
KW  - Computational linguistics
KW  - Data augmentation
KW  - Goal functions
KW  - Line of codes
KW  - Model robustness
KW  - Modeling accuracy
KW  - Modeling performance
KW  - Modular designs
KW  - Search method
KW  - Training modules
KW  - Natural language processing systems
A2  - Liu Q.
A2  - Schlangen D.
PB  - Association for Computational Linguistics (ACL)
SN  - 978-195214862-0 (ISBN)
LA  - English
J2  - EMNLP - Conf. Empir. Methods in Nat. Lang. Process., Proc. Syst. Demonstr.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 321; Conference name: 2020 System Demonstrations of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020; Conference date: 16 November 2020 through 20 November 2020; Conference code: 192531
ER  -

TY  - CONF
AU  - Su, W.
AU  - Zhu, X.
AU  - Cao, Y.
AU  - Li, B.
AU  - Lu, L.
AU  - Wei, F.
AU  - Dai, J.
TI  - VL-BERT: PRE-TRAINING OF GENERIC VISUAL-LINGUISTIC REPRESENTATIONS
PY  - 2020
T2  - 8th International Conference on Learning Representations, ICLR 2020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150638039&partnerID=40&md5=f52cb8e8b59b88a0fb25f5e00ea3257e
AD  - University of Science and Technology of China, China
AD  - Microsoft Research Asia
AB  - We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and benefit the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark. Code is released at https://github.com/jackroos/VL-BERT. © 2020 8th International Conference on Learning Representations, ICLR 2020. All rights reserved.
KW  - Linguistics
KW  - Natural language processing systems
KW  - Down-stream
KW  - Empirical analysis
KW  - Generic representation
KW  - Input image
KW  - Linguistic representations
KW  - Pre-training
KW  - Region-of-interest
KW  - Regions of interest
KW  - Simple++
KW  - Transformer modeling
KW  - Image segmentation
PB  - International Conference on Learning Representations, ICLR
LA  - English
J2  - Int. Conf. Learn. Represent., ICLR
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 438; Correspondence Address: J. Dai; Microsoft Research Asia; email: jifdai@microsoft.com; Conference name: 8th International Conference on Learning Representations, ICLR 2020; Conference code: 186995
ER  -

TY  - JOUR
AU  - Rogers, A.
AU  - Kovaleva, O.
AU  - Rumshisky, A.
TI  - A primer in bertology: What we know about how bert works
PY  - 2020
T2  - Transactions of the Association for Computational Linguistics
VL  - 8
SP  - 842
EP  - 866
DO  - 10.1162/tacl_a_00349
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098839172&doi=10.1162%2ftacl_a_00349&partnerID=40&md5=c14ea3cd386b168b85cd64948cf50d91
AD  - Center for Social Data Science, University of Copenhagen, Denmark
AD  - Dept. of Computer Science, University of Massachusetts Lowell, United States
AB  - Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research. © 2020 Association for Computational Linguistics.
KW  - 'current
KW  - Learn+
KW  - Overparameterization
KW  - State of the art
KW  - States of knowledge
PB  - MIT Press Journals
SN  - 2307387X (ISSN)
LA  - English
J2  - Trans. Assoc. Comput. Linguist.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 815
ER  -

TY  - JOUR
AU  - Raffel, C.
AU  - Shazeer, N.
AU  - Roberts, A.
AU  - Lee, K.
AU  - Narang, S.
AU  - Matena, M.
AU  - Zhou, Y.
AU  - Li, W.
AU  - Liu, P.J.
TI  - Exploring the limits of transfer learning with a unified text-to-text transformer
PY  - 2020
T2  - Journal of Machine Learning Research
VL  - 21
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092733644&partnerID=40&md5=df1a6dc84cf71b099f2476907f7c8e17
AD  - Google, Mountain View, 94043, CA, United States
AB  - Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code. ©2020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.
KW  - Attentionbased models
KW  - Deep learning
KW  - Multi-task learning
KW  - Natural language processing
KW  - Transfer learning
KW  - Classification (of information)
KW  - Learning systems
KW  - Natural language processing systems
KW  - Text processing
KW  - Language problems
KW  - Language understanding
KW  - Learning techniques
KW  - NAtural language processing
KW  - Question Answering
KW  - Systematic study
KW  - Text classification
KW  - Unified framework
KW  - Transfer learning
PB  - Microtome Publishing
SN  - 15324435 (ISSN)
LA  - English
J2  - J. Mach. Learn. Res.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 9121; Correspondence Address: C. Raffel; Google, Mountain View, 94043, United States; email: craffel@gmail.com
ER  -

TY  - CONF
AU  - Rahman, W.
AU  - Hasan, M.K.
AU  - Lee, S.
AU  - Zadeh, A.
AU  - Mao, C.
AU  - Morency, L.-P.
AU  - Hoque, E.
TI  - Integrating multimodal information in large pretrained transformers
PY  - 2020
T2  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
SP  - 2359
EP  - 2369
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114652770&partnerID=40&md5=fe44f7e01e600139f9319ca90beb6b28
AD  - Department of Computer Science, University of Rochester, United States
AD  - Language Technologies Institute, SCS, CMU, United States
AB  - Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. While finetuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication). Pre-trained models don't have the necessary components to accept two extra modalities of vision and acoustic. In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning. It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities. In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only finetuning of BERT and XLNet. On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community. © 2020 Association for Computational Linguistics
KW  - Computational linguistics
KW  - Modal analysis
KW  - Modeling languages
KW  - Contextual modeling
KW  - Contextual words
KW  - Fine tuning
KW  - Multi-modal
KW  - Multi-modal information
KW  - Multiple disciplines
KW  - Performance
KW  - Sentiment analysis
KW  - State-of-the-art performance
KW  - Word representations
KW  - Sentiment analysis
PB  - Association for Computational Linguistics (ACL)
SN  - 0736587X (ISSN); 978-195214825-5 (ISBN)
LA  - English
J2  - Proc. Annu. Meet. Assoc. Comput Linguist.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 389; Conference name: 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020; Conference date: 5 July 2020 through 10 July 2020; Conference code: 172533
ER  -

TY  - CONF
AU  - Cui, Y.
AU  - Che, W.
AU  - Liu, T.
AU  - Qin, B.
AU  - Wang, S.
AU  - Hu, G.
TI  - Revisiting pre-trained models for Chinese natural language processing
PY  - 2020
T2  - Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020
SP  - 657
EP  - 668
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118125136&partnerID=40&md5=8add7b76dc76071b3ab08c1a3af488bc
AD  - Research Center for Social Computing and Information Retrieval (SCIR), Harbin Institute of Technology, Harbin, China
AD  - State Key Laboratory of Cognitive Intelligence, iFLYTEK Research, China
AD  - iFLYTEK AI Research (Hebei), Langfang, China
AB  - Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we target on revisiting Chinese pre-trained language models to examine their effectiveness in a non-English language and release the Chinese pre-trained language model series to the community. We also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways, especially the masking strategy that adopts MLM as correction (Mac). We carried out extensive experiments on eight Chinese NLP tasks to revisit the existing pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research. © 2020 Association for Computational Linguistics
KW  - Computational linguistics
KW  - Chinese natural language processing
KW  - Language model
KW  - Non-English languages
KW  - Performance
KW  - Simple++
KW  - State-of-the-art performance
KW  - Natural language processing systems
PB  - Association for Computational Linguistics (ACL)
SN  - 978-195214890-3 (ISBN)
LA  - English
J2  - Findings Assoc. Comp. Linguist. Findings ACL: EMNLP
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 453; Conference name: Findings of the Association for Computational Linguistics, ACL 2020: EMNLP 2020; Conference date: 16 November 2020 through 20 November 2020; Conference code: 172733
ER  -

TY  - CONF
AU  - Zhang, J.
AU  - Zhao, Y.
AU  - Saleh, M.
AU  - Liu, P.J.
TI  - PEGASUS: Pre-Training with extracted gap-sentences for abstractive summarization
PY  - 2020
T2  - 37th International Conference on Machine Learning, ICML 2020
VL  - PartF168147-15
SP  - 11265
EP  - 11276
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105298429&partnerID=40&md5=0dd4c4265971fdbd66c48aa4eea5b366
AD  - Data Science Institute, Imperial College London, London, United Kingdom
AD  - Brain Team, Google Research, Mountain View, CA, United States
AB  - Recent work pre-Training Transformers with self-supervised objectives on large text corpora has shown great success when fine-Tuned on downstream NLP tasks including text summarization. However, pre-Training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-Training large Transformer-based encoder-decoder models on massive text corpora with a new selfsupervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-The-Art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-The-Art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets. © 2020 by the Authors All rights reserved.
KW  - Supervised learning
KW  - Text processing
KW  - Human evaluation
KW  - Human performance
KW  - Multiple data sets
KW  - Output sequences
KW  - State of the art
KW  - State-of-the-art performance
KW  - Systematic evaluation
KW  - Text summarization
KW  - Learning systems
A2  - Daume H.
A2  - Singh A.
PB  - International Machine Learning Society (IMLS)
SN  - 978-171382112-0 (ISBN)
LA  - English
J2  - Int. Conf. Machin. Learn., ICML
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 682; Correspondence Address: J. Zhang; Data Science Institute, Imperial College London, London, United Kingdom; email: jingqing.zhang15@imperial.ac.uk; Conference name: 37th International Conference on Machine Learning, ICML 2020; Conference date: 13 July 2020 through 18 July 2020; Conference code: 168147
ER  -

TY  - JOUR
AU  - Lee, J.
AU  - Yoon, W.
AU  - Kim, S.
AU  - Kim, D.
AU  - Kim, S.
AU  - So, C.H.
AU  - Kang, J.
TI  - BioBERT: A pre-trained biomedical language representation model for biomedical text mining
PY  - 2020
T2  - Bioinformatics
VL  - 36
IS  - 4
SP  - 1234
EP  - 1240
DO  - 10.1093/bioinformatics/btz682
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080840963&doi=10.1093%2fbioinformatics%2fbtz682&partnerID=40&md5=b8f287a6d1ca7da47309e2d4303e74e4
AD  - Department of Computer Science and Engineering, Korea University, Seoul, 02841, South Korea
AD  - Clova AI Research, Naver Corp, Seong-Nam, 13561, South Korea
AD  - Interdisciplinary Graduate Program in Bioinformatics, Korea University, Seoul, 02841, South Korea
AB  - Motivation: Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. Results: We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. © 2020 Oxford University Press. All rights reserved.
KW  - Data Mining
KW  - Language
KW  - Natural Language Processing
KW  - Software
KW  - article
KW  - human
KW  - human experiment
KW  - language
KW  - mining
KW  - data mining
KW  - language
KW  - natural language processing
KW  - software
PB  - Oxford University Press
SN  - 13674803 (ISSN)
C2  - 31501885
LA  - English
J2  - Bioinformatics
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 3495; Correspondence Address: J. Kang; Department of Computer Science and Engineering, Korea University, Seoul, 02841, South Korea; email: kangj@korea.ac.kr; CODEN: BOINF
ER  -

TY  - CONF
AU  - Zaheer, M.
AU  - Guruganesh, G.
AU  - Dubey, A.
AU  - Ainslie, J.
AU  - Alberti, C.
AU  - Ontanon, S.
AU  - Pham, P.
AU  - Ravula, A.
AU  - Wang, Q.
AU  - Yang, L.
AU  - Ahmed, A.
TI  - Big bird: Transformers for longer sequences
PY  - 2020
T2  - Advances in Neural Information Processing Systems
VL  - 2020-December
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097333449&partnerID=40&md5=45a008708bb4033f27b1b4f818cc9724
AD  - Google Research
AB  - Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BIGBIRD, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BIGBIRD is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BIGBIRD drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data. © 2020 Neural information processing systems foundation. All rights reserved.
KW  - Deep learning
KW  - Attention mechanisms
KW  - Attention model
KW  - Learning models
KW  - Novel applications
KW  - Question Answering
KW  - Sequence lengths
KW  - Turing-complete
KW  - Universal approximators
KW  - Natural language processing systems
PB  - Neural information processing systems foundation
SN  - 10495258 (ISSN)
LA  - English
J2  - Adv. neural inf. proces. syst.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 996; Conference name: 34th Conference on Neural Information Processing Systems, NeurIPS 2020; Conference date: 6 December 2020 through 12 December 2020; Conference code: 169463
ER  -

TY  - CONF
AU  - Wang, W.
AU  - Wei, F.
AU  - Dong, L.
AU  - Bao, H.
AU  - Yang, N.
AU  - Zhou, M.
TI  - MINILM: Deep self-attention distillation for task-agnostic compression of pre-trained transformers
PY  - 2020
T2  - Advances in Neural Information Processing Systems
VL  - 2020-December
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101000449&partnerID=40&md5=8b792cd5c3a9e5570b7b34d683aec21c
AD  - Microsoft Research
AB  - Pre-trained language models (e.g., BERT [12] and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer [42] based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant [26] also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model2 outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models. © 2020 Neural information processing systems foundation. All rights reserved.
KW  - Distillation
KW  - Students
KW  - Capacity constraints
KW  - Effective approaches
KW  - Language model
KW  - Real-life applications
KW  - State of the art
KW  - Student Models
KW  - Transformer models
KW  - Transformer parameters
KW  - Distilleries
PB  - Neural information processing systems foundation
SN  - 10495258 (ISSN)
LA  - English
J2  - Adv. neural inf. proces. syst.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 522; Correspondence Address: F. Wei; Microsoft Research; email: fuwei@microsoft.com; Conference name: 34th Conference on Neural Information Processing Systems, NeurIPS 2020; Conference date: 6 December 2020 through 12 December 2020; Conference code: 169463
ER  -

