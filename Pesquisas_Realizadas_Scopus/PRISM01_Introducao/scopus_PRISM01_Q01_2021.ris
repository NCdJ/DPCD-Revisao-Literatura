TY  - JOUR
AU  - Amer, F.
AU  - Jung, Y.
AU  - Golparvar-Fard, M.
TI  - Transformer machine learning language model for auto-alignment of long-term and short-term plans in construction
PY  - 2021
T2  - Automation in Construction
VL  - 132
C7  - 103929
DO  - 10.1016/j.autcon.2021.103929
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115034423&doi=10.1016%2fj.autcon.2021.103929&partnerID=40&md5=1cf96f283fb127475350e082e5bba801
AD  - Department of Civil and Environmental Engineering, University of Illinois at Urbana-Champaign, Urbana, 61801, IL, United States
AD  - Civil Engineering, Computer Science and Tech Entrepreneurship, University of Illinois at Urbana-Champaign, United States
AB  - In construction, master schedules and look-ahead plans are created at different times (monthly vs. weekly), by different personas (planner vs. superintendent), with different software (scheduling solution vs. spreadsheet), and at different levels of granularity (milestones vs. production details). Their full-alignment is essential for project coordination, progress updating, and payment application reviews, and its absence may lead to costly litigation. This paper presents the first attempt to automate linking look-ahead planning tasks to master-schedule activities following an NLP-based multi-stage ranking formulation. Our model employs distance-based matching for candidate generation and a Transformer architecture for final matching.1 Validation results from real-world projects demonstrate that the method helps planners match look-ahead planning tasks to master schedule activities by presenting a list of top-five matches with a precision of 76.5%. We also show that the method helps superintendents create look-ahead plans from a master schedule by generating lists of tasks based on activity descriptions. © 2021 Elsevier B.V.
KW  - Artificial intelligence
KW  - Construction planning
KW  - Machine learning
KW  - Natural language processing
KW  - Project controls
KW  - Laws and legislation
KW  - Learning algorithms
KW  - Machine learning
KW  - Project management
KW  - Auto alignment
KW  - Construction planning
KW  - Distance based matching
KW  - Full alignments
KW  - Language model
KW  - Multi-stages
KW  - Planning tasks
KW  - Project control
KW  - Project coordination
KW  - Software scheduling
KW  - Natural language processing systems
PB  - Elsevier B.V.
SN  - 09265805 (ISSN)
LA  - English
J2  - Autom Constr
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 39; Correspondence Address: F. Amer; Department of Civil and Environmental Engineering, University of Illinois at Urbana-Champaign, Urbana, 61801, United States; email: famer2@illinois.edu; CODEN: AUCOE
ER  -

TY  - CONF
AU  - Zheng, L.
AU  - Guha, N.
AU  - Anderson, B.R.
AU  - Henderson, P.
AU  - Ho, D.E.
TI  - When does pretraining help?: Assessing self-supervised learning for law and the CaseHOLD dataset of 53,000+ legal holdings
PY  - 2021
T2  - Proceedings of the 18th International Conference on Artificial Intelligence and Law, ICAIL 2021
SP  - 159
EP  - 168
DO  - 10.1145/3462757.3466088
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109620027&doi=10.1145%2f3462757.3466088&partnerID=40&md5=9593c72b9e78fb2d3c6462a124aae846
AD  - Stanford University, Stanford, CA, United States
AB  - While self-supervised learning has made rapid advances in natural language processing, it remains unclear when researchers should engage in resource-intensive domain-specific pretraining (domain pretraining). The law, puzzlingly, has yielded few documented instances of substantial gains to domain pretraining in spite of the fact that legal language is widely seen to be unique. We hypothesize that these existing results stem from the fact that existing legal NLP tasks are too easy and fail to meet conditions for when domain pretraining can help. To address this, we first present CaseHOLD (Case <u>H</u>oldings <u>O</u>n <u>L</u>egal <u>D</u>ecisions), a new dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case. This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second, we assess performance gains on CaseHOLD and existing legal NLP datasets. While a Transformer architecture (BERT) pretrained on a general corpus (Google Books and Wikipedia) improves performance, domain pretraining (on a corpus of 3.5M decisions across all courts in the U.S. that is larger than BERT's) with a custom legal vocabulary exhibits the most substantial performance gains with CaseHOLD (gain of 7.2% on F1, representing a 12% improvement on BERT) and consistent performance gains across two other legal tasks. Third, we show that domain pretraining may be warranted when the task exhibits sufficient similarity to the pretraining corpus: the level of performance increase in three legal tasks was directly tied to the domain specificity of the task. Our findings inform when researchers should engage in resource-intensive pretraining and show that Transformer-based architectures, too, learn embeddings suggestive of distinct legal language.  © 2021 Owner/Author.
KW  - benchmark dataset
KW  - law
KW  - natural language processing
KW  - pretraining
KW  - Linguistics
KW  - Supervised learning
KW  - Consistent performance
KW  - Domain specific
KW  - Domain specificity
KW  - Multiple choice questions
KW  - NAtural language processing
KW  - Performance Gain
KW  - Pre-training
KW  - Wikipedia
KW  - Natural language processing systems
PB  - Association for Computing Machinery, Inc
SN  - 978-145038526-8 (ISBN)
LA  - English
J2  - Proc. Int. Conf. Artif. Intell. Law, ICAIL
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 119; Conference name: 18th International Conference on Artificial Intelligence and Law, ICAIL 2021; Conference date: 21 June 2021 through 25 June 2021; Conference code: 170686
ER  -

TY  - JOUR
AU  - Wang, X.
AU  - Kou, L.
AU  - Sugumaran, V.
AU  - Luo, X.
AU  - Zhang, H.
TI  - Emotion Correlation Mining through Deep Learning Models on Natural Language Text
PY  - 2021
T2  - IEEE Transactions on Cybernetics
VL  - 51
IS  - 9
SP  - 4400
EP  - 4413
DO  - 10.1109/TCYB.2020.2987064
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115164168&doi=10.1109%2fTCYB.2020.2987064&partnerID=40&md5=8ede9ff4292d9052d9cda3379a2a1d0b
AD  - School of Computer Engineering and Science, Shanghai University, Shanghai, 200444, China
AD  - Department of Engineering Physics, Institute of Public Safety Research, Tsinghua University, Beijing, 100084, China
AD  - Department of Decision and Information Sciences, Oakland University, Rochester, 48309, MI, United States
AB  - Emotion analysis has been attracting researchers' attention. Most previous works in the artificial-intelligence field focus on recognizing emotion rather than mining the reason why emotions are not or wrongly recognized. The correlation among emotions contributes to the failure of emotion recognition. In this article, we try to fill the gap between emotion recognition and emotion correlation mining through natural language text from Web news. The correlation among emotions, expressed as the confusion and evolution of emotion, is primarily caused by human emotion cognitive bias. To mine emotion correlation from emotion recognition through text, three kinds of features and two deep neural-network models are presented. The emotion confusion law is extracted through an orthogonal basis. The emotion evolution law is evaluated from three perspectives: one-step shift, limited-step shifts, and shortest path transfer. The method is validated using three datasets: 1) the titles; 2) the bodies; and 3) the comments of news articles, covering both objective and subjective texts in varying lengths (long and short). The experimental results show that in subjective comments, emotions are easily mistaken as anger. Comments tend to arouse emotion circulations of love-anger and sadness-anger. In objective news, it is easy to recognize text emotion as love and cause fear-joy circulation. These findings could provide insights for applications regarding affective interaction, such as network public sentiment, social media communication, and human-computer interaction.  © 2013 IEEE.
KW  - Affective computing
KW  - deep neural networks
KW  - emotion correlation mining
KW  - emotion recognition
KW  - natural language processing (NLP)
KW  - Anger
KW  - Deep Learning
KW  - Emotions
KW  - Fear
KW  - Humans
KW  - Language
KW  - Character recognition
KW  - Deep neural networks
KW  - Human computer interaction
KW  - Speech recognition
KW  - Text mining
KW  - Affective interaction
KW  - Correlation mining
KW  - Emotion recognition
KW  - Natural language text
KW  - Neural network model
KW  - Orthogonal basis
KW  - Public sentiments
KW  - Recognizing emotions
KW  - anger
KW  - emotion
KW  - fear
KW  - human
KW  - language
KW  - Deep learning
PB  - Institute of Electrical and Electronics Engineers Inc.
SN  - 21682267 (ISSN)
C2  - 32413938
LA  - English
J2  - IEEE Trans. Cybern.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 53; Correspondence Address: X. Wang; School of Computer Engineering and Science, Shanghai University, Shanghai, 200444, China; email: wxz2017@shu.edu.cn
ER  -

TY  - JOUR
AU  - Ma, Q.
AU  - Sun, C.
AU  - Cui, B.
AU  - Jin, X.
TI  - A novel model for anomaly detection in network traffic based on kernel support vector machine
PY  - 2021
T2  - Computers and Security
VL  - 104
C7  - 102215
DO  - 10.1016/j.cose.2021.102215
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101619363&doi=10.1016%2fj.cose.2021.102215&partnerID=40&md5=6979b4228cb06c247b6be049109017e2
AD  - School of Cyberspace Security, Beijing University of Posts and Telecommunications, China
AD  - School of Science, Beijing University of Posts and Telecommunications, China
AD  - National Engineering Lab for Mobile Network Technology, China
AB  - Machine learning models are widely used for anomaly detection in network traffic. Effective transformation of the raw traffic data into mathematical expressions and hyper-parameter adjustment are two important steps before training the machine learning classifier, which is used to predict whether the unknown traffic is normal or abnormal. In this paper, a novel model SVM-L is proposed for anomaly detection in network traffic. In particular, raw URLs are treated as natural language, and then transformed into mathematical vectors via statistical laws and natural language processing technique. They are used as the training data for the traffic classifier, the kernel Support Vector Machine (SVM). Based on the idea of the dual formulation of kernel SVM and Linear Discriminant Analysis (LDA), we propose an optimization model to adjust the hyper-parameter of the classifier. The corresponding problem is simply one-dimensional, and is easily solved by the golden section method. Numerical tests indicate that the proposed model achieves more than 99% accuracy on all tested datasets, and outperforms the state of the arts in terms of standard evaluation measurements. © 2021 Elsevier Ltd
KW  - Anomaly detection in network traffic
KW  - Data transformation
KW  - Hyper-parameter adjustment
KW  - Kernel support vector machine
KW  - Linear discriminant analysis
KW  - Anomaly detection
KW  - Arts computing
KW  - Discriminant analysis
KW  - Learning systems
KW  - Mathematical transformations
KW  - Metadata
KW  - Natural language processing systems
KW  - Golden section method
KW  - Linear discriminant analysis
KW  - Machine learning models
KW  - Mathematical expressions
KW  - NAtural language processing
KW  - Optimization modeling
KW  - Standard evaluations
KW  - Traffic classifiers
KW  - Support vector machines
PB  - Elsevier Ltd
SN  - 01674048 (ISSN)
LA  - English
J2  - Comput Secur
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 52; Correspondence Address: C. Sun; School of Science, Beijing University of Posts and Telecommunications, China; email: suncong86@bupt.edu.cn; CODEN: CPSED
ER  -

TY  - JOUR
AU  - Davidson, J.E.
AU  - Ye, G.
AU  - Parra, M.C.
AU  - Choflet, A.
AU  - Lee, K.
AU  - Barnes, A.
AU  - Harkavy-Friedman, J.
AU  - Zisook, S.
TI  - Job-Related Problems Prior to Nurse Suicide, 2003-2017: A Mixed Methods Analysis Using Natural Language Processing and Thematic Analysis
PY  - 2021
T2  - Journal of Nursing Regulation
VL  - 12
IS  - 1
SP  - 28
EP  - 39
DO  - 10.1016/S2155-8256(21)00017-X
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103945116&doi=10.1016%2fS2155-8256%2821%2900017-X&partnerID=40&md5=eb62306985e75484d72ee030e43c2aae
AB  - Background: Nurses have a higher rate of suicide than the gender-matched general population at baseline. Quantitative data from the Centers for Disease Control and Prevention National Violent Death Reporting System have been previously analyzed to reveal that nurses have more known job-related issues prior to death by suicide. However, no known study has focused on the context of those job-related problems prior to nurse suicide. Purpose: The aim of this study was to provide context to job-related problems experienced before nurse death by suicide. Methods: Cases were selected either because they were coded as having a job-related problem prior to death or the words “job” or “work” appeared in the case investigation narrative. Natural language processing and thematic analysis of free-text medical examiner and law enforcement investigation narratives were performed to better describe nurse deaths by suicide in cases with known job-related problems prior to death. Results: Narratives from a total of 203 nurse deaths from between 2003 and 2017 were included in this study. Very little was reported regarding the actual work of being a nurse. Job-related problems of these 203 deaths focused on substance use, mental health problems, chronic pain, or job loss due to investigations for substance use or diversion of medication. Conclusions: Nurses who lose a nursing position or leave the profession because of substance use, mental health issues, or chronic pain are at risk for nurse suicide. Alternative-to-discipline programs for nurses with substance use disorder need to be improved and standardized. Earlier or more complete treatment for mental illness may help prevent suicide in this population. © 2021 National Council of State Boards of Nursing
KW  - healthy workforce
KW  - licensure
KW  - nursing
KW  - patient safety
KW  - Substance-related disorder
KW  - suicide
PB  - Elsevier Inc.
SN  - 21558256 (ISSN)
LA  - English
J2  - J. Nurs. Regul.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 39
ER  -

TY  - CONF
AU  - Liu, S.
AU  - Zhao, B.
AU  - Guo, R.
AU  - Meng, G.
AU  - Zhang, F.
AU  - Zhang, M.
TI  - Have you been properly notified? automatic compliance analysis of privacy policy text with GDPR article 13
PY  - 2021
T2  - The Web Conference 2021 - Proceedings of the World Wide Web Conference, WWW 2021
SP  - 2154
EP  - 2164
DO  - 10.1145/3442381.3450022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107924517&doi=10.1145%2f3442381.3450022&partnerID=40&md5=8d05d9fd0458e00dca89847891a84d39
AD  - Tianjin University, China
AD  - Institute of Information Engineering, University of Chinese Academy of Sciences, China
AB  - With the rapid development of web and mobile applications, as well as their wide adoption in different domains, more and more personal data is provided, consciously or unconsciously, to different application providers. Privacy policy is an important medium for users to understand what personal information has been collected and used. As data privacy protection is becoming a critical social issue, there are laws and regulations being enacted in different countries and regions, and the most representative one is the EU General Data Protection Regulation (GDPR). It is thus important to detect compliance issues among regulations, e.g., GDPR, with privacy policies, and provide intuitive results for data subjects (i.e., users), data collection party (i.e., service providers) and the regulatory authorities. In this work, we target to solve the problem of compliance analysis between GDPR (Article 13) and privacy policies. We format the task into a combination of a sentence classification step and a rule-based analysis step. We manually curate a corpus of 36,610 labeled sentences from 304 privacy policies, and benchmark our corpus with several standard sentence classifiers. We also conduct a rule-based analysis to detect compliance issues and a user study to evaluate the usability of our approach. The web-based tool AutoCompliance is publicly accessible 1.  Â© 2021 ACM.
KW  - Compliance Analysis
KW  - Natural Language Processing
KW  - Privacy
KW  - Regulatory compliance
KW  - World Wide Web
KW  - Application providers
KW  - Data privacy protections
KW  - General data protection regulations
KW  - Laws and regulations
KW  - Personal information
KW  - Regulatory authorities
KW  - Sentence classifications
KW  - Sentence classifiers
KW  - Data privacy
PB  - Association for Computing Machinery, Inc
SN  - 978-145038312-7 (ISBN)
LA  - English
J2  - Web Conf. - Proc. World Wide Web Conf., WWW
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 38; Conference name: 2021 World Wide Web Conference, WWW 2021; Conference date: 19 April 2021 through 23 April 2021; Conference code: 169366
ER  -

TY  - JOUR
AU  - Hassan, F.U.
AU  - Le, T.
AU  - Lv, X.
TI  - Addressing Legal and Contractual Matters in Construction Using Natural Language Processing: A Critical Review
PY  - 2021
T2  - Journal of Construction Engineering and Management
VL  - 147
IS  - 9
C7  - 03121004
DO  - 10.1061/(ASCE)CO.1943-7862.0002122
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109216110&doi=10.1061%2f%28ASCE%29CO.1943-7862.0002122&partnerID=40&md5=42a3202b31ec55a873263b3479861b0d
AD  - Glenn Dept. of Civil Engineering, Clemson Univ., Clemson, 29634, SC, United States
AD  - Moss School of Construction, Infrastructure and Sustainability, Florida International Univ., Miami, 33174, FL, United States
AB  - Claims, disputes, and litigations are major legal issues in construction projects, which often result in cost overruns, delays, and adverse working relationships among the contracting parties. Recent advances in natural language processing (NLP) techniques offer great potentials that can process voluminous unstructured data from legal documents to draw insightful information about the root causes of issues and prevention strategies. Several efforts have been undertaken in the last decades that used NLP to tackle a wide range of problems related to legal issues in construction such as the quality review of contracts and the identification of common patterns in legal cases. The research line on NLP-based techniques for analyzing legal texts of construction projects has progressed well recently; it, however, is still in the early stage. This paper aims to perform a critical review of recently published articles to analyze the achievements and limitations of the state of the art on NLP-based approaches to address common legal issues associated with legal documents arising across different project stages. The study also provides a roadmap for future research to expand the adoption of NLP for the processing of legal texts in construction.  © 2021 American Society of Civil Engineers.
KW  - Artificial intelligence
KW  - Claims
KW  - Contracts
KW  - Disputes
KW  - Legal issues
KW  - Linguistics
KW  - Litigation
KW  - Natural language processing (NLP)
KW  - Project requirements
KW  - Authentication
KW  - Construction projects
KW  - Critical review
KW  - NAtural language processing
KW  - Prevention strategies
KW  - Quality reviews
KW  - State of the art
KW  - Unstructured data
KW  - Working relationships
KW  - Natural language processing systems
PB  - American Society of Civil Engineers (ASCE)
SN  - 07339364 (ISSN)
LA  - English
J2  - J Constr Eng Manage
M3  - Review
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 43; Correspondence Address: T. Le; Glenn Dept. of Civil Engineering, Clemson Univ., Clemson, 29634, United States; email: tuyenl@clemson.edu; CODEN: JCEMD
ER  -

TY  - JOUR
AU  - Mumcuoğlu, E.
AU  - Öztürk, C.E.
AU  - Ozaktas, H.M.
AU  - Koç, A.
TI  - Natural language processing in law: Prediction of outcomes in the higher courts of Turkey
PY  - 2021
T2  - Information Processing and Management
VL  - 58
IS  - 5
C7  - 102684
DO  - 10.1016/j.ipm.2021.102684
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109706883&doi=10.1016%2fj.ipm.2021.102684&partnerID=40&md5=a6e8d34b014bbed6889fc00f69e8b6d4
AD  - Electrical and Electronics Engineering Department, Bilkent University, Ankara, Turkey
AD  - UMRAM, Bilkent University, Ankara, Turkey
AB  - Natural language processing (NLP) based approaches have recently received attention for legal systems of several countries. It is of interest to study the wide variety of legal systems that have so far not received any attention. In particular, for the legal system of the Republic of Turkey, codified in Turkish, no works have been published. We first review the state-of-the-art of NLP in law, and then study the problem of predicting verdicts for several different courts, using several different algorithms. This study is much broader than earlier studies in the number of different courts and the variety of algorithms it includes. Therefore it provides a reference point and baseline for further studies in this area. We further hope the scope and systematic nature of this study can set a framework that can be applied to the study of other legal systems. We present novel results on predicting the rulings of the Turkish Constitutional Court and Courts of Appeal, using only fact descriptions, and without seeing the actual rulings. The methods that are utilized are based on Decision Trees (DTs), Random Forests (RFs), Support Vector Machines (SVMs) and state-of-the-art deep learning (DL) methods; specifically Gated Recurrent Units (GRUs), Long Short-Term Memory networks (LSTMs) and bidirectional LSTMs (BiLSTMs), with the integration of an attention mechanism for each model. The prediction results for all algorithms are given in a comparative and detailed manner. We demonstrate that outcomes of the courts of Turkish legal system can be predicted with high accuracy, especially with deep learning based methods. The presented results exhibit similar performance to earlier work in the literature for other languages and legal systems. © 2021 Elsevier Ltd
KW  - AI in law
KW  - Deep learning
KW  - Law
KW  - Legal text mining
KW  - Machine learning
KW  - Natural language processing
KW  - Decision trees
KW  - Deep learning
KW  - Forecasting
KW  - Laws and legislation
KW  - Learning algorithms
KW  - Support vector machines
KW  - AI in law
KW  - Deep learning
KW  - Language processing
KW  - Law
KW  - Legal system
KW  - Legal text mining
KW  - Machine-learning
KW  - Natural languages
KW  - State of the art
KW  - Turkishs
KW  - Natural language processing systems
PB  - Elsevier Ltd
SN  - 03064573 (ISSN)
LA  - English
J2  - Inf. Process. Manage.
M3  - Article
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 46; Correspondence Address: A. Koç; Electrical and Electronics Engineering Department, Bilkent University, Ankara, Turkey; email: aykut.koc@bilkent.edu.tr; CODEN: IPMAD
ER  -

TY  - CONF
AU  - Chalkidis, I.
AU  - Fergadiotis, M.
AU  - Tsarapatsanis, D.
AU  - Aletras, N.
AU  - Androutsopoulos, I.
AU  - Malakasiotis, P.
TI  - Paragraph-level Rationale Extraction through Regularization: A case study on European Court of Human Rights Cases
PY  - 2021
T2  - NAACL-HLT 2021 - 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference
SP  - 226
EP  - 241
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107958532&partnerID=40&md5=6faab48a58207cac38d07e8134827897
AD  - EY AI Centre of Excellence in Document Intelligence, NCSR “Demokritos”, Greece
AD  - Department of Informatics, Athens University of Economics and Business, Greece
AD  - Computer Science Department, University of Sheffield, United Kingdom
AD  - Law School, University of York, United Kingdom
AB  - Interpretability or explainability is an emerging research field in NLP. From a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. To this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. We also release a new dataset comprising European Court of Human Rights cases, including annotations for paragraph-level rationales. We use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. Our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. We also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research. © 2021 Association for Computational Linguistics.
KW  - Laws and legislation
KW  - Natural language processing systems
KW  - Social aspects
KW  - Case-studies
KW  - Court case
KW  - Human rights
KW  - Interpretability
KW  - Legal texts
KW  - New applications
KW  - Regularisation
KW  - Research fields
KW  - User-centric
KW  - Word level
KW  - Extraction
PB  - Association for Computational Linguistics (ACL)
SN  - 978-195408546-6 (ISBN)
LA  - English
J2  - NAACL-HLT - Conf. N. Am. Chapter Assoc. Comput. Linguist.: Hum. Lang. Technol., Proc. Conf.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 68; Conference name: 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021; Conference date: 6 June 2021 through 11 June 2021; Conference code: 182055
ER  -

TY  - CONF
AU  - So, D.R.
AU  - Mańke, W.
AU  - Liu, H.
AU  - Dai, Z.
AU  - Shazeer, N.
AU  - Le, Q.V.
TI  - Primer: Searching for Efficient Transformers for Language Modeling
PY  - 2021
T2  - Advances in Neural Information Processing Systems
VL  - 8
SP  - 6010
EP  - 6022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131635600&partnerID=40&md5=05e334aa82c8b2ebe9c4672aa16bdc01
AD  - Google Research, Brain Team
AB  - Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant. Compared to previous approaches, our search is performed at a lower level, over the primitives that define a Transformer TensorFlow program. We identify an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling. Primer's improvements can be mostly attributed to two simple modifications: squaring ReLU activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention. Experiments show Primer's gains over Transformer increase as compute scale grows and follow a power law with respect to quality at optimal model sizes. We also verify empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning. For example, at a 500M parameter size, Primer improves the original T5 architecture on C4 auto-regressive language modeling, reducing the training cost by 4X. Furthermore, the reduced training cost means Primer needs much less compute to reach a target one-shot performance. For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer. We open source our models and several comparisons in T5 to help with reproducibility. © 2021 Neural information processing systems foundation. All rights reserved.
KW  - Computational linguistics
KW  - Cost reduction
KW  - Modeling languages
KW  - Auto-regressive
KW  - Language model
KW  - Model size
KW  - Optimal model
KW  - Performance
KW  - Power-law
KW  - Simple modifications
KW  - Small training
KW  - Training costs
KW  - Transformer modeling
KW  - Natural language processing systems
A2  - Ranzato M.
A2  - Beygelzimer A.
A2  - Dauphin Y.
A2  - Liang P.S.
A2  - Wortman Vaughan J.
PB  - Neural information processing systems foundation
SN  - 10495258 (ISSN); 978-171384539-3 (ISBN)
LA  - English
J2  - Adv. neural inf. proces. syst.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 18 December 2024; Cited By: 72; Conference name: 35th Conference on Neural Information Processing Systems, NeurIPS 2021; Conference date: 6 December 2021 through 14 December 2021; Conference code: 179642
ER  -

