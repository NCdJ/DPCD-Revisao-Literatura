<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:link="http://purl.org/rss/1.0/modules/link/"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/">
    <rdf:Description rdf:about="urn:isbn:978-1-4503-6976-3">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-6976-3</dc:identifier>
                <dc:identifier>DOI 10.1145/3357384.3358028</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>van Aken</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Winter</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Löser</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gers</foaf:surname>
                        <foaf:givenName>FA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>ACM</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_509"/>
        <link:link rdf:resource="#item_1565"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>explainability</dc:subject>
        <dc:subject>neural networks</dc:subject>
        <dc:subject>transformers</dc:subject>
        <dc:subject>question answering</dc:subject>
        <dc:subject>word representation</dc:subject>
        <dc:title>How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations</dc:title>
        <dcterms:abstract>Bidirectional Encoder Representations from Transformers (BERT) reach state-of-the-art results in a variety of Natural Language Processing tasks. However, understanding of their internal functioning is still insufficient and unsatisfactory. In order to better understand BERT and other Transformer-based models, we present a layer-wise analysis of BERT's hidden states. Unlike previous research, which mainly focuses on explaining Transformer models by their attention weights, we argue that hidden states contain equally valuable information. Specifically, our analysis focuses on models fine-tuned on the task of Question Answering (QA) as an example of a complex downstream task. We inspect how QA models transform token vectors in order to find the correct answer. To this end, we apply a set of general and QA-specific probing tasks that reveal the information stored in each representation layer. Our qualitative analysis of hidden state visualizations provides additional insights into BERT's reasoning process. Our results show that the transformations within BERT go through phases that are related to traditional pipeline tasks. The system can therefore implicitly incorporate task-specific information into its token representations. Furthermore, our analysis reveals that fine-tuning has little impact on the models' semantic abilities and that prediction errors can be recognized in the vector representations of even early layers.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000539898201088</dc:coverage>
        <bib:pages>1823-1832</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION &amp; KNOWLEDGE MANAGEMENT (CIKM '19)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_509">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;68&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;73&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;41&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1565">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1565/van Aken et al. - 2019 - How Does BERT Answer Questions A Layer-Wise Analysis of Transformer Representations.pdf"/>
        <dc:title>Versão Submetida</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1909.04925</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:38:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-950737-13-0">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-950737-13-0</dc:identifier>
                <dc:title>Google Incorporated</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Devlin</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chang</foaf:surname>
                        <foaf:givenName>MW</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Toutanova</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_655"/>
        <dc:title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</dc:title>
        <dcterms:abstract>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.
BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000900116904035</dc:coverage>
        <bib:pages>4171-4186</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_655">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;31750&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;35264&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;54&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:2945-9133">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>11856</prism:volume>
                <dc:identifier>ISBN 2945-9133</dc:identifier>
                <dc:title>Fudan University</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-32381-3_16</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qiu</foaf:surname>
                        <foaf:givenName>XP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>YG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>XJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>X</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Z</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_673"/>
        <dc:subject>BERT</dc:subject>
        <dc:subject>Text classification</dc:subject>
        <dc:subject>Transfer learning</dc:subject>
        <dc:title>How to Fine-Tune BERT for Text Classification?</dc:title>
        <dcterms:abstract>Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000578465600016</dc:coverage>
        <bib:pages>194-206</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>CHINESE COMPUTATIONAL LINGUISTICS, CCL 2019</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_673">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;689&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;775&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;29&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_682">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2307-387X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rogers</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kovaleva</foaf:surname>
                        <foaf:givenName>O</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rumshisky</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_683"/>
        <link:link rdf:resource="#item_1564"/>
        <dc:title>A Primer in BERTology: What We Know About How BERT Works</dc:title>
        <dcterms:abstract>Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000736531900054</dc:coverage>
        <bib:pages>842-866</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2307-387X">
        <prism:volume>8</prism:volume>
        <dc:title>TRANSACTIONS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS</dc:title>
        <dc:identifier>DOI 10.1162/tacl_a_00349</dc:identifier>
        <dc:identifier>ISSN 2307-387X</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_683">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;552&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;622&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;180&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1564">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1564/Rogers et al. - 2020 - A Primer in BERTology What We Know About How BERT Works.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:38:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:2159-5399">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>34</prism:volume>
                <dc:identifier>ISBN 2159-5399</dc:identifier>
                <dc:title>Peking University</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>WJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Z</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>ZR</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ju</foaf:surname>
                        <foaf:givenName>Q</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deng</foaf:surname>
                        <foaf:givenName>HT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Assoc Advancement Artificial Intelligence</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_691"/>
        <dc:title>K-BERT: Enabling Language Representation with Knowledge Graph</dc:title>
        <dcterms:abstract>Pre-trained language representation models, such as BERT, capture a general language representation from large-scale corpora, but lack domain-specific knowledge. When reading a domain text, experts make inferences with relevant knowledge. For machines to achieve this capability, we propose a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge. However, too much knowledge incorporation may divert the sentence from its correct meaning, which is called knowledge noise (KN) issue. To overcome KN, K-BERT introduces soft-position and visible matrix to limit the impact of knowledge. K-BERT can easily inject domain knowledge into the models by being equipped with a KG without pre-training by itself because it is capable of loading model parameters from the pre-trained BERT. Our investigation reveals promising results in twelve NLP tasks. Especially in domain-specific tasks (including finance, law, and medicine), K-BERT significantly outperforms BERT, which demonstrates that K-BERT is an excellent choice for solving the knowledge-driven problems that require experts.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000667722802119</dc:coverage>
        <bib:pages>2901-2908</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_691">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;342&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;400&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;25&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:1062-922X">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 1062-922X</dc:identifier>
                <dc:title>Purdue University System</dc:title>
                <dc:identifier>DOI 10.1109/smc.2019.8913898</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ye</foaf:surname>
                        <foaf:givenName>QF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Misra</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Devarapalli</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rayz</foaf:surname>
                        <foaf:givenName>JT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>IEEE</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_793"/>
        <dc:title>A SENTIMENT BASED NON-FACTOID QUESTION-ANSWERING FRAMEWORK</dc:title>
        <dcterms:abstract>With the rapid advances in Artificial Intelligence, a question of emotional intelligence of a system may become as important as its accuracy. This paper investigates whether emotions should be considered for non-factoid &quot;how&quot; Question-Answering systems with the eventual goal of enabling the system to retrieve answers in a more emotionally intelligent way. This study proposes an architecture that adds extended representation of sentiment information to questions and answers, and reports on to what extent a prediction of the best answer be improved by the proposed architecture.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000521353900061</dc:coverage>
        <bib:pages>372-377</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS (SMC)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_793">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;4&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;6&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;14&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_844">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0165-5515"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lian</foaf:surname>
                        <foaf:givenName>ZX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1342"/>
        <dcterms:isReferencedBy rdf:resource="#item_845"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>government</dc:subject>
        <dc:subject>chatbot</dc:subject>
        <dc:subject>contextualisation</dc:subject>
        <dc:subject>fuzzy logic</dc:subject>
        <dc:subject>relational graph convolutional networks</dc:subject>
        <dc:title>Government chatbot: Empowering smart conversations with enhanced contextual understanding and reasoning</dc:title>
        <dcterms:abstract>Currently, an increasing number of governments have adopted question answering systems (QASs) in public service delivery. As some citizens with limited information literacy often express their questions vaguely when interacting with a chatbot, it is necessary to improve the contextual understanding and reasoning ability of government chatbots (G-chatbots). This goal can be achieved through the optimisation of the matching between question, answer and context. By incorporating the Relational Graph Convolutional Networks (R-GCNs) and fuzzy logic, this study proposes a multi-turn dialogue model that introduces a re-question mechanism and a subgraph matching algorithm. The experiment results show that the model can improve the contextual reasoning ability of G-chatbots by about 10% and generate answers in a more explainable way. This study innovatively integrates a question-answer-context matching approach, re-question mechanism into the MTRF-G-chatbot model, reducing barriers to citizens' access to government services and enhancing contextual reasoning abilities.</dcterms:abstract>
        <dc:date>2024 SEP 18</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001315539900001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0165-5515">
        <dc:title>JOURNAL OF INFORMATION SCIENCE</dc:title>
        <dc:identifier>DOI 10.1177/01655515241268863</dc:identifier>
        <dc:identifier>ISSN 0165-5515</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1342">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_845">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;55&lt;/p&gt;</rdf:value>
    </bib:Memo>
</rdf:RDF>
