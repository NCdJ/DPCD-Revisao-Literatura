<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:link="http://purl.org/rss/1.0/modules/link/">
    <rdf:Description rdf:about="urn:isbn:0302-9743">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>10597</prism:volume>
                <dc:identifier>ISBN 0302-9743</dc:identifier>
                <dc:title>National Institute of Technology (NIT System)</dc:title>
                <dc:identifier>DOI 10.1007/978-3-319-69900-4_47</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sharma</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kumar</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kumar</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shankar</foaf:surname>
                        <foaf:givenName>BU</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ghosh</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mandal</foaf:surname>
                        <foaf:givenName>DP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ray</foaf:surname>
                        <foaf:givenName>SS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pal</foaf:surname>
                        <foaf:givenName>SK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_6"/>
        <link:link rdf:resource="#item_1547"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Emotion detection</dc:subject>
        <dc:subject>Prediction</dc:subject>
        <dc:subject>Security</dc:subject>
        <dc:title>LEXER: LEXicon Based Emotion AnalyzeR</dc:title>
        <dcterms:abstract>The huge population of India poses a challenge to government, security and law enforcement. What if we could know beforehand the consequences of any events. Social spaces, such as Twitter, Facebook, and Personal blogs, enable people to show their thoughts regarding public issues and topics. Public emotion regarding future and past events, like public gatherings, governmental policies, shows public beliefs and can be deployed to analyze the measure of support, disorder, or disrupted in such situations. Therefore, emotion analysis of Internet content may be beneficial for various organizations, particularly in government, law enforcement, and security sectors. This paper presents an extension to state-of-art-model for lexicon-based sentiment analysis algorithm for analysis of human emotions.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000450772100047</dc:coverage>
        <bib:pages>373-379</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PATTERN RECOGNITION AND MACHINE INTELLIGENCE, PREMI 2017</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_6">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;23&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;23&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;13&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1547">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1547/Sharma et al. - 2017 - LEXER LEXicon Based Emotion AnalyzeR.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.1007%2F978-3-319-69900-4_47.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:31:34</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_46">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0043-0617"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levendowski</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_47"/>
        <dcterms:isReferencedBy rdf:resource="#item_237"/>
        <dc:subject>COPY</dc:subject>
        <dc:subject>FAIR</dc:subject>
        <dc:subject>POWER</dc:subject>
        <dc:title>HOW COPYRIGHT LAW CAN FIX ARTIFICIAL INTELLIGENCE'S IMPLICIT BIAS PROBLEM</dc:title>
        <dcterms:abstract>As the use of artificial intelligence (AI) continues to spread, we have seen an increase in examples of AI systems reflecting or exacerbating societal bias, from racist facial recognition to sexist natural language processing. These biases threaten to overshadow AI's technological gains and potential benefits. While legal and computer science scholars have analyzed many sources of bias, including the unexamined assumptions of its often homogenous creators, flawed algorithms, and incomplete datasets, the role of the law itself has been largely ignored. Yet just as code and culture play significant roles in how AI agents learn about and act in the world, so too do the laws that govern them. This Article is the first to examine perhaps the most powerful law impacting AI bias: copyright.
Artificial intelligence often learns to &quot;think&quot; by reading, viewing, and listening to copies of human works. This Article first explores the problem of bias through the lens of copyright doctrine, looking at how the law's exclusion of access to certain copyrighted source materials may create or promote biased Al systems. Copyright law limits bias mitigation techniques, such as testing AI through reverse engineering, algorithmic accountability processes, and competing to convert customers. The rules of copyright law also privilege access to certain works over others, encouraging AI creators to use easily available, legally low-risk sources of data for teaching AI, even when those data are demonstrably biased. Second, it examines how a different part of copyright law the fair use doctrine has traditionally been used to address similar concerns in other technological fields, and asks whether it is equally capable of addressing them in the field of AI bias. The Article ultimately concludes that it is, in large part because the normative values embedded within traditional fair use ultimately align with the goals of mitigating Al bias and, quite literally, creating fairer AI systems.</dcterms:abstract>
        <dc:date>2018 JUN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000445990600001</dc:coverage>
        <bib:pages>579-630</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0043-0617">
        <prism:volume>93</prism:volume>
        <dc:title>WASHINGTON LAW REVIEW</dc:title>
        <prism:number>2</prism:number>
        <dc:identifier>ISSN 0043-0617</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_47">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;51&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;56&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;206&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_237">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;51&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;56&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;206&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_70">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0306-4573"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Srinivasa</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Thilagam</foaf:surname>
                        <foaf:givenName>PS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_71"/>
        <dc:subject>DISAMBIGUATION</dc:subject>
        <dc:subject>EXTRACTION</dc:subject>
        <dc:subject>Information Extraction</dc:subject>
        <dc:subject>Integration</dc:subject>
        <dc:subject>Knowledge Representation</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Ontology</dc:subject>
        <dc:subject>ONTOLOGY</dc:subject>
        <dc:subject>RECOGNITION</dc:subject>
        <dc:subject>SEMANTIC SIMILARITY</dc:subject>
        <dc:title>Crime base: Towards building a knowledge base for crime entities and their relationships from online news papers</dc:title>
        <dcterms:abstract>In the current era of internet, information related to crime is scattered across many sources namely news media, social networks, blogs, and video repositories, etc. Crime reports published in online newspapers are often considered as reliable compared to crowdsourced data like social media and contain crime information not only in the form of unstructured text but also in the form of images. Given the volume and availability of crime-related information present in online newspapers, gathering and integrating crime entities from multiple modalities and representing them as a knowledge base in machine-readable form will be useful for any law enforcement agencies to analyze and prevent criminal activities. Extant research works to generate the crime knowledge base, does not address extraction of all non-redundant entities from text and image data present in multiple newspapers. Hence, this work proposes Crime Base, an entity relationship based system to extract and integrate crime related text and image data from online newspapers with a focus towards reducing duplicity and loss of information in the knowledge base. The proposed system uses a rule-based approach to extract the entities from text and image captions. The entities extracted from text data are correlated using contextual as-well-as semantic similarity measures and image entities are correlated using low-level and high-level image features. The proposed system also presents an integrated view of these entities and their relations in the form of a knowledge base using OWL. The system is tested for a collection of crime related articles from popular Indian online newspapers.</dcterms:abstract>
        <dc:date>2019 NOV</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000487766200004</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0306-4573">
        <prism:volume>56</prism:volume>
        <dc:title>INFORMATION PROCESSING &amp; MANAGEMENT</dc:title>
        <dc:identifier>DOI 10.1016/j.ipm.2019.102059</dc:identifier>
        <prism:number>6</prism:number>
        <dc:identifier>ISSN 0306-4573</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_71">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;25&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;28&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;33&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:1049-5258">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 1049-5258</dc:identifier>
                <dc:title>University of Tokyo</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kojima</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gu</foaf:surname>
                        <foaf:givenName>SS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reid</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Matsuo</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Iwasawa</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koyejo</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mohamed</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Agarwal</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Belgrave</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cho</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oh</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_125"/>
        <dc:title>Large Language Models are Zero-Shot Reasoners</dc:title>
        <dcterms:abstract>Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding &quot;Let's think step by step&quot; before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large-scale InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001202259104027</dc:coverage>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 35, NEURIPS 2022</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_125">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;319&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;322&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;48&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:1063-6919">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 1063-6919</dc:identifier>
                <dc:title>University of Wisconsin System</dc:title>
                <dc:identifier>DOI 10.1109/CVPR52688.2022.01017</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mehta</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pal</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>V</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ravi</foaf:surname>
                        <foaf:givenName>SN</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>IEEE COMP SOC</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_139"/>
        <link:link rdf:resource="#item_1534"/>
        <dc:title>Deep Unlearning via Randomized Conditionally Independent Hessians</dc:title>
        <dcterms:abstract>Recent legislation has led to interest in machine unlearning, i.e., removing specific training samples from a predictive model as if they never existed in the training dataset. Unlearning may also be required due to corrupted/adversarial data or simply a user's updated privacy requirement. For models which require no training (k-NN), simply deleting the closest original sample can be effective. But this idea is inapplicable to models which learn richer representations. Recent ideas leveraging optimization-based updates scale poorly with the model dimension d, due to inverting the Hessian of the loss function. We use a variant of a new conditional independence coefficient, L-CODEC, to identify a subset of the model parameters with the most semantic overlap on an individual sample level. Our approach completely avoids the need to invert a (possibly) huge matrix. By utilizing a Markov blanket selection, we premise that L-CODEC is also suitable for deep unlearning, as well as other applications in vision. Compared to alternatives, L-CODEC makes approximate unlearning possible in settings that would otherwise be infeasible, including vision models used for face recognition, person re-identification and NLP models that may require unlearning samples identified for exclusion. Code is available at https://github.com/vsingh-group/LCODEC-deep-unlearning</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000870759103048</dc:coverage>
        <bib:pages>10412-10421</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_139">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;19&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;19&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;38&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1534">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1534/Mehta et al. - 2022 - Deep Unlearning via Randomized Conditionally Independent Hessians.pdf"/>
        <dc:title>PubMed Central Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://pmc.ncbi.nlm.nih.gov/articles/PMC10337718/pdf/nihms-1894549.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:26:41</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_154">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0098-5589"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cejas</foaf:surname>
                        <foaf:givenName>OA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Azeem</foaf:surname>
                        <foaf:givenName>MI</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Abualhaija</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Briand</foaf:surname>
                        <foaf:givenName>LC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_155"/>
        <link:link rdf:resource="#item_1537"/>
        <dc:subject>privacy</dc:subject>
        <dc:subject>natural language processing (NLP)</dc:subject>
        <dc:subject>the general data protection regulation (GDPR)</dc:subject>
        <dc:subject>KNOWLEDGE</dc:subject>
        <dc:subject>data processing agreement (DPA)</dc:subject>
        <dc:subject>LEGAL REQUIREMENTS</dc:subject>
        <dc:subject>regulatory compliance</dc:subject>
        <dc:subject>Requirements engineering (RE)</dc:subject>
        <dc:title>NLP-Based Automated Compliance Checking of Data Processing Agreements Against GDPR</dc:title>
        <dcterms:abstract>When the entity processing personal data (the processor) differs from the one collecting personal data (the controller), processing personal data is regulated in Europe by the General Data Protection Regulation (GDPR) through data processing agreements (DPAs). Checking the compliance of DPAs contributes to the compliance verification of software systems as DPAs are an important source of requirements for software development involving the processing of personal data. However, manually checking whether a given DPA complies with GDPR is challenging as it requires significant time and effort for understanding and identifying DPA-relevant compliance requirements in GDPR and then verifying these requirements in the DPA. Legal texts introduce additional complexity due to convoluted language and inherent ambiguity leading to potential misunderstandings. In this paper, we propose an automated solution to check the compliance of a given DPA against GDPR. In close interaction with legal experts, we first built two artifacts: (i) the &quot;shall&quot; requirements extracted from the GDPR provisions relevant to DPA compliance and (ii) a glossary table defining the legal concepts in the requirements. Then, we developed an automated solution that leverages natural language processing (NLP) technologies to check the compliance of a given DPA against these &quot;shall&quot; requirements. Specifically, our approach automatically generates phrasal-level representations for the textual content of the DPA and compares them against predefined representations of the &quot;shall&quot; requirements. By comparing these two representations, the approach not only assesses whether the DPA is GDPR compliant but it further provides recommendations about missing information in the DPA. Over a dataset of 30 actual DPAs, the approach correctly finds 618 out of 750 genuine violations while raising 76 false violations, and further correctly identifies 524 satisfied requirements. The approach has thus an average precision of 89.1%, a recall of 82.4%, and an accuracy of 84.6%. Compared to a baseline that relies on off-the-shelf NLP tools, our approach provides an average accuracy gain of approximate to 20 percentage points. The accuracy of our approach can be improved to approximate to 94% with limited manual verification effort.</dcterms:abstract>
        <dc:date>2023 SEPT 1</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001070689400002</dc:coverage>
        <bib:pages>4282-4303</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0098-5589">
        <prism:volume>49</prism:volume>
        <dc:title>IEEE TRANSACTIONS ON SOFTWARE ENGINEERING</dc:title>
        <dc:identifier>DOI 10.1109/TSE.2023.3288901</dc:identifier>
        <prism:number>9</prism:number>
        <dc:identifier>ISSN 0098-5589</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_155">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;10&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;10&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;115&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1537">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1537/Cejas et al. - 2023 - NLP-Based Automated Compliance Checking of Data Processing Agreements Against GDPR.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=10167495&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:27:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_184">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2304-070X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bozkurt</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_185"/>
        <dcterms:isReferencedBy rdf:resource="#item_349"/>
        <dcterms:isReferencedBy rdf:resource="#item_1073"/>
        <link:link rdf:resource="#item_1544"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>ethics</dc:subject>
        <dc:subject>AI</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>academic integrity</dc:subject>
        <dc:subject>academic writing</dc:subject>
        <dc:subject>AIEd</dc:subject>
        <dc:subject>authorship</dc:subject>
        <dc:subject>chatbots</dc:subject>
        <dc:subject>CHATGPT</dc:subject>
        <dc:subject>cocreating</dc:subject>
        <dc:subject>collaboration</dc:subject>
        <dc:subject>conversational agents</dc:subject>
        <dc:subject>education</dc:subject>
        <dc:subject>educational technology</dc:subject>
        <dc:subject>GenAI</dc:subject>
        <dc:subject>Generative AI</dc:subject>
        <dc:subject>generative pre-trained transformer</dc:subject>
        <dc:subject>GPT</dc:subject>
        <dc:subject>higher education</dc:subject>
        <dc:subject>large language models</dc:subject>
        <dc:subject>learning</dc:subject>
        <dc:subject>LLMs</dc:subject>
        <dc:subject>ownership</dc:subject>
        <dc:subject>teaching</dc:subject>
        <dc:subject>transparency in research</dc:subject>
        <dc:title>GenAI et al.: Cocreation, Authorship, Ownership, Academic Ethics and Integrity in a Time of Generative AI</dc:title>
        <dcterms:abstract>This paper investigates the complex interplay between generative artificial intelligence (AI) and human intellect in academic writing and publishing. It examines the 'organic versus synthetic' paradox, emphasizing the implications of using generative AI tools in educational and academic integrity contexts. The paper critiques the prevalent 'publish or perish' culture in academia, highlighting the need for systemic reevaluation due to generative AI's emerging role in academic writing and reporting. It delves into the legal and ethical challenges of authorship and ownership, especially in relation to copyright laws and AI -generated content. The paper discusses generative AI's diverse roles and advocates for transparent reporting to uphold academic integrity. Additionally, it calls for a broader examination of generative AI tools and stresses the need for new mechanisms to identify generative AI use and ensure adherence to academic integrity and ethics. The implications of generative AI are also explored, suggesting the need for innovative AI -inclusive strategies in academia. The paper concludes by emphasizing the significance of generative AI in various informationprocessing domains, highlighting the urgency to adapt and transform academic practices in an era of rapid generative AI -driven change.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001204361200009</dc:coverage>
        <bib:pages>1-10</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2304-070X">
        <prism:volume>16</prism:volume>
        <dc:title>OPEN PRAXIS</dc:title>
        <dc:identifier>DOI 10.55982/openpraxis.16.1.654</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 2304-070X</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_185">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;11&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;11&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;59&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_349">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;11&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;11&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;59&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_1073">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;11&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;11&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;59&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1544">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1544/Bozkurt - 2024 - GenAI et al. Cocreation, Authorship, Ownership, Academic Ethics and Integrity in a Time of Generati.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openpraxis.org/articles/654/files/65bcd00a4e478.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:31:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_204">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0276-9948"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ezrachi</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stucke</foaf:surname>
                        <foaf:givenName>ME</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_205"/>
        <dcterms:isReferencedBy rdf:resource="#item_905"/>
        <dc:title>ARTIFICIAL INTELLIGENCE &amp; COLLUSION: WHEN COMPUTERS INHIBIT COMPETITION</dc:title>
        <dcterms:abstract>The development of self-learning and independent computers has long captured our imagination. The HAL 9000 computer, in the 1968 film, 2001: A Space Odyssey, for example, assured, &quot;I am putting myself to the fullest possible use, which is all I think that any conscious entity can ever hope to do.&quot; Machine learning raises many challenging legal and ethical questions as to the relationship between man and machine, humans' control or lack of it over machines, and accountability for machine activities.
While these issues have long captivated our interest, few would envision the day when these developments (and the legal and ethical challenges raised by them) would become an antitrust issue. Sophisticated computers are central to the competitiveness of present and future markets. With the accelerating development of AI, they are set to change the competitive landscape and the nature of competitive restraints. As pricing mechanisms shift to computer pricing algorithms, so too will the types of collusion. We are shifting from the world where executives expressly collude in smoke-filled hotel rooms to a world where pricing algorithms continually monitor and adjust to each other's prices and market data.
Our paper addresses these developments and considers the application of competition law to an advanced &quot;computerised trade environment.&quot; After discussing the way in which computerised technology is changing the competitive landscape, we explore four scenarios where AI can foster anticompetitive collusion and the legal and ethical challenges each scenario raises.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000415132900003</dc:coverage>
        <bib:pages>1775-1810</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0276-9948">
        <dc:title>UNIVERSITY OF ILLINOIS LAW REVIEW</dc:title>
        <prism:number>5</prism:number>
        <dc:identifier>ISSN 0276-9948</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_205">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;70&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;75&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;54&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_905">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;70&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;75&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;54&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-5746-3">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-5746-3</dc:identifier>
                <dc:title>Indian Institute of Technology System (IIT System)</dc:title>
                <dc:identifier>DOI 10.1145/3194770.3194776</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Verma</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rubin</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>IEEE</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_225"/>
        <dc:title>Fairness Definitions Explained</dc:title>
        <dcterms:abstract>Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000447014100001</dc:coverage>
        <bib:pages>1-7</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2018 IEEE/ACM INTERNATIONAL WORKSHOP ON SOFTWARE FAIRNESS (FAIRWARE 2018)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_225">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;571&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;643&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;24&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_228">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1364-503X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cath</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_229"/>
        <dcterms:isReferencedBy rdf:resource="#item_909"/>
        <link:link rdf:resource="#item_1545"/>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>ethics</dc:subject>
        <dc:subject>AI</dc:subject>
        <dc:subject>law</dc:subject>
        <dc:subject>culture</dc:subject>
        <dc:subject>EXPLANATION</dc:subject>
        <dc:subject>governance</dc:subject>
        <dc:subject>technology</dc:subject>
        <dc:title>Governing artificial intelligence: ethical, legal and technical opportunities and challenges Introduction</dc:title>
        <dcterms:abstract>This paper is the introduction to the special issue entitled: 'Governing artificial intelligence: ethical, legal and technical opportunities and challenges. Artificial intelligence (AI) increasingly permeates every aspect of our society, from the critical, like urban infrastructure, law enforcement, banking, healthcare and humanitarian aid, to the mundane like dating. AI, including embodied AI in robotics and techniques like machine learning, can improve economic, social welfare and the exercise of human rights. Owing to the proliferation of AI in high-risk areas, the pressure is mounting to design and govern AI to be accountable, fair and transparent. How can this be achieved and through which frameworks? This is one of the central questions addressed in this special issue, in which eight authors present in-depth analyses of the ethical, legal-regulatory and technical challenges posed by developing governance regimes for AI systems. It also gives a brief overview of recent developments in AI governance, how much of the agenda for defining AI regulation, ethical frameworks and technical approaches is set, as well as providing some concrete suggestions to further the debate on AI governance.
This article is part of the theme issue 'Governing artificial intelligence: ethical, legal, and technical opportunities and challenges'.</dcterms:abstract>
        <dc:date>2018 NOV 28</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000447313000001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1364-503X">
        <prism:volume>376</prism:volume>
        <dc:title>PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY A-MATHEMATICAL PHYSICAL AND ENGINEERING SCIENCES</dc:title>
        <dc:identifier>DOI 10.1098/rsta.2018.0080</dc:identifier>
        <prism:number>2133</prism:number>
        <dc:identifier>ISSN 1364-503X</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_229">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;187&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;193&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;41&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_909">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;187&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;193&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;41&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1545">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1545/Cath - 2018 - Governing artificial intelligence ethical, legal and technical opportunities and challenges Introdu.pdf"/>
        <dc:title>Texto Completo</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://europepmc.org/articles/pmc6191666?pdf=render</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:31:30</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_230">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>376</prism:volume>
                <dc:title>PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY A-MATHEMATICAL PHYSICAL AND ENGINEERING SCIENCES</dc:title>
                <dc:identifier>DOI 10.1098/rsta.2018.0089</dc:identifier>
                <prism:number>2133</prism:number>
                <dc:identifier>ISSN 1364-503X</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nemitz</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_231"/>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>rule of law</dc:subject>
        <dc:subject>ethics</dc:subject>
        <dc:subject>law</dc:subject>
        <dc:subject>democracy</dc:subject>
        <dc:subject>digital power</dc:subject>
        <dc:subject>GDPR</dc:subject>
        <dc:title>Constitutional democracy and technology in the age of artificial intelligence</dc:title>
        <dcterms:abstract>Given the foreseeable pervasiveness of artificial intelligence (AI) in modern societies, it is legitimate and necessary to ask the question how this new technology must be shaped to support the maintenance and strengthening of constitutional democracy. This paper first describes the four core elements of today's digital power concentration, which need to be seen in cumulation and which, seen together, are both a threat to democracy and to functioning markets. It then recalls the experience with the lawless Internet and the relationship between technology and the law as it has developed in the Internet economy and the experience with GDPR before it moves on to the key question for AI in democracy, namely which of the challenges of AI can be safely and with good conscience left to ethics, and which challenges of AI need to be addressed by rules which are enforceable and encompass the legitimacy of democratic process, thus laws. The paper closes with a call for a new culture of incorporating the principles of democracy, rule of law and human rights by design in AI and a three-level technological impact assessment for new technologies like AI as a practical way forward for this purpose.
This article is part of a theme issue 'Governing artificial intelligence: ethical, legal, and technical opportunities and challenges'.</dcterms:abstract>
        <dc:date>2018 NOV 28</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000447313000008</dc:coverage>
    </bib:Article>
    <bib:Memo rdf:about="#item_231">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;112&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;115&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;11&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-6012-8">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-6012-8</dc:identifier>
                <dc:title>University of Canterbury</dc:title>
                <dc:identifier>DOI 10.1145/3278721.3278731</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Erdélyi</foaf:surname>
                        <foaf:givenName>OJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Goldsmith</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>ACM</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_239"/>
        <dc:subject>hard/soft law</dc:subject>
        <dc:subject>international governance</dc:subject>
        <dc:subject>international organizations</dc:subject>
        <dc:subject>Transnational legal ordering</dc:subject>
        <dc:title>Regulating Artificial Intelligence Proposal for a Global Solution</dc:title>
        <dcterms:abstract>Given the ubiquity of artificial intelligence (AI) in modern societies, it is clear that individuals, corporations, and countries will be grappling with the legal and ethical issues of its use. As global problems require global solutions, we propose the establishment of an international AI regulatory agency that - drawing on interdisciplinary expertise - could create a unified framework for the regulation of AI technologies and inform the development of AI policies around the world. We urge that such an organization be developed with all deliberate haste, as issues such as cryptocurrencies, personalized political ad hacking, autonomous vehicles and autonomous weaponized agents are already a reality, affecting international trade, politics, and war.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000510018100015</dc:coverage>
        <bib:pages>95-101</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_239">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;50&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;54&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;19&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_242">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>376</prism:volume>
                <dc:title>PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY A-MATHEMATICAL PHYSICAL AND ENGINEERING SCIENCES</dc:title>
                <dc:identifier>DOI 10.1098/rsta.2017.0360</dc:identifier>
                <prism:number>2128</prism:number>
                <dc:identifier>ISSN 1364-503X</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reed</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_243"/>
        <link:link rdf:resource="#item_1550"/>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>law</dc:subject>
        <dc:subject>regulation</dc:subject>
        <dc:subject>transparency</dc:subject>
        <dc:title>How should we regulate artificial intelligence?</dc:title>
        <dcterms:abstract>Using artificial intelligence (AI) technology to replace human decision-making will inevitably create new risks whose consequences are unforeseeable. This naturally leads to calls for regulation, but I argue that it is too early to attempt a general system of AI regulation. Instead, we should work incrementally within the existing legal and regulatory schemes which allocate responsibility, and therefore liability, to persons. Where AI clearly creates risks which current law and regulation cannot deal with adequately, then new regulation will be needed. But in most cases, the current system can work effectively if the producers of AI technology can provide sufficient transparency in explaining how AI decisions are made. Transparency ex post can often be achieved through retrospective analysis of the technology's operations, and will be sufficient if the main goal is to compensate victims of incorrect decisions. Ex ante transparency is more challenging, and can limit the use of some AI technologies such as neural networks. It should only be demanded by regulation where the AI presents risks to fundamental rights, or where society needs reassuring that the technology can safely be used. Masterly inactivity in regulation is likely to achieve a better long-term solution than a rush to regulate in ignorance.
This article is part of a discussion meeting issue 'The growing ubiquity of algorithms in society: implications, impacts and innovations'.</dcterms:abstract>
        <dc:date>2018 SEP 13</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000440870000009</dc:coverage>
    </bib:Article>
    <bib:Memo rdf:about="#item_243">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;34&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;35&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;28&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1550">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1550/Reed - 2018 - How should we regulate artificial intelligence.pdf"/>
        <dc:title>Texto Completo</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://europepmc.org/articles/pmc6107539?pdf=render</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:31:45</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_252">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0041-5650"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Katyal</foaf:surname>
                        <foaf:givenName>SK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_253"/>
        <dc:subject>LAW</dc:subject>
        <dc:subject>RISK</dc:subject>
        <dc:subject>DECISION-MAKING</dc:subject>
        <dc:subject>PROTECTION</dc:subject>
        <dc:subject>ATTRIBUTION</dc:subject>
        <dc:subject>FREQUENCY</dc:subject>
        <dc:subject>IMPLICIT BIAS</dc:subject>
        <dc:subject>INTELLECTUAL PROPERTY</dc:subject>
        <dc:subject>SELF</dc:subject>
        <dc:subject>TRADE SECRETS</dc:subject>
        <dc:title>Private Accountability in the Age of Artificial Intelligence</dc:title>
        <dcterms:abstract>In this Article, I explore the impending conflict between the protection of civil rights and artificial intelligence (AI). While both areas of law have amassed rich and well-developed areas of scholarly work and doctrinal support, a growing body of scholars are interrogating the intersection between them. This Article argues that the issues surrounding algorithmic accountability demonstrate a deeper, more structural tension within a new generation of disputes regarding law and technology. As I argue, the true promise of AI does not lie in the information we reveal to one another, but rather in the questions it raises about the interaction of technology, property, and civil rights.
For this reason, I argue that we are looking in the wrong place if we look only to the state to address issues of algorithmic accountability. Instead, given the state's reluctance to address the issue, we must turn to other ways to ensure more transparency and accountability that stem from private industry, rather than public regulation. The issue of algorithmic bias represents a crucial new world of civil rights concerns, one that is distinct in nature from the ones that preceded it. Since we are in a world where the activities of private corporations, rather than the state, are raising concerns about privacy, due process, and discrimination, we must focus on the role of private corporations in addressing the issue. Towards this end, I discuss a variety of tools to help eliminate the opacity of AI, including codes of conduct, impact statements, and whistleblower protection, which I argue carries the potential to encourage greater endogeneity in civil rights enforcement. Ultimately, by examining the relationship between private industry and civil rights, we can perhaps develop a new generation of forms of accountability in the process.</dcterms:abstract>
        <dc:date>2019 JAN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000456765700002</dc:coverage>
        <bib:pages>54-141</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0041-5650">
        <prism:volume>66</prism:volume>
        <dc:title>UCLA LAW REVIEW</dc:title>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 0041-5650</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_253">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;88&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;94&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;333&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-6201-6">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-6201-6</dc:identifier>
                <dc:identifier>DOI 10.1145/3292500.3332281</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gade</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Geyik</foaf:surname>
                        <foaf:givenName>SC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kenthapadi</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mithal</foaf:surname>
                        <foaf:givenName>V</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Taly</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Comp Machinery</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_255"/>
        <dc:title>Explainable AI in Industry</dc:title>
        <dcterms:abstract>Artificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI are far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability [6]. In addition, model explainability is a prerequisite for building trust and adoption of AI systems in high stakes domains requiring reliability and safety such as healthcare [1] and automated transportation, and critical industrial applications with significant economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling.
As a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale [8, 9, 19]. The challenges for the research community include (i) defining model explainability, (ii) formulating explainability tasks for understanding model behavior and developing solutions for these tasks, and finally (iii) designing measures for evaluating the performance of models in explainability tasks.
In this tutorial, we will present an overview of model interpretability and explainability in AI [4], key regulations/laws, and techniques/tools for providing explainability as part of AI/ML systems [7]. Then, we will focus on the application of explainability techniques in industry, wherein we present practical challenges/guidelines for using explainability techniques effectively and lessons learned from deploying explainable models for several web-scale machine learning and data mining applications. We will present case studies across different companies, spanning application domains such as search and recommendation systems, sales, lending, and fraud detection. Finally, based on our experiences in industry, we will identify open problems and research directions for the data mining/machine learning community.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000485562503051</dc:coverage>
        <bib:pages>3203-3204</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_255">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;83&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;91&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;20&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_256">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0038-3910"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kaminski</foaf:surname>
                        <foaf:givenName>ME</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_257"/>
        <dc:subject>INFORMATION</dc:subject>
        <dc:subject>RULES</dc:subject>
        <dc:subject>PRIVACY</dc:subject>
        <dc:subject>AUTOMATED DECISION-MAKING</dc:subject>
        <dc:subject>DATA PROTECTION</dc:subject>
        <dc:subject>DEFAULT</dc:subject>
        <dc:subject>PENALTY</dc:subject>
        <dc:subject>STANDARDS</dc:subject>
        <dc:subject>SURVEILLANCE</dc:subject>
        <dc:title>BINARY GOVERNANCE: LESSONS FROM THE GDPR'S APPROACH TO ALGORITHMIC ACCOUNTABILITY</dc:title>
        <dcterms:abstract>Algorithms are now used to make significant decisions about individuals, from credit determinations to hiring and firing. But they are largely unregulated under U.S. law. A quickly growing literature has split on how to address algorithmic decision-making, with individual rights and accountability to nonexpert stakeholders and to the public at the crux of the debate. In this Article, I make the case for why both individual rights and public- and stakeholder-facing accountability are not just goods in and of themselves but crucial components of effective governance. Only individual rights can fully address dignitary and justificatory concerns behind calls for regulating algorithmic decision-making. And without some form of public and stakeholder accountability, collaborative public-private approaches to systemic governance of algorithms will fail.
In this Article, I identify three categories of concern behind calls for regulating algorithmic decision-making: dignitary, justificatory, and instrumental. Dignitary concerns lead to proposals that we regulate algorithms to protect human dignity and autonomy; justificatory concerns caution that we must assess the legitimacy of algorithmic reasoning; and instrumental concerns lead to calls for regulation to prevent consequent problems such as error and bias. No one regulatory approach can effectively address all three. I therefore propose a two-pronged approach to algorithmic governance: a system of individual due process rights combined with systemic regulation achieved through collaborative governance (the use of private-public partnerships). Only through this binary approach can we effectively address all three concerns raised by algorithmic decision-making, or decision-making by Artificial Intelligence (&quot;AI&quot;).
The interplay between the two approaches will be complex. Sometimes the two systems will be complementary, and at other times, they will be in tension. The European Union's (&quot;EU's&quot;) General Data Protection Regulation (&quot;GDPR&quot;) is one such binary system. I explore the extensive collaborative governance aspects of the GDPR and how they interact with its individual rights regime. Understanding the GDPR in this way both illuminates its strengths and weaknesses and provides a model for how to construct a better governance regime for accountable algorithmic, or AI, decision-making. It shows, too, that in the absence of public and stakeholder accountability, individual rights can have a significant role to play in establishing the legitimacy of a collaborative regime.</dcterms:abstract>
        <dc:date>2019 SEP</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000501476600004</dc:coverage>
        <bib:pages>1529-1616</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0038-3910">
        <prism:volume>92</prism:volume>
        <dc:title>SOUTHERN CALIFORNIA LAW REVIEW</dc:title>
        <prism:number>6</prism:number>
        <dc:identifier>ISSN 0038-3910</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_257">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;80&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;84&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;230&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_258">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1867-299X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Buiten</foaf:surname>
                        <foaf:givenName>MC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_259"/>
        <link:link rdf:resource="#item_1540"/>
        <dc:title>Towards Intelligent Regulation of Artificial Intelligence</dc:title>
        <dcterms:abstract>Artificial intelligence (AI) is becoming a part of our daily lives at a fast pace, offering myriad benefits for society. At the same time, there is concern about the unpredictability and uncontrollability of AI. In response, legislators and scholars call for more transparency and explainability of AI. This article considers what it would mean to require transparency of AI. It advocates looking beyond the opaque concept of AI, focusing on the concrete risks and biases of its underlying technology: machine-learning algorithms. The article discusses the biases that algorithms may produce through the input data, the testing of the algorithm and the decision model. Any transparency requirement for algorithms should result in explanations of these biases that are both understandable for the prospective recipients, and technically feasible for producers. Before asking how much transparency the law should require from algorithms, we should therefore consider if the explanation that programmers could offer is useful in specific legal contexts.</dcterms:abstract>
        <dc:date>2019 MAR</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000476750200004</dc:coverage>
        <bib:pages>41-59</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1867-299X">
        <prism:volume>10</prism:volume>
        <dc:title>EUROPEAN JOURNAL OF RISK REGULATION</dc:title>
        <dc:identifier>DOI 10.1017/err.2019.8</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 1867-299X</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_259">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;78&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;87&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;89&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1540">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1540/Buiten - 2019 - Towards Intelligent Regulation of Artificial Intelligence.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.cambridge.org/core/services/aop-cambridge-core/content/view/AF1AD1940B70DB88D2B24202EE933F1B/S1867299X19000084a.pdf/div-class-title-towards-intelligent-regulation-of-artificial-intelligence-div.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:27:31</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_262">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0007-1013"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kitto</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Knight</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_263"/>
        <dc:title>Practical ethics for building learning analytics</dc:title>
        <dcterms:abstract>Artificial intelligence and data analysis (AIDA) are increasingly entering the field of education. Within this context, the subfield of learning analytics (LA) has, since its inception, had a strong emphasis upon ethics, with numerous checklists and frameworks proposed to ensure that student privacy is respected and potential harms avoided. Here, we draw attention to some of the assumptions that underlie previous work in ethics for LA, which we frame as three tensions. These assumptions have the potential of leading to both the overcautious underuse of AIDA as administrators seek to avoid risk, or the unbridled misuse of AIDA as practitioners fail to adhere to frameworks that provide them with little guidance upon the problems that they face in building LA for institutional adoption. We use three edge cases to draw attention to these tensions, highlighting places where existing ethical frameworks fail to inform those building LA solutions. We propose a pilot open database that lists edge cases faced by LA system builders as a method for guiding ethicists working in the field towards places where support is needed to inform their practice. This would provide a middle space where technical builders of systems could more deeply interface with those concerned with policy, law and ethics and so work towards building LA that encourages human flourishing across a lifetime of learning. Practitioner Notes What is already known about this topic Applied ethics has a number of well-established theoretical groundings that we can use to frame the actions of ethical agents, including, deontology, consequentialism and virtue ethics. Learning analytics has developed a number of checklists, frameworks and evaluation methodologies for supporting trusted and ethical development, but these are often not adhered to by practitioners. Laws like the General Data Protection Regulation (GDPR) apply to fields like education, but the complexity of this field can make them difficult to apply. What this paper adds Evidence of tensions and gaps in existing ethical frameworks and checklists to support the ethical development and implementation of learning analytics. A set of three edge cases that demonstrate places where existing work on the ethics of AI in education has failed to provide guidance. A &quot;practical ethics&quot; conceptualisation that draws on virtue ethics to support practitioners in building learning analytics systems. Implications for practice and/or policy Those using AIDA in education should collect and share example edge cases to support development of practical ethics in the field. A multiplicity of ethical approaches are likely to be useful in understanding how to develop and implement learning analytics ethically in practical contexts.</dcterms:abstract>
        <dc:date>2019 NOV</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000481106500001</dc:coverage>
        <bib:pages>2855-2870</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0007-1013">
        <prism:volume>50</prism:volume>
        <dc:title>BRITISH JOURNAL OF EDUCATIONAL TECHNOLOGY</dc:title>
        <dc:identifier>DOI 10.1111/bjet.12868</dc:identifier>
        <prism:number>6</prism:number>
        <dc:identifier>ISSN 0007-1013</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_263">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;62&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;69&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;41&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_266">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0951-5666"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roberts</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cowls</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Morley</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Taddeo</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>VC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Floridi</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_267"/>
        <link:link rdf:resource="#item_1548"/>
        <dc:subject>POWER</dc:subject>
        <dc:subject>LAW</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Privacy</dc:subject>
        <dc:subject>GOVERNANCE</dc:subject>
        <dc:subject>Policy</dc:subject>
        <dc:subject>China</dc:subject>
        <dc:subject>Cyber warfare</dc:subject>
        <dc:subject>Digital ethics</dc:subject>
        <dc:subject>Economic growth</dc:subject>
        <dc:subject>Governance</dc:subject>
        <dc:subject>HEALTH-CARE</dc:subject>
        <dc:subject>Innovation</dc:subject>
        <dc:subject>International competition</dc:subject>
        <dc:subject>LANDSCAPE</dc:subject>
        <dc:subject>New Generation Artificial Intelligence Development Plan</dc:subject>
        <dc:subject>RULE</dc:subject>
        <dc:subject>Social governance</dc:subject>
        <dc:title>The Chinese approach to artificial intelligence: an analysis of policy, ethics, and regulation</dc:title>
        <dcterms:abstract>In July 2017, China's State Council released the country's strategy for developing artificial intelligence (AI), entitled 'New Generation Artificial Intelligence Development Plan' ((sic)). This strategy outlined China's aims to become the world leader in AI by 2030, to monetise AI into a trillion-yuan (ca. 150 billion dollars) industry, and to emerge as the driving force in defining ethical norms and standards for AI. Several reports have analysed specific aspects of China's AI policies or have assessed the country's technical capabilities. Instead, in this article, we focus on the socio-political background and policy debates that are shaping China's AI strategy. In particular, we analyse the main strategic areas in which China is investing in AI and the concurrent ethical debates that are delimiting its use. By focusing on the policy backdrop, we seek to provide a more comprehensive and critical understanding of China's AI policy by bringing together debates and analyses of a wide array of policy documents.</dcterms:abstract>
        <dc:date>2021 MAR</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000540941600001</dc:coverage>
        <bib:pages>59-77</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0951-5666">
        <prism:volume>36</prism:volume>
        <dc:title>AI &amp; SOCIETY</dc:title>
        <dc:identifier>DOI 10.1007/s00146-020-00992-2</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 0951-5666</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_267">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;164&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;176&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;165&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1548">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1548/Roberts et al. - 2021 - The Chinese approach to artificial intelligence an analysis of policy, ethics, and regulation.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.1007%2Fs00146-020-00992-2.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:31:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-6936-7">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-6936-7</dc:identifier>
                <dc:title>Harvard University</dc:title>
                <dc:identifier>DOI 10.1145/3351095.3372860</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bietti</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Comp Machinery</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_281"/>
        <dc:subject>AI</dc:subject>
        <dc:subject>Ethics</dc:subject>
        <dc:subject>Moral Philosophy</dc:subject>
        <dc:subject>Regulation</dc:subject>
        <dc:subject>Self-regulation</dc:subject>
        <dc:subject>Technology Ethics</dc:subject>
        <dc:subject>Technology Law</dc:subject>
        <dc:title>From Ethics Washing to Ethics Bashing A View on Tech Ethics from Within Moral Philosophy</dc:title>
        <dcterms:abstract>The word 'ethics' is under siege in technology policy circles. Weaponized in support of deregulation, self-regulation or hands-off governance, &quot;ethics&quot; is increasingly identified with technology companies' self-regulatory efforts and with shallow appearances of ethical behavior. So-called &quot;ethics washing&quot; by tech companies is on the rise, prompting criticism and scrutiny from scholars and the tech community at large. In parallel to the growth of ethics washing, its condemnation has led to a tendency to engage in &quot;ethics bashing.&quot; This consists in the trivialization of ethics and moral philosophy now understood as discrete tools or pre-formed social structures such as ethics boards, self-governance schemes or stakeholder groups.
The misunderstandings underlying ethics bashing are at least threefold: (a) philosophy and &quot;ethics&quot; are seen as a communications strategy and as a form of instrumentalized cover-up or facade for unethical behavior, (b) philosophy is understood in opposition and as alternative to political representation and social organizing and (c) the role and importance of moral philosophy is downplayed and portrayed as mere &quot;ivory tower&quot; intellectualization of complex problems that need to be dealt with in practice.
This paper argues that the rhetoric of ethics and morality should not be reductively instrumentalized, either by the industry in the form of &quot;ethics washing,&quot; or by scholars and policy-makers in the form of &quot;ethics bashing.&quot; Grappling with the role of philosophy and ethics requires moving beyond both tendencies and seeing ethics as a mode of inquiry that facilitates the evaluation of competing tech policy strategies. In other words, we must resist narrow reductivism of moral philosophy as instrumentalized performance and renew our faith in its intrinsic moral value as a mode of knowled-geseeking and inquiry. Far from mandating a self-regulatory scheme or a given governance structure, moral philosophy in fact facilitates the questioning and reconsideration of any given practice, situating it within a complex web of legal, political and economic institutions. Moral philosophy indeed can shed new light on human practices by adding needed perspective, explaining the relationship between technology and other worthy goals, situating technology within the human, the social, the political. It has become urgent to start considering technology ethics also from within and not only from outside of ethics.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000620151400035</dc:coverage>
        <bib:pages>210-219</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>FAT* '20: PROCEEDINGS OF THE 2020 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_281">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;103&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;106&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;27&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_282">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2053-9517"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rességuier</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rodrigues</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_283"/>
        <link:link rdf:resource="#item_1510"/>
        <dc:subject>AI ethics</dc:subject>
        <dc:subject>ethical principles</dc:subject>
        <dc:subject>ethics washing</dc:subject>
        <dc:subject>EU HLEG on AI</dc:subject>
        <dc:subject>law of AI</dc:subject>
        <dc:subject>regulation of AI</dc:subject>
        <dc:title>AI ethics should not remain toothless! A call to bring back the teeth of ethics</dc:title>
        <dcterms:abstract>Ethics has powerful teeth, but these are barely being used in the ethics of AI today - it is no wonder the ethics of AI is then blamed for having no teeth. This article argues that 'ethics' in the current AI ethics field is largely ineffective, trapped in an 'ethical principles' approach and as such particularly prone to manipulation, especially by industry actors. Using ethics as a substitute for law risks its abuse and misuse. This significantly limits what ethics can achieve and is a great loss to the AI field and its impacts on individuals and society. This article discusses these risks and then highlights the teeth of ethics and the essential value they can - and should - bring to AI ethics now.</dcterms:abstract>
        <dc:date>2020 JUL</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000619925600001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2053-9517">
        <prism:volume>7</prism:volume>
        <dc:title>BIG DATA &amp; SOCIETY</dc:title>
        <dc:identifier>DOI 10.1177/2053951720942541</dc:identifier>
        <prism:number>2</prism:number>
        <dc:identifier>ISSN 2053-9517</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_283">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;88&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;92&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;20&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1510">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1510/Rességuier e Rodrigues - 2020 - AI ethics should not remain toothless! A call to bring back the teeth of ethics.pdf"/>
        <dc:title>SAGE PDF Full Text</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://journals.sagepub.com/doi/pdf/10.1177/2053951720942541</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 21:58:51</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_292">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0267-3649"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wachter</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mittelstadt</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Russell</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_293"/>
        <dcterms:isReferencedBy rdf:resource="#item_1105"/>
        <link:link rdf:resource="#item_1549"/>
        <dc:subject>Machine learning</dc:subject>
        <dc:subject>BIAS</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Law</dc:subject>
        <dc:subject>Algorithm</dc:subject>
        <dc:subject>BIG DATA</dc:subject>
        <dc:subject>ETHICS</dc:subject>
        <dc:subject>Bias</dc:subject>
        <dc:subject>Demographic</dc:subject>
        <dc:subject>Discrimination</dc:subject>
        <dc:subject>DISCRIMINATION</dc:subject>
        <dc:subject>European union</dc:subject>
        <dc:subject>Fairness</dc:subject>
        <dc:subject>IMPACT</dc:subject>
        <dc:subject>Non-discrimination</dc:subject>
        <dc:subject>parity</dc:subject>
        <dc:title>Why fairness cannot be automated: Bridging the gap between EU non-discrimination law and AI</dc:title>
        <dcterms:abstract>In recent years a substantial literature has emerged concerning bias, discrimination, and fairness in artificial intelligence (AI) and machine learning. Connecting this work to existing legal non-discrimination frameworks is essential to create tools and methods that are practically useful across divergent legal regimes. While much work has been undertaken from an American legal perspective, comparatively little has mapped the effects and requirements of EU law. This Article addresses this critical gap between legal, technical, and organisational notions of algorithmic fairness. Through analysis of EU non-discrimination law and jurisprudence of the European Court of Justice (ECJ) and national courts, we identify a critical incompatibility between European notions of discrimination and existing work on algorithmic and automated fairness. A clear gap exists between statistical measures of fairness as embedded in myriad fairness toolkits and governance mechanisms and the context-sensitive, often intuitive and ambiguous discrimination metrics and evidential requirements used by the ECJ; we refer to this approach as &quot;contextual equality.&quot; This Article makes three contributions. First, we review the evidential requirements to bring a claim under EU non-discrimination law. Due to the disparate nature of algorithmic and human discrimination, the EU's current requirements are too contextual, reliant on intuition, and open to judicial interpretation to be automated. Many of the concepts fundamental to bringing a claim, such as the composition of the disadvantaged and advantaged group, the severity and type of harm suffered, and requirements for the relevance and admissibility of evidence, require normative or political choices to be made by the judiciary on a caseby-case basis. We show that automating fairness or non-discrimination in Europe may be impossible because the law, by design, does not provide a static or homogenous framework suited to testing for discrimination in AI systems. Second, we show how the legal protection offered by non-discrimination law is challenged when AI, not humans, discriminate. Humans discriminate due to negative attitudes (e.g. stereotypes, prejudice) and unintentional biases (e.g. organisational practices or internalised stereotypes) which can act as a signal to victims that discrimination has occurred. Equivalent signalling mechanisms and agency do not exist in algorithmic systems. Compared to traditional forms of discrimination, automated discrimination is more abstract and unintuitive, subtle, intangible, and difficult to detect. The increasing use of algorithms disrupts traditional legal remedies and procedures for detection, investigation, prevention, and correction of discrimination which have predominantly relied upon intuition. Consistent assessment procedures that define a common standard for statistical evidence to detect and assess prima facie automated discrimination are urgently needed to support judges, regulators, system controllers and developers, and claimants. Finally, we examine how existing work on fairness in machine learning lines up with procedures for assessing cases under EU non-discrimination law. A 'gold standard' for assessment of prima facie discrimination has been advanced by the European Court of Justice but not yet translated into standard assessment procedures for automated discrimination. We propose 'conditional demographic disparity' (CDD) as a standard baseline statistical measurement that aligns with the Court's 'gold standard'.
Establishing a standard set of statistical evidence for automated discrimination cases can help ensure consistent procedures for assessment, but not judicial interpretation, of cases involving AI and automated systems. Through this proposal for procedural regularity in the identification and assessment of automated discrimination, we clarify how to build considerations of fairness into automated systems as far as possible while still respecting and enabling the contextual approach to judicial interpretation practiced under EU non-discrimination law.</dcterms:abstract>
        <dc:date>2021 JUL</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000685463100019</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0267-3649">
        <prism:volume>41</prism:volume>
        <dc:title>COMPUTER LAW &amp; SECURITY REVIEW</dc:title>
        <dc:identifier>DOI 10.1016/j.clsr.2021.105567</dc:identifier>
        <dc:identifier>ISSN 0267-3649</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_293">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;130&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;133&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;126&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_1105">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;130&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;133&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;126&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1549">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1549/Wachter et al. - 2021 - Why fairness cannot be automated Bridging the gap between EU non-discrimination law and AI.pdf"/>
        <dc:title>Versão Submetida</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2005.05906</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:31:41</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_306">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1076-9757"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ras</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>van Gerven</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Doran</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_307"/>
        <dc:subject>PREDICTION</dc:subject>
        <dc:subject>CLASSIFICATION</dc:subject>
        <dc:subject>EXPLANATIONS</dc:subject>
        <dc:subject>MODELS</dc:subject>
        <dc:subject>DECISIONS</dc:subject>
        <dc:subject>BLACK-BOX</dc:subject>
        <dc:subject>INTERPRETABILITY</dc:subject>
        <dc:title>Explainable Deep Learning: A Field Guide for the Uninitiated</dc:title>
        <dcterms:abstract>Deep neural networks (DNNs) are an indispensable machine learning tool despite the difficulty of diagnosing what aspects of a model's input drive its decisions. In countless real-world domains, from legislation and law enforcement to healthcare, such diagnosis is essential to ensure that DNN decisions are driven by aspects appropriate in the context of its use. The development of methods and studies enabling the explanation of a DNN's decisions has thus blossomed into an active and broad area of research. The field's complexity is exacerbated by competing definitions of what it means &quot;to explain&quot; the actions of a DNN and to evaluate an approach's &quot;ability to explain&quot;. This article offers a field guide to explore the space of explainable deep learning for those in the AI/ML field who are uninitiated. The field guide: i) Introduces three simple dimensions defining the space of foundational methods that contribute to explainable deep learning, ii) discusses the evaluations for model explanations, iii) places explainability in the context of other related deep learning research areas, and iv) discusses user-oriented explanation design and future directions. We hope the guide is seen as a starting point for those embarking on this research field.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000747953100001</dc:coverage>
        <bib:pages>329-396</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1076-9757">
        <prism:volume>73</prism:volume>
        <dc:title>JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH</dc:title>
        <dc:identifier>ISSN 1076-9757</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_307">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;121&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;125&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;237&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-7252-7">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-7252-7</dc:identifier>
                <dc:title>European University Viadrina Frankfurt Oder</dc:title>
                <dc:identifier>DOI 10.1145/3593013.3594067</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hacker</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Engel</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mauer</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>ASSOC COMPUTING MACHINERY</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_331"/>
        <dcterms:isReferencedBy rdf:resource="#item_743"/>
        <link:link rdf:resource="#item_1551"/>
        <dc:subject>CHALLENGES</dc:subject>
        <dc:subject>ARTIFICIAL-INTELLIGENCE</dc:subject>
        <dc:subject>MARKET</dc:subject>
        <dc:subject>OPPORTUNITIES</dc:subject>
        <dc:title>Regulating ChatGPT and other Large Generative AI Models</dc:title>
        <dcterms:abstract>Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001062819300096</dc:coverage>
        <bib:pages>1112-1123</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 6TH ACM CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, FACCT 2023</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_331">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;91&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;92&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;120&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_743">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;91&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;92&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;120&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1551">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1551/Hacker et al. - 2023 - Regulating ChatGPT and other Large Generative AI Models.pdf"/>
        <dc:title>Versão Submetida</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2302.02337</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:31:51</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_332">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1566-2535"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Díaz-Rodríguez</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Del Ser</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Coeckelbergh</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>de Prado</foaf:surname>
                        <foaf:givenName>ML</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Herrera-Viedma</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Herrera</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_333"/>
        <link:link rdf:resource="#item_1542"/>
        <dc:subject>FRAMEWORK</dc:subject>
        <dc:subject>BIG DATA</dc:subject>
        <dc:subject>PRIVACY</dc:subject>
        <dc:subject>AI ethics</dc:subject>
        <dc:subject>AI regulation</dc:subject>
        <dc:subject>METRICS</dc:subject>
        <dc:subject>QUALITY</dc:subject>
        <dc:subject>Regulatory sandbox</dc:subject>
        <dc:subject>Responsible AI systems</dc:subject>
        <dc:subject>Trustworthy AI</dc:subject>
        <dc:title>Connecting the dots in trustworthy Artificial Intelligence: From AI principles, ethics, and key requirements to responsible AI systems and regulation</dc:title>
        <dcterms:abstract>Trustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over three main pillars that should be met throughout the system's entire life cycle: it should be (1) lawful, (2) ethical, and (3) robust, both from a technical and a social perspective. However, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system's life cycle, and considers previous aspects from different lenses. A more holistic vision contemplates four essential axes: the global principles for ethical use and development of AI-based systems, a philosophical take on AI ethics, a risk-based approach to AI regulation, and the mentioned pillars and requirements. The seven requirements (human agency and oversight; robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmental wellbeing; and accountability) are analyzed from a triple perspective: What each requirement for trustworthy AI is, Why it is needed, and How each requirement can be implemented in practice. On the other hand, a practical approach to implement trustworthy AI systems allows defining the concept of responsibility of AI-based systems facing the law, through a given auditing process. Therefore, a responsible AI system is the resulting notion we introduce in this work, and a concept of utmost necessity that can be realized through auditing processes, subject to the challenges posed by the use of regulatory sandboxes. Our multidisciplinary vision of trustworthy AI culminates in a debate on the diverging views published lately about the future of AI. Our reflections in this matter conclude that regulation is a key for reaching a consensus among these views, and that trustworthy and responsible AI systems will be crucial for the present and future of our society.</dcterms:abstract>
        <dc:date>2023 NOV</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001040646900001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1566-2535">
        <prism:volume>99</prism:volume>
        <dc:title>INFORMATION FUSION</dc:title>
        <dc:identifier>DOI 10.1016/j.inffus.2023.101896</dc:identifier>
        <dc:identifier>ISSN 1566-2535</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_333">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;75&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;75&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;149&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1542">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1542/Díaz-Rodríguez et al. - 2023 - Connecting the dots in trustworthy Artificial Intelligence From AI principles, ethics, and key requ.pdf"/>
        <dc:title>Texto Completo</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://digibug.ugr.es/bitstream/10481/84557/1/1-s2.0-S1566253523002129-main.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:31:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_384">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2652-4074"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rogers</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bell</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_385"/>
        <link:link rdf:resource="#item_1539"/>
        <dc:subject>SERVICES</dc:subject>
        <dc:subject>Artificial Intelligence</dc:subject>
        <dc:subject>AI</dc:subject>
        <dc:subject>FUTURE</dc:subject>
        <dc:subject>TECHNOLOGY</dc:subject>
        <dc:subject>DEFINING ISSUES TEST</dc:subject>
        <dc:subject>FIRMS</dc:subject>
        <dc:subject>Lawyers</dc:subject>
        <dc:subject>legal practice</dc:subject>
        <dc:subject>LEGAL PROFESSION</dc:subject>
        <dc:subject>professional ethics</dc:subject>
        <dc:subject>VALUES</dc:subject>
        <dc:subject>WORK</dc:subject>
        <dc:title>The Ethical AI Lawyer: What is Required of Lawyers When They Use Automated Systems?</dc:title>
        <dcterms:abstract>This article focuses on individual lawyers' responsible use of artificial intelligence (AI) in their practice. More specifically, it examines the ways in which a lawyer's ethical capabilities and motivations are tested by the rapid growth of automated systems, both to identify the ethical risks posed by AI tools in legal services, and to uncover what is required of lawyers when they use this technology. To do so, we use psychologist James Rest's Four-component Model of Morality (FCM), which represents the necessary elements for lawyers to engage in professional conduct when utilising AL We examine issues associated with automation that most seriously challenge each component in context, as well as the skills and resolve lawyers need to adhere to their ethical duties. Importantly, this approach is grounded in social psychology. That is, by looking at human 'thinking and doing' (i.e., lawyers' motivations and capacity when using AI), this offers a different, complementary perspective to the typical, legislative approach in which the law is analysed for regulatory gaps.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000891994400006</dc:coverage>
        <bib:pages>80-99</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2652-4074">
        <prism:volume>1</prism:volume>
        <dc:title>LAW TECHNOLOGY AND HUMANS</dc:title>
        <dc:identifier>DOI 10.5204/lthj.v1i0.1324</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 2652-4074</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_385">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;5&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;5&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;126&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1539">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1539/Rogers e Bell - 2019 - The Ethical AI Lawyer What is Required of Lawyers When They Use Automated Systems.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://lthj.qut.edu.au/article/download/1324/843</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:27:29</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_386">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2284-4503"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Simoncini</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_387"/>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>GDPR</dc:subject>
        <dc:subject>algorithmic state</dc:subject>
        <dc:subject>constitutional law</dc:subject>
        <dc:subject>cybernetics</dc:subject>
        <dc:title>THE UNCONSTITUTIONAL ALGORITHM: ARTIFICIAL INTELLIGENCE AND THE FUTURE OF LIBERTIES</dc:title>
        <dcterms:abstract>Today a growing number of decisions affecting human liberties are taken by algorithms. This evidence raises a vast number of questions concerning the transparency of such tools, the legal and ethical framework for algorithmic decision-making and the societal and cognitive impacts of such algorithmic automation. Starting from Wiener's fascinating invention of &quot;cybernetics&quot;, which demostrated the original link between this new science and the very idea of &quot;public government&quot;, this paper maps the inherent tension between AI and law. It focuses firstly on the impact of &quot;cybernetic revolution&quot; on constitutional law, considering the technological switch from &quot;means&quot; to &quot;subject&quot; and then analyses some cases involving the use of predictive algorithms in the fields of criminal and administrative law. The paper then examines and criticizes the standards set by GDPR to provide effective protection for fundamental liberties. It concludes by calling for a new doctrine of &quot;precautionary constitutionalism&quot; through which protection of fundamental rights and the rule of law should be granted within designing new technologies.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>Italian</z:language>
        <dc:coverage>WOS:000462097000005</dc:coverage>
        <bib:pages>63-89</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2284-4503">
        <dc:title>BIOLAW JOURNAL-RIVISTA DI BIODIRITTO</dc:title>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 2284-4503</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_387">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;5&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;6&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;60&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_396">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1973-2937"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koulu</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_397"/>
        <link:link rdf:resource="#item_1536"/>
        <dc:subject>legal theory</dc:subject>
        <dc:subject>AI ethics</dc:subject>
        <dc:subject>automation</dc:subject>
        <dc:subject>algorithmic governance</dc:subject>
        <dc:subject>EU law</dc:subject>
        <dc:subject>human control</dc:subject>
        <dc:subject>oversight</dc:subject>
        <dc:title>HUMAN CONTROL OVER AUTOMATION: EU POLICY AND AI ETHICS</dc:title>
        <dcterms:abstract>In this article I problematize the use of algorithmic decision-making (ADM) applications to automate legal decision-making processes from the perspective of the European Union (EU) policy on trustworthy artificial intelligence (AI). Lately, the use of ADM systems across various fields, ranging from public to private, from criminal justice to credit scoring, has given rise to concerns about the negative consequences that data-driven technologies have in reinforcing and reinterpreting existing societal biases. This development has led to growing demand for ethical AI, often perceived to require human control over automation. By engaging in discussions of human-computer interaction and in post-structural policy analysis, I examine EU policy proposals to address the problematizations of AI through human oversight. I argue that the relevant policy documents do not reflect the results of earlier research which have undeniably demonstrated the shortcomings of human control over automation, which in turn leads to the reproduction of the harmful dichotomy of human versus machine in EU policy. Despite its shortcomings, the emphasis on human oversight reflects broader fears surrounding loss of control, framed as ethical concerns around digital technologies. Critical examination of these fears reveals an inherent connection between human agency and the legitimacy of legal decision-making that socio-legal scholarship needs to address.</dcterms:abstract>
        <dc:date>2020 SPR</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000567158400002</dc:coverage>
        <bib:pages>9-46</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1973-2937">
        <prism:volume>12</prism:volume>
        <dc:title>EUROPEAN JOURNAL OF LEGAL STUDIES</dc:title>
        <dc:identifier>DOI 10.2924/EJLS.2019.019</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 1973-2937</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_397">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;23&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;24&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;63&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1536">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1536/Koulu - 2020 - HUMAN CONTROL OVER AUTOMATION EU POLICY AND AI ETHICS.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://cadmus.eui.eu/bitstream/1814/66992/1/2.-EJLS-121-Koulu.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:26:56</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_412">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0924-6495"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mökander</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Floridi</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_413"/>
        <link:link rdf:resource="#item_1543"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Governance</dc:subject>
        <dc:subject>Ethics</dc:subject>
        <dc:subject>Auditing</dc:subject>
        <dc:subject>Best practice</dc:subject>
        <dc:subject>Process</dc:subject>
        <dc:title>Ethics-Based Auditing to Develop Trustworthy AI</dc:title>
        <dcterms:abstract>A series of recent developments points towards auditing as a promising mechanism to bridge the gap between principles and practice in AI ethics. Building on ongoing discussions concerning ethics-based auditing, we offer three contributions. First, we argue that ethics-based auditing can improve the quality of decision making, increase user satisfaction, unlock growth potential, enable law-making, and relieve human suffering. Second, we highlight current best practices to support the design and implementation of ethics-based auditing: To be feasible and effective, ethics-based auditing should take the form of a continuous and constructive process, approach ethical alignment from a system perspective, and be aligned with public policies and incentives for ethically desirable behaviour. Third, we identify and discuss the constraints associated with ethics-based auditing. Only by understanding and accounting for these constraints can ethics-based auditing facilitate ethical alignment of AI, while enabling society to reap the full economic and social benefits of automation.</dcterms:abstract>
        <dc:date>2021 JUN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000619696300001</dc:coverage>
        <bib:pages>323-327</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0924-6495">
        <prism:volume>31</prism:volume>
        <dc:title>MINDS AND MACHINES</dc:title>
        <dc:identifier>DOI 10.1007/s11023-021-09557-8</dc:identifier>
        <prism:number>2</prism:number>
        <dc:identifier>ISSN 0924-6495</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_413">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;45&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;46&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;4&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1543">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1543/Mökander e Floridi - 2021 - Ethics-Based Auditing to Develop Trustworthy AI.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.1007%2Fs11023-021-09557-8.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:31:25</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-8095-9">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-8095-9</dc:identifier>
                <dc:title>University System of Georgia</dc:title>
                <dc:identifier>DOI 10.1145/3411763.3441342</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ehsan</foaf:surname>
                        <foaf:givenName>U</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wintersberger</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liao</foaf:surname>
                        <foaf:givenName>QV</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mara</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Streit</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wachter</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Riener</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Riedl</foaf:surname>
                        <foaf:givenName>MO</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>ACM</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_415"/>
        <link:link rdf:resource="#item_1538"/>
        <dc:subject>Artificial Intelligence</dc:subject>
        <dc:subject>Algorithmic Fairness</dc:subject>
        <dc:subject>Critical Technical Practice</dc:subject>
        <dc:subject>Explainable Artificial Intelligence</dc:subject>
        <dc:subject>Human-centered Computing</dc:subject>
        <dc:subject>Interpretability</dc:subject>
        <dc:subject>Interpretable Machine Learning</dc:subject>
        <dc:subject>Trust in Automation</dc:subject>
        <dc:title>Operationalizing Human-Centered Perspectives in Explainable AI</dc:title>
        <dcterms:abstract>The realm of Artificial Intelligence (AI)'s impact on our lives is far reaching - with AI systems proliferating high-stakes domains such as healthcare, finance, mobility, law, etc., these systems must be able to explain their decision to diverse end-users comprehensibly. Yet the discourse of Explainable AI (XAI) has been predominantly focused on algorithm-centered approaches, suffering from gaps in meeting user needs and exacerbating issues of algorithmic opacity. To address these issues, researchers have called for human-centered approaches to XAI. There is a need to chart the domain and shape the discourse of XAI with reflective discussions from diverse stakeholders. The goal of this workshop is to examine how human-centered perspectives in XAI can be operationalized at the conceptual, methodological, and technical levels. Encouraging holistic (historical, sociological, and technical) approaches, we put an emphasis on &quot;operationalizing&quot;, aiming to produce actionable frameworks, transferable evaluation methods, concrete design guidelines, and articulate a coordinated research agenda for XAI.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000759178500033</dc:coverage>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>EXTENDED ABSTRACTS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'21)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_415">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;45&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;50&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;30&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1538">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1538/Ehsan et al. - 2021 - Operationalizing Human-Centered Perspectives in Explainable AI.pdf"/>
        <dc:title>Texto Completo</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ora.ox.ac.uk/objects/uuid:93f6c690-4a72-4cca-9b7c-784a55fa45fa/files/swh246t80b</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:27:16</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:1868-4238">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>614</prism:volume>
                <dc:identifier>ISBN 1868-4238</dc:identifier>
                <dc:identifier>DOI 10.1007/978-3-030-80847-1_1</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wong</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>MercierLaurent</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kayalica</foaf:surname>
                        <foaf:givenName>MO</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Owoc</foaf:surname>
                        <foaf:givenName>ML</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_425"/>
        <link:link rdf:resource="#item_1535"/>
        <dc:subject>Law</dc:subject>
        <dc:subject>Privacy</dc:subject>
        <dc:subject>AI</dc:subject>
        <dc:subject>Data protection</dc:subject>
        <dc:subject>Ethics</dc:subject>
        <dc:subject>Regulation</dc:subject>
        <dc:subject>Explainability</dc:subject>
        <dc:subject>Automation</dc:subject>
        <dc:subject>Employment</dc:subject>
        <dc:subject>Job transition</dc:subject>
        <dc:subject>Legal personhood</dc:subject>
        <dc:subject>Liability</dc:subject>
        <dc:subject>Robots</dc:subject>
        <dc:subject>Transparency</dc:subject>
        <dc:title>Ethics and Regulation of Artificial Intelligence</dc:title>
        <dcterms:abstract>Over the last few years, the world has deliberated and developed numerous ethical principles and frameworks. It is the general opinion that the time has arrived to move from principles and to operationalize on the ethical practice of AI. It is now recognized that principles and standards can play a universal harmonizing role for the development of AI-related legal norms across the globe. However, how do we translate and embrace these articulated values, principles and actions to guide Nation States around the world to formulate their regulatory systems, policies or other legal instruments regarding AI? Our regulatory systems have attempted to keep abreast of new technologies by recalibrating and adapting our regulatory frameworks to provide for new opportunities and risks, to confer rights and duties, safety and liability frameworks, and to ensure legal certainty for businesses. These past adaptations have been reactive and sometimes piecemeal, often with artificial delineation on rights and responsibilities and with unintended flow-on consequences. Previously, technologies have been deployed more like tools, but as autonomy and self-learning capabilities increase, robots and intelligent AI systems will feel less and less like machines and tools. There is now a significant difference, because machine learning AI systems have the ability 'to learn', adapt their performances and 'make decisions' from data and `life experiences'. This paper presented at the International Joint Conference on Artificial Intelligence - Pacific Rim International Conference on Artificial Intelligence in 2021 provides brief insights on some selected topical developments in ethical principles and frameworks, our regulatory systems and the current debates on some of the risks and challenges from the use and actions of AI, autonomous and intelligent systems [1].</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000777635100001</dc:coverage>
        <bib:pages>1-18</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>ARTIFICIAL INTELLIGENCE FOR KNOWLEDGE MANAGEMENT, AI4KM 2021</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_425">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;6&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;6&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;43&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1535">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1535/Wong - 2021 - Ethics and Regulation of Artificial Intelligence.pdf"/>
        <dc:title>Versão Submetida</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://inria.hal.science/hal-04041349/file/518231_1_En_1_Chapter.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:26:46</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-9011-8">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-9011-8</dc:identifier>
                <dc:title>University of Aegean</dc:title>
                <dc:identifier>DOI 10.1145/3494193.3494195</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mitrou</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Janssen</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Loukis</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Loukis</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Macadar</foaf:surname>
                        <foaf:givenName>MA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nielsen</foaf:surname>
                        <foaf:givenName>MM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_429"/>
        <link:link rdf:resource="#item_1546"/>
        <dc:subject>AI</dc:subject>
        <dc:subject>BIG DATA</dc:subject>
        <dc:subject>accountability</dc:subject>
        <dc:subject>CHALLENGES</dc:subject>
        <dc:subject>ALGORITHMS</dc:subject>
        <dc:subject>ARTIFICIAL-INTELLIGENCE</dc:subject>
        <dc:subject>decision-making</dc:subject>
        <dc:subject>discretion</dc:subject>
        <dc:title>Human Control and Discretion in AI-driven Decision-making in Government</dc:title>
        <dcterms:abstract>Traditionally public decision-makers have been given discretion in many of the decisions they have to make in how to comply with legislation and policies. In this way, the context and specific circumstances can be taken into account when making decisions. This enables more acceptable solutions, but at the same time, discretion might result in treating individuals differently. With the advance of AI-based decisions, the role of the decision-makers is changing. The automation might result in fully automated decisions, humans-in-the-loop or AI might only be used as recommender systems in which humans have the discretion to deviate from the suggested decision. The predictability of and the accountability of the decisions might vary in these circumstances, although humans always remain accountable. Hence, there is a need for human-control and the decision-makers should be given sufficient authority to control the system and deal with undesired outcomes. In this direction this paper analyzes the degree of discretion and human control needed in AI-driven decision-making in government. Our analysis is based on the legal requirements set/posed to the administration, by the extensive legal frameworks that have been created for its operation, concerning the rule of law, the fairness - non-discrimination, the justifiability and accountability, and the certainty/predictability.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000933151800002</dc:coverage>
        <bib:pages>10-16</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>14TH INTERNATIONAL CONFERENCE ON THEORY AND PRACTICE OF ELECTRONIC GOVERNANCE (ICEGOV 2021)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_429">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;4&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;4&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;41&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1546">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1546/Mitrou et al. - 2021 - Human Control and Discretion in AI-driven Decision-making in Government.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://repository.tudelft.nl/file/File_5ea4ac2a-76ee-497c-8138-eaf0ed24147e</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:31:34</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-9156-6">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-9156-6</dc:identifier>
                <dc:title>University System of Georgia</dc:title>
                <dc:identifier>DOI 10.1145/3491101.3503727</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ehsan</foaf:surname>
                        <foaf:givenName>U</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wintersberger</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liao</foaf:surname>
                        <foaf:givenName>QV</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Watkins</foaf:surname>
                        <foaf:givenName>EA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Manger</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Daumé</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Riener</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Riedl</foaf:surname>
                        <foaf:givenName>MO</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>ACM</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_437"/>
        <dc:subject>Artificial Intelligence</dc:subject>
        <dc:subject>Algorithmic Fairness</dc:subject>
        <dc:subject>Explainable Artificial Intelligence</dc:subject>
        <dc:subject>Interpretability</dc:subject>
        <dc:subject>Interpretable Machine Learning</dc:subject>
        <dc:subject>Trust in Automation</dc:subject>
        <dc:subject>Responsible AI</dc:subject>
        <dc:title>Human-Centered Explainable AI (HCXAI): Beyond Opening the Black-Box of AI</dc:title>
        <dcterms:abstract>Explainability of AI systems is crucial to hold them accountable because they are increasingly becoming consequential in our lives by powering high-stakes decisions in domains like healthcare and law. When it comes to Explainable AI (XAI), understanding who interacts with the black-box of AI is just as important as &quot;opening&quot; it, if not more. Yet the discourse of XAI has been predominantly centered around the black-box, suffering from deficiencies in meeting user needs and exacerbating issues of algorithmic opacity. To address these issues, researchers have called for human-centered approaches to XAI. In this second CHI workshop on Human-centered XAI (HCXAI), we build on the success of the first installment from CHI 2021 to expand the conversation around XAI. We chart the domain and shape the HCXAI discourse with reflective discussions from diverse stakeholders. The goal of the second installment is to go beyond the black box and examine how human-centered perspectives in XAI can be operationalized at the conceptual, methodological, and technical levels. Encouraging holistic (historical, sociological, and technical) approaches, we put an emphasis on &quot;operationalizing&quot;, aiming to produce actionable frameworks, transferable evaluation methods, concrete design guidelines, and articulate a coordinated research agenda for XAI.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001118038100063</dc:coverage>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>EXTENDED ABSTRACTS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2022</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_437">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;39&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;42&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;29&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_458">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1868-7865"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Parycek</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schmid</foaf:surname>
                        <foaf:givenName>V</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Novak</foaf:surname>
                        <foaf:givenName>AS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_459"/>
        <link:link rdf:resource="#item_1541"/>
        <dc:subject>LAW</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Law</dc:subject>
        <dc:subject>AI</dc:subject>
        <dc:subject>Bias</dc:subject>
        <dc:subject>SYSTEMS</dc:subject>
        <dc:subject>GOVERNMENT</dc:subject>
        <dc:subject>Automation</dc:subject>
        <dc:subject>Transparency</dc:subject>
        <dc:subject>Digital Government</dc:subject>
        <dc:subject>Digital law</dc:subject>
        <dc:subject>E-Government</dc:subject>
        <dc:subject>Legal Tech</dc:subject>
        <dc:title>Artificial Intelligence (AI) and Automation in Administrative Procedures: Potentials, Limitations, and Framework Conditions</dc:title>
        <dcterms:abstract>Integrating artificial intelligence (AI) systems into administrative procedures can revolutionize the way processes are conducted and fundamentally change established forms of action and organization in administrative law. However, implementing AI in administrative procedures requires a comprehensive evaluation of the capabilities and limitations of different systems, including considerations of transparency and data availability. Data are a crucial factor in the operation of AI systems and the validity of their predictions. It is essential to ensure that the data used to train AI algorithms are extensive, representative, and free of bias. Transparency is also an important aspect establishing trust and reliability in AI systems, particularly regarding the potential for transparent representation in rule-based and machine-learning AI systems. This paper examines the potential and challenges that arise from integrating AI into administrative procedures. In addition, the paper offers a nuanced perspective on current developments in artificial intelligence and provides a conceptual framework for its potential applications in administrative procedures. Beyond this, the paper highlights essential framework conditions that require continuous monitoring to ensure optimal results in practice.</dcterms:abstract>
        <dc:date>2024 JUN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001015490100001</dc:coverage>
        <bib:pages>8390-8415</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1868-7865">
        <prism:volume>15</prism:volume>
        <dc:title>JOURNAL OF THE KNOWLEDGE ECONOMY</dc:title>
        <dc:identifier>DOI 10.1007/s13132-023-01433-3</dc:identifier>
        <prism:number>2</prism:number>
        <dc:identifier>ISSN 1868-7865</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_459">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;7&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;7&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;78&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1541">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1541/Parycek et al. - 2024 - Artificial Intelligence (AI) and Automation in Administrative Procedures Potentials, Limitations, a.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.1007%2Fs13132-023-01433-3.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:27:34</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
</rdf:RDF>
