<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/"
 xmlns:link="http://purl.org/rss/1.0/modules/link/"
 xmlns:vcard="http://nwalsh.com/rdf/vCard#">
    <bib:Article rdf:about="#item_492">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1996-1073"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wei</foaf:surname>
                        <foaf:givenName>DQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>DC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dong</foaf:surname>
                        <foaf:givenName>ZY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>HS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>YL</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1441"/>
        <dcterms:isReferencedBy rdf:resource="#item_493"/>
        <dc:subject>natural language processing (NLP)</dc:subject>
        <dc:subject>NETWORKS</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>DIAGNOSIS</dc:subject>
        <dc:subject>CONDITION-BASED MAINTENANCE</dc:subject>
        <dc:subject>long short-term memory (LSTM)</dc:subject>
        <dc:subject>malfunction inspection report</dc:subject>
        <dc:subject>recurrent neural network (RNN)</dc:subject>
        <dc:subject>unstructured data</dc:subject>
        <dc:title>Research on Unstructured Text Data Mining and Fault Classification Based on RNN-LSTM with Malfunction Inspection Report</dc:title>
        <dcterms:abstract>This paper documents the condition-based maintenance (CBM) of power transformers, the analysis of which relies on two basic data groups: structured (e.g., numeric and categorical) and unstructured (e.g., natural language text narratives) which accounts for 80% of data required. However, unstructured data comprised of malfunction inspection reports, as recorded by operation and maintenance of the power grid, constitutes an abundant untapped source of power insights. This paper proposes a method for malfunction inspection report processing by deep learning, which combines the text data mining-oriented recurrent neural networks (RNN) with long short-term memory (LSTM). In this paper, the effectiveness of the RNN-LSTM network for modeling inspection data is established with a straightforward training strategy in which we replicate targets at each sequence step. Then, the corresponding fault labels are given in datasets, in order to calculate the accuracy of fault classification by comparison with the original data labels and output samples. Experimental results can reflect how key parameters may be selected in the configuration of the key variables to achieve optimal results. The accuracy of the fault recognition demonstrates that the method we proposed can provide a more effective way for grid inspection personnel to deal with unstructured data.</dcterms:abstract>
        <dc:date>2017 MAR</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000398736700146</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1996-1073">
        <prism:volume>10</prism:volume>
        <dc:title>ENERGIES</dc:title>
        <dc:identifier>DOI 10.3390/en10030406</dc:identifier>
        <prism:number>3</prism:number>
        <dc:identifier>ISSN 1996-1073</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1441">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_493">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;50&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;58&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;32&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:1063-6919">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 1063-6919</dc:identifier>
                <dc:title>Shanghai Jiao Tong University</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2019.00344</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>JC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Q</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ni</foaf:surname>
                        <foaf:givenName>BB</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>LG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>JX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>MD</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>Q</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>IEEE Comp Soc</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1425"/>
        <dcterms:isReferencedBy rdf:resource="#item_495"/>
        <dc:subject>NETWORKS</dc:subject>
        <dc:title>Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling</dc:title>
        <dcterms:abstract>Geometric deep learning is increasingly important thanks to the popularity of 3D sensors. Inspired by the recent advances in NLP domain, the self-attention transformer is introduced to consume the point clouds. We develop Point Attention Transformers (PATs), using a parameter-efficient Group Shuffle Attention (GSA) to replace the costly Multi-Head Attention. We demonstrate its ability to process size-varying inputs, and prove its permutation equivariance. Besides, prior work uses heuristics dependence on the input data (e.g., Furthest Point Sampling) to hierarchically select subsets of input points. Thereby, we for the first time propose an end-to-end learnable and taskagnostic sampling operation, named Gumbel Subset Sampling (GSS), to select a representative subset of input points. Equipped with Gumbel-Softmax, it produces a &quot;soft&quot; continuous subset in training phase, and a &quot;hard&quot; discrete subset in test phase. By selecting representative subsets in a hierarchical fashion, the networks learn a stronger representation of the input sets with lower computation cost. Experiments on classification and segmentation benchmarks show the effectiveness and efficiency of our methods. Furthermore, we propose a novel application, to process event camera stream as point clouds, and achieve a state-of-the-art performance on DVS128 Gesture Dataset.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000529484003051</dc:coverage>
        <bib:pages>3318-3327</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2019)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1425">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_495">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;276&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;314&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;54&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-950737-13-0">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-950737-13-0</dc:identifier>
                <dc:title>University of Washington</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>NF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gardner</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Belinkov</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Peters</foaf:surname>
                        <foaf:givenName>ME</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Smith</foaf:surname>
                        <foaf:givenName>NA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1421"/>
        <dcterms:isReferencedBy rdf:resource="#item_497"/>
        <dc:subject>CORPUS</dc:subject>
        <dc:title>Linguistic Knowledge and Transferability of Contextual Representations</dc:title>
        <dcterms:abstract>large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more taskspecific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000900116901017</dc:coverage>
        <bib:pages>1073-1094</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1421">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_497">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;238&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;266&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;72&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_498">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2169-3536"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gao</foaf:surname>
                        <foaf:givenName>ZJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Feng</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Song</foaf:surname>
                        <foaf:givenName>XY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>X</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1399"/>
        <dcterms:isReferencedBy rdf:resource="#item_499"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>sentiment analysis</dc:subject>
        <dc:subject>neural networks</dc:subject>
        <dc:title>Target-Dependent Sentiment Classification With BERT</dc:title>
        <dcterms:abstract>Research on machine assisted text analysis follows the rapid development of digital media, and sentiment analysis is among the prevalent applications. Traditional sentiment analysis methods require complex feature engineering, and embedding representations have dominated leaderboards for a long time. However, the context-independent nature limits their representative power in rich context, hurting performance in Natural Language Processing (NLP) tasks. Bidirectional Encoder Representations from Transformers (BERT), among other pre-trained language models, beats existing best results in eleven NLP tasks (including sentence-level sentiment classification) by a large margin, which makes it the new baseline of text representation. As a more challenging task, fewer applications of BERT have been observed for sentiment classification at the aspect level. We implement three target-dependent variations of the BERTbase model, with positioned output at the target terms and an optional sentence with the target built in. Experiments on three data collections show that our TD-BERT model achieves new state-of-the-art performance, in comparison to traditional feature engineering methods, embedding-based models and earlier applications of BERT. With the successful application of BERT in many NLP tasks, our experiments try to verify if its context-aware representation can achieve similar performance improvement in aspect-based sentiment analysis. Surprisingly, coupling it with complex neural networks that used to work well with embedding representations does not show much value, sometimes with performance below the vanilla BERT-FC implementation. On the other hand, incorporation of target information shows stable accuracy improvement, and the most effective way of utilizing that information is displayed through the experiment.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000510406100001</dc:coverage>
        <bib:pages>154290-154299</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2169-3536">
        <prism:volume>7</prism:volume>
        <dc:title>IEEE ACCESS</dc:title>
        <dc:identifier>DOI 10.1109/ACCESS.2019.2946594</dc:identifier>
        <dc:identifier>ISSN 2169-3536</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1399">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_499">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;206&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;231&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;51&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-6654-2418-9">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-6654-2418-9</dc:identifier>
                <dc:title>Intel Corporation</dc:title>
                <dc:identifier>DOI 10.1109/EMC2-NIPS53020.2019.00016</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zafrir</foaf:surname>
                        <foaf:givenName>O</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Boudoukh</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Izsak</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wasserblat</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>IEEE</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1440"/>
        <dcterms:isReferencedBy rdf:resource="#item_501"/>
        <dc:subject>bert</dc:subject>
        <dc:subject>language-modeling</dc:subject>
        <dc:subject>nlp</dc:subject>
        <dc:subject>quantization</dc:subject>
        <dc:subject>quantization-aware-training</dc:subject>
        <dc:subject>transformers</dc:subject>
        <dc:title>Q8BERT: Quantized 8Bit BERT</dc:title>
        <dcterms:abstract>Recently, pre-trained Transformer [I] based language models such as BERT [2] and GPT [3], have shown great improvement in many Natural Language Processing (NLP) tasks. However, these models contain a large amount of parameters. The emergence of even larger and more accurate models such as GPT2 [4] and Megatronl, suggest a trend of large pre-trained Transformer models. However, using these large models in production environments is a complex task requiring a large amount of compute, memory and power resources. In this work we show how to perform quantization-aware training during the fine-tuning phase of BERT in order to compress BERT by 4x with minimal accuracy loss. Furthermore, the produced quantized model can accelerate inference speed if it is optimized for 8bit Integer supporting hardware.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000844569200009</dc:coverage>
        <bib:pages>36-39</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>FIFTH WORKSHOP ON ENERGY EFFICIENT MACHINE LEARNING AND COGNITIVE COMPUTING - NEURIPS EDITION (EMC2-NIPS 2019)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1440">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_501">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;106&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;122&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;17&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_502">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2291-9694"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jin</foaf:surname>
                        <foaf:givenName>YH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>WS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rawat</foaf:surname>
                        <foaf:givenName>BPS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cai</foaf:surname>
                        <foaf:givenName>PS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1334"/>
        <dcterms:isReferencedBy rdf:resource="#item_503"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>NAMED ENTITY RECOGNITION</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>electronic health record note</dc:subject>
        <dc:subject>entity normalization</dc:subject>
        <dc:subject>NORMALIZATION</dc:subject>
        <dc:title>Fine-Tuning Bidirectional Encoder Representations From Transformers (BERT)-Based Models on Large-Scale Electronic Health Record Notes: An Empirical Study</dc:title>
        <dcterms:abstract>Background: The bidirectional encoder representations from transformers (BERT) model has achieved great success in many natural language processing (NLP) tasks, such as named entity recognition and question answering. However, little prior work has explored this model to be used for an important task in the biomedical and clinical domains, namely entity normalization.
Objective: We aim to investigate the effectiveness of BERT-based models for biomedical or clinical entity normalization. In addition, our second objective is to investigate whether the domains of training data influence the performances of BERT-based models as well as the degree of influence.
Methods: Our data was comprised of 1.5 million unlabeled electronic health record (EHR) notes. We first fine-tuned BioBERT on this large collection of unlabeled EHR notes. This generated our BERT-based model trained using 1.5 million electronic health record notes (EhrBERT). We then further fine-tuned EhrBERT, BioBERT, and BERT on three annotated corpora for biomedical and clinical entity normalization: the Medication, Indication, and Adverse Drug Events (MADE) 1.0 corpus, the National Center for Biotechnology Information (NCBI) disease corpus, and the Chemical-Disease Relations (CDR) corpus. We compared our models with two state-of-the-art normalization systems, namely MetaMap and disease name normalization (DNorm).
Results: EhrBERT achieved 40.95% F1 in the MADE 1.0 corpus for mapping named entities to the Medical Dictionary for Regulatory Activities and the Systematized Nomenclature of Medicine-Clinical Terms (SNOMED-CT), which have about 380,000 terms. In this corpus, EhrBERT outperformed MetaMap by 2.36% in F1. For the NCBI disease corpus and CDR corpus, EhrBERT also outperformed DNorm by improving the F1 scores from 88.37% and 89.92% to 90.35% and 93.82%, respectively. Compared with BioBERT and BERT, EhrBERT outperformed them on the MADE 1.0 corpus and the CDR corpus.
Conclusions: Our work shows that BERT-based models have achieved state-of-the-art performance for biomedical and clinical entity normalization. BERT-based models can be readily fine-tuned to normalize any kind of named entities.</dcterms:abstract>
        <dc:date>2019 JUL</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000488621000007</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2291-9694">
        <prism:volume>7</prism:volume>
        <dc:title>JMIR MEDICAL INFORMATICS</dc:title>
        <dc:identifier>DOI 10.2196/14830</dc:identifier>
        <prism:number>3</prism:number>
        <dc:identifier>ISSN 2291-9694</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1334">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_503">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;104&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;110&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;36&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_504">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1386-5056"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>XH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>YY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Q</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ren</foaf:surname>
                        <foaf:givenName>YK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qiu</foaf:surname>
                        <foaf:givenName>TL</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ma</foaf:surname>
                        <foaf:givenName>JH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Q</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1330"/>
        <dcterms:isReferencedBy rdf:resource="#item_505"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>AUTOMATED EXTRACTION</dc:subject>
        <dc:subject>Breast cancer</dc:subject>
        <dc:subject>Clinical information extraction</dc:subject>
        <dc:subject>Fine-tuning BERT</dc:subject>
        <dc:subject>Information model</dc:subject>
        <dc:title>Extracting comprehensive clinical information for breast cancer using deep learning methods</dc:title>
        <dcterms:abstract>Objective: Breast cancer is the most common malignant tumor among women. The diagnosis and treatment information of breast cancer patients is abundant in multiple types of clinical fields, including clinicopathological data, genotype and phenotype information, treatment information, and prognosis information. However, current studies are mainly focused on extracting information from one specific type of clinical field. This study defines a comprehensive information model to represent the whole-course clinical information of patients. Furthermore, deep learning approaches are used to extract the concepts and their attributes from clinical breast cancer documents by fine-tuning pretrained Bidirectional Encoder Representations from Transformers (BERT) language models.
Materials and methods: The clinical corpus that was used in this study was from one 3A cancer hospital in China, consisting of the encounter notes, operation records, pathology notes, radiology notes, progress notes and discharge summaries of 100 breast cancer patients. Our system consists of two components: a named entity recognition (NER) component and a relation recognition component. For each component, we implemented deep learning-based approaches by fine-tuning BERT, which outperformed other state-of-the-art methods on multiple natural language processing (NLP) tasks. A clinical language model is first pretrained using BERT on a large-scale unlabeled corpus of Chinese clinical text. For NER, the context embeddings that were pretrained using BERT were used as the input features of the Bi-LSTM-CRF (Bidirectional long-short-memory-conditional random fields) model and were fine-tuned using the annotated breast cancer notes. Furthermore, we proposed an approach to fine-tune BERT for relation extraction. It was considered to be a classification problem in which the two entities that were mentioned in the input sentence were replaced with their semantic types.
Results: Our best-performing system achieved F1 scores of 93.53% for the NER and 96.73% for the relation extraction. Additional evaluations showed that the deep learning-based approaches that fine-tuned BERT did outperform the traditional Bi-LSTM-CRF and CRF machine learning algorithms in NER and the attention-Bi-LSTM and SVM (support vector machines) algorithms in relation recognition.
Conclusion: In this study, we developed a deep learning approach that fine-tuned BERT to extract the breast cancer concepts and their attributes. It demonstrated its superior performance compared to traditional machine learning algorithms, thus supporting its uses in broader NER and relation extraction tasks in the medical domain.</dcterms:abstract>
        <dc:date>2019 DEC</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000492149900018</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1386-5056">
        <prism:volume>132</prism:volume>
        <dc:title>INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS</dc:title>
        <dc:identifier>DOI 10.1016/j.ijmedinf.2019.103985</dc:identifier>
        <dc:identifier>ISSN 1386-5056</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1330">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_505">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;97&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;102&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;25&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-950737-90-1">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-950737-90-1</dc:identifier>
                <dc:title>Instituto de Telecomunicacoes</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Correia</foaf:surname>
                        <foaf:givenName>GM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Niculae</foaf:surname>
                        <foaf:givenName>V</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Martins</foaf:surname>
                        <foaf:givenName>AFT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1415"/>
        <dcterms:isReferencedBy rdf:resource="#item_507"/>
        <dc:title>Adaptively Sparse Transformers</dc:title>
        <dcterms:abstract>Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter - which controls the shape and sparsity of alpha-entmax - allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000854193302031</dc:coverage>
        <bib:pages>2174-2184</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1415">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 3: desafios e limitações&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_507">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;79&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;85&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;39&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-6976-3">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-6976-3</dc:identifier>
                <dc:identifier>DOI 10.1145/3357384.3358028</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>van Aken</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Winter</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Löser</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gers</foaf:surname>
                        <foaf:givenName>FA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>ACM</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_509"/>
        <link:link rdf:resource="#item_1565"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>explainability</dc:subject>
        <dc:subject>neural networks</dc:subject>
        <dc:subject>transformers</dc:subject>
        <dc:subject>question answering</dc:subject>
        <dc:subject>word representation</dc:subject>
        <dc:title>How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations</dc:title>
        <dcterms:abstract>Bidirectional Encoder Representations from Transformers (BERT) reach state-of-the-art results in a variety of Natural Language Processing tasks. However, understanding of their internal functioning is still insufficient and unsatisfactory. In order to better understand BERT and other Transformer-based models, we present a layer-wise analysis of BERT's hidden states. Unlike previous research, which mainly focuses on explaining Transformer models by their attention weights, we argue that hidden states contain equally valuable information. Specifically, our analysis focuses on models fine-tuned on the task of Question Answering (QA) as an example of a complex downstream task. We inspect how QA models transform token vectors in order to find the correct answer. To this end, we apply a set of general and QA-specific probing tasks that reveal the information stored in each representation layer. Our qualitative analysis of hidden state visualizations provides additional insights into BERT's reasoning process. Our results show that the transformations within BERT go through phases that are related to traditional pipeline tasks. The system can therefore implicitly incorporate task-specific information into its token representations. Furthermore, our analysis reveals that fine-tuning has little impact on the models' semantic abilities and that prediction errors can be recognized in the vector representations of even early layers.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000539898201088</dc:coverage>
        <bib:pages>1823-1832</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION &amp; KNOWLEDGE MANAGEMENT (CIKM '19)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_509">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;68&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;73&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;41&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1565">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1565/van Aken et al. - 2019 - How Does BERT Answer Questions A Layer-Wise Analysis of Transformer Representations.pdf"/>
        <dc:title>Versão Submetida</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1909.04925</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:38:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:2945-9133">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>11625</prism:volume>
                <dc:identifier>ISBN 2945-9133</dc:identifier>
                <dc:identifier>DOI 10.1007/978-3-030-23204-7_39</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sung</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dhamecha</foaf:surname>
                        <foaf:givenName>TI</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mukhi</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Isotani</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Millan</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ogan</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hastings</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>McLaren</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luckin</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1369"/>
        <dcterms:isReferencedBy rdf:resource="#item_511"/>
        <dc:subject>Self-attention</dc:subject>
        <dc:subject>Student answer scoring</dc:subject>
        <dc:subject>Transfer learning</dc:subject>
        <dc:title>Improving Short Answer Grading Using Transformer-Based Pre-training</dc:title>
        <dcterms:abstract>Dialogue-based tutoring platforms have shown great promise in helping individual students improve mastery. Short answer grading is a crucial component of such platforms. However, generative short answer grading using the same platform for diverse disciplines and titles is a crucial challenge due to data distribution variations across domains and a frequent occurrence of non-sentential answers. Recent NLP research has introduced novel deep learning architectures such as the Transformer, which merely uses self-attention mechanisms. Pre-trained models based on the Transformer architecture have been used to produce impressive results across a range of NLP tasks. In this work, we experiment with fine-tuning a pre-trained self-attention language model, namely Bidirectional Encoder Representations from Transformers (BERT) applying it to short answer grading, and show that it produces superior results across multiple domains. On the benchmarking dataset of SemEval-2013, we report up to 10% absolute improvement in macro-average-F1 over state-of-the-art results. On our two psychology domain datasets, the fine-tuned model yields classification almost up to the human-agreement levels. Moreover, we study the effectiveness of fine-tuning as a function of the size of the task-specific labeled data, the number of training epochs, and its generalizability to cross-domain and join-domain scenarios.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000495604300039</dc:coverage>
        <bib:pages>469-481</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>ARTIFICIAL INTELLIGENCE IN EDUCATION (AIED 2019), PT I</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1369">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_511">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;67&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;71&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;28&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_512">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-950737-90-1</dc:identifier>
                <dc:title>University of Edinburgh</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>BA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Titov</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sennrich</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1368"/>
        <dcterms:isReferencedBy rdf:resource="#item_513"/>
        <dc:title>Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention</dc:title>
        <dcterms:abstract>The general trend in NLP is towards increasing model capacity and performance via deeper neural networks. However, simply stacking more layers of the popular Transformer architecture for machine translation results in poor convergence and high computational overhead. Our empirical analysis suggests that convergence is poor due to gradient vanishing caused by the interaction between residual connections and layer normalization. We propose depth-scaled initialization (DS-Init), which decreases parameter variance at the initialization stage, and reduces output variance of residual connections so as to ease gradient back-propagation through normalization layers. To address computational cost, we propose a merged attention sublayer (MAtt) which combines a simplified average-based self-attention sublayer and the encoderdecoder attention sublayer on the decoder side. Results on WMT and IWSLT translation tasks with five translation directions show that deep Transformers with DS-Init and MAtt can substantially outperform their base counterpart in terms of BLEU (+1.1 BLEU on average for 12-layer models), while matching the decoding speed of the baseline model thanks to the efficiency improvements of MAtt.(1)</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000854193301005</dc:coverage>
        <bib:pages>898-909</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1368">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_513">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;26&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;28&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;43&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_514">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1367-4803"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yoon</foaf:surname>
                        <foaf:givenName>W</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>So</foaf:surname>
                        <foaf:givenName>CH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kang</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1302"/>
        <dcterms:isReferencedBy rdf:resource="#item_515"/>
        <dc:subject>RECOGNITION</dc:subject>
        <dc:subject>CORPUS</dc:subject>
        <dc:title>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</dc:title>
        <dcterms:abstract>Motivation: Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.
Results: We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.</dcterms:abstract>
        <dc:date>2020 FEB 15</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000518528800035</dc:coverage>
        <bib:pages>1234-1240</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1367-4803">
        <prism:volume>36</prism:volume>
        <dc:title>BIOINFORMATICS</dc:title>
        <dc:identifier>DOI 10.1093/bioinformatics/btz682</dc:identifier>
        <prism:number>4</prism:number>
        <dc:identifier>ISSN 1367-4803</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1302">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_515">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;2403&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;2630&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;38&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-952148-25-5">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-952148-25-5</dc:identifier>
                <dc:title>University of Rochester</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rahman</foaf:surname>
                        <foaf:givenName>W</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hasan</foaf:surname>
                        <foaf:givenName>MK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zadeh</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mao</foaf:surname>
                        <foaf:givenName>CF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Morency</foaf:surname>
                        <foaf:givenName>LP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hoque</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1371"/>
        <dcterms:isReferencedBy rdf:resource="#item_517"/>
        <dc:title>Integrating Multimodal Information in Large Pretrained Transformers</dc:title>
        <dcterms:abstract>Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. While fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication). Pre-trained models don't have the necessary components to accept two extra modalities of vision and acoustic. In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning. It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities. In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of BERT and XLNet. On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000570978202058</dc:coverage>
        <bib:pages>2359-2369</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1371">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_517">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;255&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;281&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;27&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-952148-90-3">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-952148-90-3</dc:identifier>
                <dc:title>Harbin Institute of Technology</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cui</foaf:surname>
                        <foaf:givenName>YM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Che</foaf:surname>
                        <foaf:givenName>WX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qin</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>SJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hu</foaf:surname>
                        <foaf:givenName>GP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cohn</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1741"/>
        <dcterms:isReferencedBy rdf:resource="#item_519"/>
        <dc:title>Revisiting Pre-trained Models for Chinese Natural Language Processing</dc:title>
        <dcterms:abstract>Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and consecutive variants have been proposed to further improve the performance of the pretrained language models. In this paper, we target on revisiting Chinese pre-trained language models to examine their effectiveness in a non-English language and release the Chinese pre-trained language model series to the community. We also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways, especially the masking strategy that adopts MLM as correction (Mac). We carried out extensive experiments on eight Chinese NLP tasks to revisit the existing pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001181866504048</dc:coverage>
        <bib:pages>657-668</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1741">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_519">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;222&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;242&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;36&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_520">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2041-1723"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tetko</foaf:surname>
                        <foaf:givenName>IV</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Karpov</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Van Deursen</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Godin</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1398"/>
        <dcterms:isReferencedBy rdf:resource="#item_521"/>
        <dc:subject>SMILES</dc:subject>
        <dc:subject>PREDICTION</dc:subject>
        <dc:subject>SYSTEM</dc:subject>
        <dc:subject>OUTCOMES</dc:subject>
        <dc:subject>NEURAL-NETWORK</dc:subject>
        <dc:title>State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis</dc:title>
        <dcterms:abstract>We investigated the effect of different training scenarios on predicting the (retro)synthesis of chemical compounds using text-like representation of chemical reactions (SMILES) and Natural Language Processing (NLP) neural network Transformer architecture. We showed that data augmentation, which is a powerful method used in image processing, eliminated the effect of data memorization by neural networks and improved their performance for prediction of new sequences. This effect was observed when augmentation was used simultaneously for input and the target data simultaneously. The top-5 accuracy was 84.8% for the prediction of the largest fragment (thus identifying principal transformation for classical retro-synthesis) for the USPTO-50k test dataset, and was achieved by a combination of SMILES augmentation and a beam search algorithm. The same approach provided significantly better results for the prediction of direct reactions from the single-step USPTO-MIT test set. Our model achieved 90.6% top-1 and 96.1% top-5 accuracy for its challenging mixed set and 97% top-5 accuracy for the USPTO-MIT separated set. It also significantly improved results for USPTO-full set single-step retrosynthesis for both top-1 and top-10 accuracies. The appearance frequency of the most abundantly generated SMILES was well correlated with the prediction outcome and can be used as a measure of the quality of reaction prediction. Development of algorithms to predict reactant and reagents given a target molecule is key to accelerate retrosynthesis approaches. Here the authors demonstrate that applying augmentation techniques to the SMILE representation of target data significantly improves the quality of the reaction predictions.</dcterms:abstract>
        <dc:date>2020 NOV 4</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000592028600007</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2041-1723">
        <prism:volume>11</prism:volume>
        <dc:title>NATURE COMMUNICATIONS</dc:title>
        <dc:identifier>DOI 10.1038/s41467-020-19266-y</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 2041-1723</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1398">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_521">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;207&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;218&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;40&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-952148-62-0">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-952148-62-0</dc:identifier>
                <dc:title>University of Virginia</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Morris</foaf:surname>
                        <foaf:givenName>JX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lifland</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yoo</foaf:surname>
                        <foaf:givenName>JY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grigsby</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jin</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>YJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1379"/>
        <dcterms:isReferencedBy rdf:resource="#item_523"/>
        <dc:title>TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP</dc:title>
        <dcterms:abstract>While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack's modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness. TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000855177700016</dc:coverage>
        <bib:pages>119-126</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1379">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_523">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;162&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;179&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;39&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_524">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-952148-62-0</dc:identifier>
                <dc:title>Technical University of Darmstadt</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pfeiffer</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rückle</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Poth</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kamath</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vulic</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ruder</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cho</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gurevych</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1358"/>
        <dcterms:isReferencedBy rdf:resource="#item_525"/>
        <dc:title>AdapterHub: A Framework for Adapting Transformers</dc:title>
        <dcterms:abstract>The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of hundreds of millions, or even billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks. Adapters-small learnt bottleneck layers inserted within each layer of a pre-trained model-ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose AdapterHub, a framework that allows dynamic &quot;stichingin&quot; of pre-trained adapters for different tasks and languages. The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000855177700007</dc:coverage>
        <bib:pages>46-54</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1358">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_525">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;159&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;171&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;42&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:1860-949X">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>881</prism:volume>
                <dc:identifier>ISBN 1860-949X</dc:identifier>
                <dc:title>IMT - Institut Mines-Telecom</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-36687-2_77</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mozafari</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Farahbakhsh</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Crespi</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cherifi</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gaito</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mendes</foaf:surname>
                        <foaf:givenName>JF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Moro</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rocha</foaf:surname>
                        <foaf:givenName>LM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1285"/>
        <dcterms:isReferencedBy rdf:resource="#item_527"/>
        <dc:subject>NLP</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>Transfer learning</dc:subject>
        <dc:subject>Fine-tuning</dc:subject>
        <dc:subject>Hate speech detection</dc:subject>
        <dc:subject>Language modeling</dc:subject>
        <dc:subject>Social media</dc:subject>
        <dc:title>A BERT-Based Transfer Learning Approach for Hate Speech Detection in Online Social Media</dc:title>
        <dcterms:abstract>Generated hateful and toxic content by a portion of users in social media is a rising phenomenon that motivated researchers to dedicate substantial efforts to the challenging direction of hateful content identification. We not only need an efficient automatic hate speech detection model based on advanced machine learning and natural language processing, but also a sufficiently large amount of annotated data to train a model. The lack of a sufficient amount of labelled hate speech data, along with the existing biases, has been the main issue in this domain of research. To address these needs, in this study we introduce a novel transfer learning approach based on an existing pre-trained language model called BERT (Bidirectional Encoder Representations from Transformers). More specifically, we investigate the ability of BERT at capturing hateful context within social media content by using new fine-tuning methods based on transfer learning. To evaluate our proposed approach, we use two publicly available datasets that have been annotated for racism, sexism, hate, or offensive content on Twitter. The results show that our solution obtains considerable performance on these datasets in terms of precision and recall in comparison to existing approaches. Consequently, our model can capture some biases in data annotation and collection process and can potentially lead us to a more accurate model.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000843927300077</dc:coverage>
        <bib:pages>928-940</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>COMPLEX NETWORKS AND THEIR APPLICATIONS VIII, VOL 1</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1285">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_527">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;155&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;163&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;25&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-7998-4">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dcterms:isPartOf>
                   <bib:Series><dc:title>KDD '20</dc:title></bib:Series>
                </dcterms:isPartOf>
                <dc:identifier>ISBN 978-1-4503-7998-4</dc:identifier>
                <dc:title>Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</dc:title>
                <dc:identifier>DOI 10.1145/3394486.3403368</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>New York, NY, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Association for Computing Machinery</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chang</foaf:surname>
                        <foaf:givenName>WC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>HF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhong</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>YM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dhillon</foaf:surname>
                        <foaf:givenName>IS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>ASSOC COMP MACHINERY</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1749"/>
        <dcterms:isReferencedBy rdf:resource="#item_529"/>
        <link:link rdf:resource="#item_1748"/>
        <link:link rdf:resource="#item_1558"/>
        <dc:subject>eXtreme Multi-label text classification</dc:subject>
        <dc:subject>Transformer models</dc:subject>
        <dc:title>Taming Pretrained Transformers for Extreme Multi-label Text Classification</dc:title>
        <dcterms:abstract>We consider the extreme multi-label text classification (XMC) problem: given an input text, return the most relevant labels from a large label collection. For example, the input text could be a product description on Amazon.com and the labels could be product categories. XMC is an important yet challenging problem in the NLP community. Recently, deep pretrained transformer models have achieved state-of-the-art performance on many NLP tasks including sentence classification, albeit with small label sets. However, naively applying deep transformer models to the XMC problem leads to sub-optimal performance due to the large output space and the label sparsity issue. In this paper, we propose X-Transformer, the first scalable approach to fine-tuning deep transformer models for the XMC problem. The proposed method achieves new state-of-the-art results on four XMC benchmark datasets. In particular, on a Wiki dataset with around 0.5 million labels, the prec@1 of X-Transformer is 77.28%, a substantial improvement over state-of-the-art XMC approaches Parabel (linear) and AttentionXML (neural), which achieve 68.70% and 76.95% precision@1, respectively. We further apply X-Transformer to a product2query dataset from Amazon and gained 10.7% relative improvement on prec@1 over Parabel.</dcterms:abstract>
        <dc:date>agosto 20, 2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000749552303016</dc:coverage>
        <z:libraryCatalog>ACM Digital Library</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/10.1145/3394486.3403368</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <bib:pages>3163-3171</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY &amp; DATA MINING</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1749">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_529">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;122&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;137&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;32&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1748">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1748/Chang et al. - 2020 - Taming Pretrained Transformers for Extreme Multi-label Text Classification.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/pdf/10.1145/3394486.3403368</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 13:23:26</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_1558">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1558/Chang et al. - 2020 - Taming Pretrained Transformers for Extreme Multi-label Text Classification.pdf"/>
        <dc:title>Versão Submetida</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1905.02331</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:33:47</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-952148-60-6">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-952148-60-6</dc:identifier>
                <dc:title>Google Incorporated</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ainslie</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ontañón</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alberti</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cvicek</foaf:surname>
                        <foaf:givenName>V</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fisher</foaf:surname>
                        <foaf:givenName>Z</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pham</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ravula</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sanghai</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>QF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1326"/>
        <dcterms:isReferencedBy rdf:resource="#item_531"/>
        <dc:title>ETC: Encoding Long and Structured Inputs in Transformers</dc:title>
        <dcterms:abstract>Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000855160700019</dc:coverage>
        <bib:pages>268-284</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1326">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_531">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;112&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;123&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;44&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_532">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-952148-25-5</dc:identifier>
                <dc:title>University of California System</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hendrycks</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>XY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wallace</foaf:surname>
                        <foaf:givenName>ER</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dziedzic</foaf:surname>
                        <foaf:givenName>AD</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krishnan</foaf:surname>
                        <foaf:givenName>RS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Song</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1437"/>
        <dcterms:isReferencedBy rdf:resource="#item_533"/>
        <dc:title>Pretrained Transformers Improve Out-of-Distribution Robustness</dc:title>
        <dcterms:abstract>Although pretrained Transformers such as BERT achieve high accuracy on in distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers' performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000570978203004</dc:coverage>
        <bib:pages>2744-2751</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1437">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_533">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;109&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;109&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;59&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_534">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2329-9290"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cui</foaf:surname>
                        <foaf:givenName>YM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Che</foaf:surname>
                        <foaf:givenName>WX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qin</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>ZQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1436"/>
        <dcterms:isReferencedBy rdf:resource="#item_535"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>Analytical models</dc:subject>
        <dc:subject>Predictive models</dc:subject>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>Computational modeling</dc:subject>
        <dc:subject>Training</dc:subject>
        <dc:subject>Adaptation models</dc:subject>
        <dc:subject>Bit error rate</dc:subject>
        <dc:subject>Pre-trained language model</dc:subject>
        <dc:subject>representation learning</dc:subject>
        <dc:title>Pre-Training With Whole Word Masking for Chinese BERT</dc:title>
        <dcterms:abstract>Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and its consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we aim to first introduce the whole word masking (wwm) strategy for Chinese BERT, along with a series of Chinese pre-trained language models. Then we also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways. Especially, we propose a new masking strategy called MLM as correction (Mac). To demonstrate the effectiveness of these models, we create a series of Chinese pre-trained language models as our baselines, including BERT, RoBERTa, ELECTRA, RBT, etc. We carried out extensive experiments on ten Chinese NLP tasks to evaluate the created Chinese pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research. We open-source our pre-trained language models for further facilitating our research community.(1)</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000728135200002</dc:coverage>
        <bib:pages>3504-3514</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2329-9290">
        <prism:volume>29</prism:volume>
        <dc:title>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</dc:title>
        <dc:identifier>DOI 10.1109/TASLP.2021.3124365</dc:identifier>
        <dc:identifier>ISSN 2329-9290</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1436">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_535">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;591&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;705&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;40&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-6654-2812-5">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-6654-2812-5</dc:identifier>
                <dc:title>Hong Kong University of Science &amp; Technology</dc:title>
                <dc:identifier>DOI 10.1109/ICCV48922.2021.00062</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yuan</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>SP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>ZW</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>AJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>FW</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>W</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>IEEE</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1370"/>
        <dcterms:isReferencedBy rdf:resource="#item_537"/>
        <dc:title>Incorporating Convolution Designs into Visual Transformers</dc:title>
        <dcterms:abstract>Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new Convolution-enhanced image Transformer (CeiT) which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: 1) instead of the straightforward tokenization from raw input images, we design an Image-to-Tokens (I2T) module that extracts patches from generated low-level features; 2) the feed-froward network in each encoder block is replaced with a Locally-enhanced Feed-Forward (LeFF) layer that promotes the correlation among neighboring tokens in the spatial dimension; 3) a Layer-wise Class token Attention (LCA) is attached at the top of the Transformer that utilizes the multi-level representations.
Experimental results on ImageNet and seven downstream tasks show the effectiveness and generalization ability of CeiT compared with previous Transformers and state-of-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models demonstrate better convergence with 3x fewer training iterations, which can reduce the training cost significantly</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000797698900056</dc:coverage>
        <bib:pages>559-568</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1370">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_537">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;277&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;290&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;52&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_538">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0269-2821"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Acheampong</foaf:surname>
                        <foaf:givenName>FA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nunoo-Mensah</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>WY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1361"/>
        <dcterms:isReferencedBy rdf:resource="#item_539"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Transformers</dc:subject>
        <dc:subject>PERSONALITY</dc:subject>
        <dc:subject>Sentiment analysis</dc:subject>
        <dc:subject>SENTIMENT ANALYSIS</dc:subject>
        <dc:subject>Text-based emotion detection</dc:subject>
        <dc:title>Transformer models for text-based emotion detection: a review of BERT-based approaches</dc:title>
        <dcterms:abstract>We cannot overemphasize the essence of contextual information in most natural language processing (NLP) applications. The extraction of context yields significant improvements in many NLP tasks, including emotion recognition from texts. The paper discusses transformer-based models for NLP tasks. It highlights the pros and cons of the identified models. The models discussed include the Generative Pre-training (GPT) and its variants, Transformer-XL, Cross-lingual Language Models (XLM), and the Bidirectional Encoder Representations from Transformers (BERT). Considering BERT's strength and popularity in text-based emotion detection, the paper discusses recent works in which researchers proposed various BERT-based models. The survey presents its contributions, results, limitations, and datasets used. We have also provided future research directions to encourage research in text-based emotion detection using these models.</dcterms:abstract>
        <dc:date>2021 DEC</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000616046200001</dc:coverage>
        <bib:pages>5789-5829</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0269-2821">
        <prism:volume>54</prism:volume>
        <dc:title>ARTIFICIAL INTELLIGENCE REVIEW</dc:title>
        <dc:identifier>DOI 10.1007/s10462-021-09958-2</dc:identifier>
        <prism:number>8</prism:number>
        <dc:identifier>ISSN 0269-2821</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1361">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_539">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;215&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;231&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;107&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_540">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1467-5463"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Le</foaf:surname>
                        <foaf:givenName>NQK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ho</foaf:surname>
                        <foaf:givenName>QT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nguyen</foaf:surname>
                        <foaf:givenName>TTD</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ou</foaf:surname>
                        <foaf:givenName>YY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1292"/>
        <dcterms:isReferencedBy rdf:resource="#item_541"/>
        <dc:subject>BERT</dc:subject>
        <dc:subject>biological sequence</dc:subject>
        <dc:subject>contextualized word embedding</dc:subject>
        <dc:subject>convolutional neural network</dc:subject>
        <dc:subject>DNA enhancer</dc:subject>
        <dc:subject>NLP transformer</dc:subject>
        <dc:title>A transformer architecture based on BERT and 2D convolutional neural network to identify DNA enhancers from sequence information</dc:title>
        <dcterms:abstract>Recently, language representation models have drawn a lot of attention in the natural language processing field due to their remarkable results. Among them, bidirectional encoder representations from transformers (BERT) has proven to be a simple, yet powerful language model that achieved novel state-of-the-art performance. BERT adopted the concept of contextualized word embedding to capture the semantics and context of the words in which they appeared. In this study, we present a novel technique by incorporating BERT-based multilingual model in bioinformatics to represent the information of DNA sequences. We treated DNA sequences as natural sentences and then used BERT models to transform them into fixed-length numerical matrices. As a case study, we applied our method to DNA enhancer prediction, which is a well-known and challenging problem in this field. We then observed that our BERT-based features improved more than 5-10% in terms of sensitivity, specificity, accuracy and Matthews correlation coefficient compared to the current state-of-the-art features in bioinformatics. Moreover, advanced experiments show that deep learning (as represented by 2D convolutional neural networks; CNN) holds potential in learning BERT features better than other traditional machine learning techniques. In conclusion, we suggest that BERT and 2D CNNs could open a new avenue in biological modeling using sequence information.</dcterms:abstract>
        <dc:date>2021 SEP</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000709461800047</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1467-5463">
        <prism:volume>22</prism:volume>
        <dc:title>BRIEFINGS IN BIOINFORMATICS</dc:title>
        <dc:identifier>DOI 10.1093/bib/bbab005</dc:identifier>
        <prism:number>5</prism:number>
        <dc:identifier>ISSN 1467-5463</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1292">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_541">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;113&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;115&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;29&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:0302-9743">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>12908</prism:volume>
                <dc:identifier>ISBN 0302-9743</dc:identifier>
                <dc:title>Tencent</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-87237-3_5</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ma</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bi</foaf:surname>
                        <foaf:givenName>Q</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bian</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ning</foaf:surname>
                        <foaf:givenName>MN</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>NJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>YX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>HR</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>YF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>DeBruijne</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cattin</foaf:surname>
                        <foaf:givenName>PC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cotin</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Padoy</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Speidel</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Essert</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1424"/>
        <dcterms:isReferencedBy rdf:resource="#item_543"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Fundus image</dc:subject>
        <dc:subject>Multiple instance learning</dc:subject>
        <dc:subject>Vision Transformer</dc:subject>
        <dc:title>MIL-VT: Multiple Instance Learning Enhanced Vision Transformer for Fundus Image Classification</dc:title>
        <dcterms:abstract>With the advancement and prevailing success of Transformer models in the natural language processing (NLP) field, an increasing number of research works have explored the applicability of Transformer for various vision tasks and reported superior performance compared with convolutional neural networks (CNNs). However, as the proper training of Transformer generally requires an extremely large quantity of data, it has rarely been explored for the medical imaging tasks. In this paper, we attempt to adopt the Vision Transformer for the retinal disease classification tasks, by pre-training the Transformer model on a large fundus image database and then fine-tuning on downstream retinal disease classification tasks. In addition, to fully exploit the feature representations extracted by individual image patches, we propose a multiple instance learning (MIL) based 'MIL head', which can be conveniently attached to the Vision Transformer in a plug-and-play manner and effectively enhances the model performance for the downstream fundus image classification tasks. The proposed MIL-VT framework achieves superior performance over CNN models on two publicly available datasets when being trained and tested under the same setup. The implementation code and pre-trained weights are released for public access (Code link: https://github.com/greentreeys/MIL-VT).</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000712019200005</dc:coverage>
        <bib:pages>45-54</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>MEDICAL IMAGE COMPUTING AND COMPUTER ASSISTED INTERVENTION - MICCAI 2021, PT VIII</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1424">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_543">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;88&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;90&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;19&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_546">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2168-2194"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gao</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alawad</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Young</foaf:surname>
                        <foaf:givenName>MT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gounley</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schaefferkoetter</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yoon</foaf:surname>
                        <foaf:givenName>HJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>XC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Durbin</foaf:surname>
                        <foaf:givenName>EB</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Doherty</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stroup</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Coyle</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tourassi</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1420"/>
        <dcterms:isReferencedBy rdf:resource="#item_547"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>Biological system modeling</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>Data models</dc:subject>
        <dc:subject>neural networks</dc:subject>
        <dc:subject>Adaptation models</dc:subject>
        <dc:subject>Bit error rate</dc:subject>
        <dc:subject>Cancer</dc:subject>
        <dc:subject>clinical text</dc:subject>
        <dc:subject>MIMICs</dc:subject>
        <dc:subject>text classification</dc:subject>
        <dc:title>Limitations of Transformers on Clinical Text Classification</dc:title>
        <dcterms:abstract>Bidirectional Encoder Representations from Transformers (BERT) and BERT-based approaches are the current state-of-the-art in many natural language processing (NLP) tasks; however, their application to document classification on long clinical texts is limited. In this work, we introduce four methods to scale BERT, which by default can only handle input sequences up to approximately 400 words long, to perform document classification on clinical texts several thousand words long. We compare these methods against two much simpler architectures - a word-level convolutional neural network and a hierarchical self-attention network - and show that BERT often cannot beat these simpler baselines when classifying MIMIC-III discharge summaries and SEER cancer pathology reports. In our analysis, we show that two key components of BERT - pretraining and WordPiece tokenization - may actually be inhibiting BERT's performance on clinical text classification tasks where the input document is several thousand words long and where correctly identifying labels may depend more on identifying a few key words or phrases rather than understanding the contextual meaning of sequences of text.</dcterms:abstract>
        <dc:date>2021 SEP</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000692596400038</dc:coverage>
        <bib:pages>3596-3607</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2168-2194">
        <prism:volume>25</prism:volume>
        <dc:title>IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS</dc:title>
        <dc:identifier>DOI 10.1109/JBHI.2021.3062322</dc:identifier>
        <prism:number>9</prism:number>
        <dc:identifier>ISSN 2168-2194</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1420">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_547">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;75&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;76&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;44&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-954085-52-7">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-954085-52-7</dc:identifier>
                <dc:title>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</dc:title>
                <dc:identifier>DOI 10.18653/v1/2021.acl-long.197</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Online</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Association for Computational Linguistics</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yoo</foaf:surname>
                        <foaf:givenName>KM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>SG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1745"/>
        <dcterms:isReferencedBy rdf:resource="#item_549"/>
        <link:link rdf:resource="#item_1744"/>
        <dc:title>Self-Guided Contrastive Learning for BERT Sentence Representations</dc:title>
        <dcterms:abstract>Although BERT and its variants have reshaped the NLP landscape, it still remains unclear how best to derive sentence embeddings from such pre-trained Transformers. In this work, we propose a contrastive learning method that utilizes self-guidance for improving the quality of BERT sentence representations. Our method fine-tunes BERT in a self-supervised fashion, does not rely on data augmentation, and enables the usual [CLS] token embeddings to function as sentence vectors. Moreover, we redesign the contrastive learning objective (NT-Xent) and apply it to sentence representation learning. We demonstrate with extensive experiments that our approach is more effective than competitive baselines on diverse sentence-related tasks. We also show it is efficient at inference and robust to domain shifts.</dcterms:abstract>
        <dc:date>2021-08</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000698663100197</dc:coverage>
        <z:libraryCatalog>ACLWeb</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://aclanthology.org/2021.acl-long.197/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <bib:pages>2528-2540</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1745">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_549">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;75&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;79&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;46&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1744">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1744/Kim et al. - 2021 - Self-Guided Contrastive Learning for BERT Sentence Representations.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://aclanthology.org/2021.acl-long.197.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 13:21:13</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-8037-9">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-8037-9</dc:identifier>
                <dc:title>Max Planck Society</dc:title>
                <dc:identifier>DOI 10.1145/3404835.3462812</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yates</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nogueira</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>ASSOC COMP MACHINERY</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1388"/>
        <dcterms:isReferencedBy rdf:resource="#item_551"/>
        <dc:subject>Learned Dense Representations</dc:subject>
        <dc:subject>Multi-Stage Ranking</dc:subject>
        <dc:title>Pretrained Transformers for Text Ranking: BERT and Beyond</dc:title>
        <dcterms:abstract>The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This tutorial, based on a forthcoming book, provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has, without exaggeration, revolutionized the fields of natural language processing (NLP), information retrieval (IR), and beyond. We provide a synthesis of existing work as a single point of entry for both researchers and practitioners. Our coverage is grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly. Two themes pervade our treatment: techniques for handling long documents and techniques for addressing the tradeoff between effectiveness (result quality) and efficiency (query latency). Although transformer architectures and pretraining techniques are recent innovations, many aspects of their application are well understood. Nevertheless, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, we also attempt to prognosticate the future.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000719807900363</dc:coverage>
        <bib:pages>2666-2668</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1388">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_551">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;74&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;75&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;42&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-8458-2">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-8458-2</dc:identifier>
                <dc:title>Facebook Inc</dc:title>
                <dc:identifier>DOI 10.1145/3460231.3474255</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Moreira</foaf:surname>
                        <foaf:givenName>GDP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rabhi</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>JM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ak</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oldridge</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>ACM</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1377"/>
        <dcterms:isReferencedBy rdf:resource="#item_553"/>
        <dc:title>Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation</dc:title>
        <dcterms:abstract>Much of the recent progress in sequential and session-based recommendation has been driven by improvements in model architecture and pretraining techniques originating in the field of Natural Language Processing. Transformer architectures in particular have facilitated building higher-capacity models and provided data augmentation and training techniques which demonstrably improve the effectiveness of sequential recommendation. But with a thousandfold more research going on in NLP, the application of transformers for recommendation understandably lags behind. To remedy this we introduce Transformers4Rec, an open-source library built upon HuggingFace's Transformers library with a similar goal of opening up the advances of NLP based Transformers to the recommender system community and making these advancements immediately accessible for the tasks of sequential and session-based recommendation. Like its core dependency, Transformers4Rec is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments.
In order to demonstrate the usefulness of the library and the applicability of Transformer architectures in next-click prediction for user sessions, where sequence lengths are much shorter than those commonly found in NLP, we have leveraged Transformers4Rec to win two recent session-based recommendation competitions. In addition, we present in this paper the first comprehensive empirical analysis comparing many Transformer architectures and training approaches for the task of session-based recommendation. We demonstrate that the best Transformer architectures have superior performance across two e-commerce datasets while performing similarly to the baselines on two news datasets. We further evaluate in isolation the effectiveness of the different training techniques used in causal language modeling, masked language modeling, permutation language modeling and replacement token detection for a single Transformer architecture, XLNet. We establish that training XLNet with replacement token detection performs well across all datasets. Finally, we explore techniques to include side information such as item and user context features in order to establish best practices and show that the inclusion of side information uniformly improves recommendation performance.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000744461300015</dc:coverage>
        <bib:pages>143-153</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>15TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS 2021)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1377">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_553">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;72&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;75&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;67&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:2472-6737">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 2472-6737</dc:identifier>
                <dc:title>Nvidia Corporation</dc:title>
                <dc:identifier>DOI 10.1109/WACV51458.2022.00181</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hatamizadeh</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>YC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nath</foaf:surname>
                        <foaf:givenName>V</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Myronenko</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Landman</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roth</foaf:surname>
                        <foaf:givenName>HR</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>DG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>IEEE Comp Soc</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1402"/>
        <dcterms:isReferencedBy rdf:resource="#item_555"/>
        <dc:title>UNETR: Transformers for 3D Medical Image Segmentation</dc:title>
        <dcterms:abstract>Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful &quot;U-shaped&quot; network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art peiformarce on the BTCV leaderboard.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000800471201080</dc:coverage>
        <bib:pages>1748-1758</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV 2022)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1402">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_555">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;1036&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;1083&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;54&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_556">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>12962</prism:volume>
                <dc:identifier>ISBN 0302-9743</dc:identifier>
                <dc:title>Nvidia Corporation</dc:title>
                <dc:identifier>DOI 10.1007/978-3-031-08999-2_22</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hatamizadeh</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nath</foaf:surname>
                        <foaf:givenName>V</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>YC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roth</foaf:surname>
                        <foaf:givenName>HR</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>DG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Crimi</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bakas</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1384"/>
        <dcterms:isReferencedBy rdf:resource="#item_557"/>
        <dc:subject>CLASSIFICATION</dc:subject>
        <dc:subject>Brain tumor segmentation</dc:subject>
        <dc:subject>BRATS</dc:subject>
        <dc:subject>Image segmentation</dc:subject>
        <dc:subject>Swin transformer</dc:subject>
        <dc:subject>Swin UNETR</dc:subject>
        <dc:subject>UNETR</dc:subject>
        <dc:subject>Vision transformer</dc:subject>
        <dc:title>Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images</dc:title>
        <dcterms:abstract>Semantic segmentation of brain tumors is a fundamental medical image analysis task involving multiple MRI imaging modalities that can assist clinicians in diagnosing the patient and successively studying the progression of the malignant entity. In recent years, Fully Convolutional Neural Networks (FCNNs) approaches have become the de facto standard for 3D medical image segmentation. The popular &quot;U-shaped&quot; network architecture has achieved state-of-the-art performance benchmarks on different 2D and 3D semantic segmentation tasks and across various imaging modalities. However, due to the limited kernel size of convolution layers in FCNNs, their performance of modeling long-range information is sub-optimal, and this can lead to deficiencies in the segmentation of tumors with variable sizes. On the other hand, transformer models have demonstrated excellent capabilities in capturing such long-range information in multiple domains, including natural language processing and computer vision. Inspired by the success of vision transformers and their variants, we propose a novel segmentation model termed Swin UNEt TRansformers (Swin UNETR). Specifically, the task of 3D brain tumor semantic segmentation is reformulated as a sequence to sequence prediction problem wherein multi-modal input data is projected into a 1D sequence of embedding and used as an input to a hierarchical Swin transformer as the encoder. The swin transformer encoder extracts features at five different resolutions by utilizing shifted windows for computing self-attention and is connected to an FCNN-based decoder at each resolution via skip connections. We have participated in BraTS 2021 segmentation challenge, and our proposed model ranks among the top-performing approaches in the validation phase.
Code: https://monai.io/research/swin-unetr.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000878434800022</dc:coverage>
        <bib:pages>272-284</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>BRAINLESION: GLIOMA, MULTIPLE SCLEROSIS, STROKE AND TRAUMATIC BRAIN INJURIES, BRAINLES 2021, PT I</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1384">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_557">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;463&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;475&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;41&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_558">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>13662</prism:volume>
                <dc:identifier>ISBN 0302-9743</dc:identifier>
                <dc:title>Peking University</dc:title>
                <dc:identifier>DOI 10.1007/978-3-031-20086-1_35</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pang</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>W</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tay</foaf:surname>
                        <foaf:givenName>FEH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>W</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yuan</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Avidan</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brostow</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cisse</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Farinella</foaf:surname>
                        <foaf:givenName>GM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hassner</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1734"/>
        <dcterms:isReferencedBy rdf:resource="#item_559"/>
        <dc:subject>NETWORK</dc:subject>
        <dc:title>Masked Autoencoders for Point Cloud Self-supervised Learning</dc:title>
        <dcterms:abstract>As a promising scheme of self-supervised learning, masked autoencoding has significantly advanced natural language processing and computer vision. Inspired by this, we propose a neat scheme of masked autoencoders for point cloud self-supervised learning, addressing the challenges posed by point cloud's properties, including leakage of location information and uneven information density. Concretely, we divide the input point cloud into irregular point patches and randomly mask them at a high ratio. Then, a standard Transformer based autoencoder, with an asymmetric design and a shifting mask tokens operation, learns high-level latent features from unmasked point patches, aiming to reconstruct the masked point patches. Extensive experiments show that our approach is efficient during pre-training and generalizes well on various downstream tasks. The pre-trained models achieve 85.18% accuracy on ScanObjectNN and 94.04% accuracy on ModelNet40, outperforming all the other self-supervised learning methods. We show with our scheme, a simple architecture entirely based on standard Transformers can surpass dedicated Transformer models from supervised learning. Our approach also advances state-of-the-art accuracies by 1.5%-2.3% in the few-shot classification. Furthermore, our work inspires the feasibility of applying unified architectures from languages and images to the point cloud. Codes are available at https://github.com/Pang-Yatian/Point-MAE.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000899248700035</dc:coverage>
        <bib:pages>604-621</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>COMPUTER VISION - ECCV 2022, PT II</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1734">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_559">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;144&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;147&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;61&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_560">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0360-1315"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>SQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Z</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Peng</foaf:surname>
                        <foaf:givenName>X</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>ZK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1299"/>
        <dcterms:isReferencedBy rdf:resource="#item_561"/>
        <dc:subject>21st century abilities</dc:subject>
        <dc:subject>AFFECTIVE STATES</dc:subject>
        <dc:subject>Cooperative/collaborative learning</dc:subject>
        <dc:subject>Data science applications in education</dc:subject>
        <dc:subject>Distance education and online learning</dc:subject>
        <dc:subject>Evaluation methodologies</dc:subject>
        <dc:subject>ONLINE</dc:subject>
        <dc:subject>RELATIVE INCIDENCE</dc:subject>
        <dc:title>Automated detection of emotional and cognitive engagement in MOOC discussions to predict learning achievement</dc:title>
        <dcterms:abstract>In the MOOC forum discussions, emotional and cognitive engagement are two prominent aspects of learning engagement. Moreover, emotional and cognitive engagement have an interactive relationship and can jointly predict learning achievement. However, these interwoven relation-ships have not been thoroughly explored. Furthermore, the limitations on detection methods for emotional and cognitive engagement have hindered the practice and theory progress. This study aimed to develop a novel text classification model to automatically detect emotional and cognitive engagement and investigate their complex relationships with achievement, which are beneficial for improving learning engagement and historically low completion rates of MOOCs. Firstly, this study proposed a robust and interpretable NLP model called the bidirectional encoder representation from the transformers-convolutional neural network (BERT-CNN). Compared with models in previous studies, it improved the F1 values of emotional and cognitive engagement recognition tasks by 10% and 8%, respectively. Secondly, this study used BERT-CNN to analyze 8867 learners' discussions in a MOOC forum. Structural equation modeling indicated that emotional and cognitive engagement have an interactive relationship and a combined effect on learning achievement. Specifically, positive and confused emotions contributed more to higher -level cognition than negative emotions. Co-occurring emotion and cognition indicators jointly predicted learning achievement with higher reliability. In summary, this study has significant methodological implications for the automated measurement of emotional and cognitive engagement. Moreover, the study revealed the dominant role of emotional engagement on cognitive engagement and provided suggestions for improving MOOC learners' achievement.</dcterms:abstract>
        <dc:date>2022 MAY</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000793263700008</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0360-1315">
        <prism:volume>181</prism:volume>
        <dc:title>COMPUTERS &amp; EDUCATION</dc:title>
        <dc:identifier>DOI 10.1016/j.compedu.2022.104461</dc:identifier>
        <dc:identifier>ISSN 0360-1315</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1299">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_561">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;111&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;111&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;63&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_562">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1674-733X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liang</foaf:surname>
                        <foaf:givenName>DK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>XW</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>W</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bai</foaf:surname>
                        <foaf:givenName>X</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1400"/>
        <dcterms:isReferencedBy rdf:resource="#item_563"/>
        <dc:subject>crowd analysis</dc:subject>
        <dc:subject>crowd counting</dc:subject>
        <dc:subject>SCALE</dc:subject>
        <dc:subject>transformer</dc:subject>
        <dc:subject>visual transformer</dc:subject>
        <dc:subject>weakly supervised</dc:subject>
        <dc:title>TransCrowd: weakly-supervised crowd counting with transformers</dc:title>
        <dcterms:abstract>The mainstream crowd counting methods usually utilize the convolution neural network (CNN) to regress a density map, requiring point-level annotations. However, annotating each person with a point is an expensive and laborious process. During the testing phase, the point-level annotations are not considered to evaluate the counting accuracy, which means the point-level annotations are redundant. Hence, it is desirable to develop weakly-supervised counting methods that just rely on count-level annotations, a more economical way of labeling. Current weakly-supervised counting methods adopt the CNN to regress a total count of the crowd by an image-to-count paradigm. However, having limited receptive fields for context modeling is an intrinsic limitation of these weakly-supervised CNN-based methods. These methods thus cannot achieve satisfactory performance, with limited applications in the real world. The transformer is a popular sequence-to-sequence prediction model in natural language processing (NLP), which contains a global receptive field. In this paper, we propose TransCrowd, which reformulates the weakly-supervised crowd counting problem from the perspective of sequence-to-count based on transformers. We observe that the proposed TransCrowd can effectively extract the semantic crowd information by using the self-attention mechanism of transformer. To the best of our knowledge, this is the first work to adopt a pure transformer for crowd counting research. Experiments on five benchmark datasets demonstrate that the proposed TransCrowd achieves superior performance compared with all the weakly-supervised CNN-based counting methods and gains highly competitive counting performance compared with some popular fully-supervised counting methods.</dcterms:abstract>
        <dc:date>2022 JUN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000789816900002</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1674-733X">
        <prism:volume>65</prism:volume>
        <dc:title>SCIENCE CHINA-INFORMATION SCIENCES</dc:title>
        <dc:identifier>DOI 10.1007/s11432-021-3445-y</dc:identifier>
        <prism:number>6</prism:number>
        <dc:identifier>ISSN 1674-733X</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1400">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_563">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;101&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;107&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;70&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_564">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1532-0464"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kalyan</foaf:surname>
                        <foaf:givenName>KS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rajasekharan</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sangeetha</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1295"/>
        <dcterms:isReferencedBy rdf:resource="#item_565"/>
        <dc:subject>EXTRACTION</dc:subject>
        <dc:subject>RECOGNITION</dc:subject>
        <dc:subject>CORPUS</dc:subject>
        <dc:subject>Transformers</dc:subject>
        <dc:subject>BioBERT</dc:subject>
        <dc:subject>Biomedical pretrained language models</dc:subject>
        <dc:subject>DOMAIN</dc:subject>
        <dc:subject>ELECTRONIC HEALTH RECORDS</dc:subject>
        <dc:subject>PubMedBERT</dc:subject>
        <dc:subject>Self-supervised learning</dc:subject>
        <dc:subject>Survey</dc:subject>
        <dc:title>AMMU: A survey of transformer-based biomedical pretrained language models</dc:title>
        <dcterms:abstract>Transformer-based pretrained language models (PLMs) have started a new era in modern natural language processing (NLP). These models combine the power of transformers, transfer learning, and self-supervised learning (SSL). Following the success of these models in the general domain, the biomedical research commu-nity has developed various in-domain PLMs starting from BioBERT to the latest BioELECTRA and BioALBERT models. We strongly believe there is a need for a survey paper that can provide a comprehensive survey of various transformer-based biomedical pretrained language models (BPLMs). In this survey, we start with a brief overview of foundational concepts like self-supervised learning, embedding layer and transformer encoder layers. We discuss core concepts of transformer-based PLMs like pretraining methods, pretraining tasks, fine-tuning methods, and various embedding types specific to biomedical domain. We introduce a taxonomy for transformer-based BPLMs and then discuss all the models. We discuss various challenges and present possible solutions. We conclude by highlighting some of the open issues which will drive the research community to further improve transformer-based BPLMs. The list of all the publicly available transformer-based BPLMs along with their links is provided at https://mr-nlp.github.io/posts/2021/05/transformer-based-biomedical-pretra ined-language-models-list/.</dcterms:abstract>
        <dc:date>2022 FEB</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000742136900002</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1532-0464">
        <prism:volume>126</prism:volume>
        <dc:title>JOURNAL OF BIOMEDICAL INFORMATICS</dc:title>
        <dc:identifier>DOI 10.1016/j.jbi.2021.103982</dc:identifier>
        <dc:identifier>ISSN 1532-0464</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1295">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_565">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;68&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;68&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;215&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_566">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1198-0052"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tummala</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kadry</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bukhari</foaf:surname>
                        <foaf:givenName>SAC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rauf</foaf:surname>
                        <foaf:givenName>HT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1309"/>
        <dcterms:isReferencedBy rdf:resource="#item_567"/>
        <dc:subject>diagnosis</dc:subject>
        <dc:subject>brain tumor</dc:subject>
        <dc:subject>MRI</dc:subject>
        <dc:subject>vision transformer</dc:subject>
        <dc:title>Classification of Brain Tumor from Magnetic Resonance Imaging Using Vision Transformers Ensembling</dc:title>
        <dcterms:abstract>The automated classification of brain tumors plays an important role in supporting radiologists in decision making. Recently, vision transformer (ViT)-based deep neural network architectures have gained attention in the computer vision research domain owing to the tremendous success of transformer models in natural language processing. Hence, in this study, the ability of an ensemble of standard ViT models for the diagnosis of brain tumors from T1-weighted (T1w) magnetic resonance imaging (MRI) is investigated. Pretrained and finetuned ViT models (B/16, B/32, L/16, and L/32) on ImageNet were adopted for the classification task. A brain tumor dataset from figshare, consisting of 3064 T1w contrast-enhanced (CE) MRI slices with meningiomas, gliomas, and pituitary tumors, was used for the cross-validation and testing of the ensemble ViT model's ability to perform a three-class classification task. The best individual model was L/32, with an overall test accuracy of 98.2% at 384 x 384 resolution. The ensemble of all four ViT models demonstrated an overall testing accuracy of 98.7% at the same resolution, outperforming individual model's ability at both resolutions and their ensembling at 224 x 224 resolution. In conclusion, an ensemble of ViT models could be deployed for the computer-aided diagnosis of brain tumors based on T1w CE MRI, leading to radiologist relief.</dcterms:abstract>
        <dc:date>2022 OCT</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000872561600001</dc:coverage>
        <bib:pages>7498-7511</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1198-0052">
        <prism:volume>29</prism:volume>
        <dc:title>CURRENT ONCOLOGY</dc:title>
        <dc:identifier>DOI 10.3390/curroncol29100590</dc:identifier>
        <prism:number>10</prism:number>
        <dc:identifier>ISSN 1198-0052</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1309">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_567">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;67&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;69&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;50&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_568">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2522-5839"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ferruz</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Höcker</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1318"/>
        <dcterms:isReferencedBy rdf:resource="#item_569"/>
        <dc:subject>EVOLUTION</dc:subject>
        <dc:subject>MUTATIONS</dc:subject>
        <dc:title>Controllable protein design with language models</dc:title>
        <dcterms:abstract>Both proteins and natural language are essentially based on a sequential code, but feature complex interactions at multiple scales, which can be useful when transferring machine learning models from one domain to another. In this Review, Ferruz and Hocker summarize recent advances in language models, such as transformers, and their application to protein design.
The twenty-first century is presenting humankind with unprecedented environmental and medical challenges. The ability to design novel proteins tailored for specific purposes would potentially transform our ability to respond to these issues in a timely manner. Recent advances in the field of artificial intelligence are now setting the stage to make this goal achievable. Protein sequences are inherently similar to natural languages: amino acids arrange in a multitude of combinations to form structures that carry function, the same way as letters form words and sentences carry meaning. Accordingly, it is not surprising that, throughout the history of natural language processing (NLP), many of its techniques have been applied to protein research problems. In the past few years we have witnessed revolutionary breakthroughs in the field of NLP. The implementation of transformer pre-trained models has enabled text generation with human-like capabilities, including texts with specific properties such as style or subject. Motivated by its considerable success in NLP tasks, we expect dedicated transformers to dominate custom protein sequence generation in the near future. Fine-tuning pre-trained models on protein families will enable the extension of their repertoires with novel sequences that could be highly divergent but still potentially functional. The combination of control tags such as cellular compartment or function will further enable the controllable design of novel protein functions. Moreover, recent model interpretability methods will allow us to open the 'black box' and thus enhance our understanding of folding principles. Early initiatives show the enormous potential of generative language models to design functional sequences. We believe that using generative text models to create novel proteins is a promising and largely unexplored field, and we discuss its foreseeable impact on protein design.</dcterms:abstract>
        <dc:date>2022 JUN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000814485300002</dc:coverage>
        <bib:pages>521-532</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2522-5839">
        <prism:volume>4</prism:volume>
        <dc:title>NATURE MACHINE INTELLIGENCE</dc:title>
        <dc:identifier>DOI 10.1038/s42256-022-00499-z</dc:identifier>
        <prism:number>6</prism:number>
        <dc:identifier>ISSN 2522-5839</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1318">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_569">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;67&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;73&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;98&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_570">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0893-6080"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Geneva</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zabaras</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1378"/>
        <dcterms:isReferencedBy rdf:resource="#item_571"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>NEURAL-NETWORK</dc:subject>
        <dc:subject>Transformers</dc:subject>
        <dc:subject>Self-attention</dc:subject>
        <dc:subject>DYNAMICAL-SYSTEMS</dc:subject>
        <dc:subject>EFFICIENT</dc:subject>
        <dc:subject>ENCODER-DECODER NETWORKS</dc:subject>
        <dc:subject>GAUSSIAN PROCESS</dc:subject>
        <dc:subject>IDENTIFICATION</dc:subject>
        <dc:subject>Koopman</dc:subject>
        <dc:subject>Physics</dc:subject>
        <dc:subject>SHORT-TERM-MEMORY</dc:subject>
        <dc:subject>Surrogate modeling</dc:subject>
        <dc:title>Transformers for modeling physical systems</dc:title>
        <dcterms:abstract>Transformers are widely used in natural language processing due to their ability to model longer-term dependencies in text. Although these models achieve state-of-the-art performance for many language related tasks, their applicability outside of the natural language processing field has been minimal. In this work, we propose the use of transformer models for the prediction of dynamical systems representative of physical phenomena. The use of Koopman based embeddings provides a unique and powerful method for projecting any dynamical system into a vector representation which can then be predicted by a transformer. The proposed model is able to accurately predict various dynamical systems and outperform classical methods that are commonly used in the scientific machine learning literature. (C) 2021 Elsevier Ltd. All rights reserved.</dcterms:abstract>
        <dc:date>2022 FEB</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000799116300009</dc:coverage>
        <bib:pages>272-289</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0893-6080">
        <prism:volume>146</prism:volume>
        <dc:title>NEURAL NETWORKS</dc:title>
        <dc:identifier>DOI 10.1016/j.neunet.2021.11.022</dc:identifier>
        <dc:identifier>ISSN 0893-6080</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1378">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_571">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;67&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;73&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;73&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_572">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0196-2892"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Meng</foaf:surname>
                        <foaf:givenName>XC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shao</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>ST</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1373"/>
        <dcterms:isReferencedBy rdf:resource="#item_573"/>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>QUALITY</dc:subject>
        <dc:subject>Training</dc:subject>
        <dc:subject>Transformers</dc:subject>
        <dc:subject>NETWORK</dc:subject>
        <dc:subject>transformer</dc:subject>
        <dc:subject>Deep learning (DL)</dc:subject>
        <dc:subject>FUSION</dc:subject>
        <dc:subject>IMAGES</dc:subject>
        <dc:subject>Lenses</dc:subject>
        <dc:subject>pansharpening</dc:subject>
        <dc:subject>Pansharpening</dc:subject>
        <dc:subject>self-attention</dc:subject>
        <dc:subject>Standards</dc:subject>
        <dc:subject>Wavelet transforms</dc:subject>
        <dc:title>Vision Transformer for Pansharpening</dc:title>
        <dcterms:abstract>Pansharpening is a fundamental and hot-spot research topic in remote sensing image fusion. In recent years, self-attention-based transformer has attracted considerable attention in natural language processing (NLP) and introduced to attend to computer vision (CV) tasks. Inspired by great success of the vision transformer (ViT) in image classification, we propose an improved and advanced purely transformer-based model for pansharpening. In the proposed method, stacked multispectral (MS) and panchromatic (PAN) images are cropped into patches (i.e., tokens), and after a three-layer self-attention-based encoder, these tokens contain rich information. After upsampled and stitched, a high spatial resolution (HR) MS image is finally obtained. Instead of convolutional neural networks (CNNs) pursuing a short-distance dependency, our proposed method aims to build up a long-distance dependency, to make full use of more useful features. The experiments were conducted on an opening benchmark dataset, including IKONOS with four-band MS/PAN images and WorldView-2 MS images featured by eight bands. In addition, the experiments were performed on reduced and full-resolution datasets from both qualitative and quantitative evaluation aspects. The experimental results indicate the competitive performance of the proposed model than other pansharpening methods, including the state-of-the-art pansharpening algorithms based on CNN.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000790844500016</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0196-2892">
        <prism:volume>60</prism:volume>
        <dc:title>IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING</dc:title>
        <dc:identifier>DOI 10.1109/TGRS.2022.3168465</dc:identifier>
        <dc:identifier>ISSN 0196-2892</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1373">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_573">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;66&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;67&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;65&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_574">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0162-8828"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>YH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>HT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>XH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>JY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>ZH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>YH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xiao</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>CJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>YX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>ZH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>YM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tao</foaf:surname>
                        <foaf:givenName>DC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1291"/>
        <dcterms:isReferencedBy rdf:resource="#item_575"/>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>Object detection</dc:subject>
        <dc:subject>Computational modeling</dc:subject>
        <dc:subject>Transformers</dc:subject>
        <dc:subject>transformer</dc:subject>
        <dc:subject>self-attention</dc:subject>
        <dc:subject>Computer vision</dc:subject>
        <dc:subject>Encoding</dc:subject>
        <dc:subject>high-level vision</dc:subject>
        <dc:subject>low-level vision</dc:subject>
        <dc:subject>video</dc:subject>
        <dc:subject>Visualization</dc:subject>
        <dc:title>A Survey on Vision Transformer</dc:title>
        <dcterms:abstract>Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.</dcterms:abstract>
        <dc:date>2023 JAN 1</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000899419900006</dc:coverage>
        <bib:pages>87-110</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0162-8828">
        <prism:volume>45</prism:volume>
        <dc:title>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</dc:title>
        <dc:identifier>DOI 10.1109/TPAMI.2022.3152247</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 0162-8828</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1291">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_575">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;1295&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;1372&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;245&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_576">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>45</prism:volume>
                <dc:title>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</dc:title>
                <dc:identifier>DOI 10.1109/TPAMI.2022.3206148</dc:identifier>
                <prism:number>4</prism:number>
                <dc:identifier>ISSN 0162-8828</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Touvron</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bojanowski</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Caron</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cord</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>El-Nouby</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grave</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Izacard</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Joulin</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Synnaeve</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Verbeek</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jégou</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1394"/>
        <dcterms:isReferencedBy rdf:resource="#item_577"/>
        <dc:subject>NLP</dc:subject>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>Training</dc:subject>
        <dc:subject>Transformers</dc:subject>
        <dc:subject>Computer architecture</dc:subject>
        <dc:subject>computer-vision</dc:subject>
        <dc:subject>Decoding</dc:subject>
        <dc:subject>Knowledge engineering</dc:subject>
        <dc:subject>Machine translation</dc:subject>
        <dc:subject>Multi-layer perceptron</dc:subject>
        <dc:title>ResMLP: Feedforward Networks for Image Classification With Data-Efficient Training</dc:title>
        <dcterms:abstract>We present ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity trade-offs on ImageNet. We also train ResMLP models in a self-supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre-trained models and our code based on the Timm library.</dcterms:abstract>
        <dc:date>2023 APR 1</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000947840300085</dc:coverage>
        <bib:pages>5314-5321</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1394">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_577">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;202&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;210&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;72&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_578">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2162-237X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>YX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hou</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yuan</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shi</foaf:surname>
                        <foaf:givenName>ZC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fan</foaf:surname>
                        <foaf:givenName>JP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>ZQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1290"/>
        <dcterms:isReferencedBy rdf:resource="#item_579"/>
        <dc:subject>Classification</dc:subject>
        <dc:subject>computer vision (CV)</dc:subject>
        <dc:subject>DEEP</dc:subject>
        <dc:subject>detection</dc:subject>
        <dc:subject>LANGUAGE</dc:subject>
        <dc:subject>point clouds</dc:subject>
        <dc:subject>segmentation</dc:subject>
        <dc:subject>self-supervision</dc:subject>
        <dc:subject>visual Transformer</dc:subject>
        <dc:subject>visual-linguistic pretraining</dc:subject>
        <dc:title>A Survey of Visual Transformers</dc:title>
        <dcterms:abstract>Transformer, an attention-based encoder-decoder model, has already revolutionized the field of natural language processing (NLP). Inspired by such significant achievements, some pioneering works have recently been done on employing Transformer-liked architectures in the computer vision (CV) field, which have demonstrated their effectiveness on three fundamental CV tasks (classification, detection, and segmentation) as well as multiple sensory data stream (images, point clouds, and vision-language data). Because of their competitive modeling capabilities, the visual Transformers have achieved impressive performance improvements over multiple benchmarks as compared with modern Convolution Neural Networks (CNNs). In this survey, we have reviewed over one hundred of different visual Transformers comprehensively according to three fundamental CV tasks and different data stream types, where a taxonomy is proposed to organize the representative methods according to their motivations, structures, and application scenarios. Because of their differences on training settings and dedicated vision tasks, we have also evaluated and compared all these existing visual Transformers under different configurations. Furthermore, we have revealed a series of essential but unexploited aspects that may empower such visual Transformers to stand out from numerous architectures, e.g., slack high-level semantic embeddings to bridge the gap between the visual Transformers and the sequential ones. Finally, three promising research directions are suggested for future investment. We will continue to update the latest articles and their released source codes at https://github.com/liuyang-ict/awesome-visual-transformers.</dcterms:abstract>
        <dc:date>2024 JUN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000967379600001</dc:coverage>
        <bib:pages>7478-7498</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2162-237X">
        <prism:volume>35</prism:volume>
        <dc:title>IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</dc:title>
        <dc:identifier>DOI 10.1109/TNNLS.2022.3227717</dc:identifier>
        <prism:number>6</prism:number>
        <dc:identifier>ISSN 2162-237X</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1290">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_579">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;133&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;133&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;242&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_580">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2076-3417"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maurício</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Domingues</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bernardino</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1315"/>
        <dcterms:isReferencedBy rdf:resource="#item_581"/>
        <dc:subject>convolutional neural networks</dc:subject>
        <dc:subject>transformers</dc:subject>
        <dc:subject>image classification</dc:subject>
        <dc:subject>multi-head attention</dc:subject>
        <dc:subject>Vision Transformers (ViT)</dc:subject>
        <dc:title>Comparing Vision Transformers and Convolutional Neural Networks for Image Classification: A Literature Review</dc:title>
        <dcterms:abstract>Transformers are models that implement a mechanism of self-attention, individually weighting the importance of each part of the input data. Their use in image classification tasks is still somewhat limited since researchers have so far chosen Convolutional Neural Networks for image classification and transformers were more targeted to Natural Language Processing (NLP) tasks. Therefore, this paper presents a literature review that shows the differences between Vision Transformers (ViT) and Convolutional Neural Networks. The state of the art that used the two architectures for image classification was reviewed and an attempt was made to understand what factors may influence the performance of the two deep learning architectures based on the datasets used, image size, number of target classes (for the classification problems), hardware, and evaluated architectures and top results. The objective of this work is to identify which of the architectures is the best for image classification and under what conditions. This paper also describes the importance of the Multi-Head Attention mechanism for improving the performance of ViT in image classification.</dcterms:abstract>
        <dc:date>2023 APR 28</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000986981100001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2076-3417">
        <prism:volume>13</prism:volume>
        <dc:title>APPLIED SCIENCES-BASEL</dc:title>
        <dc:identifier>DOI 10.3390/app13095521</dc:identifier>
        <prism:number>9</prism:number>
        <dc:identifier>ISSN 2076-3417</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1315">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_581">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;99&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;104&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;30&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_582">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1361-8415"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>JY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>YC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Landman</foaf:surname>
                        <foaf:givenName>BA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>SK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1362"/>
        <dcterms:isReferencedBy rdf:resource="#item_583"/>
        <dc:subject>CLASSIFICATION</dc:subject>
        <dc:subject>FRAMEWORK</dc:subject>
        <dc:subject>Survey</dc:subject>
        <dc:subject>IMAGES</dc:subject>
        <dc:subject>3D</dc:subject>
        <dc:subject>ATTENTION</dc:subject>
        <dc:subject>CONVOLUTIONAL NEURAL-NETWORKS</dc:subject>
        <dc:subject>DATABASE</dc:subject>
        <dc:subject>DATASET</dc:subject>
        <dc:subject>Medical imaging</dc:subject>
        <dc:subject>SEGMENTATION</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>U-NET</dc:subject>
        <dc:title>Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives</dc:title>
        <dcterms:abstract>Transformer, one of the latest technological advances of deep learning, has gained prevalence in natural language processing or computer vision. Since medical imaging bear some resemblance to computer vision, it is natural to inquire about the status quo of Transformers in medical imaging and ask the question: can the Transformer models transform medical imaging? In this paper, we attempt to make a response to the inquiry. After a brief introduction of the fundamentals of Transformers, especially in comparison with convolutional neural networks (CNNs), and highlighting key defining properties that characterize the Transformers, we offer a comprehensive review of the state-of-the-art Transformer-based approaches for medical imaging and exhibit current research progresses made in the areas of medical image segmentation, recognition, detection, registration, reconstruction, enhancement, etc. In particular, what distinguishes our review lies in its organization based on the Transformer's key defining properties, which are mostly derived from comparing the Transformer and CNN, and its type of architecture, which specifies the manner in which the Transformer and CNN are combined, all helping the readers to best understand the rationale behind the reviewed approaches. We conclude with discussions of future perspectives.</dcterms:abstract>
        <dc:date>2023 APR</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000993037300001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1361-8415">
        <prism:volume>85</prism:volume>
        <dc:title>MEDICAL IMAGE ANALYSIS</dc:title>
        <dc:identifier>DOI 10.1016/j.media.2023.102762</dc:identifier>
        <dc:identifier>ISSN 1361-8415</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1362">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_583">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;99&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;101&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;409&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_584">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1746-8094"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xiao</foaf:surname>
                        <foaf:givenName>HG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>QY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhu</foaf:surname>
                        <foaf:givenName>XH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>QH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1376"/>
        <dcterms:isReferencedBy rdf:resource="#item_585"/>
        <dc:subject>NETWORK</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>3D segmentation</dc:subject>
        <dc:subject>COMPUTED-TOMOGRAPHY IMAGES</dc:subject>
        <dc:subject>Medical image</dc:subject>
        <dc:subject>Segmentation analysis</dc:subject>
        <dc:title>Transformers in medical image segmentation: A review</dc:title>
        <dcterms:abstract>Background and Objectives: Transformer is a model relying entirely on self-attention which has a wide range of applications in the field of natural language processing. Researchers are beginning to focus on the transformer in medical images due to the past few years having seen the rapid development of transformer in many vision fields such as vision transformer (ViT) and Swin transformer. In the last year, moreover, many scholars have applied transformer to medical image segmentation and have achieved good segmentation results. Transformer-based medical image segmentation has become one of the hot spots in this field. The purpose of this work is to categorize and review the segmentation methods of Unet-based transformer and other model based transformer in medical images.Methods: This paper summarizes the transformer-based segmentation models in the abdominal organs, heart, brain, and lung based on the relevant studies in the last two years. We described and analyzed the model structure including the position of the transformer in the model, the changes made by scholars to transformer and the combination with the model. In this work, the segmentation performance results based on Dice evaluation metrics are compared.Results: Through the help of 93 references, we find that researchers prefer to use Unet-based transformer models than others and place the transformer structure in the encoder. These new models improve the segmentation performance compared with U-Net and other segmentation models. However, there are not many related studies on lungs, which points to a new way for future research.Conclusions: We found that the combination of U-Net and transformer is more suitable for segmentation. In future research on medical image segmentation, researchers can use a suitable transformer-based segmentation method or modify the transformer structure according to the segmentation requirements. We hope that this work will be helpful for improvements of the transformer to solve clinical problems in medicine.</dcterms:abstract>
        <dc:date>2023 JUL</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000951459100001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1746-8094">
        <prism:volume>84</prism:volume>
        <dc:title>BIOMEDICAL SIGNAL PROCESSING AND CONTROL</dc:title>
        <dc:identifier>DOI 10.1016/j.bspc.2023.104791</dc:identifier>
        <dc:identifier>ISSN 1746-8094</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1376">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_585">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;94&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;95&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;93&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_586">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2731-538X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>FL</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>DZ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>ML</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>XY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shi</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1355"/>
        <dcterms:isReferencedBy rdf:resource="#item_587"/>
        <dc:subject>transformers</dc:subject>
        <dc:subject>representation learning</dc:subject>
        <dc:subject>multimodal learning</dc:subject>
        <dc:subject>pre-training</dc:subject>
        <dc:subject>Vision and language</dc:subject>
        <dc:title>VLP: A Survey on Vision-language Pre-training</dc:title>
        <dcterms:abstract>In the past few years, the emergence of pre-training models has brought uni-modal fields such as computer vision (CV) and natural language processing (NLP) to a new era. Substantial works have shown that they are beneficial for downstream uni-modal tasks and avoid training a new model from scratch. So can such pre-trained models be applied to multi-modal tasks? Researchers have explored this problem and made significant progress. This paper surveys recent advances and new frontiers in vision-language pre-training (VLP), including image-text and video-text pre-training. To give readers a better overall grasp of VLP, we first review its recent advances in five aspects: feature extraction, model architecture, pre-training objectives, pre-training datasets, and downstream tasks. Then, we summarize the specific VLP models in detail. Finally, we discuss the new frontiers in VLP. To the best of our knowledge, this is the first survey focused on VLP. We hope that this survey can shed light on future research in the VLP field.</dcterms:abstract>
        <dc:date>2023 FEB</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000934031000003</dc:coverage>
        <bib:pages>38-56</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2731-538X">
        <prism:volume>20</prism:volume>
        <dc:title>MACHINE INTELLIGENCE RESEARCH</dc:title>
        <dc:identifier>DOI 10.1007/s11633-022-1369-5</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 2731-538X</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1355">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_587">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;66&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;71&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;156&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_588">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0263-8762"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sitapure</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kwon</foaf:surname>
                        <foaf:givenName>JSI</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1329"/>
        <dcterms:isReferencedBy rdf:resource="#item_589"/>
        <dc:subject>Natural language processing (NLP)</dc:subject>
        <dc:subject>DESIGN</dc:subject>
        <dc:subject>Attention mechanism</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>1ST-PRINCIPLES</dc:subject>
        <dc:subject>CRYSTAL SHAPE</dc:subject>
        <dc:subject>CRYSTALLIZATION</dc:subject>
        <dc:subject>Long-short-term memory (LSTM)</dc:subject>
        <dc:subject>PREDICTIVE CONTROL</dc:subject>
        <dc:subject>SIZE DISTRIBUTION</dc:subject>
        <dc:subject>Time-series prediction</dc:subject>
        <dc:subject>Transformer networks</dc:subject>
        <dc:title>Exploring the potential of time-series transformers for process modeling and control in chemical systems: An inevitable paradigm shift?</dc:title>
        <dcterms:abstract>The last two years have seen groundbreaking advances in natural language processing (NLP) with the advent of applications like ChatGPT, Codex, and ChatSonic. This revolution is powered by the development of cutting-edge transformer models that leverage multiheaded attention mechanisms, positional encoding, and highly efficient transfer learning capabilities. Despite these remarkable advances, there is still work to be done to fully realize the practical applicability of transformers in chemical systems. Thus, we are excited to present our latest work, which highlights the immense potential of transformers for non-trivial multivariate time-series prediction tasks with high-value implications in process monitoring, control, and optimization. Specifically, impressive prediction capabilities of first-generation time-series transformers (TSTs) were demonstrated by developing, testing, and comparing TSTs with existing models. Further, the practical applicability of TSTs was highlighted by developing a first-of-a-kind TST-based model predictive controller (MPC). More importantly, the current work provides a concrete foundation for exploring promising new directions, such as the development of largescale TSTs leveraging transfer learning for modeling of new process equipment, and plant-level multisource aggregative cognitive models for fault prognosis and prevention. We are excited to see what the future holds as we continue to push the boundaries of what is possible with these 'transformer-tive' technologies.(c) 2023 Institution of Chemical Engineers. Published by Elsevier Ltd. All rights reserved.</dcterms:abstract>
        <dc:date>2023 JUN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001001400000001</dc:coverage>
        <bib:pages>461-477</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0263-8762">
        <prism:volume>194</prism:volume>
        <dc:title>CHEMICAL ENGINEERING RESEARCH &amp; DESIGN</dc:title>
        <dc:identifier>DOI 10.1016/j.cherd.2023.04.028</dc:identifier>
        <dc:identifier>ISSN 0263-8762</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1329">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_589">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;58&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;58&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;74&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_590">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1424-8220"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Moutik</foaf:surname>
                        <foaf:givenName>O</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sekkat</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tigani</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chehri</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saadane</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tchakoucht</foaf:surname>
                        <foaf:givenName>TA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Paul</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1319"/>
        <dcterms:isReferencedBy rdf:resource="#item_591"/>
        <dc:subject>COMPUTER VISION</dc:subject>
        <dc:subject>convolutional neural networks</dc:subject>
        <dc:subject>recurrent neural networks</dc:subject>
        <dc:subject>ATTENTION</dc:subject>
        <dc:subject>action recognition</dc:subject>
        <dc:subject>action recognitions</dc:subject>
        <dc:subject>conversational systems</dc:subject>
        <dc:subject>natural language understanding</dc:subject>
        <dc:subject>vision transformers</dc:subject>
        <dc:title>Convolutional Neural Networks or Vision Transformers: Who Will Win the Race for Action Recognitions in Visual Data?</dc:title>
        <dcterms:abstract>Understanding actions in videos remains a significant challenge in computer vision, which has been the subject of several pieces of research in the last decades. Convolutional neural networks (CNN) are a significant component of this topic and play a crucial role in the renown of Deep Learning. Inspired by the human vision system, CNN has been applied to visual data exploitation and has solved various challenges in various computer vision tasks and video/image analysis, including action recognition (AR). However, not long ago, along with the achievement of the transformer in natural language processing (NLP), it began to set new trends in vision tasks, which has created a discussion around whether the Vision Transformer models (ViT) will replace CNN in action recognition in video clips. This paper conducts this trending topic in detail, the study of CNN and Transformer for Action Recognition separately and a comparative study of the accuracy-complexity trade-off. Finally, based on the performance analysis's outcome, the question of whether CNN or Vision Transformers will win the race will be discussed.</dcterms:abstract>
        <dc:date>2023 JAN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000916353500001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1424-8220">
        <prism:volume>23</prism:volume>
        <dc:title>SENSORS</dc:title>
        <dc:identifier>DOI 10.3390/s23020734</dc:identifier>
        <prism:number>2</prism:number>
        <dc:identifier>ISSN 1424-8220</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1319">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_591">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;45&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;47&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;118&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_592">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0957-4174"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Islam</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Elmekki</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Elsebai</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bentahar</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Drawel</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rjoub</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pedrycz</foaf:surname>
                        <foaf:givenName>W</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1365"/>
        <dcterms:isReferencedBy rdf:resource="#item_593"/>
        <dc:subject>Natural language processing (NLP)</dc:subject>
        <dc:subject>CLASSIFICATION</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>RECOGNITION</dc:subject>
        <dc:subject>NEURAL-NETWORKS</dc:subject>
        <dc:subject>Self-attention</dc:subject>
        <dc:subject>IMAGES</dc:subject>
        <dc:subject>DATABASE</dc:subject>
        <dc:subject>SEGMENTATION</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>Computer vision (CV)</dc:subject>
        <dc:subject>Multi-modality</dc:subject>
        <dc:title>A comprehensive survey on applications of transformers for deep learning tasks</dc:title>
        <dcterms:abstract>Transformers are Deep Neural Networks (DNN) that utilize a self-attention mechanism to capture contextual relationships within sequential data. Unlike traditional neural networks and variants of Recurrent Neural Networks (RNNs), such as Long Short-Term Memory (LSTM), Transformer models excel at managing long dependencies among input sequence elements and facilitate parallel processing. Consequently, Transformer -based models have garnered significant attention from researchers in the field of artificial intelligence. This is due to their tremendous potential and impressive accomplishments, which extend beyond Natural Language Processing (NLP) tasks to encompass various domains, including Computer Vision (CV), audio and speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have been published, spotlighting the Transformer's contributions in specific fields, architectural disparities, or performance assessments, there remains a notable absence of a comprehensive survey paper that encompasses its major applications across diverse domains. Therefore, this paper addresses this gap by conducting an extensive survey of proposed Transformer models spanning from 2017 to 2022. Our survey encompasses the identification of the top five application domains for Transformer-based models, namely: NLP, CV, multi -modality, audio and speech processing, and signal processing. We analyze the influence of highly impactful Transformer-based models within these domains and subsequently categorize them according to their respective tasks, employing a novel taxonomy. Our primary objective is to illuminate the existing potential and future prospects of Transformers for researchers who are passionate about this area, thereby contributing to a more comprehensive understanding of this groundbreaking technology.</dcterms:abstract>
        <dc:date>2024 MAY 1</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001125930700001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0957-4174">
        <prism:volume>241</prism:volume>
        <dc:title>EXPERT SYSTEMS WITH APPLICATIONS</dc:title>
        <dc:identifier>DOI 10.1016/j.eswa.2023.122666</dc:identifier>
        <dc:identifier>ISSN 0957-4174</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1365">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_593">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;43&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;43&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;242&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_594">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>12</prism:volume>
                <dc:title>IEEE ACCESS</dc:title>
                <dc:identifier>DOI 10.1109/ACCESS.2024.3365742</dc:identifier>
                <dc:identifier>ISSN 2169-3536</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Raiaan</foaf:surname>
                        <foaf:givenName>MAK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mukta</foaf:surname>
                        <foaf:givenName>MSH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fatema</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fahad</foaf:surname>
                        <foaf:givenName>NM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sakib</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mim</foaf:surname>
                        <foaf:givenName>MMJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ahmad</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ali</foaf:surname>
                        <foaf:givenName>ME</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Azam</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1364"/>
        <dcterms:isReferencedBy rdf:resource="#item_595"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>natural language processing (NLP)</dc:subject>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>BIAS</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Linguistics</dc:subject>
        <dc:subject>Surveys</dc:subject>
        <dc:subject>Training</dc:subject>
        <dc:subject>Transformers</dc:subject>
        <dc:subject>GPT-4</dc:subject>
        <dc:subject>transformer</dc:subject>
        <dc:subject>application</dc:subject>
        <dc:subject>Cognition</dc:subject>
        <dc:subject>Information analysis</dc:subject>
        <dc:subject>Large language models (LLM)</dc:subject>
        <dc:subject>pre-trained models</dc:subject>
        <dc:subject>Question answering (information retrieval)</dc:subject>
        <dc:subject>taxonomy</dc:subject>
        <dc:subject>Taxonomy</dc:subject>
        <dc:title>A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges</dc:title>
        <dcterms:abstract>Large Language Models (LLMs) recently demonstrated extraordinary capability in various natural language processing (NLP) tasks including language translation, text generation, question answering, etc. Moreover, LLMs are new and essential part of computerized language processing, having the ability to understand complex verbal patterns and generate coherent and appropriate replies in a given context. Though this success of LLMs has prompted a substantial increase in research contributions, rapid growth has made it difficult to understand the overall impact of these improvements. Since a plethora of research on LLMs have been appeared within a short time, it is quite impossible to track all of these and get an overview of the current state of research in this area. Consequently, the research community would benefit from a short but thorough review of the recent changes in this area. This article thoroughly overviews LLMs, including their history, architectures, transformers, resources, training methods, applications, impacts, challenges, etc. This paper begins by discussing the fundamental concepts of LLMs with its traditional pipeline of the LLMs training phase. Then the paper provides an overview of the existing works, the history of LLMs, their evolution over time, the architecture of transformers in LLMs, the different resources of LLMs, and the different training methods that have been used to train them. The paper also demonstrates the datasets utilized in the studies. After that, the paper discusses the wide range of applications of LLMs, including biomedical and healthcare, education, social, business, and agriculture. The study also illustrates how LLMs create an impact on society and shape the future of AI and how they can be used to solve real-world problems. Finally, the paper also explores open issues and challenges to deploy LLMs in real-world scenario. Our review paper aims to help practitioners, researchers, and experts thoroughly understand the evolution of LLMs, pre-trained architectures, applications, challenges, and future goals.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001173153100001</dc:coverage>
        <bib:pages>26839-26874</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1364">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_595">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;49&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;50&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;187&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_596">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>35</prism:volume>
                <dc:title>IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</dc:title>
                <dc:identifier>DOI 10.1109/TNNLS.2023.3294633</dc:identifier>
                <prism:number>11</prism:number>
                <dc:identifier>ISSN 2162-237X</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Taylor</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Joyce</foaf:surname>
                        <foaf:givenName>DW</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gao</foaf:surname>
                        <foaf:givenName>ZM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kormilitzin</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nevado-Holgado</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1310"/>
        <dcterms:isReferencedBy rdf:resource="#item_597"/>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>Training</dc:subject>
        <dc:subject>Transformers</dc:subject>
        <dc:subject>Adaptation models</dc:subject>
        <dc:subject>Bit error rate</dc:subject>
        <dc:subject>Computer architecture</dc:subject>
        <dc:subject>Clinical decision support</dc:subject>
        <dc:subject>few-shot learning</dc:subject>
        <dc:subject>pretrained language models (PLMs)</dc:subject>
        <dc:subject>prompt learning</dc:subject>
        <dc:subject>Tuning</dc:subject>
        <dc:title>Clinical Prompt Learning With Frozen Language Models</dc:title>
        <dcterms:abstract>When the first transformer-based language models were published in the late 2010s, pretraining with general text and then fine-tuning the model on a task-specific dataset often achieved the state-of-the-art performance. However, more recent work suggests that for some tasks, directly prompting the pretrained model matches or surpasses fine-tuning in performance with few or no model parameter updates required. The use of prompts with language models for natural language processing (NLP) tasks is known as prompt learning. We investigated the viability of prompt learning on clinically meaningful decision tasks and directly compared this with more traditional fine-tuning methods. Results show that prompt learning methods were able to match or surpass the performance of traditional fine-tuning with up to 1000 times fewer trainable parameters, less training time, less training data, and lower computation resource requirements. We argue that these characteristics make prompt learning a very desirable alternative to traditional fine-tuning for clinical tasks, where the computational resources of public health providers are limited, and where data can often not be made available or not be used for fine-tuning due to patient privacy concerns. The complementary code to reproduce the experiments presented in this work can be found at &lt;uri&gt;https://github.com/NtaylorOX/Public_Clinical_Prompt&lt;/uri&gt;.</dcterms:abstract>
        <dc:date>2024 NOV</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001346504100111</dc:coverage>
        <bib:pages>16453-16463</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1310">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_597">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;10&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;10&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;0&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_598">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>152</prism:volume>
                <dc:title>JOURNAL OF BIOMEDICAL INFORMATICS</dc:title>
                <dc:identifier>DOI 10.1016/j.jbi.2024.104621</dc:identifier>
                <dc:identifier>ISSN 1532-0464</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>YM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tao</foaf:surname>
                        <foaf:givenName>W</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>ZH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>ZA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fenton</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tao</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1297"/>
        <dcterms:isReferencedBy rdf:resource="#item_599"/>
        <dc:subject>Natural language processing (NLP)</dc:subject>
        <dc:subject>Adverse drug event (ADE) extraction</dc:subject>
        <dc:subject>COSTS</dc:subject>
        <dc:subject>Machine learning/Deep learning</dc:subject>
        <dc:subject>named -entity recognition (NER)</dc:subject>
        <dc:subject>Pharmacovigilance</dc:subject>
        <dc:subject>Relation extraction (RE)</dc:subject>
        <dc:title>Artificial intelligence-powered pharmacovigilance: A review of machine and deep learning in clinical text-based adverse drug event detection for benchmark datasets</dc:title>
        <dcterms:abstract>Objective: The primary objective of this review is to investigate the effectiveness of machine learning and deep learning methodologies in the context of extracting adverse drug events (ADEs) from clinical benchmark datasets. We conduct an in-depth analysis, aiming to compare the merits and drawbacks of both machine learning and deep learning techniques, particularly within the framework of named-entity recognition (NER) and relation classification (RC) tasks related to ADE extraction. Additionally, our focus extends to the examination of specific features and their impact on the overall performance of these methodologies. In a broader perspective, our research extends to ADE extraction from various sources, including biomedical literature, social media data, and drug labels, removing the limitation to exclusively machine learning or deep learning methods. Methods: We conducted an extensive literature review on PubMed using the query &quot;(((machine learning [Medical Subject Headings (MeSH) Terms]) OR (deep learning [MeSH Terms])) AND (adverse drug event [MeSH Terms])) AND (extraction)&quot;, and supplemented this with a snowballing approach to review 275 references sourced from retrieved articles. Results: In our analysis, we included twelve articles for review. For the NER task, deep learning models outperformed machine learning models. In the RC task, gradient Boosting, multilayer perceptron and random forest models excelled. The Bidirectional Encoder Representations from Transformers (BERT) model consistently achieved the best performance in the end-to-end task. Future efforts in the end-to-end task should prioritize improving NER accuracy, especially for 'ADE' and 'Reason'. Conclusion: These findings hold significant implications for advancing the field of ADE extraction and pharmacovigilance, ultimately contributing to improved drug safety monitoring and healthcare outcomes.</dcterms:abstract>
        <dc:date>2024 APR</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001211489400001</dc:coverage>
    </bib:Article>
    <bib:Memo rdf:about="#item_1297">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_599">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;9&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;9&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;43&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_600">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>12</prism:volume>
                <dc:title>IEEE ACCESS</dc:title>
                <dc:identifier>DOI 10.1109/ACCESS.2024.3349952</dc:identifier>
                <dc:identifier>ISSN 2169-3536</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fields</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chovanec</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Madiraju</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1357"/>
        <dcterms:isReferencedBy rdf:resource="#item_601"/>
        <dc:subject>NLP</dc:subject>
        <dc:subject>survey</dc:subject>
        <dc:subject>ALGORITHMS</dc:subject>
        <dc:subject>MODEL</dc:subject>
        <dc:subject>transformers</dc:subject>
        <dc:subject>text classification</dc:subject>
        <dc:title>A Survey of Text Classification With Transformers: How Wide? How Large? How Long? How Accurate? How Expensive? How Safe?</dc:title>
        <dcterms:abstract>Text classification in natural language processing (NLP) is evolving rapidly, particularly with the surge in transformer-based models, including large language models (LLM). This paper presents an in-depth survey of text classification techniques across diverse benchmarks, addressing applications from sentiment analysis to chatbot-driven question-answering. Methodologically, it utilizes NLP-facilitated approaches such as co-citation and bibliographic coupling alongside traditional research techniques. Because new use cases continue to emerge in this dynamic field, the study proposes an expanded taxonomy of text classification applications, extending the focus beyond unimodal (text-only) inputs to explore the emerging field of multimodal classification. While offering a comprehensive review of text classification with LLMs, this review highlights novel questions that arise when approaching the task with transformers: It evaluates the use of multimodal data, including text, numeric, and columnar data, and discusses the evolution of text input lengths (tokens) for long text classification; it covers the historical development of transformer-based models, emphasizing recent advancements in LLMs; it evaluates model accuracy on 358 datasets across 20 applications, with results challenging the assumption that LLMs are universally superior, revealing unexpected findings related to accuracy, cost, and safety; and it explores issues related to cost and access as models become increasingly expensive. Finally, the survey discusses new social and ethical implications raised when using LLMs for text classification, including bias and copyright. Throughout, the review emphasizes the importance of a nuanced understanding of model performance and a holistic approach to deploying transformer-based models in real-world applications.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001142647500001</dc:coverage>
        <bib:pages>6518-6531</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1357">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_601">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;8&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;8&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;174&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_602">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1077-2626"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yeh</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Viégas</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wattenberg</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1298"/>
        <dcterms:isReferencedBy rdf:resource="#item_603"/>
        <dc:subject>NLP</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>Attention</dc:subject>
        <dc:subject>Computer Vision</dc:subject>
        <dc:subject>Visual Analytics</dc:subject>
        <dc:title>AttentionViz: A Global View of Transformer Attention</dc:title>
        <dcterms:abstract>Transformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we present a new visualization technique designed to help researchers understand the self-attention mechanism in transformers that allows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous attention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an interactive visualization tool, AttentionViz (demo: http://attentionviz.com), based on these joint query-key embeddings, and use it to study attention mechanisms in both language and vision transformers. We demonstrate the utility of our approach in improving model understanding and offering new insights about query-key interactions through several application scenarios and expert feedback.</dcterms:abstract>
        <dc:date>2024 JAN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001159106500105</dc:coverage>
        <bib:pages>262-272</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1077-2626">
        <prism:volume>30</prism:volume>
        <dc:title>IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS</dc:title>
        <dc:identifier>DOI 10.1109/TVCG.2023.3327163</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 1077-2626</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1298">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_603">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;8&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;8&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;62&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_604">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2196-1115"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>HZ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shafiq</foaf:surname>
                        <foaf:givenName>MO</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1353"/>
        <dcterms:isReferencedBy rdf:resource="#item_605"/>
        <dc:subject>Ensemble learning</dc:subject>
        <dc:subject>Natural language tasks</dc:subject>
        <dc:subject>Transformer model</dc:subject>
        <dc:subject>Transformer-based model</dc:subject>
        <dc:title>Survey of transformers and towards ensemble learning using transformers for natural language processing</dc:title>
        <dcterms:abstract>The transformer model is a famous natural language processing model proposed by Google in 2017. Now, with the extensive development of deep learning, many natural language processing tasks can be solved by deep learning methods. After the BERT model was proposed, many pre-trained models such as the XLNet model, the RoBERTa model, and the ALBERT model were also proposed in the research community. These models perform very well in various natural language processing tasks. In this paper, we describe and compare these well-known models. In addition, we also apply several types of existing and well-known models which are the BERT model, the XLNet model, the RoBERTa model, the GPT2 model, and the ALBERT model to different existing and well-known natural language processing tasks, and analyze each model based on their performance. There are a few papers that comprehensively compare various transformer models. In our paper, we use six types of well-known tasks, such as sentiment analysis, question answering, text generation, text summarization, name entity recognition, and topic modeling tasks to compare the performance of various transformer models. In addition, using the existing models, we also propose ensemble learning models for the different natural language processing tasks. The results show that our ensemble learning models perform better than a single classifier on specific tasks.</dcterms:abstract>
        <dc:date>2024 FEB 4</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001156909400002</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2196-1115">
        <prism:volume>11</prism:volume>
        <dc:title>JOURNAL OF BIG DATA</dc:title>
        <dc:identifier>DOI 10.1186/s40537-023-00842-0</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 2196-1115</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1353">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_605">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;7&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;7&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;54&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_606">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0360-0300"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>JJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>JX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tu</foaf:surname>
                        <foaf:givenName>XH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>JM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>AJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Laskar</foaf:surname>
                        <foaf:givenName>MTR</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bhuiyan</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1354"/>
        <dcterms:isReferencedBy rdf:resource="#item_607"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>information retrieval</dc:subject>
        <dc:title>Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges</dc:title>
        <dcterms:abstract>Recent years have witnessed a substantial increase in the use of deep learning to solve various natural language processing (NLP) problems. Early deep learning models were constrained by their sequential or unidirectional nature, such that they struggled to capture the contextual relationships across text inputs. The introduction of bidirectional encoder representations from transformers (BERT) leads to a robust encoder for the transformer model that can understand the broader context and deliver state-of-the-art performance across various NLP tasks. This has inspired researchers and practitioners to apply BERT to practical problems, such as information retrieval (IR). A survey that focuses on a comprehensive analysis of prevalent approaches that apply pretrained transformer encoders like BERT to IR can thus be useful for academia and the industry. In light of this, we revisit a variety of BERT-based methods in this survey, cover a wide range of techniques of IR, and group them into six high-level categories: (i) handling long documents, (ii) integrating semantic information, (iii) balancing effectiveness and efficiency, (iv) predicting the weights of terms, (v) query expansion, and (vi) document expansion. We also provide links to resources, including datasets and toolkits, for BERT-based IR systems. Additionally, we highlight the advantages of employing encoder-based BERT models in contrast to recent large language models like ChatGPT, which are decoder-based and demand extensive computational resources. Finally, we summarize the comprehensive outcomes of the survey and suggest directions for future research in the area.</dcterms:abstract>
        <dc:date>2024 JUL</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001208811000024</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0360-0300">
        <prism:volume>56</prism:volume>
        <dc:title>ACM COMPUTING SURVEYS</dc:title>
        <dc:identifier>DOI 10.1145/3648471</dc:identifier>
        <prism:number>7</prism:number>
        <dc:identifier>ISSN 0360-0300</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1354">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_607">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;6&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;6&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;155&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_608">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1067-5027"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sahoo</foaf:surname>
                        <foaf:givenName>SS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Plasek</foaf:surname>
                        <foaf:givenName>JM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Uzuner</foaf:surname>
                        <foaf:givenName>O</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cohen</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yetisgen</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>HF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Meystre</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>YS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1419"/>
        <dcterms:isReferencedBy rdf:resource="#item_609"/>
        <dc:subject>large language models</dc:subject>
        <dc:subject>clinical natural language processing</dc:subject>
        <dc:subject>medical informatics applications</dc:subject>
        <dc:subject>transfer learning</dc:subject>
        <dc:subject>transformer neural networks</dc:subject>
        <dc:title>Large language models for biomedicine: foundations, opportunities, challenges, and best practices</dc:title>
        <dcterms:abstract>Objectives Generative large language models (LLMs) are a subset of transformers-based neural network architecture models. LLMs have successfully leveraged a combination of an increased number of parameters, improvements in computational efficiency, and large pre-training datasets to perform a wide spectrum of natural language processing (NLP) tasks. Using a few examples (few-shot) or no examples (zero-shot) for prompt-tuning has enabled LLMs to achieve state-of-the-art performance in a broad range of NLP applications. This article by the American Medical Informatics Association (AMIA) NLP Working Group characterizes the opportunities, challenges, and best practices for our community to leverage and advance the integration of LLMs in downstream NLP applications effectively. This can be accomplished through a variety of approaches, including augmented prompting, instruction prompt tuning, and reinforcement learning from human feedback (RLHF).Target Audience Our focus is on making LLMs accessible to the broader biomedical informatics community, including clinicians and researchers who may be unfamiliar with NLP. Additionally, NLP practitioners may gain insight from the described best practices.Scope We focus on 3 broad categories of NLP tasks, namely natural language understanding, natural language inferencing, and natural language generation. We review the emerging trends in prompt tuning, instruction fine-tuning, and evaluation metrics used for LLMs while drawing attention to several issues that impact biomedical NLP applications, including falsehoods in generated text (confabulation/hallucinations), toxicity, and dataset contamination leading to overfitting. We also review potential approaches to address some of these current challenges in LLMs, such as chain of thought prompting, and the phenomena of emergent capabilities observed in LLMs that can be leveraged to address complex NLP challenge in biomedical applications.</dcterms:abstract>
        <dc:date>2024 APR 24</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001207398900001</dc:coverage>
        <bib:pages>2114-2124</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1067-5027">
        <prism:volume>31</prism:volume>
        <dc:title>JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION</dc:title>
        <dc:identifier>DOI 10.1093/jamia/ocae074</dc:identifier>
        <prism:number>9</prism:number>
        <dc:identifier>ISSN 1067-5027</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1419">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_609">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;6&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;6&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;55&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_610">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0017-8748"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chiang</foaf:surname>
                        <foaf:givenName>CC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dumkrieger</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Trivedi</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>YC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chao</foaf:surname>
                        <foaf:givenName>CJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schwedt</foaf:surname>
                        <foaf:givenName>TJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sarker</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Banerjee</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1287"/>
        <dcterms:isReferencedBy rdf:resource="#item_611"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>headache frequency</dc:subject>
        <dc:subject>large language model</dc:subject>
        <dc:subject>migraine</dc:subject>
        <dc:subject>MIGRAINE</dc:subject>
        <dc:title>A large language model-based generative natural language processing framework fine-tuned on clinical notes accurately extracts headache frequency from electronic health records</dc:title>
        <dcterms:abstract>ObjectiveTo develop a natural language processing (NLP) algorithm that can accurately extract headache frequency from free-text clinical notes.BackgroundHeadache frequency, defined as the number of days with any headache in a month (or 4 weeks), remains a key parameter in the evaluation of treatment response to migraine preventive medications. However, due to the variations and inconsistencies in documentation by clinicians, significant challenges exist to accurately extract headache frequency from the electronic health record (EHR) by traditional NLP algorithms.MethodsThis was a retrospective cross-sectional study with patients identified from two tertiary headache referral centers, Mayo Clinic Arizona and Mayo Clinic Rochester. All neurology consultation notes written by 15 specialized clinicians (11 headache specialists and 4 nurse practitioners) between 2012 and 2022 were extracted and 1915 notes were used for model fine-tuning (90%) and testing (10%). We employed four different NLP frameworks: (1) ClinicalBERT (Bidirectional Encoder Representations from Transformers) regression model, (2) Generative Pre-Trained Transformer-2 (GPT-2) Question Answering (QA) model zero-shot, (3) GPT-2 QA model few-shot training fine-tuned on clinical notes, and (4) GPT-2 generative model few-shot training fine-tuned on clinical notes to generate the answer by considering the context of included text.ResultsThe mean (standard deviation) headache frequency of our training and testing datasets were 13.4 (10.9) and 14.4 (11.2), respectively. The GPT-2 generative model was the best-performing model with an accuracy of 0.92 (0.91, 0.93, 95% confidence interval [CI]) and R2 score of 0.89 (0.87, 0.90, 95% CI), and all GPT-2-based models outperformed the ClinicalBERT model in terms of exact matching accuracy. Although the ClinicalBERT regression model had the lowest accuracy of 0.27 (0.26, 0.28), it demonstrated a high R2 score of 0.88 (0.85, 0.89), suggesting the ClinicalBERT model can reasonably predict the headache frequency within a range of &lt;= +/- 3 days, and the R2 score was higher than the GPT-2 QA zero-shot model or GPT-2 QA model few-shot training fine-tuned model.ConclusionWe developed a robust information extraction model based on a state-of-the-art large language model, a GPT-2 generative model that can extract headache frequency from EHR free-text clinical notes with high accuracy and R2 score. It overcame several challenges related to different ways clinicians document headache frequency that were not easily achieved by traditional NLP models. We also showed that GPT-2-based frameworks outperformed ClinicalBERT in terms of accuracy in extracting headache frequency from clinical notes. To facilitate research in the field, we released the GPT-2 generative model and inference code with open-source license of community use in GitHub. Additional fine-tuning of the algorithm might be required when applied to different health-care systems for various clinical use cases.
We developed a novel artificial intelligence program that can automatically and accurately extract headache frequency from doctors' notes. Figuring out how often someone gets headaches is important as it helps doctors see how bad the problem is and if treatments are working. Our method, using a powerful program called Generative Pre-Trained Transformer-2, worked better than older ways and could make big data migraine research easier.</dcterms:abstract>
        <dc:date>2024 APR</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001189935500001</dc:coverage>
        <bib:pages>400-409</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0017-8748">
        <prism:volume>64</prism:volume>
        <dc:title>HEADACHE</dc:title>
        <dc:identifier>DOI 10.1111/head.14702</dc:identifier>
        <prism:number>4</prism:number>
        <dc:identifier>ISSN 0017-8748</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1287">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_611">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;6&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;6&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;20&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_612">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2192-1962"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Al-Fraihat</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sharrab</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alzyoud</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qahmash</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tarawneh</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maaita</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1360"/>
        <dcterms:isReferencedBy rdf:resource="#item_613"/>
        <dc:subject>NEURAL-NETWORKS</dc:subject>
        <dc:subject>(NLP) Systematic Review</dc:subject>
        <dc:subject>ARCHITECTURE</dc:subject>
        <dc:subject>LSTM</dc:subject>
        <dc:subject>Speech Recognition Deep Learning (DL) Deep Neural Networks (DNNs) Natural Language Processing</dc:subject>
        <dc:title>Speech Recognition Utilizing Deep Learning: A Systematic Review of the Latest Developments</dc:title>
        <dcterms:abstract>Speech recognition is a natural language processing task that involves the computerized transcription of spoken language in real time. Numerous studies have been conducted on the utilization of deep learning (DL) models for speech recognition. However, this field is advancing rapidly. This systematic review provides an in-depth and comprehensive examination of studies published from 2019 to 2022 on speech recognition utilizing DL techniques. Initially, 575 studies were retrieved and examined. After filtration and application of the inclusion and exclusion criteria, 94 were retained for further analysis. A literature survey revealed that 17% of the studies used stand-alone models, whereas 52% used hybrid models. This indicates a shift towards the adoption of hybrid models, which were proven to achieve better results. Furthermore, most of the studies employed public datasets (56%) and used the English language (46%), whereas their environments were neutral (81%). The word error rate was the most frequently used method of evaluation, while Mel -frequency cepstral coefficients were the most frequently employed method of feature extraction. Another observation was the lack of studies utilizing transformers, which were demonstrated to be powerful models that can facilitate fast learning speeds, allow parallelization and improve the performance of low -resource languages. The results also revealed potential and interesting areas of future research that had received scant attention in earlier studies.</dcterms:abstract>
        <dc:date>2024 MAR 15</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001156990100001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2192-1962">
        <prism:volume>14</prism:volume>
        <dc:title>HUMAN-CENTRIC COMPUTING AND INFORMATION SCIENCES</dc:title>
        <dc:identifier>DOI 10.22967/HCIS.2024.14.015</dc:identifier>
        <dc:identifier>ISSN 2192-1962</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1360">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_613">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;6&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;6&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;145&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_614">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0926-6690"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Apolinário</foaf:surname>
                        <foaf:givenName>AC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>de Carvalho</foaf:surname>
                        <foaf:givenName>EM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Damasceno</foaf:surname>
                        <foaf:givenName>BPGD</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>da Silva</foaf:surname>
                        <foaf:givenName>PCD</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Converti</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pessoa</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>da Silva</foaf:surname>
                        <foaf:givenName>JA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1331"/>
        <dcterms:isReferencedBy rdf:resource="#item_615"/>
        <dc:subject>Agave sisalana</dc:subject>
        <dc:subject>Aqueous extract</dc:subject>
        <dc:subject>FIBER</dc:subject>
        <dc:subject>FRUCTANS</dc:subject>
        <dc:subject>GLASS-TRANSITION</dc:subject>
        <dc:subject>Inulin</dc:subject>
        <dc:subject>PHYSICOCHEMICAL CHARACTERIZATION</dc:subject>
        <dc:subject>POLYMERIZATION</dc:subject>
        <dc:subject>Polysaccharide</dc:subject>
        <dc:subject>POLYSACCHARIDES</dc:subject>
        <dc:subject>REBAUDIANA BERT. BERTONI</dc:subject>
        <dc:subject>Sisal</dc:subject>
        <dc:subject>STABILITY</dc:subject>
        <dc:subject>WATER</dc:subject>
        <dc:subject>WEBER VAR.-AZUL</dc:subject>
        <dc:title>Extraction, isolation and characterization of inulin from Agave sisalana boles</dc:title>
        <dcterms:abstract>Agave sisalana Perrine is a widespread species of the Brazilian Northeast region, where it is exploited only as a source of hard fiber (sisal). Although some other Agave species are sources of fructans, there is no study on this issue on A. sisalana. This paper aimed at extracting and isolating inulin from aqueous extract of A. sisalana boles. After preparation of extracts, crude inulin was precipitated with acetone at low temperature (4 degrees C). After purification by ion-exchange chromatography, a white powder was obtained by freeze-drying and characterized by X-Ray Diffraction (XRD), thermal analysis, Circular Dichroism (CD) and Matrix-Assisted Laser Desorption/Ionization Time of Flight Mass Spectrometry (MALDI-TOF-MS). Moreover, its polysaccharide structure was confirmed by Fourier Transformed Infrared (FT-IR) spectroscopy and Nuclear Magnetic Resonance (NMR). FT-IR analysis pointed out absorption at 1420 cm(-1), corresponding to deformation of CH2-OH lying on fructose ring, while absorption at 1075 cm(-1) was assigned to C-O and C-C stretching vibrations of inulin pyranose ring. NMR showed the presence of one signal in the anomeric region at 8 5.4 ppm and others between 3.1 and 4.2 ppm in the H-1 spectrum, besides a chemical shift at 104.4 ppm corresponding to the anomeric region of the C-13 spectrum of an internal beta-fructofuranose unit. XRD highlighted the amorphous state of inulin-rich powder, thermal analysis a glass transition temperature in the range between 50.0 and 55.8 degrees C, CD a good thermal stability, and MALDI-TOF-MS a prevalence of oligosaccharides with degree of polymerization in the range 5-13. These techniques revealed that A. sisalana boles contain inulin with features similar to those extracted from other commercial sources such as Agave tequilana or Agave atrovirens, which extends the economic importance of this species beyond its simple use as a fiber source.</dcterms:abstract>
        <dc:date>2017 DEC 1</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000412959800041</dc:coverage>
        <bib:pages>355-362</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0926-6690">
        <prism:volume>108</prism:volume>
        <dc:title>INDUSTRIAL CROPS AND PRODUCTS</dc:title>
        <dc:identifier>DOI 10.1016/j.indcrop.2017.06.045</dc:identifier>
        <dc:identifier>ISSN 0926-6690</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1331">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_615">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;63&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;65&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;54&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_616">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0753-3322"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Latha</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chaudhary</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ray</foaf:surname>
                        <foaf:givenName>RS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1345"/>
        <dcterms:isReferencedBy rdf:resource="#item_617"/>
        <dc:subject>ANTIOXIDANT ACTIVITY</dc:subject>
        <dc:subject>FREE-RADICALS</dc:subject>
        <dc:subject>Hepatoprotective</dc:subject>
        <dc:subject>HEPATOPROTECTIVE ACTIVITY</dc:subject>
        <dc:subject>Inflammation</dc:subject>
        <dc:subject>KAPPA-B</dc:subject>
        <dc:subject>Lipopolysaccharide</dc:subject>
        <dc:subject>NITRIC-OXIDE SYNTHASE</dc:subject>
        <dc:subject>OXIDATIVE DNA-DAMAGE</dc:subject>
        <dc:subject>Oxidative stress</dc:subject>
        <dc:subject>PLANT-EXTRACTS</dc:subject>
        <dc:subject>PRO-INFLAMMATORY CYTOKINES</dc:subject>
        <dc:subject>PROTECTS MOUSE-LIVER</dc:subject>
        <dc:subject>Stevia rebaudiana</dc:subject>
        <dc:subject>Stevioside</dc:subject>
        <dc:subject>TNF-ALPHA</dc:subject>
        <dc:title>Hydroalcoholic extract of Stevia rebaudiana bert. leaves and stevioside ameliorates lipopolysaccharide induced acute liver injury in rats</dc:title>
        <dcterms:abstract>Oxidative stress and hepatic inflammatory response is primarily implicated in the pathogenesis of LPS induced acute liver injury. Stevioside, a diterpenoidal glycoside isolated from the Stevia rebaudiana leaves, exerts potent anti-oxidant, anti-inflammatory and immunomodulatory activities. The present study was aimed to investigate the hepatoprotective effect of hydroalcoholic extract of Stevia rebaudiana leaves (STE EXT) and its major phytochemical constituent, stevioside (STE) in LPS induced acute liver injury. The hepatoprotective activity of STE EXT (500 mg/kg p.o) and STE (250 mg/kg p.o) was investigated in lipopolysaccharide (LPS 5 mg/kg i.p.) induced acute liver injury in male wistar rats. Our results revealed that both STE EXT and STE treatment ameliorated LPS induced hepatic oxidative stress, evident from altered levels of reduced SOD, Catalase, GSH, MDA, NO. Histopathological observations revealed that both STE EXT and STE attenuated LPS induced structural changes and hepatocellular apoptosis providing additional evidence for its hepatoprotective effect. Further, STE EXT and STE significantly restored the elevated serum and tissue levels of AST and ALT in LPS treated rats. Furthermore, both STE EXT and STE rescued hepatocellular dysfunctions to normal by altering the level of proinflammatory cytokines such as TNF-alpha, IL-1 beta and IL-6 exhibiting its anti-inflammatory potential. In conclusion, both STE EXT and STE demonstrated excellent hepatoprotective effects against endotoxemia induced acute liver injury possibly through suppression of hepatic inflammatory response and oxidative stress, attributing to its medicinal importance in treating various liver ailments.</dcterms:abstract>
        <dc:date>2017 NOV</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000417744400119</dc:coverage>
        <bib:pages>1040-1050</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0753-3322">
        <prism:volume>95</prism:volume>
        <dc:title>BIOMEDICINE &amp; PHARMACOTHERAPY</dc:title>
        <dc:identifier>DOI 10.1016/j.biopha.2017.08.082</dc:identifier>
        <dc:identifier>ISSN 0753-3322</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1345">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_617">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;51&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;54&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;95&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_618">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0040-0262"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>van Reine</foaf:surname>
                        <foaf:givenName>WFP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1392"/>
        <dcterms:isReferencedBy rdf:resource="#item_619"/>
        <dc:subject>CONSERVED TYPE</dc:subject>
        <dc:subject>CYANOPHYCEAE</dc:subject>
        <dc:subject>DINOPHYCEAE</dc:subject>
        <dc:subject>PROPOSAL</dc:subject>
        <dc:subject>REJECT</dc:subject>
        <dc:title>Report of the Nomenclature Committee for Algae: 15</dc:title>
        <dcterms:abstract>Three names are recommended for conservation against the names indicated: Gelidium bipectinatum (Rhodophyceae) against Fucus serra; Scrippsiella (Dinophyceae) against Heteraulacus and Goniodoma; and Gloeobacter violaceus (Cyanophyceae) against Aphanothece caldariorum, Gloeothece coerulea, and Gloeothece linearis, while two names are recommended for conservation with a conserved type: Chara hispida (Characeae) and Gloeothece (Cyanophyceae). Three names are recommended for outright rejection, Jania verrucosa (Rhodophyceae), Cyanospira (Cyanophyceae) and Goniodomataceae (Dinophyceae), but the name Gonyaulax catenella (Dinophyceae) should not be rejected. The Committee concluded that the names Geisleria Nitschke (Fungi) and Geissleria Lange-Bert. &amp; Metzeltin (Bacillariophyceae) are not sufficiently alike to be confused.</dcterms:abstract>
        <dc:date>2017 FEB</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000397389100016</dc:coverage>
        <bib:pages>191-192</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0040-0262">
        <prism:volume>66</prism:volume>
        <dc:title>TAXON</dc:title>
        <dc:identifier>DOI 10.12705/661.16</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 0040-0262</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1392">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_619">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;48&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;52&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;15&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_620">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0955-3002"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vardhan</foaf:surname>
                        <foaf:givenName>PV</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shukla</foaf:surname>
                        <foaf:givenName>LI</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1337"/>
        <dcterms:isReferencedBy rdf:resource="#item_621"/>
        <dc:subject>ANTIOXIDANT ACTIVITY</dc:subject>
        <dc:subject>CALLUS-CULTURES</dc:subject>
        <dc:subject>camptothecin</dc:subject>
        <dc:subject>CAMPTOTHECIN PRODUCTION</dc:subject>
        <dc:subject>CELL-SUSPENSION CULTURES</dc:subject>
        <dc:subject>Gamma irradiation</dc:subject>
        <dc:subject>ginsenosides</dc:subject>
        <dc:subject>HAIRY ROOT CULTURES</dc:subject>
        <dc:subject>MOLECULAR-BIOLOGY</dc:subject>
        <dc:subject>OXIDATIVE STRESS</dc:subject>
        <dc:subject>PANAX-GINSENG</dc:subject>
        <dc:subject>phenolic compounds</dc:subject>
        <dc:subject>PHENYLALANINE AMMONIA-LYASE</dc:subject>
        <dc:subject>secondary metabolites</dc:subject>
        <dc:subject>shikonins</dc:subject>
        <dc:subject>TOTAL PHENOLIC CONTENT</dc:subject>
        <dc:title>Gamma irradiation of medicinally important plants and the enhancement of secondary metabolite production</dc:title>
        <dcterms:abstract>Purpose: The profitable production of some important plant-based secondary metabolites (ginsenosides, saponins, camptothecin, shikonins etc.) in vitro by gamma irradiation is a current area of interest. We reviewed different types of secondary metabolites, their mode of synthesis and effect of gamma-radiation on their yield for different plants, organs and in vitro cultures (callus, suspension, hairy root). Special effort has been made to review the biochemical mechanisms underlying the increase in secondary metabolites. A comparison of yield improvement with biotic and abiotic stresses was made.
Results: Phenolic compounds increase with gamma-irradiation in whole plants/plant parts; psoralen content in the common herb babchi (Psoralea corylifolia) was increased as high as 32-fold with gamma-irradiation of seeds at 20 kGy. The capsaicinoids, a phenolic compound increased about 10% with 10 kGy in paprika (Capsicum annum L.). The in vitro studies show all the three types of secondary metabolites are reported to increase with gamma-irradiation. Stevioside, total phenolic and flavonoids content were slightly increased in 15 Gy-treated callus cultures of stevia (Stevia rebaudiana Bert.). In terpenoids, total saponin and ginsenosides content were increased 1.4- and 1.8-fold, respectively, with 100 Gy for wild ginseng (Panax ginseng Meyer) hairy root cultures. In alkaloids, camptothecin yield increased as high as 20-fold with 20 Gy in callus cultures of ghanera (Nothapodytes foetida). Shikonins increased up to 4-fold with 16 Gy in suspension cultures of purple gromwell (Lithospermum erythrorhizon S.). The enzymes associated with secondary metabolite production were increased with gamma-irradiation of 20 Gy; namely, phenylalanine ammonia-lyase (PAL) for phenolics, chalcone synthase (CHS) for flavonoids, squalene synthase (SS), squalene epoxidase (SE) and oxidosqualene cyclases (OSC) for ginsenosides and PHB (p-hydroxylbenzoic acid) geranyl transferase for shikonins.
Conclusions: An increase in secondary metabolites in response to various biotic and abiotic stresses is compared with ionizing radiation. A similar to 5- to 20-fold increase is noted with similar to 20 Gy irradiation dose. It increases the yield of secondary metabolites by enhancing the activity of certain key biosynthetic enzymes. Identification of the optimum dose is the important step in the large-scale production of secondary metabolites at industrial level.</dcterms:abstract>
        <dc:date>2017 SEP</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000408781700010</dc:coverage>
        <bib:pages>967-979</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0955-3002">
        <prism:volume>93</prism:volume>
        <dc:title>INTERNATIONAL JOURNAL OF RADIATION BIOLOGY</dc:title>
        <dc:identifier>DOI 10.1080/09553002.2017.1344788</dc:identifier>
        <prism:number>9</prism:number>
        <dc:identifier>ISSN 0955-3002</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1337">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_621">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;47&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;50&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;131&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_622">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2045-2322"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Parmar</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Paul</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vashist</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Swarnkar</foaf:surname>
                        <foaf:givenName>MK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kumar</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>AK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kumar</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sharma</foaf:surname>
                        <foaf:givenName>RK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1426"/>
        <dcterms:isReferencedBy rdf:resource="#item_623"/>
        <dc:subject>IDENTIFICATION</dc:subject>
        <dc:subject>ACID</dc:subject>
        <dc:subject>DIVERSIFICATION</dc:subject>
        <dc:subject>GENES</dc:subject>
        <dc:subject>GERMINATION</dc:subject>
        <dc:subject>GLUCOSYLATION</dc:subject>
        <dc:subject>METABOLITES</dc:subject>
        <dc:subject>PATHWAY</dc:subject>
        <dc:subject>RNA-SEQ</dc:subject>
        <dc:subject>SAPONIN BIOSYNTHESIS</dc:subject>
        <dc:title>Molecular dissection of transcriptional reprogramming of steviol glycosides synthesis in leaf tissue during developmental phase transitions in Stevia rebaudiana Bert</dc:title>
        <dcterms:abstract>Stevia is a natural source of commercially important steviol glycosides (SGs), which share biosynthesis route with gibberellic acids (GAs) through plastidal MEP and cytosolic MVA pathways. Ontogeny-dependent deviation in SGs biosynthesis is one of the key factor for global cultivation of Stevia, has not been studied at transcriptional level. To dissect underlying molecular mechanism, we followed a global transcriptome sequencing approach and generated more than 100 million reads. Annotation of 41,262 de novo assembled transcripts identified all the genes required for SGs and GAs biosynthesis. Differential gene expression and quantitative analysis of important pathway genes (DXS, HMGR, KA13H) and gene regulators (WRKY, MYB, NAC TFs) indicated developmental phase dependent utilization of metabolic flux between SGs and GAs synthesis. Further, identification of 124 CYPs and 45 UGTs enrich the genomic resources, and their PPI network analysis with SGs/GAs biosynthesis proteins identifies putative candidates involved in metabolic changes, as supported by their developmental phase-dependent expression. These putative targets can expedite molecular breeding and genetic engineering efforts to enhance SGs content, biomass and yield. Futuristically, the generated dataset will be a useful resource for development of functional molecular markers for diversity characterization, genome mapping and evolutionary studies in Stevia.</dcterms:abstract>
        <dc:date>2017 SEP 19</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000411165100012</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2045-2322">
        <prism:volume>7</prism:volume>
        <dc:title>SCIENTIFIC REPORTS</dc:title>
        <dc:identifier>DOI 10.1038/s41598-017-12025-y</dc:identifier>
        <dc:identifier>ISSN 2045-2322</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1426">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_623">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;45&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;48&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;68&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_624">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1545-102X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zicko</foaf:surname>
                        <foaf:givenName>JM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schroeder</foaf:surname>
                        <foaf:givenName>RA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Byers</foaf:surname>
                        <foaf:givenName>WS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Taylor</foaf:surname>
                        <foaf:givenName>AM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Spence</foaf:surname>
                        <foaf:givenName>DL</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1301"/>
        <dcterms:isReferencedBy rdf:resource="#item_625"/>
        <dc:subject>assault</dc:subject>
        <dc:subject>behavioral emergency</dc:subject>
        <dc:subject>confidence</dc:subject>
        <dc:subject>knowledge</dc:subject>
        <dc:subject>mental health setting</dc:subject>
        <dc:subject>nonpsychiatric</dc:subject>
        <dc:subject>response team</dc:subject>
        <dc:subject>restraints</dc:subject>
        <dc:subject>security</dc:subject>
        <dc:subject>support</dc:subject>
        <dc:title>Behavioral Emergency Response Team: Implementation Improves Patient Safety, Staff Safety, and Staff Collaboration</dc:title>
        <dcterms:abstract>BackgroundStaff members working on our nonmental health (non-MH) units (i.e., medical-surgical [MS] units) were not educated in recognizing or deescalating behavioral emergencies. Published evidence suggests a behavioral emergency response team (BERT) composed of MH experts who assist with deescalating behavioral emergencies may be beneficial in these situations. Therefore, we sought to implement a BERT on the inpatient non-MH units at our military treatment facility.
AimsThe objectives of this evidence-based practice process improvement project were to determine how implementation of a BERT affects staff and patient safety and to examine nursing staffs' level of knowledge, confidence, and support in caring for psychiatric patients and patients exhibiting behavioral emergencies.
MethodsA BERT was piloted on one MS unit for 5 months and expanded to two additional units for 3 months. Pre- and postimplementation staff surveys were conducted, and the number of staff assaults and injuries, restraint usage, and security intervention were compared.
ResultsThe BERT responded to 17 behavioral emergencies. The number of assaults decreased from 10 (pre) to 1 (post); security intervention decreased from 14 to 1; and restraint use decreased from 8 to 1. MS staffs' level of BERT knowledge and rating of support between MH staff and their staff significantly increased. Both MS and MH nurses rated the BERT as supportive and effective.
Linking Evidence to ActionA BERT can assist with deescalating behavioral emergencies, and improve staff collaboration and patient and staff safety.</dcterms:abstract>
        <dc:date>2017 OCT</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000412070100006</dc:coverage>
        <bib:pages>377-384</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1545-102X">
        <prism:volume>14</prism:volume>
        <dc:title>WORLDVIEWS ON EVIDENCE-BASED NURSING</dc:title>
        <dc:identifier>DOI 10.1111/wvn.12225</dc:identifier>
        <prism:number>5</prism:number>
        <dc:identifier>ISSN 1545-102X</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1301">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_625">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;37&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;42&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;11&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_626">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0936-5907"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brône</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oben</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jehoul</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vranjes</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Feyaerts</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1332"/>
        <dcterms:isReferencedBy rdf:resource="#item_627"/>
        <dc:subject>BEHAVIOR</dc:subject>
        <dc:subject>CONSTRUCTION</dc:subject>
        <dc:subject>COORDINATION</dc:subject>
        <dc:subject>CROSS RECURRENCE</dc:subject>
        <dc:subject>DISCOURSE</dc:subject>
        <dc:subject>eye-tracking</dc:subject>
        <dc:subject>GESTURE</dc:subject>
        <dc:subject>interaction management</dc:subject>
        <dc:subject>JOINT ATTENTION</dc:subject>
        <dc:subject>multimodality</dc:subject>
        <dc:subject>RESOURCES</dc:subject>
        <dc:subject>SPEAKERS</dc:subject>
        <dc:subject>synchronization</dc:subject>
        <dc:subject>TURN-TAKING</dc:subject>
        <dc:title>Eye gaze and viewpoint in multimodal interaction management</dc:title>
        <dcterms:abstract>In this paper, we present an embodiment perspective on viewpoint by exploring the role of eye gaze in face-to-face conversation, in relation to and interaction with other expressive modalities. More specifically, we look into gaze patterns, as well as gaze synchronization with speech, as instruments in the negotiation of participant roles in interaction. In order to obtain fine-grained information on the different modalities under scrutiny, we used the InSight Interaction Corpus (Brone, Geert &amp; Bert Oben. 2015. Insight Interaction: A multimodal and multifocal dialogue corpus. Language Resources and Evaluation 49, 195-214.). This multimodal video corpus consists of two- and three-party interactions (in Dutch), with head-mounted scene cameras and eye-trackers tracking all participants' visual behavior, providing a unique 'speaker-internal' perspective on the conversation. The analysis of interactional sequences from the corpus (dyads and triads) reveals specific patterns of gaze distribution related to the temporal organization of viewpoint in dialogue. Different dialogue acts typically display specific gaze events at crucial points in time, as, e.g., in the case of brief gaze aversion associated with turn-holding, and shared gaze between interlocutors at the critical point of turn-taking. In addition, the data show a strong correlation and temporal synchronization between eye gaze and speech in the realization of specific dialogue acts, as shown by means of a series of cross-recurrence analyses for specific turn-holding mechanisms (e.g., verbal fillers co-occurring with brief moments of gaze aversion).</dcterms:abstract>
        <dc:date>2017 AUG</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000407804500004</dc:coverage>
        <bib:pages>449-483</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0936-5907">
        <prism:volume>28</prism:volume>
        <dc:title>COGNITIVE LINGUISTICS</dc:title>
        <dc:identifier>DOI 10.1515/cog-2016-0119</dc:identifier>
        <prism:number>3</prism:number>
        <dc:identifier>ISSN 0936-5907</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1332">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_627">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;28&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;31&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;94&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_628">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0145-5680"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ghorbani</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kahrizi</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saeidi</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Arji</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1320"/>
        <dcterms:isReferencedBy rdf:resource="#item_629"/>
        <dc:subject>Gene expression</dc:subject>
        <dc:subject>GLYCOSIDE BIOSYNTHESIS</dc:subject>
        <dc:subject>PLANTS</dc:subject>
        <dc:subject>Semi-quantitative RT-PCR</dc:subject>
        <dc:subject>Stevia rebaudiana Bertoni</dc:subject>
        <dc:subject>Sucrose</dc:subject>
        <dc:subject>Tissue culture</dc:subject>
        <dc:title>Effect of sucrose concentrations on Stevia rebaudiana Bertoni tissue culture and gene expression</dc:title>
        <dcterms:abstract>Stevia rebaudiana (Bert.) Bertoni is known as sweet plant which it contains a high level of steviol glycosides in the leaves. This plant has been used from centuries ago as a sweetener for tea. One of the most important steviol glycosides is stevioside that is attractive for diabetic persons. Tissue culture is the only rapid process for the mass propagation of stevia. One of the most important factors in the medium is sucrose that is a necessary for plant growth. In the present study, we use nodal segments of the stem as explants in mediums with different sucrose concentration (50 mM, 100 mM and 150 mM). Several morphological traits were measured in a 28 day period. Results analysis showed a significant variation between treatments. The highest growth rate, rooting and leaf production was obtained in medium with 100mM sucrose. The correlation between measured traits was significant at the 0.01 level. To investigation of UGT74G1, UGT76G1, UGT85C2 and KS genes expression that are involved in the synthesis of SGs, RT-PCR was done with the housekeeping gene of as internal control. There were significant differences between all media. The results showed thatsucrose 100 mM containing media was more desirable than others for expression of UGT76G1 and UGT85C2 genes. Whereas, the best medium for expression of UGT74G1 was sucrose 150 mM and sucrose 50 mM for KS gene. Totally, it seems that sucrose at a concentration of 100 mMprovides the best condition for stevia growth and steviol glycosides production.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000418145400008</dc:coverage>
        <bib:pages>33-37</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0145-5680">
        <prism:volume>63</prism:volume>
        <dc:title>CELLULAR AND MOLECULAR BIOLOGY</dc:title>
        <dc:identifier>DOI 10.14715/cmb/2017.63.8.8</dc:identifier>
        <prism:number>8</prism:number>
        <dc:identifier>ISSN 0145-5680</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1320">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_629">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;27&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;27&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;24&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_630">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0010-3616"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Haus</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Procesi</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1417"/>
        <dcterms:isReferencedBy rdf:resource="#item_631"/>
        <dc:subject>GROWTH</dc:subject>
        <dc:subject>DYNAMICS</dc:subject>
        <dc:subject>SYSTEMS</dc:subject>
        <dc:subject>EQUATIONS</dc:subject>
        <dc:subject>LEWIS-TYPE THEOREM</dc:subject>
        <dc:subject>LINEAR SCHRODINGER</dc:subject>
        <dc:subject>PERIODIC-SOLUTIONS</dc:subject>
        <dc:subject>SOBOLEV NORMS</dc:subject>
        <dc:subject>TORI</dc:subject>
        <dc:title>KAM for Beating Solutions of the Quintic NLS</dc:title>
        <dcterms:abstract>We consider the nonlinear Schrodinger equation of degree five on the circle . We prove the existence of quasi-periodic solutions with four frequencies which bifurcate from &quot;resonant&quot; solutions [studied in Gr,bert and Thomann (Ann Inst Henri Poincar, Anal Non Lin,aire 29(3):455-477, 2012)] of the system obtained by truncating the Hamiltonian after one step of Birkhoff normal form, exhibiting recurrent exchange of energy between some Fourier modes. The existence of these quasi-periodic solutions is a purely nonlinear effect.</dcterms:abstract>
        <dc:date>2017 SEP</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000404984600007</dc:coverage>
        <bib:pages>1101-1132</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0010-3616">
        <prism:volume>354</prism:volume>
        <dc:title>COMMUNICATIONS IN MATHEMATICAL PHYSICS</dc:title>
        <dc:identifier>DOI 10.1007/s00220-017-2925-7</dc:identifier>
        <prism:number>3</prism:number>
        <dc:identifier>ISSN 0010-3616</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1417">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_631">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;25&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;26&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;33&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_632">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0094-2405"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Patel</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bronk</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guan</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Peeler</foaf:surname>
                        <foaf:givenName>CR</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brons</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dokic</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Abdollahi</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rittmüller</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jäkel</foaf:surname>
                        <foaf:givenName>O</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grosshans</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mohan</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Titt</foaf:surname>
                        <foaf:givenName>U</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1429"/>
        <dcterms:isReferencedBy rdf:resource="#item_633"/>
        <dc:subject>MODEL</dc:subject>
        <dc:subject>CHARGED-PARTICLES</dc:subject>
        <dc:subject>EFFECTIVENESS RBE VALUES</dc:subject>
        <dc:subject>GEANT4</dc:subject>
        <dc:subject>high throughput experiments</dc:subject>
        <dc:subject>LINEAR-ENERGY-TRANSFER</dc:subject>
        <dc:subject>Monte Carlo simulations</dc:subject>
        <dc:subject>PROTON THERAPY</dc:subject>
        <dc:subject>protons</dc:subject>
        <dc:subject>RADIOTHERAPY</dc:subject>
        <dc:subject>relative biological effectiveness</dc:subject>
        <dc:subject>SPECTRA</dc:subject>
        <dc:title>Optimization of Monte Carlo particle transport parameters and validation of a novel high throughput experimental setup to measure the biological effects of particle beams</dc:title>
        <dcterms:abstract>PurposeAccurate modeling of the relative biological effectiveness (RBE) of particle beams requires increased systematic invitro studies with human cell lines with care towards minimizing uncertainties in biologic assays as well as physical parameters. In this study, we describe a novel high-throughput experimental setup and an optimized parameterization of the Monte Carlo (MC) simulation technique that is universally applicable for accurate determination of RBE of clinical ion beams. Clonogenic cell-survival measurements on a human lung cancer cell line (H460) are presented using proton irradiation.
MethodsExperiments were performed at the Heidelberg Ion Therapy Center (HIT) with support from the Deutsches Krebsforschungszentrum (DKFZ) in Heidelberg, Germany using a mono-energetic horizontal proton beam. A custom-made variable range selector was designed for the horizontal beam line using the Geant4 MC toolkit. This unique setup enabled a high-throughput clonogenic assay investigation of multiple, well defined dose and linear energy transfer (LETs) per irradiation for human lung cancer cells (H460) cultured in a 96-well plate. Sensitivity studies based on application of different physics lists in conjunction with different electromagnetic constructors and production threshold values to the MC simulations were undertaken for accurate assessment of the calculated dose and the dose-averaged LET (LETd). These studies were extended to helium and carbon ion beams.
ResultsSensitivity analysis of the MC parameterization revealed substantial dependence of the dose and LETd values on both the choice of physics list and the production threshold values. While the dose and LETd calculations using FTFP_BERT_LIV, FTFP_BERT_EMZ, FTFP_BERT_PEN and QGSP_BIC_EMY physics lists agree well with each other for all three ions, they show large differences when compared to the FTFP_BERT physics list with the default electromagnetic constructor. For carbon ions, the dose corresponding to the largest LETd value is observed to differ by as much as 78% between FTFP_BERT and FTFP_BERT_LIV. Furthermore, between the production threshold of 700m and 5m, proton dose varies by as much as 19% corresponding to the largest LETd value sampled in the current investigation. Based on the sensitivity studies, the FTFP_BERT physics list with the low energy Livermore electromagnetic constructor and a production threshold of 5m was employed for determining accurate dose and LETd. The optimized MC parameterization results in a different LETd dependence of the RBE curve for 10% SF of the H460 cell line irradiated with proton beam when compared with the results from a previous study using the same cell line. When the MC parameters are kept consistent between the studies, the proton RBE results agree well with each other within the experimental uncertainties.
ConclusionsA custom high-throughput, high-accuracy experimental design for accurate invitro cell survival measurements was employed at a horizontal beam line. High sensitivity of the physics-based optimization establishes the importance of accurate MC parameterization and hence the conditioning of the MC system on a case-by-case basis. The proton RBE results from current investigations are observed to agree with a previous measurement made under different experimental conditions. This establishes the consistency of our experimental findings across different experiments and institutions.</dcterms:abstract>
        <dc:date>2017 NOV</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000414970800049</dc:coverage>
        <bib:pages>6061-6073</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0094-2405">
        <prism:volume>44</prism:volume>
        <dc:title>MEDICAL PHYSICS</dc:title>
        <dc:identifier>DOI 10.1002/mp.12568</dc:identifier>
        <prism:number>11</prism:number>
        <dc:identifier>ISSN 0094-2405</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1429">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_633">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;24&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;24&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;28&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_634">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1083-351X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bachmann</foaf:surname>
                        <foaf:givenName>AS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Geerts</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1434"/>
        <dcterms:isReferencedBy rdf:resource="#item_635"/>
        <dc:subject>ANTIZYME INHIBITOR</dc:subject>
        <dc:subject>C-MYC</dc:subject>
        <dc:subject>CIRCADIAN CLOCK</dc:subject>
        <dc:subject>N-MYC</dc:subject>
        <dc:subject>ORNITHINE-DECARBOXYLASE GENE</dc:subject>
        <dc:subject>PROTEIN-SYNTHESIS</dc:subject>
        <dc:subject>SIGNALING PATHWAYS</dc:subject>
        <dc:subject>STRUCTURAL BASIS</dc:subject>
        <dc:subject>THERAPEUTIC TARGET</dc:subject>
        <dc:subject>TRANSCRIPTIONAL PROGRAM</dc:subject>
        <dc:title>Polyamine synthesis as a target of MYC oncogenes</dc:title>
        <dcterms:abstract>This paper is in recognition of the 100th birthday of Dr. Her-bert Tabor, a true pioneer in the polyamine field for over 70 years, who served as the editor-in-chief of the Journal of Biological Chemistry from 1971 to 2010. Wereview current knowledge of MYC proteins (c-MYC, MYCN, and MYCL) and focus on ornithine decarboxylase 1 (ODC1), an important bona fide gene target of MYC, which encodes the sentinel, rate- limiting enzyme in polyamine biosynthesis. Although notable advances have been made in designing inhibitors against the &quot;undruggable&quot; MYCs, their downstream targets and pathways are currently the main avenue for therapeutic anticancer interventions. To this end, the MYC-ODCaxis presents an attractive target for managing cancers such as neuroblastoma, a pediatric malignancy in which MYCN gene amplification correlates with poor prognosis and high-risk disease. ODC and polyamine levels are often up-regulated and contribute to tumor hyperproliferation, especially of MYC-driven cancers. We therefore had proposed to repurpose alpha-difluoromethylornithine (DFMO), an FDA-approved, orally available ODC inhibitor, for management of neuroblastoma, and this intervention is now being pursued in several clinical trials. We discuss the regulation of ODC and polyamines, which besides their well-known interactions with DNA and tRNA/rRNA, are involved in regulating RNA transcription and translation, ribosome function, proteasomal degradation, the circadian clock, and immunity, events that are also controlled by MYC proteins.</dcterms:abstract>
        <dc:date>2018 NOV 30</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000458467300031</dc:coverage>
        <bib:pages>18757-18769</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1083-351X">
        <prism:volume>293</prism:volume>
        <dc:title>JOURNAL OF BIOLOGICAL CHEMISTRY</dc:title>
        <dc:identifier>DOI 10.1074/jbc.TM118.003336</dc:identifier>
        <prism:number>48</prism:number>
        <dc:identifier>ISSN 1083-351X</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1434">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_635">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;93&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;104&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;170&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_638">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0972-1525"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gantait</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Das</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Banerjee</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1339"/>
        <dcterms:isReferencedBy rdf:resource="#item_639"/>
        <dc:subject>REBAUDIANA BERT. BERTONI</dc:subject>
        <dc:subject>Stevioside</dc:subject>
        <dc:subject>PLANTS</dc:subject>
        <dc:subject>ASTERACEAE</dc:subject>
        <dc:subject>CULTIVATION</dc:subject>
        <dc:subject>Floral biology</dc:subject>
        <dc:subject>FOREIGN POLLINATION</dc:subject>
        <dc:subject>INTRASPECIFIC POLLINATIONS</dc:subject>
        <dc:subject>Medicinal plant</dc:subject>
        <dc:subject>POLLEN-STIGMA INTERACTIONS</dc:subject>
        <dc:subject>Pollination</dc:subject>
        <dc:subject>Propagation</dc:subject>
        <dc:subject>STEVIOSIDE CONTENT</dc:subject>
        <dc:subject>TUBE GROWTH</dc:subject>
        <dc:subject>WALL PROTEINS</dc:subject>
        <dc:title>Geographical Distribution, Botanical Description and Self-Incompatibility Mechanism of Genus Stevia</dc:title>
        <dcterms:abstract>Stevia rebaudiana Bertoni, popularly known as 'candy leaf', is a sweet native herb of Paraguay. It became economically important for its significant contribution to the sugar and beverage industry throughout the world. This plant has been known to contain a calorie-free natural sugar in its leaves, which is an alternative to other artificially produced sugar substitutes. Stevia is conventionally propagated through seed and cutting, owing to its self-incompatibility, insufficient pollinator activity, and poor seed set, which results in the origination of heterozygous plants with varying concentration of glucosides in leaves, with low multiplication rate. This article compiles the literatures and depicts an overview on the geographical distribution, morphological, reproductive and cytological features, along with incompatibility mechanism of Stevia that would assist researchers to explore further and genetically refine this potential herb with immense medicinal importance.</dcterms:abstract>
        <dc:date>2018 FEB</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000419715300001</dc:coverage>
        <bib:pages>1-10</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0972-1525">
        <prism:volume>20</prism:volume>
        <dc:title>SUGAR TECH</dc:title>
        <dc:identifier>DOI 10.1007/s12355-017-0563-1</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 0972-1525</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1339">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_639">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;37&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;40&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;70&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_640">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>125</prism:volume>
                <dc:title>INDUSTRIAL CROPS AND PRODUCTS</dc:title>
                <dc:identifier>DOI 10.1016/j.indcrop.2018.09.029</dc:identifier>
                <dc:identifier>ISSN 0926-6690</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shahverdi</foaf:surname>
                        <foaf:givenName>MA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Omidi</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tabatabaei</foaf:surname>
                        <foaf:givenName>SJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1432"/>
        <dcterms:isReferencedBy rdf:resource="#item_641"/>
        <dc:subject>ACCUMULATION</dc:subject>
        <dc:subject>Leaf mass ratio</dc:subject>
        <dc:subject>Leaf yield</dc:subject>
        <dc:subject>LEAVES</dc:subject>
        <dc:subject>LIGHT INTERCEPTION</dc:subject>
        <dc:subject>MECHANISMS</dc:subject>
        <dc:subject>METABOLISM</dc:subject>
        <dc:subject>Micronutrients</dc:subject>
        <dc:subject>MICRONUTRIENTS</dc:subject>
        <dc:subject>RESPONSES</dc:subject>
        <dc:subject>SALINITY</dc:subject>
        <dc:subject>Salt-stress</dc:subject>
        <dc:subject>Selenium</dc:subject>
        <dc:subject>Steviol glycosides accumulation</dc:subject>
        <dc:subject>TOLERANCE</dc:subject>
        <dc:subject>YIELD</dc:subject>
        <dc:title>Plant growth and steviol glycosides as affected by foliar application of selenium, boron, and iron under NaCl stress in Stevia rebaudiana Bertoni</dc:title>
        <dcterms:abstract>Stevia (Stevia rebaudiana Bert.) is well known for its high content of sweet steviol glycosides (SVglys) in its leaves (about 4-20% of dry weight) depending on the environmental, climatic, and growth conditions. The present study aimed to investigate the effects of foliar iron, selenium, and boron on stevia, irrigated with different concentrations of saline water. The experiment was conducted during two consecutive years in Firuzabad and Anzali, Iran. Results indicated that plant height, number of branches, leaf and biological yield and leaf mass ratio decreased by increasing the NaCl concentration of irrigation water. In both places, total SVglys content, and SVglys yield increased when the plants were irrigated with a 30 mM of saline water while higher salinities (90 mM) resulted in decreasing the traits. In addition, the foliar application of selenium alone and in combination with boron, and iron played a significant role in growth, yield parameters and SVglys accumulation including rebaudioside-A,-B and -C, Stevioside, and Dulcoside-A. Firuzabad had the highest mean values of vegetative traits, leaf yield, and SVglys yield in the second year of the study. Regarding Firuzabad, leaf yield and SVglys percentages had a compensatory relationship, which can play a role in the observed increase in the SVglys percentage under a moderate level of NaCl (30 mM) stress. Finally, the foliar application of selenium either alone or in combination with iron could alleviate the adverse effects of NaCl stress on the studied traits. However, further studies are required to recognize the roles of selenium in SVglys accumulation and growth of stevia.</dcterms:abstract>
        <dc:date>2018 DEC 1</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000448095400047</dc:coverage>
        <bib:pages>408-415</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1432">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_641">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;35&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;36&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;48&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_642">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2053-2733"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>van Laar</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schenk</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1380"/>
        <dcterms:isReferencedBy rdf:resource="#item_643"/>
        <dc:subject>PROGRAM</dc:subject>
        <dc:subject>CRYSTAL-STRUCTURE</dc:subject>
        <dc:subject>EARTH METAL URANATES</dc:subject>
        <dc:subject>NEUTRON-DIFFRACTION</dc:subject>
        <dc:subject>PATTERNS</dc:subject>
        <dc:subject>powder profile refinement</dc:subject>
        <dc:subject>profile refinement</dc:subject>
        <dc:subject>Rietveld refinement</dc:subject>
        <dc:subject>X-RAY</dc:subject>
        <dc:title>The development of powder profile refinement at the Reactor Centre Netherlands at Petten</dc:title>
        <dcterms:abstract>With thousands of references to 'Rietveld refinement' it is forgotten that the method did not suddenly appear in a flash of inspiration of a single person, but was the result of the work of three individuals working in the 1960s at the Reactor Centre Netherlands at Petten, Loopstra, van Laar and Rietveld. This paper outlines the origins of 'profile refinement', as it was called at Petten, and also looks at why it took so long for the scientific community to recognize its importance. With the recent passing of Hugo Rietveld, the death of Bert Loopstra in 1998 and before other pioneers also disappear, it is important to set down a first-hand account.</dcterms:abstract>
        <dc:date>2018 MAR</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000426592500002</dc:coverage>
        <bib:pages>88-92</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2053-2733">
        <prism:volume>74</prism:volume>
        <dc:title>ACTA CRYSTALLOGRAPHICA A-FOUNDATION AND ADVANCES</dc:title>
        <dc:identifier>DOI 10.1107/S2053273317018435</dc:identifier>
        <dc:identifier>ISSN 2053-2733</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1380">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_643">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;33&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;34&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;32&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_644">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1751-8741"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ghazal</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saif</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Farid</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khan</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rehman</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reshma</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fazal</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ali</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ahmad</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rahman</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ahmad</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1381"/>
        <dcterms:isReferencedBy rdf:resource="#item_645"/>
        <dc:subject>GROWTH</dc:subject>
        <dc:subject>CALLUS-CULTURES</dc:subject>
        <dc:subject>BIOMASS</dc:subject>
        <dc:subject>ELICITATION</dc:subject>
        <dc:subject>IRRADIATION</dc:subject>
        <dc:subject>LEAF EXPLANTS</dc:subject>
        <dc:subject>ORGANOGENESIS</dc:subject>
        <dc:subject>PRUNELLA-VULGARIS L</dc:subject>
        <dc:subject>SILVER</dc:subject>
        <dc:subject>STEVIOSIDE</dc:subject>
        <dc:title>Stimulation of secondary metabolites by copper and gold nanoparticles in submerge adventitious root cultures of Stevia rebaudiana (Bert.)</dc:title>
        <dcterms:abstract>Nanotechnology is one of the advance technologies that almost found implications in every field of science. The importance is due to the unique properties of nanoparticles. In this study, bimetallic alloys of copper (Cu) and gold (Au) were tested in submerge root cultures of Stevia rebaudiana for production of biomass and secondary metabolites. A known amount of inoculum roots were submerged in liquid Murashige and Skoog medium containing combination of naphthalene acetic acid (NAA; 0.5 mg l(-1)) and different ratios of nanoparticles (NPs). NAA augmented medium was used as control. The addition of nanoparticles (30 mu g l(-1)) stimulated biomass accumulation (1.447 g/flask) on 27th day of log phases. The maximum total phenolics content (TPC; 16.17 mg/g-DW) and total flavonoids content (TFC; 4.20 mg/g-DW) were displayed using AuCu-NPs (1:3) and NAA. The same combinations enhanced total phenolic production (TPP; 116 mg/L) and total flavonoid production (TFP; 29.5 mg/L) in submerged cultures. A strong correlation was observed between phenolics, flavonoids and dry biomass. Moreover, maximum 1, 1-diphenyl-2-picrylhydrazyl (DPPH) activity of 79% was displayed by addition of AuCu (1:3) nanoparticles. In conclusion, nanoparticles application has shown a positive effect in enhancing biomass and secondary metabolites production in adventitious root cultures of Stevia rebaudiana.</dcterms:abstract>
        <dc:date>2018 AUG</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000441256400005</dc:coverage>
        <bib:pages>569-573</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1751-8741">
        <prism:volume>12</prism:volume>
        <dc:title>IET NANOBIOTECHNOLOGY</dc:title>
        <dc:identifier>DOI 10.1049/iet-nbt.2017.0093</dc:identifier>
        <prism:number>5</prism:number>
        <dc:identifier>ISSN 1751-8741</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1381">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_645">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;32&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;32&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;26&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_646">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0031-9155"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wulff</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Baumann</foaf:surname>
                        <foaf:givenName>KS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Verbeek</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bäumer</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Timmermann</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zink</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1374"/>
        <dcterms:isReferencedBy rdf:resource="#item_647"/>
        <dc:subject>ALGORITHM</dc:subject>
        <dc:subject>Monte Carlo simulations</dc:subject>
        <dc:subject>CONSISTENCY</dc:subject>
        <dc:subject>DOSIMETRY</dc:subject>
        <dc:subject>ELECTRON-TRANSPORT</dc:subject>
        <dc:subject>ionization chambers</dc:subject>
        <dc:subject>MONTE-CARLO CALCULATIONS</dc:subject>
        <dc:subject>MULTIPLE COULOMB SCATTERING</dc:subject>
        <dc:subject>PENELOPE</dc:subject>
        <dc:subject>PERTURBATION</dc:subject>
        <dc:subject>proton dosimetry</dc:subject>
        <dc:subject>QUALITY CORRECTION FACTORS</dc:subject>
        <dc:subject>SIMULATION</dc:subject>
        <dc:title>TOPAS/Geant4 configuration for ionization chamber calculations in proton beams</dc:title>
        <dcterms:abstract>Monte Carlo (MC) calculations are a fundamental tool for the investigation of ionization chambers (ICs) in radiation fields, and for calculations in the scope of IC reference dosimetry. Geant4, as used for the toolkit TOPAS, is a major general purpose code, generally suitable for investigating ICs in primary proton beams. To provide reliable results, the impact of parameter settings and the limitations of the underlying condensed history (CH) algorithm need to be known.
A Fano cavity test was implemented in Geant4 (10.03.pl) for protons, based on the existing version for electrons distributed with the Geant4 release. This self-consistent test allows the calculation to be compared with the expected result for the typical IC-like geometry of an air-filled cavity surrounded by a higher density material. Various user-selectable parameters of the CH implementation in the EMStandardOpt4 physics-list were tested for incident proton energies between 30 and 250 MeV. Using TOPAS (3.1.pl) the influence of production cuts was investigated for bare air-cavities in water, irradiated by primary protons. Detailed IC geometries for an NACP-02 plane-parallel chamber and an NE2571 Farmer-chamber were created. The overall factor f(Q) as a ratio between the dose-to-water and dose to the sensitive air-volume was calculated for incident proton energies between 70 and 250 MeV.
The Fano test demonstrated the EMStandardOpt4 physics-list with the WentzelIV multiple scattering model as appropriate for IC calculations. If protons start perpendicular to the air cavity, no further step-size limitations are required to pass the test within 0.1 %. For an isotropic source, limitations of the maximum step length within the air cavity and its surrounding as well as a limitation of the maximum fractional energy loss per step were required to pass within 0.2%. A production cut of &lt;= 5 mu m or similar to 15 keV for all particles yielded a constant result for f(Q) of bare air-filled cavities. The overall factor f(Q) for the detailed NACP-02 and NE2571 chamber models calculated with TOPAS agreed with the values of Goma et al (2016 Phys. Med. Biol. 61 2389) within statistical uncertainties (1 sigma) of &lt;0.3% for almost all energies with a maximum deviation of 0.6% at 250 MeV for the NE2571. The selection of hadronic scattering models (QGSP_BIC versus QGSP_BERT) in TOPAS impacted the results at the highest energies by 0.3% +/- 0.1%.
Based on the Fano cavity test, the Geant4/TOPAS Monte Carlo code, in its investigated version, can provide reliable results for IC calculations. Agreement with the detailed IC models and the published values of Goma et al can be achieved when production cuts are reduced from the TOPAS default values. The calculations confirm the reported agreement of Goma et al for k(Q,Q0) with IAEA-TRS398 values within the given uncertainties. An additional uncertainty for the MC-calculated k(Q,Q0) of similar to 0.3% by hadronic interaction models should be considered.</dcterms:abstract>
        <dc:date>2018 JUN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000434760000003</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0031-9155">
        <prism:volume>63</prism:volume>
        <dc:title>PHYSICS IN MEDICINE AND BIOLOGY</dc:title>
        <dc:identifier>DOI 10.1088/1361-6560/aac30e</dc:identifier>
        <prism:number>11</prism:number>
        <dc:identifier>ISSN 0031-9155</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1374">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_647">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;32&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;35&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;41&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_648">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0981-9428"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Debnath</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ashwath</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hill</foaf:surname>
                        <foaf:givenName>CB</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Callahan</foaf:surname>
                        <foaf:givenName>DL</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dias</foaf:surname>
                        <foaf:givenName>DA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jayasinghe</foaf:surname>
                        <foaf:givenName>NS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Midmore</foaf:surname>
                        <foaf:givenName>DJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roessner</foaf:surname>
                        <foaf:givenName>U</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1314"/>
        <dcterms:isReferencedBy rdf:resource="#item_649"/>
        <dc:subject>Stevia rebaudiana</dc:subject>
        <dc:subject>Stevioside</dc:subject>
        <dc:subject>ACCUMULATION</dc:subject>
        <dc:subject>LEAVES</dc:subject>
        <dc:subject>MECHANISMS</dc:subject>
        <dc:subject>TOLERANCE</dc:subject>
        <dc:subject>CONTRASTING RESPONSES</dc:subject>
        <dc:subject>GLYCOSIDES</dc:subject>
        <dc:subject>Hydroponics</dc:subject>
        <dc:subject>IMPROVEMENT</dc:subject>
        <dc:subject>Metabolite profiling</dc:subject>
        <dc:subject>PROLINE</dc:subject>
        <dc:subject>Rebaudioside</dc:subject>
        <dc:subject>Salinity</dc:subject>
        <dc:subject>SALT-STRESS</dc:subject>
        <dc:subject>Steviol glycoside</dc:subject>
        <dc:subject>SWEETENERS</dc:subject>
        <dc:title>Comparative metabolic and ionomic profiling of two cultivars of Stevia rebaudiana Bert. (Bertoni) grown under salinity stress</dc:title>
        <dcterms:abstract>This study provides a comprehensive investigation on the impact of increasing NaCl concentrations on hydroponically grown Stevia rebaudiana cultivars (Shoutian-2 and Fengtian). Growth parameters including plant height, biomass and physiological responses including osmotic potential were measured. In addition, the levels of steviol glycosides, elements and primary metabolites were measured and statistically evaluated. The cultivar Fengtian grew faster, accumulated less Na+ and compatible organic solutes, and more K+ in the leaves, as compared to the cv. Shoutian-2. Metabolite analysis identified 81 differentially accumulated metabolites, indicating an alteration in the metabolite phenotype of both cultivars upon exposure to salinity A general increase in many amino acids, amines, sugars and sugar phosphates with a concurrent decrease in most organic acids; including tricarboxylic acid (TCA) cycle intermediates, was observed. In the more salt tolerant cv. Fengtian, the levels of hexose phosphates and metabolites involved in cellular protection increased in response to salinity. These metabolites remained unchanged in the sensitive cv. Shoutian-2. Interestingly, salt treatment notably increased the rebaudioside A concentration by 53% while at the same time stevioside decreased by 38% in Fengtian which has important implications for controlling the relative amounts of reboudioside A and stevioside. The findings of this study leads to the conclusion that mild salinity stress can increase the yield of sweetener compounds, which is dependent on the cultivar and the level of salinity stress.</dcterms:abstract>
        <dc:date>2018 AUG</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000439683200006</dc:coverage>
        <bib:pages>56-70</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0981-9428">
        <prism:volume>129</prism:volume>
        <dc:title>PLANT PHYSIOLOGY AND BIOCHEMISTRY</dc:title>
        <dc:identifier>DOI 10.1016/j.plaphy.2018.05.001</dc:identifier>
        <dc:identifier>ISSN 0981-9428</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1314">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_649">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;29&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;34&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;67&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_650">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>64</prism:volume>
                <dc:title>CELLULAR AND MOLECULAR BIOLOGY</dc:title>
                <dc:identifier>DOI 10.14715/cmb/2018.64.2.9</dc:identifier>
                <prism:number>2</prism:number>
                <dc:identifier>ISSN 0145-5680</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Keshvari</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Najaphy</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kahrizi</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zebarjadi</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1305"/>
        <dcterms:isReferencedBy rdf:resource="#item_651"/>
        <dc:subject>Artificial Seed</dc:subject>
        <dc:subject>BIOCHEMICAL CHARACTERISTICS</dc:subject>
        <dc:subject>Callus</dc:subject>
        <dc:subject>GENE-EXPRESSION</dc:subject>
        <dc:subject>IN-VITRO CONDITIONS</dc:subject>
        <dc:subject>Somatic Embryo</dc:subject>
        <dc:subject>Stevia</dc:subject>
        <dc:subject>Tissue Culture</dc:subject>
        <dc:title>Callus induction and somatic embryogenesis in Stevia rebaudiana Bertoni as a medicinal plant</dc:title>
        <dcterms:abstract>Stevia rebaudiana (Bert.) from Asteraceae family is a useful medicinal plant that prevents and cures diabetes, blood pressure, weight gain and tooth decay. Due to self-incompatibility in stevia, somatic embryo investigation for artificial seed production is valuable in this plant. In order to evaluate the callus induction characteristics in stevia, a factorial experiment was laid out based on a completely randomized design with three replications. The factors included ten hormone combinations and control, two kinds of media (MS and B5) and two types of explants (leaf and internode). Callus induction characters including the percentage of callus formation, days to callus induction, fresh and dry callus weight were recorded. Analysis of variance showed significant differences (p&lt;0.01) among hormone combinations, media and explant types as well as their interactions. The best treatment for callus induction with minimum time to callus formation was 1 mg/l NAA+1 mg/l BAP. The highest fresh and dry callus weight were obtained on B5 medium supplemented by 1 mg/l 2,4-D+1 mg/l BAP (in leaf explant) and 0.25 mg/l 2,4-D+0.1 mg/l BAP (in internode explant). These results can be used in suspension culture. To induce somatic embryogenesis in suspension culture, six hormone treatments were investigated. The highest somatic embryogenesis percentage was obtained in MS medium supplemented by 2 mg/l 2,4-D+0.5 mg/l NAA+0.5 mg/l BAP.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000425228900009</dc:coverage>
        <bib:pages>46-49</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1305">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_651">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;26&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;28&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;20&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_652">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1631-0691"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Idrees</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sania</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hafsa</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kumari</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khan</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fazal</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ahmad</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Akbar</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ahmad</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ali</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ahmad</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1382"/>
        <dcterms:isReferencedBy rdf:resource="#item_653"/>
        <dc:subject>GROWTH</dc:subject>
        <dc:subject>Stevia rebaudiana</dc:subject>
        <dc:subject>CALLUS-CULTURES</dc:subject>
        <dc:subject>CELL-SUSPENSION CULTURES</dc:subject>
        <dc:subject>ORGANOGENESIS</dc:subject>
        <dc:subject>Antioxidants</dc:subject>
        <dc:subject>BIOCHEMICAL VARIATIONS</dc:subject>
        <dc:subject>Biomass</dc:subject>
        <dc:subject>BLUE-LIGHT</dc:subject>
        <dc:subject>MICROPROPAGATION</dc:subject>
        <dc:subject>Polyphenolics</dc:subject>
        <dc:subject>PROPAGATION</dc:subject>
        <dc:subject>RED</dc:subject>
        <dc:subject>REGENERATION</dc:subject>
        <dc:subject>Spectral lights</dc:subject>
        <dc:title>Spectral lights trigger biomass accumulation and production of antioxidant secondary metabolites in adventitious root cultures of Stevia rebaudiana (Bert.)</dc:title>
        <dcterms:abstract>Stevia rebaudiana (S. rebaudiana) is the most important therapeutic plant species and has been accepted as such worldwide. It has a tendency to accumulate steviol glycosides, which are 300 times sweeter than marketable sugar. Recently, diabetic patients commonly use this plant as a sugar substitute for sweet taste. In the present study, the effects of different spectral lights were investigated on biomass accumulation and production of secondary metabolites in adventitious root cultures of S. rebaudiana. For callus development, leaf explants were excised from seed-derived plantlets and inoculated on a Murashige and Skoog (MS) medium containing the combination of 2,4-dichlorophenoxy acetic acid (2, 4-D, 2.0 mg/l) and 6-benzyladenine (BA, 2.0 mg/l), while 0.5 mg/l naphthalene acetic acid (NAA) was used for adventitious root culture. Adventitious root cultures were exposed to different spectral lights (blue, green, violet, red and yellow) for a 30-day period. White light was used as control. The growth kinetics was studied for 30 days with 3-day intervals. In this study, the violet light showed the maximum accumulation of fresh biomass (2.495 g/flask) as compared to control (1.63 g/flask), while red light showed growth inhibition (1.025 g/flask) as compared to control. The blue light enhanced the highest accumulation of phenolic content (TPC; 6.56 mg GAE/g DW), total phenolic production (TPP; 101 mg/flask) as compared to control (5.44 mg GAE/g DW; 82.2 mg GAE/g DW), and exhibited a strong correlation with dry biomass. Blue light also improved the accumulation of total flavonoid content (TFC; 4.33 mg RE/g DW) and total flavonoid production (TFP; 65 mg/flask) as compared to control. The violet light showed the highest DPPH inhibition (79.72%), while the lowest antioxidant activity was observed for control roots (73.81%). Hence, we concluded that the application of spectral lights is an auspicious strategy for the enhancement of the required antioxidant secondary metabolites in adventitious root cultures of S. rebaudiana and of other medicinal plants. (C) 2018 Academie des sciences. Published by Elsevier Masson SAS. All rights reserved.</dcterms:abstract>
        <dc:date>2018 JUL</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000438954500004</dc:coverage>
        <bib:pages>334-342</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1631-0691">
        <prism:volume>341</prism:volume>
        <dc:title>COMPTES RENDUS BIOLOGIES</dc:title>
        <dc:identifier>DOI 10.1016/j.crvi.2018.05.003</dc:identifier>
        <prism:number>6</prism:number>
        <dc:identifier>ISSN 1631-0691</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1382">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_653">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;22&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;22&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;32&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_654">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-950737-13-0</dc:identifier>
                <dc:title>Google Incorporated</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Devlin</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chang</foaf:surname>
                        <foaf:givenName>MW</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Toutanova</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_655"/>
        <dc:title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</dc:title>
        <dcterms:abstract>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.
BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000900116904035</dc:coverage>
        <bib:pages>4171-4186</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_655">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;31750&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;35264&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;54&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_656">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1047-7047"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>YH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ott</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Goyal</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Du</foaf:surname>
                        <foaf:givenName>JF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Joshi</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>DQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levy</foaf:surname>
                        <foaf:givenName>O</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lewis</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zettlemoyer</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stoyanov</foaf:surname>
                        <foaf:givenName>V</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1742"/>
        <dcterms:isReferencedBy rdf:resource="#item_657"/>
        <link:link rdf:resource="#item_1557"/>
        <dc:title>RoBERTa: A Robustly Optimized BERT Pretraining Approach</dc:title>
        <dcterms:abstract>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.1</dcterms:abstract>
        <dc:date>2019 JUL 26</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001050220600001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1047-7047">
        <dc:title>INFORMATION SYSTEMS RESEARCH</dc:title>
        <dc:identifier>DOI 10.48550/arXiv.1907.11692</dc:identifier>
        <dc:identifier>ISSN 1047-7047</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1742">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_657">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;3206&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;3304&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;50&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1557">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1557/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining Approach.pdf"/>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/1907.11692v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:33:45</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:1049-5258">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>32</prism:volume>
                <dc:identifier>ISBN 1049-5258</dc:identifier>
                <dc:title>Carnegie Mellon University</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>ZL</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dai</foaf:surname>
                        <foaf:givenName>ZH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>YM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Carbonell</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Salakhutdinov</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Le</foaf:surname>
                        <foaf:givenName>QV</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wallach</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Larochelle</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Beygelzimer</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>d'Alche-Buc</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fox</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Garnett</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1404"/>
        <dcterms:isReferencedBy rdf:resource="#item_659"/>
        <dc:title>XLNet: Generalized Autoregressive Pretraining for Language Understanding</dc:title>
        <dcterms:abstract>With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000534424305072</dc:coverage>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1404">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_659">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;2624&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;2665&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;39&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_660">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-950737-90-1</dc:identifier>
                <dc:title>Technical University of Darmstadt</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reimers</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gurevych</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1746"/>
        <dcterms:isReferencedBy rdf:resource="#item_661"/>
        <dc:title>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</dc:title>
        <dcterms:abstract>BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (similar to 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.
In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.
We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.(1)</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000854193304015</dc:coverage>
        <bib:pages>3982-3992</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1746">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_661">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;2551&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;2604&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;38&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_662">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>32</prism:volume>
                <dc:identifier>ISBN 1049-5258</dc:identifier>
                <dc:title>University System of Georgia</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>JS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Batra</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Parikh</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wallach</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Larochelle</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Beygelzimer</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>d'Alche-Buc</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fox</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Garnett</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1403"/>
        <dcterms:isReferencedBy rdf:resource="#item_663"/>
        <dc:title>ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</dc:title>
        <dcterms:abstract>We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, processing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks - visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval - by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000534424300002</dc:coverage>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1403">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_663">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;1744&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;1879&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;47&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_664">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:identifier>ISBN 978-1-950737-90-1</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Beltagy</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lo</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cohan</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1383"/>
        <dcterms:isReferencedBy rdf:resource="#item_665"/>
        <dc:subject>CORPUS</dc:subject>
        <dc:title>SCIBERT: A Pretrained Language Model for Scientific Text</dc:title>
        <dcterms:abstract>Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of highquality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000854193303089</dc:coverage>
        <bib:pages>3615-3620</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1383">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_665">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;1394&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;1536&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;30&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:2640-3498">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>97</prism:volume>
                <dc:identifier>ISBN 2640-3498</dc:identifier>
                <dc:title>Jagiellonian University</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Houlsby</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Giurgiu</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jastrzebski</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Morrone</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>de Laroussilhe</foaf:surname>
                        <foaf:givenName>Q</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gesmundo</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Attariyan</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gelly</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chaudhuri</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Salakhutdinov</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1739"/>
        <dcterms:isReferencedBy rdf:resource="#item_667"/>
        <dc:title>Parameter-Efficient Transfer Learning for NLP</dc:title>
        <dcterms:abstract>Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000684034302095</dc:coverage>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 97</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1739">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_667">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;1111&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;1200&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;52&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-950737-48-2">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-950737-48-2</dc:identifier>
                <dc:title>Technical University of Darmstadt</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reimers</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schiller</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Beck</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Daxenberger</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stab</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gurevych</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>ACL</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Korhonen</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Traum</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Marquez</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1308"/>
        <dcterms:isReferencedBy rdf:resource="#item_669"/>
        <dc:title>Classification and Clustering of Arguments with Contextualized Word Embeddings</dc:title>
        <dcterms:abstract>We experiment with two recent contextualized word embedding methods (ELMo and BERT) in the context of open-domain argument search. For the first time, we show how to leverage the power of contextualized word embeddings to classify and cluster topic-dependent arguments, achieving impressive results on both tasks and across multiple datasets. For argument classification, we improve the state-of-the-art for the UKP Sentential Argument Mining Corpus by 20.8 percentage points and for the IBM Debater - Evidence Sentences dataset by 7.4 percentage points. For the understudied task of argument clustering, we propose a pre-training step which improves by 7.8 percentage points over strong baselines on a novel dataset, and by 12.3 percentage points for the Argument Facet Similarity (AFS) Corpus.(1)</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000493046101007</dc:coverage>
        <bib:pages>567-578</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1308">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_669">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;827&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;1066&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;22&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://aclanthology.org/D19-1250/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-950737-90-1</dc:identifier>
                <dc:title>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</dc:title>
                <dc:identifier>DOI 10.18653/v1/D19-1250</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Hong Kong, China</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Association for Computational Linguistics</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Petroni</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rocktäschel</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lewis</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bakhtin</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>YX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Miller</foaf:surname>
                        <foaf:givenName>AH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Riedel</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1750"/>
        <dcterms:isReferencedBy rdf:resource="#item_671"/>
        <link:link rdf:resource="#item_1731"/>
        <dc:title>Language Models as Knowledge Bases?</dc:title>
        <dcterms:abstract>Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as &quot;fillin-the-blank&quot; cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-theart pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https: //github.com/facebookresearch/LAMA</dcterms:abstract>
        <dc:date>2019-11</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000854193302058</dc:coverage>
        <z:libraryCatalog>ACLWeb</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://aclanthology.org/D19-1250/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <bib:pages>2463-2473</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1750">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_671">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;761&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;860&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;38&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1731">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1731/Petroni et al. - 2019 - Language Models as Knowledge Bases.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://aclanthology.org/D19-1250.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 13:13:23</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_672">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>11856</prism:volume>
                <dc:identifier>ISBN 2945-9133</dc:identifier>
                <dc:title>Fudan University</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-32381-3_16</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qiu</foaf:surname>
                        <foaf:givenName>XP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>YG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>XJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>X</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Z</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_673"/>
        <dc:subject>BERT</dc:subject>
        <dc:subject>Text classification</dc:subject>
        <dc:subject>Transfer learning</dc:subject>
        <dc:title>How to Fine-Tune BERT for Text Classification?</dc:title>
        <dcterms:abstract>Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000578465600016</dc:coverage>
        <bib:pages>194-206</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>CHINESE COMPUTATIONAL LINGUISTICS, CCL 2019</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_673">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;689&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;775&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;29&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_676">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2307-387X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Joshi</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>DQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>YH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Weld</foaf:surname>
                        <foaf:givenName>DS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zettlemoyer</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levy</foaf:surname>
                        <foaf:givenName>O</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1397"/>
        <dcterms:isReferencedBy rdf:resource="#item_677"/>
        <dc:title>SpanBERT: Improving Pre-training by Representing and Predicting Spans</dc:title>
        <dcterms:abstract>We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.(1)</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000736531900005</dc:coverage>
        <bib:pages>64-77</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2307-387X">
        <prism:volume>8</prism:volume>
        <dc:title>TRANSACTIONS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS</dc:title>
        <dc:identifier>DOI 10.1162/tacl_a_00300</dc:identifier>
        <dc:identifier>ISSN 2307-387X</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1397">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_677">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;857&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;991&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;55&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_678">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>INFORMATION SYSTEMS RESEARCH</dc:title>
                <dc:identifier>DOI 10.48550/arXiv.2003.10555</dc:identifier>
                <dc:identifier>ISSN 1047-7047</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Clark</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luong</foaf:surname>
                        <foaf:givenName>MT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Le</foaf:surname>
                        <foaf:givenName>QV</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Manning</foaf:surname>
                        <foaf:givenName>CD</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1321"/>
        <dcterms:isReferencedBy rdf:resource="#item_679"/>
        <dc:title>ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS</dc:title>
        <dcterms:abstract>Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.</dcterms:abstract>
        <dc:date>2020 MAR 23</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001046971300001</dc:coverage>
    </bib:Article>
    <bib:Memo rdf:about="#item_1321">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_679">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;815&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;840&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;44&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_680">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1674-7321"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qiu</foaf:surname>
                        <foaf:givenName>XP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>TX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>YG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shao</foaf:surname>
                        <foaf:givenName>YF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dai</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>XJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1351"/>
        <dcterms:isReferencedBy rdf:resource="#item_681"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>neural network</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>ANSWER</dc:subject>
        <dc:subject>distributed representation</dc:subject>
        <dc:subject>language modelling</dc:subject>
        <dc:subject>pre-trained model</dc:subject>
        <dc:subject>self-supervised learning</dc:subject>
        <dc:subject>word embedding</dc:subject>
        <dc:title>Pre-trained models for natural language processing: A survey</dc:title>
        <dcterms:abstract>Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy from four different perspectives. Next, we describe how to adapt the knowledge of PTMs to downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.</dcterms:abstract>
        <dc:date>2020 OCT</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000571751900005</dc:coverage>
        <bib:pages>1872-1897</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1674-7321">
        <prism:volume>63</prism:volume>
        <dc:title>SCIENCE CHINA-TECHNOLOGICAL SCIENCES</dc:title>
        <dc:identifier>DOI 10.1007/s11431-020-1647-3</dc:identifier>
        <prism:number>10</prism:number>
        <dc:identifier>ISSN 1674-7321</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1351">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_681">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;710&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;841&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;223&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_682">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>8</prism:volume>
                <dc:title>TRANSACTIONS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS</dc:title>
                <dc:identifier>DOI 10.1162/tacl_a_00349</dc:identifier>
                <dc:identifier>ISSN 2307-387X</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rogers</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kovaleva</foaf:surname>
                        <foaf:givenName>O</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rumshisky</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_683"/>
        <link:link rdf:resource="#item_1564"/>
        <dc:title>A Primer in BERTology: What We Know About How BERT Works</dc:title>
        <dcterms:abstract>Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000736531900054</dc:coverage>
        <bib:pages>842-866</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_683">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;552&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;622&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;180&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1564">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1564/Rogers et al. - 2020 - A Primer in BERTology What We Know About How BERT Works.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:38:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-8016-4">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-8016-4</dc:identifier>
                <dc:title>Stanford University</dc:title>
                <dc:identifier>DOI 10.1145/3397271.3401075</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khattab</foaf:surname>
                        <foaf:givenName>O</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zaharia</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>ACM</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1312"/>
        <dcterms:isReferencedBy rdf:resource="#item_685"/>
        <dc:subject>BERT</dc:subject>
        <dc:subject>Deep Language Models</dc:subject>
        <dc:subject>Efficiency</dc:subject>
        <dc:subject>Neural IR</dc:subject>
        <dc:title>ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</dc:title>
        <dcterms:abstract>Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000722377700012</dc:coverage>
        <bib:pages>39-48</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1312">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_685">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;517&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;549&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;42&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_686">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-952148-90-3</dc:identifier>
                <dc:title>Huazhong University of Science &amp; Technology</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiao</foaf:surname>
                        <foaf:givenName>XQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yin</foaf:surname>
                        <foaf:givenName>YC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shang</foaf:surname>
                        <foaf:givenName>LF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiang</foaf:surname>
                        <foaf:givenName>X</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>X</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>LL</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Q</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cohn</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1444"/>
        <dcterms:isReferencedBy rdf:resource="#item_687"/>
        <dc:title>TinyBERT: Distilling BERT for Natural Language Understanding</dc:title>
        <dcterms:abstract>Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resourcerestricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large '' teacher '' BERT can be effectively transferred to a small '' student '' TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT.
TinyBERT 41 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT BASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT (4) is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only similar to 28% parameters and similar to 31% inference time of them. Moreover, TinyBERT 6 with 6 layers performs on-par with its teacher BERT BASE</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001181866503040</dc:coverage>
        <bib:pages>4163-4174</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1444">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_687">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;482&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;527&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;51&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:2159-5399">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>34</prism:volume>
                <dc:identifier>ISBN 2159-5399</dc:identifier>
                <dc:title>Massachusetts Institute of Technology (MIT)</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jin</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jin</foaf:surname>
                        <foaf:givenName>ZJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>JTY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Szolovits</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Assoc Advancement Artificial Intelligence</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1416"/>
        <dcterms:isReferencedBy rdf:resource="#item_689"/>
        <dc:title>Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment</dc:title>
        <dcterms:abstract>Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TEXTFOOLER, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective-it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving-it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient-it generates adversarial text with computational complexity linear to the text length.(1)</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000668126800048</dc:coverage>
        <bib:pages>8018-8025</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1416">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_689">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;355&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;400&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;32&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_690">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>34</prism:volume>
                <dc:identifier>ISBN 2159-5399</dc:identifier>
                <dc:title>Peking University</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>WJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Z</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>ZR</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ju</foaf:surname>
                        <foaf:givenName>Q</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deng</foaf:surname>
                        <foaf:givenName>HT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Assoc Advancement Artificial Intelligence</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_691"/>
        <dc:title>K-BERT: Enabling Language Representation with Knowledge Graph</dc:title>
        <dcterms:abstract>Pre-trained language representation models, such as BERT, capture a general language representation from large-scale corpora, but lack domain-specific knowledge. When reading a domain text, experts make inferences with relevant knowledge. For machines to achieve this capability, we propose a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge. However, too much knowledge incorporation may divert the sentence from its correct meaning, which is called knowledge noise (KN) issue. To overcome KN, K-BERT introduces soft-position and visible matrix to limit the impact of knowledge. K-BERT can easily inject domain knowledge into the models by being equipped with a KG without pre-training by itself because it is capable of loading model parameters from the pre-trained BERT. Our investigation reveals promising results in twelve NLP tasks. Especially in domain-specific tasks (including finance, law, and medicine), K-BERT significantly outperforms BERT, which demonstrates that K-BERT is an excellent choice for solving the knowledge-driven problems that require experts.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000667722802119</dc:coverage>
        <bib:pages>2901-2908</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_691">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;342&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;400&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;25&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_692">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>34</prism:volume>
                <dc:identifier>ISBN 2159-5399</dc:identifier>
                <dc:title>Baidu</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>SH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>YK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Feng</foaf:surname>
                        <foaf:givenName>SK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>HF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Assoc Advancement Artificial Intelligence</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1324"/>
        <dcterms:isReferencedBy rdf:resource="#item_693"/>
        <dc:title>ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding</dc:title>
        <dcterms:abstract>Recently pre-trained models have achieved state-of-the-art results in various language understanding tasks. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring information, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entities, semantic closeness and discourse relations. In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning. Based on this framework, we construct several tasks and train the ERNIE 2.0 model to capture lexical, syntactic and semantic aspects of information in the training data. Experimental results demonstrate that ERNIE 2.0 model outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several similar tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000668126801050</dc:coverage>
        <bib:pages>8968-8975</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1324">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_693">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;309&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;366&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;25&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-8309-7">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-8309-7</dc:identifier>
                <dc:title>University of Washington</dc:title>
                <dc:identifier>DOI 10.1145/3442188.3445922</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bender</foaf:surname>
                        <foaf:givenName>EM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gebru</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>McMillan-Major</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shmitchell</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>ACM</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1428"/>
        <dcterms:isReferencedBy rdf:resource="#item_695"/>
        <dc:subject>GENDER</dc:subject>
        <dc:subject>STEREOTYPES</dc:subject>
        <dc:title>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</dc:title>
        <dcterms:abstract>The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001074649900058</dc:coverage>
        <bib:pages>610-623</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 2021 ACM CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, FACCT 2021</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1428">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_695">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;1627&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;1756&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;158&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_696">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 1063-6919</dc:identifier>
                <dc:title>Peking University</dc:title>
                <dc:identifier>DOI 10.1109/CVPR46437.2021.01212</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>HT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>YH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>TY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deng</foaf:surname>
                        <foaf:givenName>YP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>ZH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ma</foaf:surname>
                        <foaf:givenName>SW</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>CJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gao</foaf:surname>
                        <foaf:givenName>W</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>IEEE COMP SOC</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1386"/>
        <dcterms:isReferencedBy rdf:resource="#item_697"/>
        <dc:title>Pre-Trained Image Processing Transformer</dc:title>
        <dcterms:abstract>As the computing power of modern hardware is increasing strongly, pre-trained deep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have shown their effectiveness over conventional methods. The big progress is mainly contributed to the representation ability of transformer and its variant architectures. In this paper, we study the low-level computer vision task (e.g., denoising, super-resolution and deraining) and develop a new pre-trained model, namely, image processing transformer (IPT). To maximally excavate the capability of transformer, we present to utilize the well-known ImageNet benchmark for generating a large amount of corrupted image pairs. The IPT model is trained on these images with multi-heads and multi-tails. In addition, the contrastive learning is introduced for well adapting to different image processing tasks. The pre-trained model can therefore efficiently employed on desired task after fine-tuning. With only one pre-trained model, IPT outperforms the current state-of-the-art methods on various low-level benchmarks.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000742075002049</dc:coverage>
        <bib:pages>12294-12305</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, CVPR 2021</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1386">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_697">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;988&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;1043&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;86&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_698">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>29</prism:volume>
                <dc:title>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</dc:title>
                <dc:identifier>DOI 10.1109/TASLP.2021.3122291</dc:identifier>
                <dc:identifier>ISSN 2329-9290</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hsu</foaf:surname>
                        <foaf:givenName>WN</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bolte</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tsai</foaf:surname>
                        <foaf:givenName>YHH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lakhotia</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Salakhutdinov</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mohamed</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1343"/>
        <dcterms:isReferencedBy rdf:resource="#item_699"/>
        <dc:subject>Predictive models</dc:subject>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>Computational modeling</dc:subject>
        <dc:subject>Data models</dc:subject>
        <dc:subject>Bit error rate</dc:subject>
        <dc:subject>Self-supervised learning</dc:subject>
        <dc:subject>Acoustics</dc:subject>
        <dc:subject>MARKOV MODEL</dc:subject>
        <dc:subject>MODEL VARIATIONAL AUTOENCODER</dc:subject>
        <dc:subject>Speech processing</dc:subject>
        <dc:title>HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</dc:title>
        <dcterms:abstract>Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960 h) and Libri-light (60,000 h) benchmarks with 10 min, 1 h, 10 h, 100 h, and 960 h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.(1)(2)</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000722716400001</dc:coverage>
        <bib:pages>3451-3460</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1343">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_699">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;736&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;773&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;64&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-955917-09-4">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-955917-09-4</dc:identifier>
                <dc:title>Salesforce</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>WS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Joty</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hoi</foaf:surname>
                        <foaf:givenName>SCH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1311"/>
        <dcterms:isReferencedBy rdf:resource="#item_703"/>
        <dc:title>CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation</dc:title>
        <dcterms:abstract>Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000860727002063</dc:coverage>
        <bib:pages>8696-8708</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1311">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_703">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;367&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;378&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;35&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_704">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>34</prism:volume>
                <dc:identifier>ISBN 1049-5258</dc:identifier>
                <dc:title>University of California System</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>LL</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rajeswaran</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grover</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Laskin</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Abbeel</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Srinivas</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mordatch</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ranzato</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Beygelzimer</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dauphin</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liang</foaf:surname>
                        <foaf:givenName>PS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vaughan</foaf:surname>
                        <foaf:givenName>JW</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1367"/>
        <dcterms:isReferencedBy rdf:resource="#item_705"/>
        <dc:title>Decision Transformer: Reinforcement Learning via Sequence Modeling</dc:title>
        <dcterms:abstract>We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.
[GRAPHICS]</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000901616406051</dc:coverage>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 34 (NEURIPS 2021)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1367">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_705">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;363&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;404&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;66&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_706">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2666-6510"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>X</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>ZY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ding</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gu</foaf:surname>
                        <foaf:givenName>YX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>X</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huo</foaf:surname>
                        <foaf:givenName>YQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qiu</foaf:surname>
                        <foaf:givenName>JZ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yao</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>WT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>ML</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jin</foaf:surname>
                        <foaf:givenName>Q</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lan</foaf:surname>
                        <foaf:givenName>YY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>ZY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>ZW</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qiu</foaf:surname>
                        <foaf:givenName>XP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Song</foaf:surname>
                        <foaf:givenName>RH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wen</foaf:surname>
                        <foaf:givenName>JR</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yuan</foaf:surname>
                        <foaf:givenName>JH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>WX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhu</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1387"/>
        <dcterms:isReferencedBy rdf:resource="#item_707"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Language models</dc:subject>
        <dc:subject>Transfer learning</dc:subject>
        <dc:subject>Self-supervised learning</dc:subject>
        <dc:subject>LANGUAGE</dc:subject>
        <dc:subject>GENERATION</dc:subject>
        <dc:subject>Multimodal processing</dc:subject>
        <dc:subject>Pre-trained models</dc:subject>
        <dc:title>Pre-trained models: Past, present and future</dc:title>
        <dcterms:abstract>Large-scale pre -trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre -training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre -training, especially its special relation with transfer learning and self -supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001220553400001</dc:coverage>
        <bib:pages>225-250</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2666-6510">
        <prism:volume>2</prism:volume>
        <dc:title>AI OPEN</dc:title>
        <dc:identifier>DOI 10.1016/j.aiopen.2021.08.002</dc:identifier>
        <dc:identifier>ISSN 2666-6510</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1387">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_707">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;303&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;334&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;319&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_708">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2398-6352"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rasmy</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xiang</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>ZQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tao</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhi</foaf:surname>
                        <foaf:givenName>DG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1423"/>
        <dcterms:isReferencedBy rdf:resource="#item_709"/>
        <dc:subject>BIG DATA</dc:subject>
        <dc:subject>ARTIFICIAL-INTELLIGENCE</dc:subject>
        <dc:subject>CARE</dc:subject>
        <dc:title>Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction</dc:title>
        <dcterms:abstract>Deep learning (DL)-based predictive models from electronic health records (EHRs) deliver impressive performance in many clinical tasks. Large training cohorts, however, are often required by these models to achieve high accuracy, hindering the adoption of DL-based models in scenarios with limited training data. Recently, bidirectional encoder representations from transformers (BERT) and related models have achieved tremendous successes in the natural language processing domain. The pretraining of BERT on a very large training corpus generates contextualized embeddings that can boost the performance of models trained on smaller datasets. Inspired by BERT, we propose Med-BERT, which adapts the BERT framework originally developed for the text domain to the structured EHR domain. Med-BERT is a contextualized embedding model pretrained on a structured EHR dataset of 28,490,650 patients. Fine-tuning experiments showed that Med-BERT substantially improves the prediction accuracy, boosting the area under the receiver operating characteristics curve (AUC) by 1.21-6.14% in two disease prediction tasks from two clinical databases. In particular, pretrained Med-BERT obtains promising performances on tasks with small fine-tuning training sets and can boost the AUC by more than 20% or obtain an AUC as high as a model trained on a training set ten times larger, compared with deep learning models without Med-BERT. We believe that Med-BERT will benefit disease prediction studies with small local training datasets, reduce data collection expenses, and accelerate the pace of artificial intelligence aided healthcare.</dcterms:abstract>
        <dc:date>2021 MAY 20</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000652608900001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2398-6352">
        <prism:volume>4</prism:volume>
        <dc:title>NPJ DIGITAL MEDICINE</dc:title>
        <dc:identifier>DOI 10.1038/s41746-021-00455-y</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 2398-6352</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1423">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_709">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;301&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;331&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;62&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_710">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1380-7501"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kaliyar</foaf:surname>
                        <foaf:givenName>RK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Goswami</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Narang</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1333"/>
        <dcterms:isReferencedBy rdf:resource="#item_711"/>
        <dc:subject>CLASSIFICATION</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>REPRESENTATIONS</dc:subject>
        <dc:subject>CONVOLUTIONAL NEURAL-NETWORK</dc:subject>
        <dc:subject>Social media</dc:subject>
        <dc:subject>Fake news</dc:subject>
        <dc:subject>Neural network</dc:subject>
        <dc:title>FakeBERT: Fake news detection in social media with a BERT-based deep learning approach</dc:title>
        <dcterms:abstract>In the modern era of computing, the news ecosystem has transformed from old traditional print media to social media outlets. Social media platforms allow us to consume news much faster, with less restricted editing results in the spread of fake news at an incredible pace and scale. In recent researches, many useful methods for fake news detection employ sequential neural networks to encode news content and social context-level information where the text sequence was analyzed in a unidirectional way. Therefore, a bidirectional training approach is a priority for modelling the relevant information of fake news that is capable of improving the classification performance with the ability to capture semantic and long-distance dependencies in sentences. In this paper, we propose a BERT-based (Bidirectional Encoder Representations from Transformers) deep learning approach (FakeBERT) by combining different parallel blocks of the single-layer deep Convolutional Neural Network (CNN) having different kernel sizes and filters with the BERT. Such a combination is useful to handle ambiguity, which is the greatest challenge to natural language understanding. Classification results demonstrate that our proposed model (FakeBERT) outperforms the existing models with an accuracy of 98.90%.</dcterms:abstract>
        <dc:date>2021 MAR</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000642237200002</dc:coverage>
        <bib:pages>11765-11788</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1380-7501">
        <prism:volume>80</prism:volume>
        <dc:title>MULTIMEDIA TOOLS AND APPLICATIONS</dc:title>
        <dc:identifier>DOI 10.1007/s11042-020-10183-2</dc:identifier>
        <prism:number>8</prism:number>
        <dc:identifier>ISSN 1380-7501</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1333">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_711">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;267&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;279&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;54&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_714">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>44</prism:volume>
                <dc:title>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</dc:title>
                <dc:identifier>DOI 10.1109/TPAMI.2021.3095381</dc:identifier>
                <prism:number>10</prism:number>
                <dc:identifier>ISSN 0162-8828</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Elnaggar</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Heinzinger</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dallago</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rehawi</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jones</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gibbs</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Feher</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Angerer</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Steinegger</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bhowmik</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rost</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1439"/>
        <dcterms:isReferencedBy rdf:resource="#item_715"/>
        <dc:subject>PREDICTION</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>NEURAL-NETWORKS</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>Computational modeling</dc:subject>
        <dc:subject>Training</dc:subject>
        <dc:subject>Amino acids</dc:subject>
        <dc:subject>Computational biology</dc:subject>
        <dc:subject>Databases</dc:subject>
        <dc:subject>high performance computing</dc:subject>
        <dc:subject>language modeling</dc:subject>
        <dc:subject>LOCALIZATION</dc:subject>
        <dc:subject>PROTEIN SECONDARY STRUCTURE</dc:subject>
        <dc:subject>Proteins</dc:subject>
        <dc:subject>Three-dimensional displays</dc:subject>
        <dc:title>ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning</dc:title>
        <dcterms:abstract>Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models (LMs) taken from Natural Language Processing (NLP). These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The protein LMs (pLMs) were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw pLM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks: (1) a per-residue (per-token) prediction of protein secondary structure (3-state accuracy Q3=81%-87%); (2) per-protein (pooling) predictions of protein sub-cellular location (ten-state accuracy: Q10=81%) and membrane versus water-soluble (2-state accuracy Q2=91%). For secondary structure, the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without multiple sequence alignments (MSAs) or evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that pLMs learned some of the grammar of the language of life. All our models are available through https://github.com/agemagician/ProtTrans.</dcterms:abstract>
        <dc:date>2022 OCT 1</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000853875300088</dc:coverage>
        <bib:pages>7112-7127</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1439">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_715">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;423&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;435&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;99&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_716">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>23</prism:volume>
                <dc:title>BRIEFINGS IN BIOINFORMATICS</dc:title>
                <dc:identifier>DOI 10.1093/bib/bbac409</dc:identifier>
                <prism:number>6</prism:number>
                <dc:identifier>ISSN 1467-5463</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>RQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>LA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xia</foaf:surname>
                        <foaf:givenName>YC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qin</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Poon</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>TY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1303"/>
        <dcterms:isReferencedBy rdf:resource="#item_717"/>
        <dc:subject>text mining</dc:subject>
        <dc:subject>biomedical literature</dc:subject>
        <dc:subject>generative pre-trained language model</dc:subject>
        <dc:subject>text generation</dc:subject>
        <dc:title>BioGPT: generative pre-trained transformer for biomedical text generation and mining</dc:title>
        <dcterms:abstract>Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.</dcterms:abstract>
        <dc:date>2022 NOV</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000859127700001</dc:coverage>
    </bib:Article>
    <bib:Memo rdf:about="#item_1303">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_717">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;236&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;245&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;53&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_718">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 1063-6919</dc:identifier>
                <dc:title>Tsinghua University</dc:title>
                <dc:identifier>DOI 10.1109/CVPR52688.2022.01871</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>XM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>LL</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rao</foaf:surname>
                        <foaf:givenName>YM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>TJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>JW</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>IEEE COMP SOC</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1433"/>
        <dcterms:isReferencedBy rdf:resource="#item_719"/>
        <dc:title>Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling</dc:title>
        <dcterms:abstract>We present Point-BERT, a new paradigm for learning Transformers to generalize the concept of BERT [8] to 3D point cloud. Inspired by BERT, we devise a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically, we first divide a point cloud into several local point patches, and a point cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to generate discrete point tokens containing meaningful local information. Then, we randomly mask out some patches of input point clouds and feed them into the backbone Transformers. The pre-training objective is to recover the original point tokens at the masked locations under the supervision of point tokens obtained by the Tokenizer. Extensive experiments demonstrate that the proposed BERT-style pre-training strategy significantly improves the performance of standard point cloud Transformers. Equipped with our pre-training strategy, we show that a pure Transformer architecture attains 93.8% accuracy on ModelNet40 and 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully designed point cloud models with much fewer hand-made designs. We also demonstrate that the representations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. The code and pre-trained models are available at https://github.com/lulutang0608/Point-BERT.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000870783005012</dc:coverage>
        <bib:pages>19291-19300</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2022)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1433">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_719">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;210&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;217&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;66&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-955917-21-6">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-955917-21-6</dc:identifier>
                <dc:title>Tsinghua University</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Du</foaf:surname>
                        <foaf:givenName>ZX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qian</foaf:surname>
                        <foaf:givenName>YJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>X</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ding</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qiu</foaf:surname>
                        <foaf:givenName>JZ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>ZL</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Assoc Computat Linguist</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1341"/>
        <dcterms:isReferencedBy rdf:resource="#item_721"/>
        <dc:title>GLM: General Language Model Pretraining with Autoregressive Blank Infilling</dc:title>
        <dcterms:abstract>There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25x parameters of BERTLarge, demonstrating its generalizability to different downstream tasks.(1)</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000828702300026</dc:coverage>
        <bib:pages>320-335</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1341">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_721">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;199&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;222&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;50&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-9385-0">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-9385-0</dc:identifier>
                <dc:title>Tsinghua University</dc:title>
                <dc:identifier>DOI 10.1145/3534678.3539321</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hou</foaf:surname>
                        <foaf:givenName>ZY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>X</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cen</foaf:surname>
                        <foaf:givenName>YK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dong</foaf:surname>
                        <foaf:givenName>YX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>HX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>CJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>ACM</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1344"/>
        <dcterms:isReferencedBy rdf:resource="#item_723"/>
        <dc:subject>Graph Neural Networks</dc:subject>
        <dc:subject>Graph Representation Learning</dc:subject>
        <dc:subject>Pre-Training</dc:subject>
        <dc:subject>Self-Supervised Learning</dc:subject>
        <dc:title>GraphMAE: Self-Supervised Masked Graph Autoencoders</dc:title>
        <dcterms:abstract>Self-supervised learning (SSL) has been extensively explored in recent years. Particularly, generative SSL has seen emerging success in natural language processing and other AI fields, such as the wide adoption of BERT and GPT. Despite this, contrastive learning-which heavily relies on structural data augmentation and complicated training strategies-has been the dominant approach in graph SSL, while the progress of generative SSL on graphs, especially graph autoencoders (GAEs), has thus far not reached the potential as promised in other fields. In this paper, we identify and examine the issues that negatively impact the development of GAEs, including their reconstruction objective, training robustness, and error metric. We present a masked graph autoencoder GraphMAE(1) that mitigates these issues for generative self-supervised graph learning. Instead of reconstructing graph structures, we propose to focus on feature reconstruction with both a masking strategy and scaled cosine error that benefit the robust training of Graph-MAE. We conduct extensive experiments on 21 public datasets for three different graph learning tasks. The results manifest that GraphMAE-a simple graph autoencoder with careful designs-can consistently generate outperformance over both contrastive and generative state-of-the-art baselines. This study provides an understanding of graph autoencoders and demonstrates the potential of generative self-supervised learning on graphs.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001119000300060</dc:coverage>
        <bib:pages>594-604</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 28TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2022</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1344">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_723">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;162&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;169&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;61&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-9132-0">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-9132-0</dc:identifier>
                <dc:title>University of Queensland</dc:title>
                <dc:identifier>DOI 10.1145/3488560.3498433</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qiu</foaf:surname>
                        <foaf:givenName>RH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Z</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yin</foaf:surname>
                        <foaf:givenName>HZ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>ZJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>ASSOC COMP MACHINERY</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1317"/>
        <dcterms:isReferencedBy rdf:resource="#item_725"/>
        <dc:subject>NETWORKS</dc:subject>
        <dc:subject>contrastive learning</dc:subject>
        <dc:subject>sequential recommendation</dc:subject>
        <dc:title>Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation</dc:title>
        <dcterms:abstract>Recent advancements of sequential deep learning models such as Transformer and BERT have significantly facilitated the sequential recommendation. However, according to our study, the distribution of item embeddings generated by these models tends to degenerate into an anisotropic shape, which may result in high semantic similarities among embeddings. In this paper, both empirical and theoretical investigations of this representation degeneration problem are first provided, based on which a novel recommender model DuoRec is proposed to improve the item embeddings distribution. Specifically, in light of the uniformity property of contrastive learning, a contrastive regularization is designed for DuoRec to reshape the distribution of sequence representations. Given the convention that the recommendation task is performed by measuring the similarity between sequence representations and item embeddings in the same space via dot product, the regularization can be implicitly applied to the item embedding distribution. Existing contrastive learning methods mainly rely on data level augmentation for user-item interaction sequences through item cropping, masking, or reordering and can hardly provide semantically consistent augmentation samples. In DuoRec, a model-level augmentation is proposed based on Dropout to enable better semantic preserving. Furthermore, a novel sampling strategy is developed, where sequences having the same target item are chosen hard positive samples. Extensive experiments conducted on five datasets demonstrate the superior performance of the proposed DuoRec model compared with baseline methods. Visualization results of the learned representations validate that DuoRec can largely alleviate the representation degeneration problem.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000810504300088</dc:coverage>
        <bib:pages>813-823</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>WSDM'22: PROCEEDINGS OF THE FIFTEENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1317">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_725">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;162&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;173&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;58&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_726">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 1049-5258</dc:identifier>
                <dc:title>Stanford University</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dao</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fu</foaf:surname>
                        <foaf:givenName>DY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ermon</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rudra</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ré</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koyejo</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mohamed</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Agarwal</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Belgrave</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cho</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oh</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1335"/>
        <dcterms:isReferencedBy rdf:resource="#item_727"/>
        <dc:subject>MODEL</dc:subject>
        <dc:subject>COMPILER</dc:subject>
        <dc:subject>SET</dc:subject>
        <dc:title>FLASHATTENTION: Fast and Memory-Efficient Exact Attention with IO-Awareness</dc:title>
        <dcterms:abstract>Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware-accounting for reads and writes between levels of GPU memory. We propose FLASHATTENTION, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FLASHATTENTION, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FLASHATTENTION to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FLASHATTENTION trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 x speedup on GPT-2 (seq. length 1K), and 2.4 x speedup on long-range arena (seq. length 1K-4K). FLASHATTENTION and block-sparse FLASHATTENTION enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001213927503043</dc:coverage>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 35 (NEURIPS 2022)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1335">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_727">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;158&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;164&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;98&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_728">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>4</prism:volume>
                <dc:title>NATURE MACHINE INTELLIGENCE</dc:title>
                <dc:identifier>DOI 10.1038/s42256-022-00534-z</dc:identifier>
                <prism:number>10</prism:number>
                <dc:identifier>ISSN 2522-5839</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>WC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fang</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>DY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>JZ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yao</foaf:surname>
                        <foaf:givenName>JH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1442"/>
        <dcterms:isReferencedBy rdf:resource="#item_729"/>
        <dc:title>scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data</dc:title>
        <dcterms:abstract>Annotating cell types on the basis of single-cell RNA-seq data is a prerequisite for research on disease progress and tumour microenvironments. Here we show that existing annotation methods typically suffer from a lack of curated marker gene lists, improper handling of batch effects and difficulty in leveraging the latent gene-gene interaction information, impairing their generalization and robustness. We developed a pretrained deep neural network-based model, single-cell bidirectional encoder representations from transformers (scBERT), to overcome the challenges. Following BERT's approach to pretraining and fine-tuning, scBERT attains a general understanding of gene-gene interactions by being pretrained on huge amounts of unlabelled scRNA-seq data; it is then transferred to the cell type annotation task of unseen and user-specific scRNA-seq data for supervised fine-tuning. Extensive and rigorous benchmark studies validated the superior performance of scBERT on cell type annotation, novel cell type discovery, robustness to batch effects and model interpretability.
Cell type annotation is a core task for single cell RNA-sequencing, but current bioinformatic tools struggle with some of the underlying challenges, including high dimensionality, data sparsity, batch effects and a lack of labels. In a self-supervised approach, a transformer model called scBERT is pretrained on millions of unlabelled public single cell RNA-seq data and then fine-tuned with a small number of labelled samples for cell annotation tasks.</dcterms:abstract>
        <dc:date>2022 OCT</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000859605000002</dc:coverage>
        <bib:pages>852-+</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1442">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_729">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;128&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;133&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;60&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-9096-5">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-9096-5</dc:identifier>
                <dc:title>Chinese Academy of Sciences</dc:title>
                <dc:identifier>DOI 10.1145/3485447.3512217</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>XJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xiong</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gou</foaf:surname>
                        <foaf:givenName>GP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Z</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shi</foaf:surname>
                        <foaf:givenName>JZ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>ACM</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1325"/>
        <dcterms:isReferencedBy rdf:resource="#item_731"/>
        <dc:subject>NETWORK</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>Encrypted Traffic Classification</dc:subject>
        <dc:subject>Masked BURST Model</dc:subject>
        <dc:subject>Pre-training</dc:subject>
        <dc:subject>Same-origin BURST Prediction</dc:subject>
        <dc:title>ET-BERT: A Contextualized Datagram Representation with Pre-training Transformers for Encrypted Traffic Classification</dc:title>
        <dcterms:abstract>Encrypted traffic classification requires discriminative and robust traffic representation captured from content-invisible and imbalanced traffic data for accurate classification, which is challenging but indispensable to achieve network security and network management. The major limitation of existing solutions is that they highly rely on the deep features, which are overly dependent on data size and hard to generalize on unseen data. How to leverage the open-domain unlabeled traffic data to learn representation with strong generalization ability remains a key challenge. In this paper, we propose a new traffic representation model called Encrypted Traffic Bidirectional Encoder Representations from Transformer (ET-BERT), which pre-trains deep contextualized datagram-level representation from large-scale unlabeled data. The pre-trained model can be fine-tuned on a small number of task-specific labeled data and achieves state-of-the-art performance across five encrypted traffic classification tasks, remarkably pushing the F1 of ISCX-VPN-Service to 98.9% (5.2%.), Cross-Platform (Android) to 92.5% (5.4%.), CSTNET-TLS 1.3 to 97.4% (10.0%.). Notably, we provide explanation of the empirically powerful pre-training model by analyzing the randomness of ciphers. It gives us insights in understanding the boundary of classification ability over encrypted traffic. The code is available at: https://github.com/linwhitehat/ET-BERT.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000852713000064</dc:coverage>
        <bib:pages>633-642</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1325">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_731">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;112&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;118&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;42&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_734">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0263-6352"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mancia</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kreutz</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brunström</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Burnier</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grassi</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Januszewicz</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Muiesan</foaf:surname>
                        <foaf:givenName>ML</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tsioufis</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Agabiti-Rosei</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Algharably</foaf:surname>
                        <foaf:givenName>EA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Azizi</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Benetos</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Borghi</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hitij</foaf:surname>
                        <foaf:givenName>JB</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cifkova</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Coca</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cornelissen</foaf:surname>
                        <foaf:givenName>V</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cruickshank</foaf:surname>
                        <foaf:givenName>JK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cunha</foaf:surname>
                        <foaf:givenName>PG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Danser</foaf:surname>
                        <foaf:givenName>AHJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>de Pinho</foaf:surname>
                        <foaf:givenName>RM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Delles</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dominiczak</foaf:surname>
                        <foaf:givenName>AF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dorobantu</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Doumas</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fernández-Alfonso</foaf:surname>
                        <foaf:givenName>MS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Halimi</foaf:surname>
                        <foaf:givenName>JM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Járai</foaf:surname>
                        <foaf:givenName>Z</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jelakovic</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jordan</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kuznetsova</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Laurent</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lovic</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lurbe</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mahfoud</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Manolis</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Miglinas</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Narkiewicz</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Niiranen</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Palatini</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Parati</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pathak</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Persu</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Polonia</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Redon</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sarafidis</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schmieder</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Spronck</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stabouli</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stergiou</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Taddei</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Thomopoulos</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tomaszewski</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Van de Borne</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wanner</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Weber</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Williams</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>ZY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kjeldsen</foaf:surname>
                        <foaf:givenName>SE</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1283"/>
        <dcterms:isReferencedBy rdf:resource="#item_735"/>
        <dc:subject>AMBULATORY BLOOD-PRESSURE</dc:subject>
        <dc:subject>antihypertensive device interventions</dc:subject>
        <dc:subject>antihypertensive drug therapy</dc:subject>
        <dc:subject>blood pressure</dc:subject>
        <dc:subject>cardiac disease</dc:subject>
        <dc:subject>cardiovascular disease</dc:subject>
        <dc:subject>CHRONIC KIDNEY-DISEASE</dc:subject>
        <dc:subject>CONVERTING-ENZYME-INHIBITORS</dc:subject>
        <dc:subject>drug combinations</dc:subject>
        <dc:subject>END-POINT REDUCTION</dc:subject>
        <dc:subject>guidelines</dc:subject>
        <dc:subject>heart failure</dc:subject>
        <dc:subject>hypertension</dc:subject>
        <dc:subject>INTIMA-MEDIA THICKNESS</dc:subject>
        <dc:subject>ISOLATED SYSTOLIC HYPERTENSION</dc:subject>
        <dc:subject>kidney disease</dc:subject>
        <dc:subject>LEFT-VENTRICULAR HYPERTROPHY</dc:subject>
        <dc:subject>lifestyle interventions</dc:subject>
        <dc:subject>OBSTRUCTIVE SLEEP-APNEA</dc:subject>
        <dc:subject>organ damage</dc:subject>
        <dc:subject>patient's follow-up</dc:subject>
        <dc:subject>PULSE-WAVE VELOCITY</dc:subject>
        <dc:subject>secondary hypertension</dc:subject>
        <dc:subject>stroke</dc:subject>
        <dc:subject>WHITE-COAT HYPERTENSION</dc:subject>
        <dc:title>2023 ESH Guidelines for the management of arterial hypertension The Task Force for the management of arterial hypertension of the European Society of Hypertension Endorsed by the International Society of Hypertension (ISH) and the European Renal Association (ERA)</dc:title>
        <dcterms:abstract>Document Reviewers:Luis Alcocer (Mexico), Christina Antza (Greece), Mustafa Arici (Turkey), Eduardo Barbosa (Brazil), Adel Berbari (Lebanon), Luis Bronze (Portugal), John Chalmers (Australia), Tine De Backer (Belgium), Alejandro de la Sierra (Spain), Kyriakos Dimitriadis (Greece), Dorota Drozdz (Poland), Beatrice Duly-Bouhanick (France), Brent M. Egan (USA), Serap Erdine (Turkey), Claudio Ferri (Italy), Slavomira Filipova (Slovak Republic), Anthony Heagerty (UK), Michael Hecht Olsen (Denmark), Dagmara Hering (Poland), Sang Hyun Ihm (South Korea), Uday Jadhav (India), Manolis Kallistratos (Greece), Kazuomi Kario (Japan), Vasilios Kotsis (Greece), Adi Leiba (Israel), Patricio Lopez-Jaramillo (Colombia), Hans-Peter Marti (Norway), Terry McCormack (UK), Paolo Mulatero (Italy), Dike B. Ojji (Nigeria), Sungha Park (South Korea), Priit Pauklin (Estonia), Sabine Perl (Austria), Arman Postadzhian (Bulgaria), Aleksander Prejbisz (Poland), Venkata Ram (India), Ramiro Sanchez (Argentina), Markus Schlaich (Australia), Alta Schutte (Australia), Cristina Sierra (Spain), Sekib Sokolovic (Bosnia and Herzegovina), Jonas Spaak (Sweden), Dimitrios Terentes-Printzios (Greece), Bruno Trimarco (Italy), Thomas Unger (The Netherlands), Bert-Jan van den Born (The Netherlands), Anna Vachulova (Slovak Republic), Agostino Virdis (Italy), Jiguang Wang (China), Ulrich Wenzel (Germany), Paul Whelton (USA), Jiri Widimsky (Czech Republic), Jacek Wolf (Poland), Gregoire Wuerzner (Switzerland), Eugene Yang (USA), Yuqing Zhang (China).</dcterms:abstract>
        <dc:date>2023 DEC</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001100908200002</dc:coverage>
        <bib:pages>1874-2071</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0263-6352">
        <prism:volume>41</prism:volume>
        <dc:title>JOURNAL OF HYPERTENSION</dc:title>
        <dc:identifier>DOI 10.1097/HJH.0000000000003480</dc:identifier>
        <prism:number>12</prism:number>
        <dc:identifier>ISSN 0263-6352</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1283">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_735">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;387&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;412&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;1728&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_738">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>45</prism:volume>
                <dc:title>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</dc:title>
                <dc:identifier>DOI 10.1109/TPAMI.2023.3275156</dc:identifier>
                <prism:number>10</prism:number>
                <dc:identifier>ISSN 0162-8828</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhu</foaf:surname>
                        <foaf:givenName>XT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Clifton</foaf:surname>
                        <foaf:givenName>DA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1350"/>
        <dcterms:isReferencedBy rdf:resource="#item_739"/>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>INTEGRATION</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>transformer</dc:subject>
        <dc:subject>LANGUAGE</dc:subject>
        <dc:subject>taxonomy</dc:subject>
        <dc:subject>introductory</dc:subject>
        <dc:subject>Multimodal learning</dc:subject>
        <dc:subject>VISION</dc:subject>
        <dc:title>Multimodal Learning With Transformers: A Survey</dc:title>
        <dcterms:abstract>Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and Big Data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal Big Data era, (2) a systematic review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community.</dcterms:abstract>
        <dc:date>2023 OCT</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001068816800038</dc:coverage>
        <bib:pages>12113-12132</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1350">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_739">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;141&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;147&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;310&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_740">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>45</prism:volume>
                <dc:title>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</dc:title>
                <dc:identifier>DOI 10.1109/TPAMI.2022.3148210</dc:identifier>
                <prism:number>1</prism:number>
                <dc:identifier>ISSN 0162-8828</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stefanini</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cornia</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Baraldi</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cascianelli</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fiameni</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cucchiara</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1336"/>
        <dcterms:isReferencedBy rdf:resource="#item_741"/>
        <dc:subject>MODELS</dc:subject>
        <dc:subject>survey</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>LANGUAGE</dc:subject>
        <dc:subject>GENERATION</dc:subject>
        <dc:subject>Image captioning</dc:subject>
        <dc:subject>TRANSFORMER</dc:subject>
        <dc:subject>vision-and-language</dc:subject>
        <dc:title>From Show to Tell: A Survey on Deep Learning-Based Image Captioning</dc:title>
        <dcterms:abstract>Connecting Vision and Language plays an essential role in Generative Intelligence. For this reason, large research efforts have been devoted to image captioning, i.e. describing images with syntactically and semantically meaningful sentences. Starting from 2015 the task has generally been addressed with pipelines composed of a visual encoder and a language model for text generation. During these years, both components have evolved considerably through the exploitation of object regions, attributes, the introduction of multi-modal connections, fully-attentive approaches, and BERT-like early-fusion strategies. However, regardless of the impressive results, research in image captioning has not reached a conclusive answer yet. This work aims at providing a comprehensive overview of image captioning approaches, from visual encoding and text generation to training strategies, datasets, and evaluation metrics. In this respect, we quantitatively compare many relevant state-of-the-art approaches to identify the most impactful technical innovations in architectures and training strategies. Moreover, many variants of the problem and its open challenges are discussed. The final goal of this work is to serve as a tool for understanding the existing literature and highlighting the future directions for a research area where Computer Vision and Natural Language Processing can find an optimal synergy.</dcterms:abstract>
        <dc:date>2023 JAN 1</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000899419900033</dc:coverage>
        <bib:pages>539-559</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1336">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_741">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;100&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;106&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;254&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_744">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0007-1013"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yan</foaf:surname>
                        <foaf:givenName>LX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sha</foaf:surname>
                        <foaf:givenName>LL</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>LX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>YH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Martinez-Maldonado</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>GL</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>XY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jin</foaf:surname>
                        <foaf:givenName>YQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gasevic</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1435"/>
        <dcterms:isReferencedBy rdf:resource="#item_745"/>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>PRINCIPLES</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>GPT-3</dc:subject>
        <dc:subject>education</dc:subject>
        <dc:subject>large language models</dc:subject>
        <dc:subject>GENERATION</dc:subject>
        <dc:subject>pre-trained language models</dc:subject>
        <dc:subject>systematic scoping review</dc:subject>
        <dc:title>Practical and ethical challenges of large language models in education: A systematic scoping review</dc:title>
        <dcterms:abstract>Educational technology innovations leveraging large language models (LLMs) have shown the potential to automate the laborious process of generating and analysing textual content. While various innovations have been developed to automate a range of educational tasks (eg, question generation, feedback provision, and essay grading), there are concerns regarding the practicality and ethicality of these innovations. Such concerns may hinder future research and the adoption of LLMs-based innovations in authentic educational contexts. To address this, we conducted a systematic scoping review of 118 peer-reviewed papers published since 2017 to pinpoint the current state of research on using LLMs to automate and support educational tasks. The findings revealed 53 use cases for LLMs in automating education tasks, categorised into nine main categories: profiling/labelling, detection, grading, teaching support, prediction, knowledge representation, feedback, content generation, and recommendation. Additionally, we also identified several practical and ethical challenges, including low technological readiness, lack of replicability and transparency and insufficient privacy and beneficence considerations. The findings were summarised into three recommendations for future studies, including updating existing innovations with state-of-the-art models (eg, GPT-3/4), embracing the initiative of open-sourcing models/systems, and adopting a human-centred approach throughout the developmental process. As the intersection of AI and education is continuously evolving, the findings of this study can serve as an essential reference point for researchers, allowing them to leverage the strengths, learn from the limitations, and uncover potential research opportunities enabled by ChatGPT and other generative AI models.</dcterms:abstract>
        <dc:date>2024 JAN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001043538000001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0007-1013">
        <prism:volume>55</prism:volume>
        <dc:title>BRITISH JOURNAL OF EDUCATIONAL TECHNOLOGY</dc:title>
        <dc:identifier>DOI 10.1111/bjet.13370</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 0007-1013</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1435">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_745">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;73&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;73&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;89&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_746">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1526-1492"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>WF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>SY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cai</foaf:surname>
                        <foaf:givenName>ZH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>RY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yin</foaf:surname>
                        <foaf:givenName>LR</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1755"/>
        <dcterms:isReferencedBy rdf:resource="#item_747"/>
        <link:link rdf:resource="#item_1555"/>
        <dc:subject>ALBERT</dc:subject>
        <dc:subject>BiLSTM</dc:subject>
        <dc:subject>network pruning</dc:subject>
        <dc:subject>PAL-BERT</dc:subject>
        <dc:subject>pretraining language models</dc:subject>
        <dc:subject>pruning model</dc:subject>
        <dc:subject>question answering model</dc:subject>
        <dc:subject>TextCNN</dc:subject>
        <dc:title>PAL-BERT: An Improved Question Answering Model</dc:title>
        <dcterms:abstract>In the field of natural language processing (NLP), there have been various pre-training language models in recent years, with question answering systems gaining significant attention. However, as algorithms, data, and computing power advance, the issue of increasingly larger models and a growing number of parameters has surfaced. Consequently, model training has become more costly and less efficient. To enhance the efficiency and accuracy of the training process while reducing the model volume, this paper proposes a first-order pruning model PAL-BERT based on the ALBERT model according to the characteristics of question-answering (QA) system and language model. Firstly, a first-order network pruning method based on the ALBERT model is designed, and the PAL-BERT model is formed. Then, the parameter optimization strategy of the PAL-BERT model is formulated, and the Mish function was used as an activation function instead of ReLU to improve the performance. Finally, after comparison experiments with traditional deep learning models TextCNN and BiLSTM, it is confirmed that PALBERT is a pruning model compression method that can significantly reduce training time and optimize training efficiency. Compared with traditional models, PAL-BERT significantly improves the NLP task's performance.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001136017600001</dc:coverage>
        <bib:pages>2729-2745</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1526-1492">
        <prism:volume>139</prism:volume>
        <dc:title>CMES-COMPUTER MODELING IN ENGINEERING &amp; SCIENCES</dc:title>
        <dc:identifier>DOI 10.32604/cmes.2023.046692</dc:identifier>
        <prism:number>3</prism:number>
        <dc:identifier>ISSN 1526-1492</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1755">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_747">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;60&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;60&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;35&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1555">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1555/Zheng et al. - 2024 - PAL-BERT An Improved Question Answering Model.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://file.techscience.com/files/CMES/2024/TSP_CMES-139-3/TSP_CMES_46692/TSP_CMES_46692.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:33:34</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_750">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>23</prism:volume>
                <dc:title>SENSORS</dc:title>
                <dc:identifier>DOI 10.3390/s23010506</dc:identifier>
                <prism:number>1</prism:number>
                <dc:identifier>ISSN 1424-8220</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bello</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ng</foaf:surname>
                        <foaf:givenName>SC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Leung</foaf:surname>
                        <foaf:givenName>MF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1284"/>
        <dcterms:isReferencedBy rdf:resource="#item_751"/>
        <dc:subject>BERT</dc:subject>
        <dc:subject>sentiment analysis</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>CNN</dc:subject>
        <dc:subject>LSTM</dc:subject>
        <dc:subject>tweets</dc:subject>
        <dc:title>A BERT Framework to Sentiment Analysis of Tweets</dc:title>
        <dcterms:abstract>Sentiment analysis has been widely used in microblogging sites such as Twitter in recent decades, where millions of users express their opinions and thoughts because of its short and simple manner of expression. Several studies reveal the state of sentiment which does not express sentiment based on the user context because of different lengths and ambiguous emotional information. Hence, this study proposes text classification with the use of bidirectional encoder representations from transformers (BERT) for natural language processing with other variants. The experimental findings demonstrate that the combination of BERT with CNN, BERT with RNN, and BERT with BiLSTM performs well in terms of accuracy rate, precision rate, recall rate, and F1-score compared to when it was used with Word2vec and when it was used with no variant.</dcterms:abstract>
        <dc:date>2023 JAN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000908738500001</dc:coverage>
    </bib:Article>
    <bib:Memo rdf:about="#item_1284">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_751">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;54&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;55&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;31&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_752">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1549-9596"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Raza</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Uddin</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Almuhaimeed</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Akbar</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zou</foaf:surname>
                        <foaf:givenName>Q</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ahmad</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1294"/>
        <dcterms:isReferencedBy rdf:resource="#item_753"/>
        <dc:subject>CLASSIFICATION</dc:subject>
        <dc:subject>MODEL</dc:subject>
        <dc:subject>IDENTIFICATION</dc:subject>
        <dc:subject>ANTICANCER PEPTIDES</dc:subject>
        <dc:subject>AUTOIMMUNE</dc:subject>
        <dc:subject>DEEP NEURAL-NETWORK</dc:subject>
        <dc:subject>FEATURE-SELECTION</dc:subject>
        <dc:subject>INFLAMMATION</dc:subject>
        <dc:subject>OVERSAMPLING TECHNIQUE</dc:subject>
        <dc:subject>SMOTE</dc:subject>
        <dc:title>AIPs-SnTCN: Predicting Anti-Inflammatory Peptides Using fastText and Transformer Encoder-Based Hybrid Word Embedding with Self-Normalized Temporal Convolutional Networks</dc:title>
        <dcterms:abstract>Inflammation is a biologically resistant response to harmful stimuli, such as infection, damaged cells, toxic chemicals, or tissue injuries. Its purpose is to eradicate pathogenic micro-organisms or irritants and facilitate tissue repair. Prolonged inflammation can result in chronic inflammatory diseases. However, wet-laboratory-based treatments are costly and time-consuming and may have adverse side effects on normal cells. In the past decade, peptide therapeutics have gained significant attention due to their high specificity in targeting affected cells without affecting healthy cells. Motivated by the significance of peptide-based therapies, we developed a highly discriminative prediction model called AIPs-SnTCN to predict anti-inflammatory peptides accurately. The peptide samples are encoded using word embedding techniques such as skip-gram and attention-based bidirectional encoder representation using a transformer (BERT). The conjoint triad feature (CTF) also collects structure-based cluster profile features. The fused vector of word embedding and sequential features is formed to compensate for the limitations of single encoding methods. Support vector machine-based recursive feature elimination (SVM-RFE) is applied to choose the ranking-based optimal space. The optimized feature space is trained by using an improved self-normalized temporal convolutional network (SnTCN). The AIPs-SnTCN model achieved a predictive accuracy of 95.86% and an AUC of 0.97 by using training samples. In the case of the alternate training data set, our model obtained an accuracy of 92.04% and an AUC of 0.96. The proposed AIPs-SnTCN model outperformed existing models with an similar to 19% higher accuracy and an similar to 14% higher AUC value. The reliability and efficacy of our AIPs-SnTCN model make it a valuable tool for scientists and may play a beneficial role in pharmaceutical design and research academia.</dcterms:abstract>
        <dc:date>2023 OCT 31</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001098424600001</dc:coverage>
        <bib:pages>6537-6554</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1549-9596">
        <prism:volume>63</prism:volume>
        <dc:title>JOURNAL OF CHEMICAL INFORMATION AND MODELING</dc:title>
        <dc:identifier>DOI 10.1021/acs.jcim.3c01563</dc:identifier>
        <prism:number>21</prism:number>
        <dc:identifier>ISSN 1549-9596</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1294">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_753">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;53&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;53&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;108&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_754">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>56</prism:volume>
                <dc:title>ACM COMPUTING SURVEYS</dc:title>
                <dc:identifier>DOI 10.1145/3605943</dc:identifier>
                <prism:number>2</prism:number>
                <dc:identifier>ISSN 0360-0300</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Min</foaf:surname>
                        <foaf:givenName>BN</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ross</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sulem</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ben Veyseh</foaf:surname>
                        <foaf:givenName>AP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nguyen</foaf:surname>
                        <foaf:givenName>TH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sainz</foaf:surname>
                        <foaf:givenName>O</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Agirre</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Heintz</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roth</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1352"/>
        <dcterms:isReferencedBy rdf:resource="#item_755"/>
        <dc:subject>generative AI</dc:subject>
        <dc:subject>neural networks</dc:subject>
        <dc:subject>foundational models</dc:subject>
        <dc:subject>Large language models</dc:subject>
        <dc:title>Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey</dc:title>
        <dcterms:abstract>Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.</dcterms:abstract>
        <dc:date>2024 FEB</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001085637600005</dc:coverage>
    </bib:Article>
    <bib:Memo rdf:about="#item_1352">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_755">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;209&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;222&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;225&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_756">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1364-503X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Katz</foaf:surname>
                        <foaf:givenName>DM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bommarito</foaf:surname>
                        <foaf:givenName>MJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gao</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Arredondo</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_757"/>
        <link:link rdf:resource="#item_1563"/>
        <dc:subject>LAW</dc:subject>
        <dc:subject>large language models</dc:subject>
        <dc:subject>GPT-4</dc:subject>
        <dc:subject>Bar Exam</dc:subject>
        <dc:subject>legal complexity</dc:subject>
        <dc:subject>legal language</dc:subject>
        <dc:subject>legal services</dc:subject>
        <dc:title>GPT-4 passes the bar exam</dc:title>
        <dcterms:abstract>In this paper, we experimentally evaluate the zero-shot performance of GPT-4 against prior generations of GPT on the entire uniform bar examination (UBE), including not only the multiple-choice multistate bar examination (MBE), but also the open-ended multistate essay exam (MEE) and multistate performance test (MPT) components. On the MBE, GPT-4 significantly outperforms both human test-takers and prior models, demonstrating a 26% increase over ChatGPT and beating humans in five of seven subject areas. On the MEE and MPT, which have not previously been evaluated by scholars, GPT-4 scores an average of 4.2/6.0 when compared with much lower scores for ChatGPT. Graded across the UBE components, in the manner in which a human test-taker would be, GPT-4 scores approximately 297 points, significantly in excess of the passing threshold for all UBE jurisdictions. These findings document not just the rapid and remarkable advance of large language model performance generally, but also the potential for such models to support the delivery of legal services in society.This article is part of the theme issue 'A complexity science approach to law and governance'.</dcterms:abstract>
        <dc:date>2024 APR 15</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001175989800012</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1364-503X">
        <prism:volume>382</prism:volume>
        <dc:title>PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY A-MATHEMATICAL PHYSICAL AND ENGINEERING SCIENCES</dc:title>
        <dc:identifier>DOI 10.1098/rsta.2023.0254</dc:identifier>
        <prism:number>2270</prism:number>
        <dc:identifier>ISSN 1364-503X</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_757">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;67&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;69&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;85&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1563">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1563/Katz et al. - 2024 - GPT-4 passes the bar exam.pdf"/>
        <dc:title>PubMed Central Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://pmc.ncbi.nlm.nih.gov/articles/PMC10894685/pdf/rsta.2023.0254.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:34:32</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_758">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0169-2607"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>JN</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dada</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Puladi</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kleesiek</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Egger</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1307"/>
        <dcterms:isReferencedBy rdf:resource="#item_759"/>
        <dc:subject>NLP</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>Healthcare</dc:subject>
        <dc:subject>MEDICINE</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>Taxonomy</dc:subject>
        <dc:subject>AUTHOR</dc:subject>
        <dc:subject>Bard</dc:subject>
        <dc:subject>LANGUAGE MODELS</dc:subject>
        <dc:subject>LLaMA</dc:subject>
        <dc:subject>LLM</dc:subject>
        <dc:subject>OpenAI</dc:subject>
        <dc:title>ChatGPT in healthcare: A taxonomy and systematic review</dc:title>
        <dcterms:abstract>The recent release of ChatGPT, a chat bot research project/product of natural language processing (NLP) by OpenAI, stirs up a sensation among both the general public and medical professionals, amassing a phenomenally large user base in a short time. This is a typical example of the 'productization' of cutting-edge technologies, which allows the general public without a technical background to gain firsthand experience in artificial intelligence (AI), similar to the AI hype created by AlphaGo (DeepMind Technologies, UK) and self-driving cars (Google, Tesla, etc.). However, it is crucial, especially for healthcare researchers, to remain prudent amidst the hype. This work provides a systematic review of existing publications on the use of ChatGPT in healthcare, elucidating the 'status quo' of ChatGPT in medical applications, for general readers, healthcare professionals as well as NLP scientists. The large biomedical literature database PubMed is used to retrieve published works on this topic using the keyword 'ChatGPT'. An inclusion criterion and a taxonomy are further proposed to filter the search results and categorize the selected publications, respectively. It is found through the review that the current release of ChatGPT has achieved only moderate or 'passing' performance in a variety of tests, and is unreliable for actual clinical deployment, since it is not intended for clinical applications by design. We conclude that specialized NLP models trained on (bio)medical datasets still represent the right direction to pursue for critical clinical applications.</dcterms:abstract>
        <dc:date>2024 MAR</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001175237600001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0169-2607">
        <prism:volume>245</prism:volume>
        <dc:title>COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE</dc:title>
        <dc:identifier>DOI 10.1016/j.cmpb.2024.108013</dc:identifier>
        <dc:identifier>ISSN 0169-2607</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1307">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_759">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;58&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;58&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;146&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_760">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0933-3657"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Akbar</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zou</foaf:surname>
                        <foaf:givenName>Q</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Raza</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alarfaj</foaf:surname>
                        <foaf:givenName>FK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1346"/>
        <dcterms:isReferencedBy rdf:resource="#item_761"/>
        <dc:subject>CLASSIFICATION</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>IDENTIFICATION</dc:subject>
        <dc:subject>Antifungal peptides</dc:subject>
        <dc:subject>Bidirectional temporal convolutional networks</dc:subject>
        <dc:subject>CORROSION TYPE</dc:subject>
        <dc:subject>Feature selection</dc:subject>
        <dc:subject>PROTEIN</dc:subject>
        <dc:subject>Word embedding</dc:subject>
        <dc:title>iAFPs-Mv-BiTCN: Predicting antifungal peptides using self-attention transformer embedding and transform evolutionary based multi-view features with bidirectional temporal convolutional networks</dc:title>
        <dcterms:abstract>Globally, fungal infections have become a major health concern in humans. Fungal diseases generally occur due to the invading fungus appearing on a specific portion of the body and becoming hard for the human immune system to resist. The recent emergence of COVID-19 has intensely increased different nosocomial fungal infections. The existing wet -laboratory -based medications are expensive, time-consuming, and may have adverse side effects on normal cells. In the last decade, peptide therapeutics have gained significant attention due to their high specificity in targeting affected cells without affecting healthy cells. Motivated by the significance of peptide -based therapies, we developed a highly discriminative prediction scheme called iAFPs-Mv-BiTCN to predict antifungal peptides correctly. The training peptides are encoded using word embedding methods such as skip -gram and attention mechanism -based bidirectional encoder representation using transformer. Additionally, transform -based evolutionary features are generated using the Pseduo position -specific scoring matrix using discrete wavelet transform (PsePSSM-DWT). The fused vector of word embedding and evolutionary descriptors is formed to compensate for the limitations of single encoding methods. A Shapley Additive exPlanations (SHAP) based global interpolation approach is applied to reduce training costs by choosing the optimal feature set. The selected feature set is trained using a bi-directional temporal convolutional network (BiTCN). The proposed iAFPs-Mv-BiTCN model achieved a predictive accuracy of 98.15 % and an AUC of 0.99 using training samples. In the case of the independent samples, our model obtained an accuracy of 94.11 % and an AUC of 0.98. Our iAFPsMv-BiTCN model outperformed existing models with a -4 % and -5 % higher accuracy using training and independent samples, respectively. The reliability and efficacy of the proposed iAFPs-Mv-BiTCN model make it a valuable tool for scientists and may perform a beneficial role in pharmaceutical design and research academia.</dcterms:abstract>
        <dc:date>2024 MAY</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001240521300001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0933-3657">
        <prism:volume>151</prism:volume>
        <dc:title>ARTIFICIAL INTELLIGENCE IN MEDICINE</dc:title>
        <dc:identifier>DOI 10.1016/j.artmed.2024.102860</dc:identifier>
        <dc:identifier>ISSN 0933-3657</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1346">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_761">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;32&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;32&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;84&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_762">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>14</prism:volume>
                <dc:title>SCIENTIFIC REPORTS</dc:title>
                <dc:identifier>DOI 10.1038/s41598-024-51615-5</dc:identifier>
                <prism:number>1</prism:number>
                <dc:identifier>ISSN 2045-2322</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hassan</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Abd El-Hafeez</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shams</foaf:surname>
                        <foaf:givenName>MY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1430"/>
        <dcterms:isReferencedBy rdf:resource="#item_763"/>
        <dc:subject>PEPTIDES</dc:subject>
        <dc:title>Optimizing classification of diseases through language model analysis of symptoms</dc:title>
        <dcterms:abstract>This paper investigated the use of language models and deep learning techniques for automating disease prediction from symptoms. Specifically, we explored the use of two Medical Concept Normalization-Bidirectional Encoder Representations from Transformers (MCN-BERT) models and a Bidirectional Long Short-Term Memory (BiLSTM) model, each optimized with a different hyperparameter optimization method, to predict diseases from symptom descriptions. In this paper, we utilized two distinct dataset called Dataset-1, and Dataset-2. Dataset-1 consists of 1,200 data points, with each point representing a unique combination of disease labels and symptom descriptions. While, Dataset-2 is designed to identify Adverse Drug Reactions (ADRs) from Twitter data, comprising 23,516 rows categorized as ADR (1) or Non-ADR (0) tweets. The results indicate that the MCN-BERT model optimized with AdamP achieved 99.58% accuracy for Dataset-1 and 96.15% accuracy for Dataset-2. The MCN-BERT model optimized with AdamW performed well with 98.33% accuracy for Dataset-1 and 95.15% for Dataset-2, while the BiLSTM model optimized with Hyperopt achieved 97.08% accuracy for Dataset-1 and 94.15% for Dataset-2. Our findings suggest that language models and deep learning techniques have promise for supporting earlier detection and more prompt treatment of diseases, as well as expanding remote diagnostic capabilities. The MCN-BERT and BiLSTM models demonstrated robust performance in accurately predicting diseases from symptoms, indicating the potential for further related research.</dcterms:abstract>
        <dc:date>2024 JAN 17</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001184473900065</dc:coverage>
    </bib:Article>
    <bib:Memo rdf:about="#item_1430">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_763">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;28&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;29&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;50&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_764">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1041-4347"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liang</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>SH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tu</foaf:surname>
                        <foaf:givenName>WX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wen</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>XH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dong</foaf:surname>
                        <foaf:givenName>XJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>XW</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1751"/>
        <dcterms:isReferencedBy rdf:resource="#item_765"/>
        <link:link rdf:resource="#item_1554"/>
        <dc:subject>Graph learning</dc:subject>
        <dc:subject>knowledge graph embedding</dc:subject>
        <dc:subject>self-supervised contrastive learning</dc:subject>
        <dc:subject>symmetrical property</dc:subject>
        <dc:title>Knowledge Graph Contrastive Learning Based on Relation-Symmetrical Structure</dc:title>
        <dcterms:abstract>Knowledge graph embedding (KGE) aims at learning powerful representations to benefit various artificial intelligence applications. Meanwhile, contrastive learning has been widely leveraged in graph learning as an effective mechanism to enhance the discriminative capacity of the learned representations. However, the complex structures of KG make it hard to construct appropriate contrastive pairs. Only a few attempts have integrated contrastive learning strategies with KGE. But, most of them rely on language models (e.g., Bert) for contrastive pair construction instead of fully mining information underlying the graph structure, hindering expressive ability. Surprisingly, we find that the entities within a relational symmetrical structure are usually similar and correlated. To this end, we propose a knowledge graph contrastive learning framework based on relation-symmetrical structure, KGE-SymCL, which mines symmetrical structure information in KGs to enhance the discriminative ability of KGE models. Concretely, a plug-and-play approach is proposed by taking entities in the relation-symmetrical positions as positive pairs. Besides, a self-supervised alignment loss is designed to pull together positive pairs. Experimental results on link prediction and entity classification datasets demonstrate that our KGE-SymCL can be easily adopted to various KGE models for performance improvements. Moreover, extensive experiments show that our model could outperform other state-of-the-art baselines.</dcterms:abstract>
        <dc:date>2024 JAN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001124222100015</dc:coverage>
        <bib:pages>226-238</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1041-4347">
        <prism:volume>36</prism:volume>
        <dc:title>IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING</dc:title>
        <dc:identifier>DOI 10.1109/TKDE.2023.3282989</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 1041-4347</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1751">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_765">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;26&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;27&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;93&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1554">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1554/Liang et al. - 2024 - Knowledge Graph Contrastive Learning Based on Relation-Symmetrical Structure.pdf"/>
        <dc:title>Versão Submetida</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2211.10738</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:33:28</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_766">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2574-3805"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zaretsky</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>JM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Baskharoun</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>YN</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Austrian</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Aphinyanaphongs</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gupta</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Blecker</foaf:surname>
                        <foaf:givenName>SB</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Feldman</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1338"/>
        <dcterms:isReferencedBy rdf:resource="#item_767"/>
        <dc:subject>INFORMATION</dc:subject>
        <dc:subject>ACTIVATION</dc:subject>
        <dc:subject>READABILITY</dc:subject>
        <dc:subject>UNDERSTANDABILITY</dc:subject>
        <dc:title>Generative Artificial Intelligence to Transform Inpatient Discharge Summaries to Patient-Friendly Language and Format</dc:title>
        <dcterms:abstract>Importance By law, patients have immediate access to discharge notes in their medical records. Technical language and abbreviations make notes difficult to read and understand for a typical patient. Large language models (LLMs [eg, GPT-4]) have the potential to transform these notes into patient-friendly language and format. Objective To determine whether an LLM can transform discharge summaries into a format that is more readable and understandable. Design, Setting, and Participants This cross-sectional study evaluated a sample of the discharge summaries of adult patients discharged from the General Internal Medicine service at NYU (New York University) Langone Health from June 1 to 30, 2023. Patients discharged as deceased were excluded. All discharge summaries were processed by the LLM between July 26 and August 5, 2023. Interventions A secure Health Insurance Portability and Accountability Act-compliant platform, Microsoft Azure OpenAI, was used to transform these discharge summaries into a patient-friendly format between July 26 and August 5, 2023. Main Outcomes and Measures Outcomes included readability as measured by Flesch-Kincaid Grade Level and understandability using Patient Education Materials Assessment Tool (PEMAT) scores. Readability and understandability of the original discharge summaries were compared with the transformed, patient-friendly discharge summaries created through the LLM. As balancing metrics, accuracy and completeness of the patient-friendly version were measured. Results Discharge summaries of 50 patients (31 female [62.0%] and 19 male [38.0%]) were included. The median patient age was 65.5 (IQR, 59.0-77.5) years. Mean (SD) Flesch-Kincaid Grade Level was significantly lower in the patient-friendly discharge summaries (6.2 [0.5] vs 11.0 [1.5]; P &lt; .001). PEMAT understandability scores were significantly higher for patient-friendly discharge summaries (81% vs 13%; P &lt; .001). Two physicians reviewed each patient-friendly discharge summary for accuracy on a 6-point scale, with 54 of 100 reviews (54.0%) giving the best possible rating of 6. Summaries were rated entirely complete in 56 reviews (56.0%). Eighteen reviews noted safety concerns, mostly involving omissions, but also several inaccurate statements (termed hallucinations). Conclusions and Relevance The findings of this cross-sectional study of 50 discharge summaries suggest that LLMs can be used to translate discharge summaries into patient-friendly language and formats that are significantly more readable and understandable than discharge summaries as they appear in electronic health records. However, implementation will require improvements in accuracy, completeness, and safety. Given the safety concerns, initial implementation will require physician review.</dcterms:abstract>
        <dc:date>2024 MAR 11</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001185758300008</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2574-3805">
        <prism:volume>7</prism:volume>
        <dc:title>JAMA NETWORK OPEN</dc:title>
        <dc:identifier>DOI 10.1001/jamanetworkopen.2024.0357</dc:identifier>
        <prism:number>3</prism:number>
        <dc:identifier>ISSN 2574-3805</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1338">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_767">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;17&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;18&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;37&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_768">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2666-3899"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>JY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deng</foaf:surname>
                        <foaf:givenName>WY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Weng</foaf:surname>
                        <foaf:givenName>CH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>YY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1323"/>
        <dcterms:isReferencedBy rdf:resource="#item_769"/>
        <dc:subject>BIOMEDICAL TEXT</dc:subject>
        <dc:title>Enhancing phenotype recognition in clinical notes using large language models: PhenoBCBERT and PhenoGPT</dc:title>
        <dcterms:abstract>To enhance phenotype recognition in clinical notes of genetic diseases, we developed two models-PhenoBCBERT and PhenoGPT-for expanding the vocabularies of Human Phenotype Ontology (HPO) terms. While HPO offers a standardized vocabulary for phenotypes, existing tools often fail to capture the full scope of phenotypes due to limitations from traditional heuristic or rule -based approaches. Our models leverage large language models to automate the detection of phenotype terms, including those not in the current HPO. We compare these models with PhenoTagger, another HPO recognition tool, and found that our models identify a wider range of phenotype concepts, including previously uncharacterized ones. Our models also show strong performance in case studies on biomedical literature. We evaluate the strengths and weaknesses of BERT- and GPT-based models in aspects such as architecture and accuracy. Overall, our models enhance automated phenotype detection from clinical texts, improving downstream analyses on human diseases.</dcterms:abstract>
        <dc:date>2024 JAN 12</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001165274200001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2666-3899">
        <prism:volume>5</prism:volume>
        <dc:title>PATTERNS</dc:title>
        <dc:identifier>DOI 10.1016/j.patter.2023.100887</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 2666-3899</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1323">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_769">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;14&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;15&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;72&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_770">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 2159-5399</dc:identifier>
                <dc:title>Chinese Academy of Sciences</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hu</foaf:surname>
                        <foaf:givenName>BZ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sheng</foaf:surname>
                        <foaf:givenName>Q</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shi</foaf:surname>
                        <foaf:givenName>YH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>DD</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wooldridge</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dy</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Natarajan</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1300"/>
        <dcterms:isReferencedBy rdf:resource="#item_771"/>
        <dc:title>Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection</dc:title>
        <dcterms:abstract>Detecting fake news requires both a delicate sense of diverse clues and a profound understanding of the real-world background, which remains challenging for detectors based on small language models (SLMs) due to their knowledge and capability limitations. Recent advances in large language models (LLMs) have shown remarkable performance in various tasks, but whether and how LLMs could help with fake news detection remains underexplored. In this paper, we investigate the potential of LLMs in fake news detection. First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a gap to the LLM's inability to select and integrate rationales properly to conclude. Based on these findings, we propose that current LLMs may not substitute fine-tuned SLMs in fake news detection but can be a good advisor for SLMs by providing multi-perspective instructive rationales. To instantiate this proposal, we design an adaptive rationale guidance network for fake news detection (ARG), in which SLMs selectively acquire insights on news analysis from the LLMs' rationales. We further derive a rationale-free version of ARG by distillation, namely ARG-D, which services cost-sensitive scenarios without querying LLMs. Experiments on two real-world datasets demonstrate that ARG and ARG-D outperform three types of baseline methods, including SLM-based, LLM-based, and combinations of small and large language models.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001239985800026</dc:coverage>
        <bib:pages>22105-22113</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>THIRTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 38 NO 20</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1300">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_771">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;14&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;15&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;48&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_772">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>219</prism:volume>
                <dc:title>COMPUTERS &amp; EDUCATION</dc:title>
                <dc:identifier>DOI 10.1016/j.compedu.2024.105109</dc:identifier>
                <dc:identifier>ISSN 0360-1315</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Z</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>QH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ouyang</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Long</foaf:surname>
                        <foaf:givenName>TT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>SNYY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1438"/>
        <dcterms:isReferencedBy rdf:resource="#item_773"/>
        <dc:subject>BEHAVIOR</dc:subject>
        <dc:subject>COMPLEXITY</dc:subject>
        <dc:subject>OUTCOMES</dc:subject>
        <dc:subject>Data science applications in education</dc:subject>
        <dc:subject>Distance education and online learning</dc:subject>
        <dc:subject>Evaluation methodologies</dc:subject>
        <dc:subject>ONLINE</dc:subject>
        <dc:subject>Applications in subject areas</dc:subject>
        <dc:subject>COMMUNITIES</dc:subject>
        <dc:subject>DISCIPLINE</dc:subject>
        <dc:subject>INQUIRY</dc:subject>
        <dc:subject>Learning communities</dc:subject>
        <dc:title>Profiling students' learning engagement in MOOC discussions to identify learning achievement: An automated configurational approach</dc:title>
        <dcterms:abstract>In the Massive Online Open Course (MOOC) forum, learning engagement encompasses three fundamental dimensions-cognitive, emotional, and behavioral engagement-that intricately interact to jointly influence students' learning achievements. However, the interplay between multiple engagement dimensions and their correlations with learning achievement remain understudied, particularly across different academic disciplines. This study adopts an automated configurational approach that integrates bidirectional encoder representation from transformers (BERT) and fuzzy set qualitative comparative analysis (fsQCA) to explore the configurations of learning engagement, their connections with learning achievement, and variations across disciplines. Our analysis reveals a nuanced profile of learners' learning engagement, indicating the high-achieving individuals demonstrated more frequent posting and commenting behaviors and the high-level cognitive engagement than low-achieving individuals. Second, our analysis revealed multiple configurations where the coexistence or absence of factors at different levels of the cognitive, behavioral, and emotional dimensions significantly impacted learning achievement. Learners who conducted posting and replying behaviors, expressed positive emotions, and engaged in deep cognitive engagement tended to achieve superior learning outcomes. Third, there were significant differences in behavioral and emotional engagement among learners across different academic disciplines. Specifically, pure discipline learners were more inclined to engage in posting behaviors than the applied discipline learners. Across academic disciplines, positive emotions correlated strongly with higher achievement. These findings deepen our understanding of the multifaceted characteristics of learning engagement in MOOCs and highlight the importance of disciplinary distinctions, providing a foundation for educators and designers to optimize learners' MOOC effects and tailor learning experiences in diverse disciplinary contexts.</dcterms:abstract>
        <dc:date>2024 OCT</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001269314100001</dc:coverage>
    </bib:Article>
    <bib:Memo rdf:about="#item_1438">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_773">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;13&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;13&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;81&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-5386-3260-4">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:identifier>ISBN 978-1-5386-3260-4</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>XM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Long</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>IEEE</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1289"/>
        <dcterms:isReferencedBy rdf:resource="#item_775"/>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>SEMANTIC WEB</dc:subject>
        <dc:subject>Robotics</dc:subject>
        <dc:subject>LANGUAGE</dc:subject>
        <dc:subject>COMPUTER</dc:subject>
        <dc:subject>Multi-modal interaction</dc:subject>
        <dc:subject>question Answering</dc:subject>
        <dc:title>A Survey of Multi-modal Question Answering Systems for Robotics</dc:title>
        <dcterms:abstract>Multi-modal question answering system aims at automatically giving precise answers. The system is usually used for robotics with multi-modal interface. Question answering is the core technology of the system which involves natural language processing, semantic search, artificial intelligence, machine learning, information retrieval, and database management. This paper presents a brief overview of multi-modal question answering system.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000426453700034</dc:coverage>
        <bib:pages>189-194</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2017 2ND INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS AND MECHATRONICS (ICARM)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1289">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_775">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;1&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;1&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;86&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_776">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1751-8806"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tasar</foaf:surname>
                        <foaf:givenName>CO</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Komesli</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Unalir</foaf:surname>
                        <foaf:givenName>MO</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1385"/>
        <dcterms:isReferencedBy rdf:resource="#item_777"/>
        <dc:subject>semantic analysis</dc:subject>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>database management systems</dc:subject>
        <dc:subject>exclusion criteria</dc:subject>
        <dc:subject>inclusion criteria</dc:subject>
        <dc:subject>input questions</dc:subject>
        <dc:subject>Linked Data</dc:subject>
        <dc:subject>linked data technologies</dc:subject>
        <dc:subject>ontologies (artificial intelligence)</dc:subject>
        <dc:subject>question answering (information retrieval)</dc:subject>
        <dc:subject>question answering frameworks</dc:subject>
        <dc:subject>research questions</dc:subject>
        <dc:subject>semantic endpoints</dc:subject>
        <dc:subject>sentence-level recognition</dc:subject>
        <dc:subject>systematic mapping</dc:subject>
        <dc:title>Systematic mapping study on question answering frameworks over linked data</dc:title>
        <dcterms:abstract>Employing linked data technologies and semantic endpoints for question answering systems are expanding approaches among the researchers. Therefore, systems that combine syntactic and semantic analysis and enrich input questions by sentence-level recognition are examined. A systematic mapping study is conducted to identify and analyse the studies from major databases, journals and proceedings of conferences or workshops published between 2010 and 2017. With a set of 14 research questions, inclusion and exclusion criteria are specified. 53 studies are selected as primary studies from an initial set of 845 papers. This study provides a mapping while focusing on the methods and identifying the gaps between required and existing approaches. Popular approaches which have gained the most attention among researchers are given as a conclusion. Moreover, a comparison between the authors' study and related work in the literature is given to point out the differences and the contributions of their study. As the result of the comparison, it is concluded that the study is a novel and original topic on question answering frameworks.</dcterms:abstract>
        <dc:date>2018 DEC</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000452742700003</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1751-8806">
        <prism:volume>12</prism:volume>
        <dc:title>IET SOFTWARE</dc:title>
        <dc:identifier>DOI 10.1049/iet-sen.2018.5105</dc:identifier>
        <prism:number>6</prism:number>
        <dc:identifier>ISSN 1751-8806</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1385">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_777">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;3&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;3&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;32&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-351-12414-0">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-351-12414-0</dc:identifier>
                <dc:title>Government Engineering College Palakkad</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sandhini</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Binu</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vanchipura</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiji</foaf:surname>
                        <foaf:givenName>KS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1349"/>
        <dcterms:isReferencedBy rdf:resource="#item_779"/>
        <dc:title>Classification of question answering systems: A survey</dc:title>
        <dcterms:abstract>Question Answering Systems (QAS) are a unique kind of information retrieval. In Question Answering (QA) the system retrieves the precise answer to the questions asked by the user in natural language. QA is multidisciplinary. It involves information retrieval, natural language processing, linguistics, knowledge representation, databases, software engineering, artificial intelligence and so on. This paper classifies QAS based on different techniques used in answer ranking and answer extraction. The survey also provides the main contributions, experimental results, and limitations of various approaches. Finally, we discuss our perspective on the future direction of QAS.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000469287500106</dc:coverage>
        <bib:pages>779-784</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>EMERGING TRENDS IN ENGINEERING, SCIENCE AND TECHNOLOGY FOR SOCIETY, ENERGY AND ENVIRONMENT</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1349">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_779">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;1&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;1&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;24&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-6012-8">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4503-6012-8</dc:identifier>
                <dc:title>University System of Georgia</dc:title>
                <dc:identifier>DOI 10.1145/3278721.3278797</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Eicher</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>ACM</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1340"/>
        <dcterms:isReferencedBy rdf:resource="#item_781"/>
        <dc:subject>education access</dc:subject>
        <dc:subject>ethics of artificial intelligence</dc:subject>
        <dc:subject>intelligent tutoring systems</dc:subject>
        <dc:subject>metacognition</dc:subject>
        <dc:subject>online education</dc:subject>
        <dc:subject>question-answering systems</dc:subject>
        <dc:subject>theory of mind</dc:subject>
        <dc:subject>Turing test</dc:subject>
        <dc:subject>virtual teaching assistant</dc:subject>
        <dc:title>Giving AI a Theory of Mind</dc:title>
        <dcterms:abstract>Effective collaboration between humans and artificially intelligent agents will require that the two are equipped to build a sense of mutual understanding with each other. When humans have an intuitive understanding of the motives and intentions of other humans, it is known as Theory of Mind. My work revolves around designing artificial intelligence to leverage this capacity to improve human collaborations with artificial agents.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000510018100063</dc:coverage>
        <bib:pages>364-365</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1340">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_781">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;9&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:1742-6588">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1069</prism:volume>
                <dc:identifier>ISBN 1742-6588</dc:identifier>
                <dc:title>Shanghai University of Political Science &amp; Law</dc:title>
                <dc:identifier>DOI 10.1088/1742-6596/1069/1/012105</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pan</foaf:surname>
                        <foaf:givenName>XH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tseng</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kotenko</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1393"/>
        <dcterms:isReferencedBy rdf:resource="#item_783"/>
        <dc:title>Research on E-Commerce Automatic Question Answering System Model Based on Data Mining</dc:title>
        <dcterms:abstract>With the rapid development of the Internet and artificial intelligence, intelligent question answering systems have become current research hotspots because they can provide users with accurate answers and intelligent services. They are gradually entering the e-commerce field to replace some manual work. In this paper, we proposed several models including a user intent recognition model, a pattern of association rule mining model and a model of the entire e-commerce auto answering system. In addition, the application effects of these models were analysed. The models and analysis in this work are useful for constructing e-commerce automatic question answering system or making personalized recommendations and related services.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000443503700105</dc:coverage>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>3RD ANNUAL INTERNATIONAL CONFERENCE ON INFORMATION SYSTEM AND ARTIFICIAL INTELLIGENCE (ISAI2018)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1393">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_783">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;9&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_784">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>27</prism:volume>
                <dc:title>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</dc:title>
                <dc:identifier>DOI 10.1109/TASLP.2019.2926125</dc:identifier>
                <prism:number>10</prism:number>
                <dc:identifier>ISSN 2329-9290</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lan</foaf:surname>
                        <foaf:givenName>YS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>SH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiang</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1753"/>
        <dcterms:isReferencedBy rdf:resource="#item_785"/>
        <link:link rdf:resource="#item_1553"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>knowledge base question answering</dc:subject>
        <dc:title>Knowledge Base Question Answering With a Matching-Aggregation Model and Question-Specific Contextual Relations</dc:title>
        <dcterms:abstract>Making use of knowledge bases to answer questions (KBQA) is a key direction in question answering systems. Researchers have developed a diverse range of methods to address this problem, but there are still some limitations with the existing methods. Specifically, the existing neural network-based methods for KBQA have not taken advantage of the recent &quot;matching-aggregation&quot; framework for the sequence matching, and when representing a candidate answer entity, they may not choose the most useful context of the candidate for matching. In this paper, we explore the use of a &quot;matching-aggregation&quot; framework to match candidate answers with questions. We further make use of question-specific contextual relations to enhance the representations of candidate answer entities. Our complete method is able to achieve state-of-the-art performance on two benchmark datasets: WebQuestions and SimpleQuestions.</dcterms:abstract>
        <dc:date>2019 OCT</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000477727900002</dc:coverage>
        <bib:pages>1629-1638</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1753">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_785">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;32&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;35&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;51&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1553">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1553/Lan et al. - 2019 - Knowledge Base Question Answering With a Matching-Aggregation Model and Question-Specific Contextual.pdf"/>
        <dc:title>Versão Aceite</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?params=/context/sis_research/article/5904/&amp;path_info=TASLP_final.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:33:19</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_786">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>11717</prism:volume>
                <dc:identifier>ISBN 0302-9743</dc:identifier>
                <dc:title>University of Alberta</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-31605-1_14</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kano</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>MY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yoshioka</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rabelo</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kiyota</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Goebel</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Satoh</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kojima</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sakamoto</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mineshima</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Satoh</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1313"/>
        <dcterms:isReferencedBy rdf:resource="#item_787"/>
        <dc:subject>AI and law</dc:subject>
        <dc:subject>COLIEE</dc:subject>
        <dc:subject>Juris-informatics</dc:subject>
        <dc:subject>Legal information retrieval</dc:subject>
        <dc:subject>Legal question answering</dc:subject>
        <dc:subject>Legal textual entailment</dc:subject>
        <dc:title>COLIEE-2018: Evaluation of the Competition on Legal Information Extraction and Entailment</dc:title>
        <dcterms:abstract>We summarize the evaluation of the 5th Competition on Legal Information Extraction/Entailment 2018 (COLIEE-2018). The COLIEE-2018 tasks include two tasks in each of statute law and case law. The case law component includes an information retrieval (Task 1), and the confirmation of an entailment relation between an existing case and an unseen case (Task 2). The statute law component includes information retrieval (Task 3) and entailment/question answering (Task 4). Participation was open to any group based on any approach. 13 teams participated in the case law competition, and we received results from 7 teams where 6 submissions to Task 1 (12 runs), and 4 submissions to Task 2 (8 runs). Regarding the statute law, there were submissions of 17 runs from 8 teams (including 2 organizers' runs) for Task 3 and 7 runs from 3 teams for Task 4. We describe each team's approaches, our official evaluation, and analysis on our data and submission results. We also discuss possibilities for future competition tasks.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000772142300014</dc:coverage>
        <bib:pages>177-192</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>NEW FRONTIERS IN ARTIFICIAL INTELLIGENCE (JSAI-ISAI 2018)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1313">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_787">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;15&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;15&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;15&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_788">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>7</prism:volume>
                <dc:title>IEEE ACCESS</dc:title>
                <dc:identifier>DOI 10.1109/ACCESS.2019.2948081</dc:identifier>
                <dc:identifier>ISSN 2169-3536</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Honda</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hagiwara</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1390"/>
        <dcterms:isReferencedBy rdf:resource="#item_789"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Knowledge discovery</dc:subject>
        <dc:subject>Neural networks</dc:subject>
        <dc:subject>knowledge base</dc:subject>
        <dc:subject>Knowledge based systems</dc:subject>
        <dc:subject>neural machine translation</dc:subject>
        <dc:subject>prolog</dc:subject>
        <dc:subject>question answering system</dc:subject>
        <dc:subject>Reflective binary codes</dc:subject>
        <dc:subject>symbolic processing</dc:subject>
        <dc:subject>Training data</dc:subject>
        <dc:subject>Word2Vec</dc:subject>
        <dc:title>Question Answering Systems With Deep Learning-Based Symbolic Processing</dc:title>
        <dcterms:abstract>The authors propose methods to learn symbolic processing with deep learning and to build question answering systems by means of learned models. Symbolic processing, performed by the Prolog processing systems which execute unification, resolution, and list operations, is learned by a combination of deep learning models, Neural Machine Translation (NMT) and Word2Vec training. To our knowledge, the implementation of a Prolog-like processing system using deep learning is a new experiment that has not been conducted in the past. The results of their experiments revealed that the proposed methods are superior to the conventional methods because symbolic processing (1) has rich representations, (2) can interpret inputs even if they include unknown symbols, and (3) can be learned with a small amount of training data. In particular (2), handling of unknown data, which is a major task in artificial intelligence research, is solved using Word2Vec. Furthermore, question answering systems can be built from knowledge bases written in Prolog with learned symbolic processing, which, with conventional methods, is extremely difficult to accomplish. Their proposed systems can not only answer questions through powerful inferences by utilizing facts that harbor unknown data but also have the potential to build knowledge bases from a large amount of data, including unknown data, on the Web. The proposed systems are a completely new trial, there is no state-of-the-art methods in the sense of newest. Therefore, to evaluate their efficiency, they are compared with the most traditional and robust system i.e., the Prolog system. This is new research that encompasses the subjects of conventional artificial intelligence and neural network, and their systems have higher potential to build applications such as FAQ chatbots, decision support systems and energy-efficient estimation using a large amount of information on the Web. Mining hidden information through these applications will provide great value.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000497163000160</dc:coverage>
        <bib:pages>152368-152378</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1390">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_789">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;8&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;8&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;56&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_790">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>7</prism:volume>
                <dc:title>IEEE ACCESS</dc:title>
                <dc:identifier>DOI 10.1109/ACCESS.2019.2949993</dc:identifier>
                <dc:identifier>ISSN 2169-3536</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Su</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fan</foaf:surname>
                        <foaf:givenName>ZY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guizani</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1752"/>
        <dcterms:isReferencedBy rdf:resource="#item_791"/>
        <link:link rdf:resource="#item_1559"/>
        <dc:subject>Knowledge discovery</dc:subject>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>Internet of Things</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Cognition</dc:subject>
        <dc:subject>Knowledge based systems</dc:subject>
        <dc:subject>dynamic memory network</dc:subject>
        <dc:subject>human-machine interaction</dc:subject>
        <dc:subject>Internet of things</dc:subject>
        <dc:subject>knowledge base question answering systems</dc:subject>
        <dc:subject>LARGE-SCALE</dc:subject>
        <dc:subject>Man-machine systems</dc:subject>
        <dc:title>Answer Acquisition for Knowledge Base Question Answering Systems Based on Dynamic Memory Network</dc:title>
        <dcterms:abstract>In recent years, with the rapid growth of Artificial Intelligence (AI) and the Internet of Things (IoT), the question answering systems for human-machine interaction based on deep learning have become a research hotspot of the IoT. Different from the structured query method in traditional Knowledge Base Question Answering (KBQA) systems based on templates or rules, representation learning is one of the most promising approaches to solving the problems of data sparsity and semantic gaps. In this paper, an answer acquisition method for KBQA systems based on a dynamic memory network is proposed, in which representation learning is employed to represent the natural language questions that are raised by users and the knowledge base subgraphs of the related entities. These representations are taken as inputs of the dynamic memory network. The correct answers are obtained by utilizing the memory and inferential capabilities. The experimental results demonstrate the effectiveness of the proposed approach.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000497169800034</dc:coverage>
        <bib:pages>161329-161339</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1752">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_791">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;5&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;5&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;41&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1559">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1559/Su et al. - 2019 - Answer Acquisition for Knowledge Base Question Answering Systems Based on Dynamic Memory Network.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=8887234&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:34:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:1062-922X">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 1062-922X</dc:identifier>
                <dc:title>Purdue University System</dc:title>
                <dc:identifier>DOI 10.1109/smc.2019.8913898</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ye</foaf:surname>
                        <foaf:givenName>QF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Misra</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Devarapalli</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rayz</foaf:surname>
                        <foaf:givenName>JT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>IEEE</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_793"/>
        <dc:title>A SENTIMENT BASED NON-FACTOID QUESTION-ANSWERING FRAMEWORK</dc:title>
        <dcterms:abstract>With the rapid advances in Artificial Intelligence, a question of emotional intelligence of a system may become as important as its accuracy. This paper investigates whether emotions should be considered for non-factoid &quot;how&quot; Question-Answering systems with the eventual goal of enabling the system to retrieve answers in a more emotionally intelligent way. This study proposes an architecture that adds extended representation of sentiment information to questions and answers, and reports on to what extent a prediction of the best answer be improved by the proposed architecture.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000521353900061</dc:coverage>
        <bib:pages>372-377</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS (SMC)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_793">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;4&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;6&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;14&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-1-7281-4876-2">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-7281-4876-2</dc:identifier>
                <dc:title>Shri Mata Vaishno Devi University</dc:title>
                <dc:identifier>DOI 10.1109/iciccs48265.2020.9121015</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Farooq</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kaushik</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>IEEE</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1740"/>
        <dcterms:isReferencedBy rdf:resource="#item_795"/>
        <dc:subject>natural language processing (NLP)</dc:subject>
        <dc:subject>recurrent neural network (RNN)</dc:subject>
        <dc:subject>Bi-directional long short term memory (Bi LSTM)</dc:subject>
        <dc:subject>convolutional neural network (CNN)</dc:subject>
        <dc:subject>gated recurrent neural network (GRU)</dc:subject>
        <dc:subject>Reading comprehension (MRC)</dc:subject>
        <dc:title>Review of Deep Learning Techniques for Improving the Performance of Machine Reading Comprehension Problem</dc:title>
        <dcterms:abstract>The amazing research of Artificial Intelligence is natural language processing (NLP) and the mesmerizing field in NLP is machine reading comprehension (MRC). MRC alleviates the efforts of making machines behave like a human as it helps information accessing in natural language by developing Question answering systems. MRC is summarized as a task to read a piece of text, understand it, and answer the related question of the text. Reading text can be cloze style reading (fill in the blanks from the text) as well as open style reading (separate question) and understanding the piece of text as well as the query is accomplished by contextual representation and Attention mechanism. In the MRC literature, various methodologies have been used for extracting answers from the given text including primitive methods to the deep learning methods to have a step towards deploying machine intelligence. The introduction of deep learning and large datasets in the recent few years has encouraged the success of MRC. This paper gives a recent review of MRC models based on deep learning, datasets on which they have been evaluated, and also their word representations.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000609825100159</dc:coverage>
        <bib:pages>928-935</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND CONTROL SYSTEMS (ICICCS 2020)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1740">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_795">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;2&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;3&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;48&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_796">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1532-0626"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Aydin</foaf:surname>
                        <foaf:givenName>BI</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yilmaz</foaf:surname>
                        <foaf:givenName>YS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Demirbas</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1286"/>
        <dcterms:isReferencedBy rdf:resource="#item_797"/>
        <dc:subject>question answering</dc:subject>
        <dc:subject>choicem</dc:subject>
        <dc:subject>crowdsourcing</dc:subject>
        <dc:subject>multiple‐</dc:subject>
        <dc:title>A crowdsourced &quot;Who wants to be a millionaire?&quot; player</dc:title>
        <dcterms:abstract>Question answering is a fundamental problem for artificial intelligence research. State-of-the-art question answering systems, such as search engines, can answer well-formed factual questions. However, they fail on nonfactual and natural language queries. On the other hand, crowdsourcing leverages human intelligence to present solutions for problems, which are hard for computers. In the process of crowdsourcing, aggregating the responses from the crowd is a big challenge of itself. This work presents a model that integrates artificial intelligence with crowdsourcing to answer difficult natural language multiple choice questions. We build a crowdsourced mobile gaming experience for &quot;Who wants to be a millionaire?&quot; TV quiz show to test our methods. We use lightweight artificial intelligence models in aggregating the crowd responses. Our methods are able to answer even the hardest questions with very high accuracy. The experiment results suggest that building a super player for &quot;Who wants to be a millionaire?&quot; is feasible using our models. This study shows that facilitating crowdsourcing with artificial intelligence is an important tool for answering questions, which trouble state-of-the-art question answering systems.</dcterms:abstract>
        <dc:date>2021 APR 25</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000636346200018</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1532-0626">
        <prism:volume>33</prism:volume>
        <dc:title>CONCURRENCY AND COMPUTATION-PRACTICE &amp; EXPERIENCE</dc:title>
        <dc:identifier>DOI 10.1002/cpe.4168</dc:identifier>
        <prism:number>8</prism:number>
        <dc:identifier>ISSN 1532-0626</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1286">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_797">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;5&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;5&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;72&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_798">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>12699</prism:volume>
                <dc:identifier>ISBN 2945-9133</dc:identifier>
                <dc:title>Tallinn University of Technology</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-79876-5_29</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tammet</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Draheim</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Järv</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Platzer</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sutcliffe</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1316"/>
        <dcterms:isReferencedBy rdf:resource="#item_799"/>
        <dc:subject>LOGIC</dc:subject>
        <dc:title>Confidences for Commonsense Reasoning</dc:title>
        <dcterms:abstract>Commonsense reasoning has long been considered one of the holy grails of artificial intelligence. Our goal is to develop a logic-based component for hybrid - machine learning plus logic - commonsense question answering systems. A critical feature for the component is estimating the confidence in the statements derived from knowledge bases containing uncertain contrary and supporting evidence obtained from different sources. Instead of computing exact probabilities or designing a new calculus we focus on extending the methods and algorithms used by the existing automated reasoners for full classical first-order logic. The paper presents the CONFER framework and implementation for confidence estimation of derived answers.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000693448800029</dc:coverage>
        <bib:pages>507-524</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>AUTOMATED DEDUCTION, CADE 28</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1316">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_799">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;4&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;4&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;37&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_800">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2158-107X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alanazi</foaf:surname>
                        <foaf:givenName>SS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Elfadil</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jarajreh</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Algarni</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1391"/>
        <dcterms:isReferencedBy rdf:resource="#item_801"/>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>systematic literature review</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>knowledge systems</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>syntax</dc:subject>
        <dc:title>Question Answering Systems: A Systematic Literature Review</dc:title>
        <dcterms:abstract>Question answering systems (QAS) are developed to answer questions presented in natural language by extracting the answer. The development of QAS is aimed at making the Web more suited to human use by eliminating the need to sift through a lot of search results manually to determine the correct answer to a question. Accordingly, the aim of this study was to provide an overview of the current state of QAS research. It also aimed at highlighting the key limitations and gaps in the existing body of knowledge relating to QAS. Furthermore, it intended to identify the most effective methods utilized in the design of QAS. The systematic review of literature research method was selected as the most appropriate methodology for studying the research topic. This method differs from the conventional literature review as it is more comprehensive and objective. Based on the findings, QAS is a highly active area of research, with scholars taking diverse approaches in the development of their systems. Some of the limitations observed in these studies encompass the focused nature of current QAS, weaknesses associated with models that are used as building blocks for QAS, the need for standard datasets and question formats hence limiting the applicability of the QAS in practical settings, and the failure of researchers to examine their QAS solutions comprehensively. The most effective methods for designing QAS include focusing on syntax and context, utilizing word encoding and knowledge systems, leveraging deep learning, and using elements such as machine learning and artificial intelligence. Going forward, modular designs ought to be encouraged to foster collaboration in the creation of QAS.</dcterms:abstract>
        <dc:date>2021 MAR</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000639020900060</dc:coverage>
        <bib:pages>495-502</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2158-107X">
        <prism:volume>12</prism:volume>
        <dc:title>INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS</dc:title>
        <prism:number>3</prism:number>
        <dc:identifier>ISSN 2158-107X</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1391">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_801">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;3&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;3&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;84&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:1865-0929">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1389</prism:volume>
                <dc:identifier>ISBN 1865-0929</dc:identifier>
                <dc:title>University of Salerno</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-70629-6_15</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Monteleone</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bekavac</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kocijan</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Silberztein</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sojat</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1427"/>
        <dcterms:isReferencedBy rdf:resource="#item_803"/>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Artificial Intelligence</dc:subject>
        <dc:subject>Ontologies</dc:subject>
        <dc:subject>Formal Semantics</dc:subject>
        <dc:subject>Fuzzy Logic</dc:subject>
        <dc:subject>Lexicon-Grammar</dc:subject>
        <dc:subject>Machine Learning</dc:subject>
        <dc:subject>NooJ</dc:subject>
        <dc:subject>NooJ FSA/FSTs</dc:subject>
        <dc:subject>NooJ local grammars</dc:subject>
        <dc:subject>Question-answering systems</dc:subject>
        <dc:title>NooJ for Artificial Intelligence: An Anthropic Approach</dc:title>
        <dcterms:abstract>Today, Artificial Intelligence (AI) appears as a topic widely referred to for commercial purposes, while from a technical-scientific point of view, it remains of niche interest. Unfortunately, commercial purposes often lead to confusion in defining the functionalities of human-machine interfaces (HMIs), whereas such AI tools should have to efficiently imitate or replace human beings and body parts in specific cognitive tasks. Instead, AI is often defined too vaguely, that is, not from an anthropic point of view, and therefore, it remains quite distant from the human activities it tries to imitate. Today, AI is categorized among the cognitive sciences, i.e., those sciences that call upon computational neuro-biology (particularly neural networks), mathematical logic (as a part of mathematics and philosophy), and computer science.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001294555700015</dc:coverage>
        <bib:pages>173-184</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>FORMALISING NATURAL LANGUAGES: APPLICATIONS TO NATURAL LANGUAGE PROCESSING AND DIGITAL HUMANITIES, NOOJ 2020</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1427">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_803">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;2&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;2&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;18&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_804">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2509-4971"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>CC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1327"/>
        <dcterms:isReferencedBy rdf:resource="#item_805"/>
        <dc:subject>Explainable artificial intelligence</dc:subject>
        <dc:subject>Healthcare informatics</dc:subject>
        <dc:subject>Human-computer interactions</dc:subject>
        <dc:subject>Predictive modeling</dc:subject>
        <dc:title>Explainable Artificial Intelligence for Predictive Modeling in Healthcare</dc:title>
        <dcterms:abstract>The principle behind artificial intelligence is mimicking human intelligence in the way that it can perform tasks, recognize patterns, or predict outcomes through learning from the acquired data of various sources. Artificial intelligence and machine learning algorithms have been widely used in autonomous driving, recommender systems in electronic commerce and social media, fintech, natural language understanding, and question answering systems. Artificial intelligence is also gradually changing the landscape of healthcare research (Yu et al. in Biomed Eng 2:719-731, 25). The rule-based approach that relied on the curation of medical knowledge and the construction of robust decision rules had drawn significant attention in diagnosing diseases and clinical decision support since half a century ago. In recent years, machine learning algorithms such as deep learning that can account for complex interactions between features is shown to be promising in predictive modeling in healthcare (Deo in Circulation 132:1920-1930, 26). Although many of these artificial intelligence and machine learning algorithms can achieve remarkably high performance, it is often difficult to be completely adopted in practical clinical environments due to the lack of explainability in some of these algorithms. Explainable artificial intelligence (XAI) is emerging to assist in the communication of internal decisions, behavior, and actions to health care professionals. Through explaining the prediction outcomes, XAI gains the trust of the clinicians as they may learn how to apply the predictive modeling in practical situations instead of blindly following the predictions. There are still many scenarios to explore how to make XAI effective in clinical settings due to the complexity of medical knowledge.</dcterms:abstract>
        <dc:date>2022 JUN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000754125900001</dc:coverage>
        <bib:pages>228-239</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2509-4971">
        <prism:volume>6</prism:volume>
        <dc:title>JOURNAL OF HEALTHCARE INFORMATICS RESEARCH</dc:title>
        <dc:identifier>DOI 10.1007/s41666-022-00114-1</dc:identifier>
        <prism:number>2</prism:number>
        <dc:identifier>ISSN 2509-4971</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1327">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_805">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;57&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;61&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;27&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_806">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1046-8188"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>RQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>JF</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fan</foaf:surname>
                        <foaf:givenName>YX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cheng</foaf:surname>
                        <foaf:givenName>XQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1363"/>
        <dcterms:isReferencedBy rdf:resource="#item_807"/>
        <dc:subject>survey</dc:subject>
        <dc:subject>AUTOMATIC-GENERATION</dc:subject>
        <dc:subject>natural language generation</dc:subject>
        <dc:subject>Question generation</dc:subject>
        <dc:title>A Review on Question Generation from Natural Language Text</dc:title>
        <dcterms:abstract>Question generation is an important yet challenging problem in Artificial Intelligence (AI), which aims to generate natural and relevant questions from various input formats, e.g., natural language text, structure database, knowledge base, and image. In this article, we focus on question generation from natural language text, which has received tremendous interest in recent years due to the widespread applications such as data augmentation for question answering systems. During the past decades, many different question generation models have been proposed, from traditional rule-based methods to advanced neural network-based methods. Since there have been a large variety of research works proposed, we believe it is the right time to summarize the current status, learn from existing methodologies, and gain some insights for future development. In contrast to existing reviews, in this survey, we try to provide a more comprehensive taxonomy of question generation tasks from three different perspectives, i.e., the types of the input context text, the target answer, and the generated question. We take a deep look into existing models from different dimensions to analyze their underlying ideas, major design principles, and training strategies We compare these models through benchmark tasks to obtain an empirical understanding of the existing techniques. Moreover, we discuss what is missing in the current literature and what are the promising and desired future directions.</dcterms:abstract>
        <dc:date>2022 JAN</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000770678900014</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1046-8188">
        <prism:volume>40</prism:volume>
        <dc:title>ACM TRANSACTIONS ON INFORMATION SYSTEMS</dc:title>
        <dc:identifier>DOI 10.1145/3468889</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 1046-8188</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1363">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_807">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;23&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;24&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;262&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_808">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>10</prism:volume>
                <dc:title>IEEE ACCESS</dc:title>
                <dc:identifier>DOI 10.1109/ACCESS.2022.3155521</dc:identifier>
                <dc:identifier>ISSN 2169-3536</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fuad</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Al-Yahya</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1359"/>
        <dcterms:isReferencedBy rdf:resource="#item_809"/>
        <dc:subject>Licenses</dc:subject>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>MODELS</dc:subject>
        <dc:subject>chatbots</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Databases</dc:subject>
        <dc:subject>Arabic language</dc:subject>
        <dc:subject>Chatbots</dc:subject>
        <dc:subject>Conversational AI systems</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>QUESTION</dc:subject>
        <dc:subject>question answering systems</dc:subject>
        <dc:subject>task-oriented dialogue systems</dc:subject>
        <dc:title>Recent Developments in Arabic Conversational AI: A Literature Review</dc:title>
        <dcterms:abstract>Conversational AI is one of the most active research areas in AI, and it has gained more attention from academia as well as industry. Given recent advancements in several conversational AI systems in addition to the availability of several datasets, the aim of this study is to explore the landscape of Arabic text-based conversational AI systems. In this work, we provide a thorough review of recent Arabic conversational AI systems. We group them into three categories based on their functionality: (1) question-answering (QA) systems, (2) task-oriented dialogue systems (DS), and (3) chatbots. Furthermore, we describe the common datasets used in building and evaluating conversational AI systems in Arabic. Few surveys have targeted the conversational AI field for the Arabic language, and we aim to cover this gap with this study. Our contribution focuses on reviewing and analyzing the literature in the field and highlighting future research directions towards human-like conversational AI systems in Arabic.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000766548700001</dc:coverage>
        <bib:pages>23842-23859</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1359">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_809">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;7&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;7&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;90&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_810">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>10</prism:volume>
                <dc:title>IEEE ACCESS</dc:title>
                <dc:identifier>DOI 10.1109/ACCESS.2022.3157289</dc:identifier>
                <dc:identifier>ISSN 2169-3536</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kazemi</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mozafari</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nematbakhsh</foaf:surname>
                        <foaf:givenName>MA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1431"/>
        <dcterms:isReferencedBy rdf:resource="#item_811"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>Training</dc:subject>
        <dc:subject>question answering</dc:subject>
        <dc:subject>Machine translation</dc:subject>
        <dc:subject>Buildings</dc:subject>
        <dc:subject>Dataset</dc:subject>
        <dc:subject>Encyclopedias</dc:subject>
        <dc:subject>Internet</dc:subject>
        <dc:subject>machine reading comprehension</dc:subject>
        <dc:subject>Online services</dc:subject>
        <dc:subject>Persian</dc:subject>
        <dc:title>PersianQuAD: The Native Question Answering Dataset for the Persian Language</dc:title>
        <dcterms:abstract>Developing Question Answering systems (QA) is one of the main goals in Artificial Intelligence. With the advent of Deep Learning (DL) techniques, QA systems have witnessed significant advances. Although DL performs very well on QA, it requires a considerable amount of annotated data for training. Many annotated datasets have been built for the QA task; most of them are exclusively in English. In order to address the need for a high-quality QA dataset in the Persian language, we present PersianQuAD, the native QA dataset for the Persian language. We create PersianQuAD in four steps: 1) Wikipedia article selection, 2) question-answer collection, 3) three-candidates test set preparation, and 4) Data Quality Monitoring. PersianQuAD consists of approximately 20,000 questions and answers made by native annotators on a set of Persian Wikipedia articles. The answer to each question is a segment of the corresponding article text. To better understand PersianQuAD and ensure its representativeness, we analyze PersianQuAD and show it contains questions of varying types and difficulties. We also present three versions of a deep learning-based QA system trained with PersianQuAD. Our best system achieves an F1 score of 82.97% which is comparable to that of QA systems on English SQuAD, made by the Stanford University. This shows that PersianQuAD performs well for training deep-learning-based QA systems. Human performance on PersianQuAD is significantly better (96.49%), demonstrating that PersianQuAD is challenging enough and there is still plenty of room for future improvement. PersianQuAD and all QA models implemented in this paper are freely available.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000769961400001</dc:coverage>
        <bib:pages>26045-26057</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1431">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_811">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;7&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;7&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;41&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_812">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>3</prism:volume>
                <dc:title>AI OPEN</dc:title>
                <dc:identifier>DOI 10.1016/j.aiopen.2022.11.002</dc:identifier>
                <dc:identifier>ISSN 2666-6510</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>HG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>TY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>FX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>ZZ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>YY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>YY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vicente</foaf:surname>
                        <foaf:givenName>MA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_813"/>
        <link:link rdf:resource="#item_1560"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Text classification</dc:subject>
        <dc:subject>Attention mechanism</dc:subject>
        <dc:subject>Bilinear model</dc:subject>
        <dc:subject>Convolutional Neural Networks</dc:subject>
        <dc:subject>Judicial Examination</dc:subject>
        <dc:title>BCA: Bilinear Convolutional Neural Networks and Attention Networks for legal question answering</dc:title>
        <dcterms:abstract>The National Judicial Examination of China is an essential examination for selecting legal practitioners. In recent years, people have tried to use machine learning algorithms to answer examination questions. With the proposal of JEC-QA (Zhong et al. 2020), the judicial examination becomes a particular legal task. The data of judicial examination contains two types, i.e., Knowledge-Driven questions and Case-Analysis questions. Both require complex reasoning and text comprehension, thus challenging computers to answer judicial examination questions. We propose B ilinear C onvolutional Neural Networks and A ttention Networks ( BCA ) in this paper, which is an improved version based on the model proposed by our team on the Challenge of AI in Law 2021 judicial examination task. It has two essential modules, K nowledge- D riven M odule ( KDM ) for local features extraction and C ase- A nalysis M odule ( CAM ) for the semantic difference clarification between the question stem and the options. We also add a post-processing module to correct the results in the final stage. The experimental results show that our system achieves state -of -the -art in the offline test of the judicial examination task.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001219458300001</dc:coverage>
        <bib:pages>172-181</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_813">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;2&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;2&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;54&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1560">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1560/Zhang et al. - 2022 - BCA Bilinear Convolutional Neural Networks and Attention Networks for legal question answering.pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S2666651022000171/pdfft?download=true</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:34:18</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-6654-6403-1">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-6654-6403-1</dc:identifier>
                <dc:title>National University of Defense Technology - China</dc:title>
                <dc:identifier>DOI 10.1109/ICRSS57469.2022.00024</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shi</foaf:surname>
                        <foaf:givenName>YQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>YW</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>RP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>YT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>IEEE</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1754"/>
        <dcterms:isReferencedBy rdf:resource="#item_815"/>
        <link:link rdf:resource="#item_1728"/>
        <link:link rdf:resource="#item_1729"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Knowledge graphs</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>Artificial Intelligence</dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Robots</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Cognition</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Question answering (information retrieval)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>Deep Learning</dc:subject>
        <dc:subject>Entity Completion</dc:subject>
        <dc:subject>Knowledge Graph</dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>History</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Representation learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Resource description framework</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Knowledge Graph Completion Technology Research</dc:title>
        <dcterms:abstract>In recent years, knowledge graphs have been widely used in the fields of language cognition, search and recommendation, question answering systems, etc. However, with the continuous expansion of the scale of many knowledge graphs constructed by RDF, how to solve the incompleteness of knowledge graphs has become a research hotspot. knowledge graph completion technology is used to complete the missing content in the knowledge graph, mainly including entity completion and relationship completion. This paper mainly focuses on the completion problem, and sorts out completion methods based on deep representation learning. Through the analysis of the research history and latest progress of completion technology, the practical challenges and future development directions of the technology are put forward.</dcterms:abstract>
        <dc:date>2022-12</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000995890800014</dc:coverage>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/10086474</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <bib:pages>68-73</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 International Conference on Computing, Robotics and System Sciences (ICRSS)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1754">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_815">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;38&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1728">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1728/Shi et al. - 2022 - Knowledge Graph Completion Technology Research.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=10086474&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 13:10:23</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_1729">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1729/10086474.html"/>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/10086474</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 13:10:35</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_816">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1942-4787"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Budler</foaf:surname>
                        <foaf:givenName>LC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gosak</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stiglic</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1395"/>
        <dcterms:isReferencedBy rdf:resource="#item_817"/>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>conversational agents</dc:subject>
        <dc:subject>ASSISTANT</dc:subject>
        <dc:subject>CHATBOT</dc:subject>
        <dc:subject>EMBODIED CONVERSATIONAL AGENT</dc:subject>
        <dc:subject>FEASIBILITY</dc:subject>
        <dc:subject>health care</dc:subject>
        <dc:title>Review of artificial intelligence-based question-answering systems in healthcare</dc:title>
        <dcterms:abstract>Use of conversational agents, like chatbots, avatars, and robots is increasing worldwide. Yet, their effectiveness in health care is largely unknown. The aim of this advanced review was to assess the use and effectiveness of conversational agents in various fields of health care. A literature search, analysis, and synthesis were conducted in February 2022 in PubMed and CINAHL. The included evidence was analyzed narratively by employing the principles of thematic analysis. We reviewed articles on artificial intelligence-based question-answering systems in health care. Most of the identified articles report its effectiveness; less is known about its use. We outlined study findings and explored directions of future research, to provide evidence-based knowledge about artificial intelligence-based question-answering systems.This article is categorized under:Fundamental Concepts of Data and Knowledge &gt; Human Centricity and User InteractionApplication Areas &gt; Health CareTechnologies &gt; Artificial Intelligence</dcterms:abstract>
        <dc:date>2023 MAR</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000909897600001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1942-4787">
        <prism:volume>13</prism:volume>
        <dc:title>WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY</dc:title>
        <dc:identifier>DOI 10.1002/widm.1487</dc:identifier>
        <prism:number>2</prism:number>
        <dc:identifier>ISSN 1942-4787</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1395">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_817">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;19&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;19&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;86&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_818">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>11</prism:volume>
                <dc:title>IEEE ACCESS</dc:title>
                <dc:identifier>DOI 10.1109/ACCESS.2023.3291592</dc:identifier>
                <dc:identifier>ISSN 2169-3536</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tohma</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Okur</foaf:surname>
                        <foaf:givenName>HI</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kutlu</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sertbas</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1443"/>
        <dcterms:isReferencedBy rdf:resource="#item_819"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>sentiment analysis</dc:subject>
        <dc:subject>question answering</dc:subject>
        <dc:title>Sentiment Analysis in Turkish Question Answering Systems: An Application of Human-Robot Interaction</dc:title>
        <dcterms:abstract>The use of the sentiment analysis technique, which aims to extract emotions and thoughts from texts, has become a remarkable research topic today, where the importance of human-robot interaction is gradually increasing. In this study, a new hybrid sentiment analysis model is proposed using machine learning algorithms to increase emotional performance for Turkish question and answer systems. In this context, as a first, we apply text preprocessing steps to the Turkish question-answer-emotion dataset. Subsequently, we convert the preprocessed question and answer texts into text vector form using Pretrained Turkish BERT Model and two different word representation methods, TF-IDF and word2vec. Additionally, we incorporate pre-determined polarity vectors containing the positive and negative scores of words into the question-answer text vector. As a result of this study, we propose a new hybrid sentiment analysis model. We separate vectorized and expanded question-answer text vectors into training and testing data and train and test them with machine learning algorithms. By employing this previously unused method in Turkish question-answering systems, we achieve an accuracy value of up to 91.05% in sentiment analysis. Consequently, this study contributes to making human-robot interactions in Turkish more realistic and sensitive.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001028871500001</dc:coverage>
        <bib:pages>66522-66534</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1443">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_819">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;5&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;5&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;61&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_820">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1674-1056"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>ZY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>FK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wan</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yuan</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>ZG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Meng</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>YG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1422"/>
        <dcterms:isReferencedBy rdf:resource="#item_821"/>
        <dc:subject>materials science</dc:subject>
        <dc:subject>01.50.hv</dc:subject>
        <dc:subject>81.05.Zx</dc:subject>
        <dc:subject>81.16.Be</dc:subject>
        <dc:subject>generative artificial intelligence</dc:subject>
        <dc:subject>MatChat</dc:subject>
        <dc:title>MatChat: A large language model and application service platform for materials science</dc:title>
        <dcterms:abstract>The prediction of chemical synthesis pathways plays a pivotal role in materials science research. Challenges, such as the complexity of synthesis pathways and the lack of comprehensive datasets, currently hinder our ability to predict these chemical processes accurately. However, recent advancements in generative artificial intelligence (GAI), including automated text generation and question-answering systems, coupled with fine-tuning techniques, have facilitated the deployment of large-scale AI models tailored to specific domains. In this study, we harness the power of the LLaMA2-7B model and enhance it through a learning process that incorporates 13878 pieces of structured material knowledge data. This specialized AI model, named MatChat, focuses on predicting inorganic material synthesis pathways. MatChat exhibits remarkable proficiency in generating and reasoning with knowledge in materials science. Although MatChat requires further refinement to meet the diverse material design needs, this research undeniably highlights its impressive reasoning capabilities and innovative potential in materials science. MatChat is now accessible online and open for use, with both the model and its application framework available as open source. This study establishes a robust foundation for collaborative innovation in the integration of generative AI in materials science.</dcterms:abstract>
        <dc:date>2023 NOV 1</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001121059200001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1674-1056">
        <prism:volume>32</prism:volume>
        <dc:title>CHINESE PHYSICS B</dc:title>
        <dc:identifier>DOI 10.1088/1674-1056/ad04cb</dc:identifier>
        <prism:number>11</prism:number>
        <dc:identifier>ISSN 1674-1056</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1422">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_821">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;4&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;5&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;33&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_822">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2376-5992"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alrayzah</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alsolami</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saleh</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1306"/>
        <dcterms:isReferencedBy rdf:resource="#item_823"/>
        <dc:subject>Transformer models</dc:subject>
        <dc:subject>Answer extraction</dc:subject>
        <dc:subject>Arabic question answering</dc:subject>
        <dc:subject>Arabic question answering dataset</dc:subject>
        <dc:subject>Information retrieval</dc:subject>
        <dc:title>Challenges and opportunities for Arabic question-answering systems: current techniques and future directions</dc:title>
        <dcterms:abstract>Artificial intelligence-based question-answering (QA) systems can expedite the performance of various tasks. These systems either read passages and answer questions given in natural languages or if a question is given, they extract the most accurate answer from documents retrieved from the internet. Arabic is spoken by Arabs and Muslims and is located in the middle of the Arab world, which encompasses the Middle East and North Africa. It is difficult to use natural language processing techniques to process modern Arabic owing to the language's complex morphology, orthographic ambiguity, regional variations in spoken Arabic, and limited linguistic and technological resources. Only a few Arabic QA experiments and systems have been designed on small datasets, some of which are yet to be made available. Although several reviews of Arabic QA studies have been conducted, the number of studies covered has been limited and recent trends have not been included. To the best of our knowledge, only two systematic reviews focused on Arabic QA have been published to date. One covered only 26 primary studies without considering recent techniques, while the other covered only nine studies conducted for Holy Qur'an QA systems. Here, the included studies were analyzed in terms of the datasets used, domains covered, types of Arabic questions asked, information retrieved, the mechanism used to extract answers, and the techniques used. Based on the results of the analysis, several limitations, concerns, and recommendations for future research were identified. Additionally, a novel taxonomy was developed to categorize the techniques used based on the domains and approaches of the QA system.</dcterms:abstract>
        <dc:date>2023 OCT 20</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001087568300001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2376-5992">
        <prism:volume>9</prism:volume>
        <dc:title>PEERJ COMPUTER SCIENCE</dc:title>
        <dc:identifier>DOI 10.7717/peerj-cs.1633</dc:identifier>
        <dc:identifier>ISSN 2376-5992</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1306">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_823">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;1&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;1&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;127&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_824">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2413-9351"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saoudi</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gammoudi</foaf:surname>
                        <foaf:givenName>MM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1401"/>
        <dcterms:isReferencedBy rdf:resource="#item_825"/>
        <dc:subject>Performance evaluation</dc:subject>
        <dc:subject>LLMs</dc:subject>
        <dc:subject>Taxonomy</dc:subject>
        <dc:subject>Arabic conversational AI challenges</dc:subject>
        <dc:subject>Arabic question answering systems</dc:subject>
        <dc:subject>Chatbot</dc:subject>
        <dc:subject>Generative artificial intelligence</dc:subject>
        <dc:title>TRENDS AND CHALLENGES OF ARABIC CHATBOTS: LITERATURE REVIEW</dc:title>
        <dcterms:abstract>Conversational systems have recently garnered increased attention due to advancements in Large Language Models (LLMs) and Language Models for Dialogue Applications (LaMDA). However, conversational Artificial Intelligence (AI) research focuses primarily on English. Despite Arabic being one of the most widely used languages on the Internet, only a few studies have concentrated on Arabic conversational dialogue systems thus far. This study presents a comprehensive qualitative analysis of critical research works in this domain to examine the strengths and limitations of existing approaches. The analysis begins with an overview of chatbot history and classification, then explores the language challenges encountered when developing Generative Arabic Conversational AI. Rule-based/Retrieval-based and deep learning-based approaches for Arabic chatbots are also examined. Furthermore, the study investigates the evolution of Generative Conversational AI with the advancements in deep-learning techniques. It also comprehensively reviews various metrics used to assess conversational systems.</dcterms:abstract>
        <dc:date>2023 SEP</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001088472900006</dc:coverage>
        <bib:pages>261-286</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2413-9351">
        <prism:volume>9</prism:volume>
        <dc:title>JORDANIAN JOURNAL OF COMPUTERS AND INFORMATION TECHNOLOGY</dc:title>
        <prism:number>3</prism:number>
        <dc:identifier>ISSN 2413-9351</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1401">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_825">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;1&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;1&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;147&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_826">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1787-5021"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>ZG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ligeti-Nagy</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1304"/>
        <dcterms:isReferencedBy rdf:resource="#item_827"/>
        <dc:title>Building machine reading comprehension model from scratch</dc:title>
        <dcterms:abstract>In this paper, we introduce a machine reading comprehension model and how we built this model from scratch. Reading comprehension is a crucial requisite for artificial intelligence applications, such as Question-Answering systems, chatbots, virtual assistants etc. Reading comprehension task requires the highest complexity of natural language processing methods. In recent years, the transformer neural architecture could achieve the ability to solve high complexity tasks. To make these applications available in Hungarian as well it is inevitable to design a Hungarian corpus of reading comprehension so that the pretrained models can be fine-tuned on this dataset.
In our research, we have created the HuRC (Hungarian Reading Comprehension) corpus, which is the first dataset in Hungarian aiming to train, test and evaluate language models on a reading comprehension task. We built such a dataset based on the English ReCoRD corpus. This is a dataset of 120,000 examples consisting of news articles containing a passage and a close-style query, where a named entity is masked and the reference answer has to be found in a list.
Using the evaluated dataset and transformers' question-answering library, we have built the first neural machine reading comprehension models in commonsense reasoning task for Hungarian.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001081200200008</dc:coverage>
        <bib:pages>107-123</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1787-5021">
        <prism:volume>57</prism:volume>
        <dc:title>ANNALES MATHEMATICAE ET INFORMATICAE</dc:title>
        <dc:identifier>DOI 10.33039/ami.2023.03.001</dc:identifier>
        <dc:identifier>ISSN 1787-5021</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1304">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_827">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;1&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;1&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;27&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="#item_828">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>14120</prism:volume>
                <dc:identifier>ISBN 2945-9133</dc:identifier>
                <dc:title>Guangxi Normal University</dc:title>
                <dc:identifier>DOI 10.1007/978-3-031-40292-0_21</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>GB</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>XD</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhu</foaf:surname>
                        <foaf:givenName>JL</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jin</foaf:surname>
                        <foaf:givenName>Z</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiang</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Buchmann</foaf:surname>
                        <foaf:givenName>RA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bi</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ghiran</foaf:surname>
                        <foaf:givenName>AM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ma</foaf:surname>
                        <foaf:givenName>W</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1716"/>
        <dcterms:isReferencedBy rdf:resource="#item_829"/>
        <dc:subject>Attention mechanism</dc:subject>
        <dc:subject>Pre-trained language model</dc:subject>
        <dc:subject>Binary classification</dc:subject>
        <dc:subject>Legal intelligence</dc:subject>
        <dc:subject>Question answering</dc:subject>
        <dc:title>A Legal Multi-Choice Question Answering Model Based on BERT and Attention</dc:title>
        <dcterms:abstract>Legal question answering is one of the critical topics in the field of legal intelligence, and the judicial examination is a multi-choice question-answering task. However, previous scoring-based methods for the multi-choice task suffer from the classification bias problem and can not fully exploit the relationship between reference books, questions, and multi-choice answers. To address the issues, in this paper, we propose a BERT and attention based model for the judicial examination task. Specifically, we first use the BM25 algorithm to retrieve articles in the reference law books to provide certain information to answer a given question. Second, we input a question, its multi-choice answers, and the retrieved relevant articles to BERT for encoding. Third, we input the encoded information to the module of information fuse and match through attention mechanism to find deep relationships in articles, questions, and multi-choice answers. Finally, we concatenate the output of the attention mechanism and input it to a binary classifier to determine whether an answer option to the question is correct. We have done extensive experiments to show that our model significantly outperforms the baselines in accuracy. We also experimentally demonstrate the effectiveness of some components of our model. Moreover, we applied our method in the Challenge of AI in Law 2022 (CAIL 2022) and won the third prize in the judicial examination task in this competition.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001295192800021</dc:coverage>
        <bib:pages>250-266</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>KNOWLEDGE SCIENCE, ENGINEERING AND MANAGEMENT, PT IV, KSEM 2023</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1716">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser de acesso pago&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_829">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;26&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_830">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1220-1758"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khensous</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Labed</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Labed</foaf:surname>
                        <foaf:givenName>Z</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1328"/>
        <dcterms:isReferencedBy rdf:resource="#item_831"/>
        <dc:subject>NLP</dc:subject>
        <dc:subject>AI</dc:subject>
        <dc:subject>Deep Learning</dc:subject>
        <dc:subject>Education</dc:subject>
        <dc:title>Exploring the evolution and applications of natural language processing in education</dc:title>
        <dcterms:abstract>Natural Language Processing (NLP) is a branch of computer science, artificial intelligence, information engineering, and linguistics that focuses on enabling machines to understand, interpret, and generate human language. Machine translation, sentiment analysis, speech recognition, and question-answering systems are just a few examples of NLP applications. NLP has advanced rapidly in recent years, owing to the availability of large amounts of text data and the development of deep learning models, which has resulted in increased accuracy and efficiency in NLP tasks. This paper tackles the state of the art of NLP, its methodology and techniques, and its applications in different fields. The paper also discusses the role of NLP to improve education.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001072199300005</dc:coverage>
        <bib:pages>61-74</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1220-1758">
        <prism:volume>33</prism:volume>
        <dc:title>ROMANIAN JOURNAL OF INFORMATION TECHNOLOGY AND AUTOMATIC CONTROL-REVISTA ROMANA DE INFORMATICA SI AUTOMATICA</dc:title>
        <dc:identifier>DOI 10.33436/v33i2y202305</dc:identifier>
        <prism:number>2</prism:number>
        <dc:identifier>ISSN 1220-1758</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1328">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_831">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;41&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:979-8-4007-0830-5">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 979-8-4007-0830-5</dc:identifier>
                <dc:title>China Southern Power Grid</dc:title>
                <dc:identifier>DOI 10.1145/3650400.3650526</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>WQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>XM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Q</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>QY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>XS</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>ASSOC COMPUTING MACHINERY</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1418"/>
        <dcterms:isReferencedBy rdf:resource="#item_833"/>
        <dc:subject>Knowledge Graph</dc:subject>
        <dc:subject>Electrical Engineering</dc:subject>
        <dc:subject>Large Language Models</dc:subject>
        <dc:title>Knowledge Graph-Based Credibility Evaluation Method for Electric Grid Large Language Model Knowledge Question-Answering</dc:title>
        <dcterms:abstract>In the field of electricity, specialized terminology is often intricate and complex, making it challenging for non-experts to comprehend. However, with the advancement of artificial intelligence technology, the emergence of large language models provides a new technological solution to address this issue. Large language models, based on deep learning techniques, have the capability to quickly understand and interpret specialized terminology in the electricity domain through learning from a vast corpus of professional literature and data. They can then be applied to various domains, including question-answering systems. However, existing large language models still face issues of unreliable outputs, necessitating a method to evaluate their results and improve the quality of their applications. We propose a knowledge graph-based credibility evaluation method for electric grid large language model knowledge question-answering. This method aligns the answers generated by large language models with the knowledge graph of a local knowledge base and calculates their cosine similarity and Pearson correlation coefficient. We batch-process the answers from the large language model into an electricity dataset and validate them using this method. Experimental results demonstrate that this method can accurately and efficiently reflect the relevance between texts, providing a reliable scoring basis for question-answering by large models in vertical domains. Future research can focus on exploring other embedding methods that can better extract semantic relationships between texts and validating the feasibility of this method in vertical domains other than electricity.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001283896700126</dc:coverage>
        <bib:pages>754-759</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF 2023 7TH INTERNATIONAL CONFERENCE ON ELECTRONIC INFORMATION TECHNOLOGY AND COMPUTER ENGINEERING, EITCE 2023</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1418">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_833">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;20&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:979-8-4007-0124-5">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 979-8-4007-0124-5</dc:identifier>
                <dc:title>Beijing Information Science &amp; Technology University</dc:title>
                <dc:identifier>DOI 10.1145/3583780.3614767</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ma</foaf:surname>
                        <foaf:givenName>DH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chang</foaf:surname>
                        <foaf:givenName>KCC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>YG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lv</foaf:surname>
                        <foaf:givenName>XQ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>ACM</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1288"/>
        <dcterms:isReferencedBy rdf:resource="#item_835"/>
        <dc:subject>Bayesian inference</dc:subject>
        <dc:subject>Paraphrasing</dc:subject>
        <dc:subject>Pointwise mutual information</dc:subject>
        <dc:title>A Principled Decomposition of Pointwise Mutual Information for Intention Template Discovery</dc:title>
        <dcterms:abstract>With the rise of Artificial Intelligence (AI), question answering systems have become common for users to interact with computers, e.g., ChatGPT and Siri. These systems require a substantial amount of labeled data to train their models. However, the labeled data is scarce and challenging to be constructed. The construction process typically involves two stages: discovering potential sample candidates and manually labeling these candidates. To discover high-quality candidate samples, we study the intention paraphrase template discovery task: Given some seed questions or templates of an intention, discover new paraphrase templates that describe the intention and are diverse to the seeds enough in text. As the first exploration of the task, we identify the new quality requirements, i.e., relevance, divergence and popularity, and identify the new challenges, i.e., the paradox of divergent yet relevant paraphrases, and the conflict of popular yet relevant paraphrases. To untangle the paradox of divergent yet relevant paraphrases, in which the traditional bag of words falls short, we develop usage-centric modeling, which represents a question/template/answer as a bag of usages that users engaged (e.g., up-votes), and uses a usage-flow graph to interrelate templates, questions and answers. To balance the conflict of popular yet relevant paraphrases, we propose a new and principled decomposition for the well-known Pointwise Mutual Information from the usage perspective (usage-PMI), and then develop a Bayesian inference framework over the usage-flow graph to estimate the usage-PMI. Extensive experiments over three large CQA corpora show strong performance advantage over the baselines adopted from paraphrase identification task. We release 885, 000 paraphrase templates of high quality discovered by our proposed PMI decomposition model, and the data is available in site https://github.com/Para-Questions/Intention_template_discovery.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001161549501082</dc:coverage>
        <bib:pages>1746-1755</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 32ND ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2023</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1288">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_835">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;38&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:979-8-4007-0602-8">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 979-8-4007-0602-8</dc:identifier>
                <dc:title>Dublin City University</dc:title>
                <dc:identifier>DOI 10.1145/3652583.3658890</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mai</foaf:surname>
                        <foaf:givenName>TT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tran</foaf:surname>
                        <foaf:givenName>QL</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tran</foaf:surname>
                        <foaf:givenName>LD</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ninh</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dang-Nguyen</foaf:surname>
                        <foaf:givenName>DT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gurrin</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>ASSOC COMPUTING MACHINERY</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1375"/>
        <dcterms:isReferencedBy rdf:resource="#item_837"/>
        <dc:subject>Artificial Intelligence</dc:subject>
        <dc:subject>Large Language Models</dc:subject>
        <dc:subject>Multimedia</dc:subject>
        <dc:subject>Question Answering Systems</dc:subject>
        <dc:title>The First ACM Workshop on AI-Powered Question Answering Systems for Multimedia</dc:title>
        <dcterms:abstract>The advent of large language models (LLMs) has energised research in Question-Answering (QA) tasks, enabling responses across varied domains like economics and mathematics. Despite their capabilities, LLMs often lack explainability due to their complex parameter embeddings. Additionally, integrating multimedia data into QA systems introduces challenges in processing and interpreting diverse data types such as text, images, audio, and video. This necessitates sophisticated algorithms for accurate information retrieval across media while ensuring the reliability of the data and responses remains a significant challenge. The AIQAM workshop aims to bring together researchers and practitioners to address these challenges and enhance QA systems with multimedia data. The focus is on promoting innovations that improve the accuracy, explainability, and trustworthiness of QA systems, contributing to the development of the field.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001282078400169</dc:coverage>
        <bib:pages>1328-1329</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 4TH ANNUAL ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2024</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1375">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_837">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;7&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;7&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;7&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_838">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0921-3449"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mei</foaf:surname>
                        <foaf:givenName>YP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>L</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>HH</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>YY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1296"/>
        <dcterms:isReferencedBy rdf:resource="#item_839"/>
        <dc:subject>Knowledge graphs</dc:subject>
        <dc:subject>Large language models</dc:subject>
        <dc:subject>Intelligent agriculture</dc:subject>
        <dc:subject>Question answering system</dc:subject>
        <dc:title>Application of question answering systems for intelligent agriculture production and sustainable management: A review</dc:title>
        <dcterms:abstract>The increasing application of artificial intelligence in agriculture production and management has generated a large amount of data, leading to a demand for processing this data. This review focuses on the knowledge storage approaches in agricultural question answering systems, namely corpora, knowledge graphs, and large language models. These systems are built on massive amounts of data and aim to process and retrieve information effectively in the context of sustainable agriculture. Corpora refer to large collections of diverse documents that serve as foundational resources for training and fine-tuning question answering systems. Knowledge graphs capture structured and interconnected knowledge by representing entities, relationships, and attributes, enabling efficient organization and querying of information. Large language models, such as GPT-4, enhance the capacity of question answering systems to provide accurate and relevant responses. By exploring these three prominent knowledge storage approaches, this review analyses the methodology and impact of agricultural question answering systems, highlighting their applications in the production process. The findings provide important implications for future research in agriculture, and potential directions for further exploration.</dcterms:abstract>
        <dc:date>2024 MAY</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001196852900001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0921-3449">
        <prism:volume>204</prism:volume>
        <dc:title>RESOURCES CONSERVATION AND RECYCLING</dc:title>
        <dc:identifier>DOI 10.1016/j.resconrec.2024.107497</dc:identifier>
        <dc:identifier>ISSN 0921-3449</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1296">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_839">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;5&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;5&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;78&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_840">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>56</prism:volume>
                <dc:title>ACM COMPUTING SURVEYS</dc:title>
                <dc:identifier>DOI 10.1145/3657631</dc:identifier>
                <prism:number>9</prism:number>
                <dc:identifier>ISSN 0360-0300</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Biancofiore</foaf:surname>
                        <foaf:givenName>GM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deldjoo</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Di Noia</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Di Sciascio</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Narducci</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1356"/>
        <dcterms:isReferencedBy rdf:resource="#item_841"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>LANGUAGE</dc:subject>
        <dc:subject>ATTENTION</dc:subject>
        <dc:subject>large language model</dc:subject>
        <dc:subject>Question answering</dc:subject>
        <dc:subject>human-computer interaction</dc:subject>
        <dc:subject>interactive systems</dc:subject>
        <dc:title>Interactive Question Answering Systems: Literature Review</dc:title>
        <dcterms:abstract>Question-answering systems are recognized as popular and frequently effective means of information seeking on the web. In such systems, information seekers can receive a concise response to their queries by presenting their questions in natural language. Interactive question answering is a recently proposed and increasingly popular solution that resides at the intersection of question answering and dialogue systems. On the one hand, the user can ask questions in normal language and locate the actual response to her inquiry; on the other hand, the system can prolong the question-answering session into a dialogue if there are multiple probable replies, very few, or ambiguities in the initial request. By permitting the user to ask more questions, interactive question answering enables users to interact with the system and receive more precise results dynamically. This survey offers a detailed overview of the interactive question-answering methods that are prevalent in current literature. It begins by explaining the foundational principles of question-answering systems, hence defining new notations and taxonomies to combine all identified works inside a unified framework. The reviewed published work on interactive question-answering systems is then presented and examined in terms of its proposed methodology, evaluation approaches, and dataset/application domain. We also describe trends surrounding specific tasks and issues raised by the community, so shedding light on the future interests of scholars. Our work is further supported by a GitHub page synthesizing all the major topics covered in this literature study. https://sisinflab.github.io/interactive-question-answering-systems-survey/</dcterms:abstract>
        <dc:date>2024 SEP</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001230135700024</dc:coverage>
    </bib:Article>
    <bib:Memo rdf:about="#item_1356">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito sem relação direta com o tema&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_841">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;4&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;4&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;173&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_842">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0219-6492"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>QZ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1366"/>
        <dcterms:isReferencedBy rdf:resource="#item_843"/>
        <dc:subject>NEURAL-NETWORKS</dc:subject>
        <dc:subject>entity recognition model</dc:subject>
        <dc:subject>intelligent question-answering system</dc:subject>
        <dc:subject>Knowledge management services</dc:subject>
        <dc:subject>precision</dc:subject>
        <dc:subject>question classification model</dc:subject>
        <dc:title>Construction of Intelligent Question-Answering System to Improve Knowledge Management Service from the Perspective of Education Informatization</dc:title>
        <dcterms:abstract>With the continuous reform of education informatization, modern information technology gradually became a key technology in education. An intelligent question-answering system was constructed in this research for knowledge management services. It included an entity recognition model based on bidirectional long short-term memory attention conditional random field and a question classification model based on a bidirectional encoder using Transformers text convolutional neural network. The entity recognition model added a reverse long short-term memory propagation structure, while introducing an attention mechanism and conditional random field model. The question classification model utilized a preprocessed structure to output a feature matrix with richer semantic information, while utilizing a bidirectional encoder to extract local features of short text sentences. These experiments confirmed that the constructed model had a high precision of 0.945, which was much higher than other models. The recall of this proposed model was 0.077 higher than the control models. For precision, the proposed model had good recognition performance for methods, files and modules, with values of 0.9040, 0.9050 and 0.9160, respectively. For the recall, the question classification model had the best recognition effect on files, at 0.9060. For the F1-score, the proposed model had the best recognition performance for modules, at 0.9100. The named entity recognition model constructed had high accuracy and F1-score, reaching 0.931 and 0.923, indicating that it had the best classification performance in the dataset. Through intelligent question-answering systems, automatic answering of questions can be achieved, effectively improving knowledge services. This research has integrated knowledge and technology from multiple disciplines such as computer science, artificial intelligence, and natural language processing to promote the cross-integration of these disciplines in knowledge management. This interdisciplinary method not only broadens the research perspective of knowledge management but also provides new ideas and methods for the development of related fields.</dcterms:abstract>
        <dc:date>2024 SEP 25</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001321116000002</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0219-6492">
        <dc:title>JOURNAL OF INFORMATION &amp; KNOWLEDGE MANAGEMENT</dc:title>
        <dc:identifier>DOI 10.1142/S0219649224501028</dc:identifier>
        <dc:identifier>ISSN 0219-6492</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1366">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_843">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;34&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_844">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0165-5515"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lian</foaf:surname>
                        <foaf:givenName>ZX</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1342"/>
        <dcterms:isReferencedBy rdf:resource="#item_845"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>government</dc:subject>
        <dc:subject>chatbot</dc:subject>
        <dc:subject>contextualisation</dc:subject>
        <dc:subject>fuzzy logic</dc:subject>
        <dc:subject>relational graph convolutional networks</dc:subject>
        <dc:title>Government chatbot: Empowering smart conversations with enhanced contextual understanding and reasoning</dc:title>
        <dcterms:abstract>Currently, an increasing number of governments have adopted question answering systems (QASs) in public service delivery. As some citizens with limited information literacy often express their questions vaguely when interacting with a chatbot, it is necessary to improve the contextual understanding and reasoning ability of government chatbots (G-chatbots). This goal can be achieved through the optimisation of the matching between question, answer and context. By incorporating the Relational Graph Convolutional Networks (R-GCNs) and fuzzy logic, this study proposes a multi-turn dialogue model that introduces a re-question mechanism and a subgraph matching algorithm. The experiment results show that the model can improve the contextual reasoning ability of G-chatbots by about 10% and generate answers in a more explainable way. This study innovatively integrates a question-answer-context matching approach, re-question mechanism into the MTRF-G-chatbot model, reducing barriers to citizens' access to government services and enhancing contextual reasoning abilities.</dcterms:abstract>
        <dc:date>2024 SEP 18</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001315539900001</dc:coverage>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0165-5515">
        <dc:title>JOURNAL OF INFORMATION SCIENCE</dc:title>
        <dc:identifier>DOI 10.1177/01655515241268863</dc:identifier>
        <dc:identifier>ISSN 0165-5515</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1342">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_845">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;55&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_846">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1367-0751"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chuganskaya</foaf:surname>
                        <foaf:givenName>AA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kovalev</foaf:surname>
                        <foaf:givenName>AK</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Panov</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1396"/>
        <dcterms:isReferencedBy rdf:resource="#item_847"/>
        <dc:subject>action</dc:subject>
        <dc:subject>perception</dc:subject>
        <dc:subject>scenario</dc:subject>
        <dc:subject>sign-based world model</dc:subject>
        <dc:subject>social interaction</dc:subject>
        <dc:subject>Visual question answering</dc:subject>
        <dc:title>Sign-based image criteria for social interaction visual question answering</dc:title>
        <dcterms:abstract>The multi-modal tasks have started to play a significant role in the research on artificial intelligence. A particular example of that domain is visual-linguistic tasks, such as visual question answering. The progress of modern machine learning systems is determined, among other things, by the data on which these systems are trained. Most modern visual question answering data sets contain limited type questions that can be answered either by directly accessing the image itself or by using external data. At the same time, insufficient attention is paid to the issues of social interactions between people, which limits the scope of visual question answering systems. In this paper, we propose criteria by which images suitable for social interaction visual question answering can be selected for composing such questions, based on psychological research. We believe this should serve the progress of visual question answering systems.</dcterms:abstract>
        <dc:date>2024 AUG</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001189422100001</dc:coverage>
        <bib:pages>656-670</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1367-0751">
        <prism:volume>32</prism:volume>
        <dc:title>LOGIC JOURNAL OF THE IGPL</dc:title>
        <dc:identifier>DOI 10.1093/jigpal/jzae026</dc:identifier>
        <prism:number>4</prism:number>
        <dc:identifier>ISSN 1367-0751</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1396">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_847">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;40&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_848">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1994-5639"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zakharov</foaf:surname>
                        <foaf:givenName>AA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zakharova</foaf:surname>
                        <foaf:givenName>IG</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shabalin</foaf:surname>
                        <foaf:givenName>AM</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khanbekov</foaf:surname>
                        <foaf:givenName>SI</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dzhalilzoda</foaf:surname>
                        <foaf:givenName>DB</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1372"/>
        <dcterms:isReferencedBy rdf:resource="#item_849"/>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>CHALLENGES</dc:subject>
        <dc:subject>data interpretation</dc:subject>
        <dc:subject>digital educational resource</dc:subject>
        <dc:subject>EDUCATION</dc:subject>
        <dc:subject>inclusive design</dc:subject>
        <dc:subject>ontology</dc:subject>
        <dc:subject>question-answering system</dc:subject>
        <dc:subject>visually impaired students</dc:subject>
        <dc:subject>voice assistant</dc:subject>
        <dc:title>INTELLIGENT VOICE ASSISTANT AS AN EXAMPLE OF INCLUSIVE DESIGN METHODOLOGY IMPLEMENTATION</dc:title>
        <dcterms:abstract>Introduction . The development of artificial intelligence methods and technologies aimed at speech recognition contributes to the creation of specialised programmes - namely voice assistants - which are capable of conducting a dialogue in natural language. Such services are of particular relevance in inclusive education in order to support students with visual impairments. The research problem lies in the individualised support of students' independent work based on a voice assistant and is determined by the contradiction between the widespread use of various question -answer systems in business and everyday life (including those with a voice interface), on the one hand, and insufficient knowledge their didactic possibilities, on the other hand. Aim. The present research aims to investigate and practically implement the methodology of inclu - sive design of digital educational resources on the example of creating an intelligent voice assistant for students' independent work in the course &quot;Computer Networks&quot;. Methodology and research methods . The current research is based on the methodology of inclusive design in combination with an ontological approach in relation to the creation of digital educational resources, speech recognition methods, methods for designing intelligent systems and knowledge bases, methods and technologies for designing and implementing automated learning systems with feedback. To recognise questions, search for answers, and support dialogues carried out by a voice assistant, the authors applied natural language text analysis methods and classification models created using machine learning methods. Results. The authors have developed and substantiated the requirements that a digital educational resource must meet in accordance with the principles of inclusive design, linking an ontological approach to content development, automatic individualised support for students, and monitoring the achievement of educational results. According to the formulated requirements, the authors have developed an inter - active computer service - an intelligent voice assistant that provides support for students' independent work when performing practical tasks, using the &quot;Computer Networks&quot; course as an example. The service supports voice input and subsequent interpretation of questions, search for answers in the knowledge base with voice output of the result, and implements the execution of operations according to certain rules. Scientific novelty. The authors have clarified the content of the concept of &quot;inclusive design&quot; in the context of digital educational resources, when the key feature is the focus on continuous improvement of the didactic capabilities of a particular product. The authors have shown that this can be achieved through a conceptually based content structure and initially provided feedback. This approach has prov - en to be effective in designing an intelligent voice assistant to answer student questions and to automat - ically perform operations on a computer. Practical significance. The use of a voice assistant by students of the Institute of Mathematics and Computer Science of the University of Tyumen in the process of studying the course &quot;Computer Net - works&quot; demonstrated the relevance of developing similar question -answering systems to accompany the independent work of students, including those with visual impairments in online and blended learning. The developed service is universal and can be used with any knowledge base that provides answers to students' questions.</dcterms:abstract>
        <dc:date>2024 MAR</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001223185700005</dc:coverage>
        <bib:pages>149-175</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1994-5639">
        <prism:volume>26</prism:volume>
        <dc:title>OBRAZOVANIE I NAUKA-EDUCATION AND SCIENCE</dc:title>
        <dc:identifier>DOI 10.17853/1994-5639-2024-3-149-175</dc:identifier>
        <prism:number>3</prism:number>
        <dc:identifier>ISSN 1994-5639</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1372">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_849">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;37&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_850">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>31</prism:volume>
                <dc:title>JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION</dc:title>
                <dc:identifier>DOI 10.1093/jamia/ocae015</dc:identifier>
                <prism:number>4</prism:number>
                <dc:identifier>ISSN 1067-5027</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kell</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roberts</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Umansky</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qian</foaf:surname>
                        <foaf:givenName>LL</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ferrari</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Soboczenski</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wallace</foaf:surname>
                        <foaf:givenName>BC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Patel</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Marshall</foaf:surname>
                        <foaf:givenName>IJ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1389"/>
        <dcterms:isReferencedBy rdf:resource="#item_851"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>INFORMATION</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>question answering</dc:subject>
        <dc:subject>clinical decision support</dc:subject>
        <dc:subject>CLINICAL QUESTIONS</dc:subject>
        <dc:subject>evidence-based medicine</dc:subject>
        <dc:title>Question answering systems for health professionals at the point of care-a systematic review</dc:title>
        <dcterms:abstract>Objectives Question answering (QA) systems have the potential to improve the quality of clinical care by providing health professionals with the latest and most relevant evidence. However, QA systems have not been widely adopted. This systematic review aims to characterize current medical QA systems, assess their suitability for healthcare, and identify areas of improvement.Materials and methods We searched PubMed, IEEE Xplore, ACM Digital Library, ACL Anthology, and forward and backward citations on February 7, 2023. We included peer-reviewed journal and conference papers describing the design and evaluation of biomedical QA systems. Two reviewers screened titles, abstracts, and full-text articles. We conducted a narrative synthesis and risk of bias assessment for each study. We assessed the utility of biomedical QA systems.Results We included 79 studies and identified themes, including question realism, answer reliability, answer utility, clinical specialism, systems, usability, and evaluation methods. Clinicians' questions used to train and evaluate QA systems were restricted to certain sources, types and complexity levels. No system communicated confidence levels in the answers or sources. Many studies suffered from high risks of bias and applicability concerns. Only 8 studies completely satisfied any criterion for clinical utility, and only 7 reported user evaluations. Most systems were built with limited input from clinicians.Discussion While machine learning methods have led to increased accuracy, most studies imperfectly reflected real-world healthcare information needs. Key research priorities include developing more realistic healthcare QA datasets and considering the reliability of answer sources, rather than merely focusing on accuracy.</dcterms:abstract>
        <dc:date>2024 APR 3</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001163650100001</dc:coverage>
        <bib:pages>1009-1024</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1389">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_851">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;126&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="#item_852">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>12</prism:volume>
                <dc:title>IEEE ACCESS</dc:title>
                <dc:identifier>DOI 10.1109/ACCESS.2024.3378300</dc:identifier>
                <dc:identifier>ISSN 2169-3536</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Awais</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nawab</foaf:surname>
                        <foaf:givenName>RMA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1293"/>
        <dcterms:isReferencedBy rdf:resource="#item_853"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Machine learning</dc:subject>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>Social networking (online)</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Unsupervised learning</dc:subject>
        <dc:subject>Benchmark testing</dc:subject>
        <dc:subject>large language models</dc:subject>
        <dc:subject>Convolutional neural networks</dc:subject>
        <dc:subject>Abstractive text summarization</dc:subject>
        <dc:subject>Abstracts</dc:subject>
        <dc:subject>BART</dc:subject>
        <dc:subject>corpus</dc:subject>
        <dc:subject>deep learning models</dc:subject>
        <dc:subject>GPT-3.5</dc:subject>
        <dc:subject>Long short term memory</dc:subject>
        <dc:subject>Publishing</dc:subject>
        <dc:subject>Text analysis</dc:subject>
        <dc:subject>Text detection</dc:subject>
        <dc:subject>Text summarization</dc:subject>
        <dc:subject>Urdu</dc:subject>
        <dc:title>Abstractive Text Summarization for the Urdu Language: Data and Methods</dc:title>
        <dcterms:abstract>The task of abstractive text summarization aims to automatically generate a short and concise summary of a given source article. In recent years, automatic abstractive text summarization has attracted the attention of researchers because large volumes of digital text are readily available in multiple languages on a wide range of topics. Automatically generating precise summaries from large text has potential application in the generation of news headlines, a summary of research articles, the moral of the stories, media marketing, search engine optimization, financial research, social media marketing, question-answering systems, and chatbots. In literature, the problem of abstractive text summarization has been mainly investigated for English and some other languages. However, it has not been thoroughly explored for the Urdu language despite having a huge amount of data available in digital format. To fulfill this gap, this paper presents a large benchmark corpus of 2,067,784 Urdu news articles for the Urdu abstractive text summarization task. As a secondary contribution, we applied a range of deep learning (LSTM, Bi-LSTM, LSTM with attention, GRU, Bi-GRU, and GRU with attention), and large language models (BART and GPT-3.5) on our proposed corpus. Our extensive evaluation on 20,000 test instances showed that GRU with attention model outperforms the other models with ROUGE- 1 = 46.7 , ROUGE- 2 = 24.1 , and ROUGE-L = 48.7. To foster research in Urdu, our proposed corpus is publically and freely available for research purposes under the Creative Common Licence.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001214282400001</dc:coverage>
        <bib:pages>61198-61210</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1293">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_853">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;59&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:979-8-3503-8678-3">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 979-8-3503-8678-3</dc:identifier>
                <dc:title>University of Southern California</dc:title>
                <dc:identifier>DOI 10.1109/ICCEA62105.2024.10603938</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>JP</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>HY</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tan</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>HT</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>IEEE</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1322"/>
        <dcterms:isReferencedBy rdf:resource="#item_855"/>
        <dc:subject>Large Vision-Language Model</dc:subject>
        <dc:subject>Medical Question-Answering Systems</dc:subject>
        <dc:subject>Qwen-VL 7B Model</dc:subject>
        <dc:title>Enhanced Qwen-VL 7B Model via Instruction Finetuning on Chinese Medical Dataset</dc:title>
        <dcterms:abstract>The integration of artificial intelligence (AI) into healthcare is revolutionizing medical information access and professional advice delivery. This study focuses on the development of a medical-specific question-answering model leveraging the Qwen-VL 7B model, an advanced Large Vision-Language Model (LVLM), to enhance the understanding and generation of medical texts. The Qwen-VL 7B model's capabilities in both language and visual comprehension make it an ideal candidate for medical question-answering systems, which require a deep grasp of medical knowledge and language. The research objectives include adapting the Qwen-VL 7B model to medical terminology, utilizing its visual understanding for medical imaging analysis, optimizing question-answering systems for medical scenarios, and evaluating the performance of these models. To achieve these goals, we employed methodologies such as dataset construction, model fine-tuning, and user studies. Results showed that the Qwen-VL-Mediacal model achieved a Rouge-1 score of 0.6147, indicating its potential for medical applications. However, challenges remain, such as understanding complex scenes and abstract concepts. Future research will aim to improve the model's adaptability and reasoning capabilities.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001291294600102</dc:coverage>
        <bib:pages>526-530</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2024 5TH INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING AND APPLICATION, ICCEA 2024</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1322">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_855">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;10&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://aclanthology.org/2020.findings-emnlp.261/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-952148-90-3</dc:identifier>
                <dc:title>Findings of the Association for Computational Linguistics: EMNLP 2020</dc:title>
                <dc:identifier>DOI 10.18653/v1/2020.findings-emnlp.261</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Online</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Association for Computational Linguistics</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chalkidis</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fergadiotis</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Malakasiotis</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Aletras</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Androutsopoulos</foaf:surname>
                        <foaf:givenName>I</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cohn</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_861"/>
        <link:link rdf:resource="#item_1733"/>
        <dc:title>LEGAL-BERT: The Muppets straight out of Law School</dc:title>
        <dcterms:abstract>BERT has achieved impressive performance in several NLP tasks. However, there has been limited investigation on its adaptation guidelines in specialised domains. Here we focus on the legal domain, where we explore several approaches for applying BERT models to downstream legal tasks, evaluating on multiple datasets. Our findings indicate that the previous guidelines for pre-training and fine-tuning, often blindly followed, do not always generalize well in the legal domain. Thus we propose a systematic investigation of the available strategies when applying BERT in specialised domains. These are: (a) use the original BERT out of the box, (b) adapt BERT by additional pre-training on domain-specific corpora, and (c) pre-train BERT from scratch on domain-specific corpora. We also propose a broader hyper-parameter search space when fine-tuning for downstream tasks and we release LEGAL- BERT, a family of BERT models intended to assist legal NLP research, computational law, and legal technology applications.</dcterms:abstract>
        <dc:date>2020-11</dc:date>
        <z:language>English</z:language>
        <z:shortTitle>LEGAL-BERT</z:shortTitle>
        <dc:coverage>WOS:001181866502003</dc:coverage>
        <z:libraryCatalog>ACLWeb</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://aclanthology.org/2020.findings-emnlp.261/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <bib:pages>2898–2904</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_861">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;41&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;41&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;26&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1733">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1733/Chalkidis et al. - 2020 - LEGAL-BERT The Muppets straight out of Law School.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://aclanthology.org/2020.findings-emnlp.261.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 13:14:24</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:979-10-95546-34-4">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 979-10-95546-34-4</dc:identifier>
                <dc:title>German Research Center for Artificial Intelligence (DFKI)</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Marseille, France</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>European Language Resources Association</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Moreno-Schneider</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rehm</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Montiel-Ponsoda</foaf:surname>
                        <foaf:givenName>E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rodriguez-Doncel</foaf:surname>
                        <foaf:givenName>Ví</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Revenko</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Karampatakis</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khvalchik</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sageder</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gracia</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maganza</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Calzolari</foaf:surname>
                        <foaf:givenName>N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bechet</foaf:surname>
                        <foaf:givenName>F</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Blache</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Choukri</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cieri</foaf:surname>
                        <foaf:givenName>C</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Declerck</foaf:surname>
                        <foaf:givenName>T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Goggi</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Isahara</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maegaard</foaf:surname>
                        <foaf:givenName>B</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mariani</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mazo</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Moreno</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Odijk</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Piperidis</foaf:surname>
                        <foaf:givenName>S</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_863"/>
        <link:link rdf:resource="#item_1736"/>
        <dc:subject>Applications</dc:subject>
        <dc:subject>Knowledge Discovery/Representation</dc:subject>
        <dc:subject>Systems</dc:subject>
        <dc:subject>Text Analytics</dc:subject>
        <dc:subject>Tools</dc:subject>
        <dc:title>Orchestrating NLP Services for the Legal Domain</dc:title>
        <dcterms:abstract>Legal technology is currently receiving a lot of attention from various angles. In this contribution we describe the main technical components of a system that is currently under development in the European innovation project Lynx, which includes partners from industry and research. The key contribution of this paper is a workflow manager that enables the flexible orchestration of workflows based on a portfolio of Natural Language Processing and Content Curation services as well as a Multilingual Legal Knowledge Graph that contains semantic information and meaningful references to legal documents. We also describe different use cases with which we experiment and develop prototypical solutions.</dcterms:abstract>
        <dc:date>2020-05</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000724697203037</dc:coverage>
        <z:libraryCatalog>ACLWeb</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://aclanthology.org/2020.lrec-1.284/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <bib:pages>2332-2340</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2020)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_863">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;7&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;7&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;31&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1736">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1736/Moreno-Schneider et al. - 2020 - Orchestrating NLP Services for the Legal Domain.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://aclanthology.org/2020.lrec-1.284.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 13:16:47</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_864">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>12</prism:volume>
                <dc:title>APPLIED SCIENCES-BASEL</dc:title>
                <dc:identifier>DOI 10.3390/app122010200</dc:identifier>
                <prism:number>20</prism:number>
                <dc:identifier>ISSN 2076-3417</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Francia</foaf:surname>
                        <foaf:givenName>OAA</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nunez-del-Prado</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alatrista-Salas</foaf:surname>
                        <foaf:givenName>H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1661"/>
        <dcterms:isReferencedBy rdf:resource="#item_865"/>
        <link:link rdf:resource="#item_1562"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>LEGAL</dc:subject>
        <dc:subject>OUTCOMES</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>ARTIFICIAL-INTELLIGENCE</dc:subject>
        <dc:subject>COURT</dc:subject>
        <dc:subject>judicial prediction</dc:subject>
        <dc:subject>legal prediction</dc:subject>
        <dc:subject>legal tech</dc:subject>
        <dc:subject>SCIENCE</dc:subject>
        <dc:title>Survey of Text Mining Techniques Applied to Judicial Decisions Prediction</dc:title>
        <dcterms:abstract>This paper reviews the most recent literature on experiments with different Machine Learning, Deep Learning and Natural Language Processing techniques applied to predict judicial and administrative decisions. Among the most outstanding findings, we have that the most used data mining techniques are Support Vector Machine (SVM), K Nearest Neighbours (K-NN) and Random Forest (RF), and in terms of the most used deep learning techniques, we found Long-Term Memory (LSTM) and transformers such as BERT. An important finding in the papers reviewed was that the use of machine learning techniques has prevailed over those of deep learning. Regarding the place of origin of the research carried out, we found that 64% of the works belong to studies carried out in English-speaking countries, 8% in Portuguese and 28% in other languages (such as German, Chinese, Turkish, Spanish, etc.). Very few works of this type have been carried out in Spanish-speaking countries. The classification criteria of the works have been based, on the one hand, on the identification of the classifiers used to predict situations (or events with legal interference) or judicial decisions and, on the other hand, on the application of classifiers to the phenomena regulated by the different branches of law: criminal, constitutional, human rights, administrative, intellectual property, family law, tax law and others. The corpus size analyzed in the reviewed works reached 100,000 documents in 2020. Finally, another important finding lies in the accuracy of these predictive techniques, reaching predictions of over 60% in different branches of law.</dcterms:abstract>
        <dc:date>2022 OCT</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000872322400001</dc:coverage>
    </bib:Article>
    <bib:Memo rdf:about="#item_1661">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_865">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;11&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;11&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;77&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1562">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1562/Francia et al. - 2022 - Survey of Text Mining Techniques Applied to Judicial Decisions Prediction.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.mdpi.com/2076-3417/12/20/10200/pdf?version=1665483458</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:34:31</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-8732-3">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dcterms:isPartOf>
                   <bib:Series><dc:title>SIGIR '22</dc:title></bib:Series>
                </dcterms:isPartOf>
                <dc:identifier>ISBN 978-1-4503-8732-3</dc:identifier>
                <dc:title>Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</dc:title>
                <dc:identifier>DOI 10.1145/3477495.3536329</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>New York, NY, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Association for Computing Machinery</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vianna</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>de Moura</foaf:surname>
                        <foaf:givenName>ES</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>ACM</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_867"/>
        <link:link rdf:resource="#item_1738"/>
        <dc:subject>language models</dc:subject>
        <dc:subject>law tech</dc:subject>
        <dc:subject>legal cases</dc:subject>
        <dc:subject>topic model</dc:subject>
        <dc:title>Organizing Portuguese Legal Documents through Topic Discovery</dc:title>
        <dcterms:abstract>A significant challenge in the legal domain is to organize and summarize a constantly growing collection of legal documents, uncovering hidden topics, or themes, that later can support tasks such as legal case retrieval and legal judgment prediction. This massive amount of digital legal documents, combined with the inherent complexity of judiciary systems worldwide, presents a promising scenario for Machine Learning solutions, mainly those taking advantage of all the advancements in the area of Natural Language Processing (NLP). It is in this scenario that Jusbrasil, the largest legal tech company in Brazil, is situated. Using a dataset partially curated by the Jusbrasil legal team, we explore topic modeling solutions using state of the art language models, trained with legal Portuguese documents, to automatically organize and summarize this complex collection of documents. Instead of using an entire legal case, which usually is composed of many pages, we show that it is possible to efficiently organize the collection using the syllabus (in Portuguese, ementa jurisprudencial) from each court decision as they concisely summarize the main points presented by the entire decision.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:000852715903060</dc:coverage>
        <z:libraryCatalog>ACM Digital Library</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/10.1145/3477495.3536329</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <bib:pages>3388-3392</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_867">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;2&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;3&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;20&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1738">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1738/Vianna e Silva de Moura - 2022 - Organizing Portuguese Legal Documents through Topic Discovery.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/pdf/10.1145/3477495.3536329</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 13:17:35</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:2165-0608">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 2165-0608</dc:identifier>
                <dc:title>Ihsan Dogramaci Bilkent University</dc:title>
                <dc:identifier>DOI 10.1109/SIU55565.2022.9864970</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Aras</foaf:surname>
                        <foaf:givenName>AC</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Öztürk</foaf:surname>
                        <foaf:givenName>CE</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koç</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>IEEE</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_869"/>
        <link:link rdf:resource="#item_1721"/>
        <link:link rdf:resource="#item_1722"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Feature extraction</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Law</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>legal tech</dc:subject>
        <dc:subject>Feedforward neural networks</dc:subject>
        <dc:subject>legal NLP</dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Prediction algorithms</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Signal processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Prediction methods</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Signal processing algorithms</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Soft sensors</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Feedforward Neural Network Based Case Prediction in Turkish Higher Courts</dc:title>
        <dcterms:abstract>Thanks to natural language processing (NLP) methods, legal texts can be processed by computers and decision prediction applications can be developed in the legal tech field. Increase in the available data sources in the Turkish legal system provides an opportunity to develop NLP applications as well. In order to develop these applications, the necessary corpora and datasets should be created. In this work, legal case texts from the Turkish Higher Courts that are open to public access and free from personal data are used to develop decision prediction methods. Feedforward neural networks (FFNN) are deployed using word embeddings and the features extracted from texts via the Principal Component Analysis (PCA) algorithm. %85.4 Macro F1 score level is achieved.</dcterms:abstract>
        <dc:date>2022-05</dc:date>
        <z:language>Turkish</z:language>
        <dc:coverage>WOS:001307163400308</dc:coverage>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://ieeexplore.ieee.org/document/9864970</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>ISSN: 2165-0608</dc:description>
        <bib:pages>1-4</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 30th Signal Processing and Communications Applications Conference (SIU)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_869">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;23&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1721">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1721/Aras et al. - 2022 - Feedforward Neural Network Based Case Prediction in Turkish Higher Courts.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=9864970&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 13:07:45</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_1722">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1722/9864970.html"/>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://ieeexplore.ieee.org/document/9864970</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 13:07:57</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_870">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 2165-0608</dc:identifier>
                <dc:title>Ihsan Dogramaci Bilkent University</dc:title>
                <dc:identifier>DOI 10.1109/SIU55565.2022.9864914</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Öztürk</foaf:surname>
                        <foaf:givenName>CE</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Özçelik</foaf:surname>
                        <foaf:givenName>SB</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koç</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>IEEE</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_871"/>
        <link:link rdf:resource="#item_1556"/>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Law</dc:subject>
        <dc:subject>Legal text mining</dc:subject>
        <dc:subject>Machine Learning</dc:subject>
        <dc:subject>Deep Learning</dc:subject>
        <dc:subject>AI in Law</dc:subject>
        <dc:title>Predicting Outcomes of the Court of Cassation of Turkey with Recurrent Neural Networks</dc:title>
        <dcterms:abstract>Natural Language Processing (NLP) based approaches have recently become very popular for studies in legal domain. In this work, the outcomes of the cases of the Court of Cassation of Turkey were predicted with the use of Deep Learning models. These models are GRU, LSTM and BiLSTM which are variants of the recurrent neural network. Models saw only fact description parts of the case decision texts during training. Firstly, the models were trained with the word embeddings that were created from the texts from daily language. Then, the models were trained with the word embeddings that were created from downloaded legal cases from Turkish courts. The results of the experiments on the models are given in a comparative and detailed manner. It is observed based on this study and the past studies that the outcomes of the Court of Cassation can be predicted with higher accuracy than most of the courts that were investigated in previous legal NLP studies. The model which is best at predicting decisions is GRU. The GRU model achieves 96.8% accuracy in the decision prediction task.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>Turkish</z:language>
        <dc:coverage>WOS:001307163400252</dc:coverage>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 30TH SIGNAL PROCESSING AND COMMUNICATIONS APPLICATIONS CONFERENCE, SIU</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_871">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;30&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1556">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1556/Öztürk et al. - 2022 - Predicting Outcomes of the Court of Cassation of Turkey with Recurrent Neural Networks.pdf"/>
        <dc:title>Texto Completo</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://repository.bilkent.edu.tr/bitstream/11693/111316/1/Predicting_Outcomes_of_the_Court_of_Cassation_of_Turkey_with_Recurrent_Neural_Networks.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:33:40</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S0267364923001188">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0267-3649"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Licari</foaf:surname>
                        <foaf:givenName>D</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Comandè</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_873"/>
        <link:link rdf:resource="#item_1725"/>
        <link:link rdf:resource="#item_1724"/>
        <dc:subject>Italian Legal BERT</dc:subject>
        <dc:subject>Italian Legal NLP</dc:subject>
        <dc:subject>Legal AI</dc:subject>
        <dc:subject>Pre-trained Language Models</dc:subject>
        <dc:title>ITALIAN-LEGAL-BERT models for improving natural language processing tasks in the Italian legal domain</dc:title>
        <dcterms:abstract>Legal-BERT models are based on the BERT architecture (or its variants) and have been developed specifically for the legal domain. They have reached the state of the art in complex legal tasks such as legal research, document synthesis, contract analysis, argument extraction, and legal prediction. In this paper, we proposed four versions of Legal-BERT models pre-trained on the Italian legal domain. They aim to improve NLP applications in the Italian legal context. We have shown that they outperforms the Italian &quot;generalpurpose&quot; BERT in several domain-specific tasks, such as named entity recognition, sentence classification, semantic similarity with Bi-encoders, and document classification.</dcterms:abstract>
        <dc:date>2024-04-01</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001102220200001</dc:coverage>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0267364923001188</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <bib:pages>105908</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0267-3649">
        <prism:volume>52</prism:volume>
        <dc:title>Computer Law &amp; Security Review</dc:title>
        <dc:identifier>DOI 10.1016/j.clsr.2023.105908</dc:identifier>
        <dcterms:alternative>Computer Law &amp; Security Review</dcterms:alternative>
        <dc:identifier>ISSN 0267-3649</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_873">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;4&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;4&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;38&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1725">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1725/Licari e Comandè - 2024 - ITALIAN-LEGAL-BERT models for improving natural language processing tasks in the Italian legal domai.pdf"/>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://pdf.sciencedirectassets.com/271884/1-s2.0-S0267364923X00053/1-s2.0-S0267364923001188/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEP3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQCtkBcL8YOkq%2BqKQ8NjqIIZZqPGhXkOejqiZU4q3oaJTQIhAJ8zh5HLqad1k7yUWADO9nawHhSNYb4NNVJmEPlJ%2BSMXKrsFCNX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBRoMMDU5MDAzNTQ2ODY1Igwlw5eUtVYntnk6EucqjwUxNZTTMcDF5HhopYso4YFW8DT3GcHXb7yM%2FZcjUPOHudF5TAzwvbFsPrmu8ofc7%2Feyaj0WD3KI1sWa79DyCc0ksnqKkK9BmHfOIWwkoEISkd7YyTZbdVQxrczDjxfZr8b8rm2F%2FrmLwxf%2F6Bbj150o76iOryxVRgcsi38CIWuKxzK95IbO3IA9jvwDr2CxXEQYI0ivO9l8bLfKcOGVAz1fY97WE%2FnAqmvCPMh1we3Shkj6r3yDa1JvU1IOVQmBuX4tgXAM4HWmUpli3qhdgXHTGfKEBE1nvvchWEM1BjZp4HxXn5wBJfOcjvTlY7OH7S4HC9dt2tZYQ1hXk%2Bzg%2FhpOrFMUd1J1skvzX8wV2Bq%2FivbwTB3cGf40Hjk4xHEzHfGCbmDg%2BnWJwWL91fTQjcc2t%2Bn100%2FMIZmHDizJQfQxMRbZVRWtl5BruufN%2F38Qrlg6WaRNzsCvYEHaKnyO2bfpA3X0zh98jF%2FXswUkXn7mbm8%2BRAPqDCPVKLZ3cO8BBzslbZKo8ixVFi7HU2tAQ0zqBJYhtzMrJY43n8zXE5EdkjHxbCGVtAaci4WMk18BPPfShQY9NxcjmXp08%2Bn20aHvmnzjGT9tmu68dxrSqWn9m87%2BXu0%2BD9ku822giig4no8prscUPfizEy6XflZRXSxzsszewRtpG4S3wgonitBgYyARM2nljZlRGsdSsqVI0U4w3rd5HEvUqLYN1Y%2BFlqBxmM0vawO%2BssLGMRc3Ib%2BTV1quzgiAc%2FTmfN%2FeDOCsAgqtQH6hYwjAQYvl0JJi6VelLefpBGbqNDseiU1okJ1VwPKSmX1GDFBqQeGwyMLWgeRoMyVbUonnQKllILMBwlE2Uc5HiADWMtUrNMY99nXDMJ6H2rsGOrABgAZZNZUQ5wnTHRB88rG65lQa6H9mcShxU%2Btf1FlZqBZ69CHxTAjFRi0KWA1UPHYCX8TkGGpFYSydFHv1iro7AFpUuOazA69fOUhfxTzfDMQUqfO9aui%2BPYv3fpIKmK77%2BHw4P3%2BFerDpBrQxE1LbuZ1FDEGgzjlFfL%2BY%2Bh1apppeRlX%2FjJRWKBvnRV5h0PXYFxgO%2FbfEQgXGIr3dqApUYvi2Hui7l4K6%2BpiHd95tgvU%3D&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20250102T130846Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=299&amp;X-Amz-Credential=ASIAQ3PHCVTY6DZNPOPS%2F20250102%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=a3db03954579393c966fa52faed2d542e0fd2bdef18639c44336ab303daebc7d&amp;hash=22e269153b6b749d1498b060c9edba8bd7021792bc5ed29fcb1c45ba7d465288&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S0267364923001188&amp;tid=spdf-17d59936-4d73-41d9-83e0-fa0ec9d17aad&amp;sid=af8ef9f4253fd043dd586b081af4b12954d3gxrqb&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=16145e02040309040005&amp;rr=8fbafacf9e7403f2&amp;cc=pt</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 13:08:57</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_1724">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1724/S0267364923001188.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0267364923001188?via%3Dihub</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 13:08:39</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_874">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>13856</prism:volume>
                <dc:identifier>ISBN 2945-9133</dc:identifier>
                <dc:identifier>DOI 10.1007/978-3-031-36190-6_3</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kalamkar</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Venugopalan</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Raghavan</foaf:surname>
                        <foaf:givenName>V</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yada</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Takama</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mineshima</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Satoh</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1660"/>
        <dcterms:isReferencedBy rdf:resource="#item_875"/>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Legal Text Processing</dc:subject>
        <dc:subject>NLP Benchmarks</dc:subject>
        <dc:title>Benchmarks for Indian Legal NLP: A Survey</dc:title>
        <dcterms:abstract>Legal text is significantly different from English text (e.g. Wikipedia, News) used for training most natural language processing (NLP) algorithms. As a result, the state of the art algorithms (e.g. GPT3, BERT derivatives), need additional effort (e.g. fine-tuning and further pre-training) to achieve optimal performance on legal text. Hence there is a need to create separate NLP data sets and benchmarks for legal text which are challenging and focus on tasks specific to legal systems. This will spur innovation in applications of NLP for legal text and will benefit AI community and legal fraternity. This paper focuses on an empirical review of the existing work in the use of NLP in Indian legal text and proposes ideas to create new benchmarks for Indian Legal NLP.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001298764700003</dc:coverage>
        <bib:pages>33-48</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>NEW FRONTIERS IN ARTIFICIAL INTELLIGENCE, JSAI-ISAI 2021 WORKSHOPS, JURISIN 2021, LENLS18, SCIDOCA 2021, KANSEI-AI 2021, AND AI-BIZ 2021</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1660">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_875">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;42&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:0922-6389">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>379</prism:volume>
                <dc:identifier>ISBN 0922-6389</dc:identifier>
                <dc:title>Indraprastha Institute of Information Technology Delhi</dc:title>
                <dc:identifier>DOI 10.3233/FAIA230971</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khatri</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sheik</foaf:surname>
                        <foaf:givenName>R</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wadhwa</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Satija</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kumar</foaf:surname>
                        <foaf:givenName>Y</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shah</foaf:surname>
                        <foaf:givenName>RR</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kumaraguru</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Spanakis</foaf:surname>
                        <foaf:givenName>J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>VanDijck</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sileno</foaf:surname>
                        <foaf:givenName>G</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_877"/>
        <link:link rdf:resource="#item_1561"/>
        <dc:subject>Legal NLP</dc:subject>
        <dc:subject>Classification</dc:subject>
        <dc:subject>Citation</dc:subject>
        <dc:title>CiteCaseLAW: Citation Worthiness Detection in Caselaw for Legal Assistive Writing</dc:title>
        <dcterms:abstract>Complex legal language, filled with jargon, nuanced language semantics, and a high level of domain specificity, poses a significant challenge for automation in handling various legal tasks. In the realm of legal document composition, a pivotal component revolves around accurately referencing case laws and other sources to substantiate assertions and arguments. Understanding the legal domain and identifying appropriate citation context or cite-worthy sentences automatically is challenging. Our research is centered on the issue of citation-worthiness identification of a given sentence. This serves as the initial phase in contemporary citation recommendation systems, aimed at alleviating the effort involved in extracting a suitable array of citation contexts. To address this, we first introduce a labeled dataset comprising 178 million sentences, specifically tailored for detecting citation-worthy content within the legal domain. This dataset is curated from the Caselaw Access Project (CAP)(2) (https://case.law/). We proceeded to assess the performance of a range of deep learning models on this novel dataset. Among the models examined, the domain-specific pre-trained model consistently demonstrated superior performance, achieving an 88% F1-score in the task of detecting citation-worthy material. To enhance our insights, we employed inputXGradient explainable AI techniques to dissect the predictions, thereby identifying the tokens that contribute to specific citation classes.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001175464100031</dc:coverage>
        <bib:pages>257-262</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>LEGAL KNOWLEDGE AND INFORMATION SYSTEMS</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_877">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;22&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1561">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1561/Khatri et al. - 2023 - CiteCaseLAW Citation Worthiness Detection in Caselaw for Legal Assistive Writing.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA230971</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:34:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/10223938/?arnumber=10223938">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 2165-0608</dc:identifier>
                <dc:title>Ihsan Dogramaci Bilkent University</dc:title>
                <dc:identifier>DOI 10.1109/SIU59756.2023.10223938</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Öztürk</foaf:surname>
                        <foaf:givenName>CE</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Özçelik</foaf:surname>
                        <foaf:givenName>SB</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koç</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>IEEE</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_879"/>
        <link:link rdf:resource="#item_1686"/>
        <link:link rdf:resource="#item_1685"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Task analysis</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Law</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Transformers</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Information retrieval</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>legal tech</dc:subject>
        <dc:subject>legal NLP</dc:subject>
        <dc:subject>prior legal case retrieval</dc:subject>
        <dc:subject>Turkish NLP</dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Support vector machines</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Barium</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Signal processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>A Transformer-based Prior Legal Case Retrieval Method</dc:title>
        <dcterms:abstract>In this work, BERTurk-Legal, a transformer-based language model, is introduced to retrieve prior legal cases. BERTurk-Legal is pre-trained on a dataset from the Turkish legal domain. This dataset does not contain any labels related to the prior court case retrieval task. Masked language modeling is used to train BERTurk-Legal in a self-supervised manner. With zero-shot classification, BERTurk-Legal provides state-of-the-art results on the dataset consisting of legal cases of the Court of Cassation of Turkey. The results of the experiments show the necessity of developing language models specific to the Turkish law domain.</dcterms:abstract>
        <dc:date>2023-07</dc:date>
        <z:language>Turkish</z:language>
        <dc:coverage>WOS:001062571000166</dc:coverage>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/10223938/?arnumber=10223938</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>ISSN: 2165-0608</dc:description>
        <bib:pages>1-4</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2023 31ST SIGNAL PROCESSING AND COMMUNICATIONS APPLICATIONS CONFERENCE, SIU</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_879">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;28&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1686">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1686/Öztürk et al. - 2023 - A Transformer-Based Prior Legal Case Retrieval Method.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=10223938&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-28 18:07:59</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_1685">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1685/10223938.html"/>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/10223938/?arnumber=10223938</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-28 18:07:57</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_882">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2444-250X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gomez</foaf:surname>
                        <foaf:givenName>FJD</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Galende</foaf:surname>
                        <foaf:givenName>JN</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_883"/>
        <link:link rdf:resource="#item_1552"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>LAW</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>data protection</dc:subject>
        <dc:subject>GPT-4</dc:subject>
        <dc:subject>OpenAI</dc:subject>
        <dc:subject>legal tech</dc:subject>
        <dc:subject>intellectual property</dc:subject>
        <dc:subject>legal industry innovation</dc:subject>
        <dc:title>ChatGPT and GPT-4: utilities in the legal sector, functioning, limitations and risks of foundational models</dc:title>
        <dcterms:abstract>Artificial intelligence systems such as ChatGPT, the OpenAI chatbot, based on the language model family GPT (generative pre -trained transformers), as well as other solutions built on this technology and fine-tuned for specific tasks, have generated considerable interest across various sectors, including the legal sector. However, such models still feature important limitations and legal risks associated to their use, which must be considered in order to make a proper and legally responsible use of this technology. This work aims to familiarize the readers (men and women) with the configuration, architecture, and functioning of these systems, as well as their functionalities in the legal sector. It includes a review of their associated legal limitations and risks, with crucial practical implications in their application.</dcterms:abstract>
        <dc:date>2024 MAY</dc:date>
        <z:language>Spanish</z:language>
        <dc:coverage>WOS:001222663200006</dc:coverage>
        <bib:pages>45-88</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2444-250X">
        <dc:title>TECNOLOGIA CIENCIA Y EDUCACION</dc:title>
        <dc:identifier>DOI 10.51302/tce.2024.19081</dc:identifier>
        <prism:number>28</prism:number>
        <dc:identifier>ISSN 2444-250X</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_883">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;202&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1552">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1552/Gomez e Galende - 2024 - ChatGPT and GPT-4 utilities in the legal sector, functioning, limitations and risks of foundational.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.tecnologia-ciencia-educacion.com/index.php/TCE/article/download/19081/22121</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-12-25 22:33:06</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:979-8-3503-8635-6">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 979-8-3503-8635-6</dc:identifier>
                <dc:title>2024 4th International Conference on Pervasive Computing and Social Networking (ICPCSN)</dc:title>
                <dc:identifier>DOI 10.1109/ICPCSN62568.2024.00022</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vayadande</foaf:surname>
                        <foaf:givenName>K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bhat</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bachhav</foaf:surname>
                        <foaf:givenName>P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bhoyar</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Charoliya</foaf:surname>
                        <foaf:givenName>Z</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chavan</foaf:surname>
                        <foaf:givenName>A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>IEEE COMPUTER SOC</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_885"/>
        <link:link rdf:resource="#item_1718"/>
        <link:link rdf:resource="#item_1719"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Named entity recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>LAW</dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Social networking (online)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Law</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Sentiment analysis</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>Irregularities Detection</dc:subject>
        <dc:subject>Legal Document Processing</dc:subject>
        <dc:subject>Legal Technology</dc:subject>
        <dc:subject>Natural Language Processing (NLP)</dc:subject>
        <dc:subject>OpenAI Embeddings</dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Documentation</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Pervasive computing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Technological innovation</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>AI-Powered Legal Documentation Assistant</dc:title>
        <dcterms:abstract>Legal systems worldwide vary in structure and principles, reflecting the diverse legal traditions of different countries. The legal system, inherently complex and reliant on meticulous documentation, often faces challenges related to time-consuming manual processes and the potential for human errors. The system proposed provides a transformative solution to the above problems. The system emerges as a groundbreaking solution within the intricate landscape of legal systems which responds to these challenges by seamlessly integrating advanced AI techniques. At its core, OpenAI embeddings takes center stage, demonstrating unparalleled proficiency in document generation, comprehension, and abnormality detection, addressing the complexities ingrained in legal documentation. In contrast to traditional approaches, this system maximizes the versatility of ChatGPT 3.5, allowing it to not only issue commands but also proficiently generate a diverse array of legal documents. By incorporating an understanding module equipped with PyPDF, Amazon Textract, and langchain utilities, the system adeptly handles document intricacies. The utilization of OpenAI Embeddings further enhances natural language understanding. Leveraging sentiment analysis and Named Entity Recognition (NER) in its natural language processing (NLP) toolkit, the system employs an intuitive web interface for irregularities detection. The exploration of AI for automated irregularity detection showcases its transformative potential in ensuring document accuracy within the legal domain. This project, therefore, stands as a beacon of innovation, promising to reshape the dynamics of legal document processing by merging advanced AI capabilities with the unique demands of legal systems.</dcterms:abstract>
        <dc:date>2024-05</dc:date>
        <z:language>English</z:language>
        <dc:coverage>WOS:001289477900015</dc:coverage>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/10607645</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <bib:pages>84-91</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2024 4th International Conference on Pervasive Computing and Social Networking (ICPCSN)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_885">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;0&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;16&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_1718">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1718/Vayadande et al. - 2024 - AI-Powered Legal Documentation Assistant.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=10607645&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 13:06:47</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_1719">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/1719/10607645.html"/>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/10607645</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 13:06:58</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Collection rdf:about="#collection_10">
        <dc:title>Rejeitados_Q2</dc:title>
        <dcterms:hasPart rdf:resource="#item_492"/>
        <dcterms:hasPart rdf:resource="urn:isbn:1063-6919"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-950737-13-0"/>
        <dcterms:hasPart rdf:resource="#item_498"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-6654-2418-9"/>
        <dcterms:hasPart rdf:resource="#item_502"/>
        <dcterms:hasPart rdf:resource="#item_504"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-950737-90-1"/>
        <dcterms:hasPart rdf:resource="urn:isbn:2945-9133"/>
        <dcterms:hasPart rdf:resource="#item_512"/>
        <dcterms:hasPart rdf:resource="#item_514"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-952148-25-5"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-952148-90-3"/>
        <dcterms:hasPart rdf:resource="#item_520"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-952148-62-0"/>
        <dcterms:hasPart rdf:resource="#item_524"/>
        <dcterms:hasPart rdf:resource="urn:isbn:1860-949X"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-7998-4"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-952148-60-6"/>
        <dcterms:hasPart rdf:resource="#item_532"/>
        <dcterms:hasPart rdf:resource="#item_534"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-6654-2812-5"/>
        <dcterms:hasPart rdf:resource="#item_538"/>
        <dcterms:hasPart rdf:resource="#item_540"/>
        <dcterms:hasPart rdf:resource="urn:isbn:0302-9743"/>
        <dcterms:hasPart rdf:resource="#item_546"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-954085-52-7"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-8037-9"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-8458-2"/>
        <dcterms:hasPart rdf:resource="urn:isbn:2472-6737"/>
        <dcterms:hasPart rdf:resource="#item_556"/>
        <dcterms:hasPart rdf:resource="#item_558"/>
        <dcterms:hasPart rdf:resource="#item_560"/>
        <dcterms:hasPart rdf:resource="#item_562"/>
        <dcterms:hasPart rdf:resource="#item_564"/>
        <dcterms:hasPart rdf:resource="#item_566"/>
        <dcterms:hasPart rdf:resource="#item_568"/>
        <dcterms:hasPart rdf:resource="#item_570"/>
        <dcterms:hasPart rdf:resource="#item_572"/>
        <dcterms:hasPart rdf:resource="#item_574"/>
        <dcterms:hasPart rdf:resource="#item_576"/>
        <dcterms:hasPart rdf:resource="#item_578"/>
        <dcterms:hasPart rdf:resource="#item_580"/>
        <dcterms:hasPart rdf:resource="#item_582"/>
        <dcterms:hasPart rdf:resource="#item_584"/>
        <dcterms:hasPart rdf:resource="#item_586"/>
        <dcterms:hasPart rdf:resource="#item_588"/>
        <dcterms:hasPart rdf:resource="#item_590"/>
        <dcterms:hasPart rdf:resource="#item_592"/>
        <dcterms:hasPart rdf:resource="#item_594"/>
        <dcterms:hasPart rdf:resource="#item_596"/>
        <dcterms:hasPart rdf:resource="#item_598"/>
        <dcterms:hasPart rdf:resource="#item_600"/>
        <dcterms:hasPart rdf:resource="#item_602"/>
        <dcterms:hasPart rdf:resource="#item_604"/>
        <dcterms:hasPart rdf:resource="#item_606"/>
        <dcterms:hasPart rdf:resource="#item_608"/>
        <dcterms:hasPart rdf:resource="#item_610"/>
        <dcterms:hasPart rdf:resource="#item_612"/>
        <dcterms:hasPart rdf:resource="#item_614"/>
        <dcterms:hasPart rdf:resource="#item_616"/>
        <dcterms:hasPart rdf:resource="#item_618"/>
        <dcterms:hasPart rdf:resource="#item_620"/>
        <dcterms:hasPart rdf:resource="#item_622"/>
        <dcterms:hasPart rdf:resource="#item_624"/>
        <dcterms:hasPart rdf:resource="#item_626"/>
        <dcterms:hasPart rdf:resource="#item_628"/>
        <dcterms:hasPart rdf:resource="#item_630"/>
        <dcterms:hasPart rdf:resource="#item_632"/>
        <dcterms:hasPart rdf:resource="#item_634"/>
        <dcterms:hasPart rdf:resource="#item_638"/>
        <dcterms:hasPart rdf:resource="#item_640"/>
        <dcterms:hasPart rdf:resource="#item_642"/>
        <dcterms:hasPart rdf:resource="#item_644"/>
        <dcterms:hasPart rdf:resource="#item_646"/>
        <dcterms:hasPart rdf:resource="#item_648"/>
        <dcterms:hasPart rdf:resource="#item_650"/>
        <dcterms:hasPart rdf:resource="#item_652"/>
        <dcterms:hasPart rdf:resource="#item_656"/>
        <dcterms:hasPart rdf:resource="urn:isbn:1049-5258"/>
        <dcterms:hasPart rdf:resource="#item_660"/>
        <dcterms:hasPart rdf:resource="#item_662"/>
        <dcterms:hasPart rdf:resource="#item_664"/>
        <dcterms:hasPart rdf:resource="urn:isbn:2640-3498"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-950737-48-2"/>
        <dcterms:hasPart rdf:resource="https://aclanthology.org/D19-1250/"/>
        <dcterms:hasPart rdf:resource="#item_676"/>
        <dcterms:hasPart rdf:resource="#item_678"/>
        <dcterms:hasPart rdf:resource="#item_680"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-8016-4"/>
        <dcterms:hasPart rdf:resource="#item_686"/>
        <dcterms:hasPart rdf:resource="urn:isbn:2159-5399"/>
        <dcterms:hasPart rdf:resource="#item_692"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-8309-7"/>
        <dcterms:hasPart rdf:resource="#item_696"/>
        <dcterms:hasPart rdf:resource="#item_698"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-955917-09-4"/>
        <dcterms:hasPart rdf:resource="#item_704"/>
        <dcterms:hasPart rdf:resource="#item_706"/>
        <dcterms:hasPart rdf:resource="#item_708"/>
        <dcterms:hasPart rdf:resource="#item_710"/>
        <dcterms:hasPart rdf:resource="#item_714"/>
        <dcterms:hasPart rdf:resource="#item_716"/>
        <dcterms:hasPart rdf:resource="#item_718"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-955917-21-6"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-9385-0"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-9132-0"/>
        <dcterms:hasPart rdf:resource="#item_726"/>
        <dcterms:hasPart rdf:resource="#item_728"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-9096-5"/>
        <dcterms:hasPart rdf:resource="#item_734"/>
        <dcterms:hasPart rdf:resource="#item_738"/>
        <dcterms:hasPart rdf:resource="#item_740"/>
        <dcterms:hasPart rdf:resource="#item_744"/>
        <dcterms:hasPart rdf:resource="#item_746"/>
        <dcterms:hasPart rdf:resource="#item_750"/>
        <dcterms:hasPart rdf:resource="#item_752"/>
        <dcterms:hasPart rdf:resource="#item_754"/>
        <dcterms:hasPart rdf:resource="#item_758"/>
        <dcterms:hasPart rdf:resource="#item_760"/>
        <dcterms:hasPart rdf:resource="#item_762"/>
        <dcterms:hasPart rdf:resource="#item_764"/>
        <dcterms:hasPart rdf:resource="#item_766"/>
        <dcterms:hasPart rdf:resource="#item_768"/>
        <dcterms:hasPart rdf:resource="#item_770"/>
        <dcterms:hasPart rdf:resource="#item_772"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-5386-3260-4"/>
        <dcterms:hasPart rdf:resource="#item_776"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-351-12414-0"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-6012-8"/>
        <dcterms:hasPart rdf:resource="urn:isbn:1742-6588"/>
        <dcterms:hasPart rdf:resource="#item_784"/>
        <dcterms:hasPart rdf:resource="#item_786"/>
        <dcterms:hasPart rdf:resource="#item_788"/>
        <dcterms:hasPart rdf:resource="#item_790"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-7281-4876-2"/>
        <dcterms:hasPart rdf:resource="#item_796"/>
        <dcterms:hasPart rdf:resource="#item_798"/>
        <dcterms:hasPart rdf:resource="#item_800"/>
        <dcterms:hasPart rdf:resource="urn:isbn:1865-0929"/>
        <dcterms:hasPart rdf:resource="#item_804"/>
        <dcterms:hasPart rdf:resource="#item_806"/>
        <dcterms:hasPart rdf:resource="#item_808"/>
        <dcterms:hasPart rdf:resource="#item_810"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-6654-6403-1"/>
        <dcterms:hasPart rdf:resource="#item_816"/>
        <dcterms:hasPart rdf:resource="#item_818"/>
        <dcterms:hasPart rdf:resource="#item_820"/>
        <dcterms:hasPart rdf:resource="#item_822"/>
        <dcterms:hasPart rdf:resource="#item_824"/>
        <dcterms:hasPart rdf:resource="#item_826"/>
        <dcterms:hasPart rdf:resource="#item_828"/>
        <dcterms:hasPart rdf:resource="#item_830"/>
        <dcterms:hasPart rdf:resource="urn:isbn:979-8-4007-0830-5"/>
        <dcterms:hasPart rdf:resource="urn:isbn:979-8-4007-0124-5"/>
        <dcterms:hasPart rdf:resource="urn:isbn:979-8-4007-0602-8"/>
        <dcterms:hasPart rdf:resource="#item_838"/>
        <dcterms:hasPart rdf:resource="#item_840"/>
        <dcterms:hasPart rdf:resource="#item_842"/>
        <dcterms:hasPart rdf:resource="#item_846"/>
        <dcterms:hasPart rdf:resource="#item_848"/>
        <dcterms:hasPart rdf:resource="#item_850"/>
        <dcterms:hasPart rdf:resource="#item_852"/>
        <dcterms:hasPart rdf:resource="urn:isbn:979-8-3503-8678-3"/>
        <dcterms:hasPart rdf:resource="#item_864"/>
        <dcterms:hasPart rdf:resource="#item_874"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_11">
        <dc:title>Rever_Q2</dc:title>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-6976-3"/>
        <dcterms:hasPart rdf:resource="#item_654"/>
        <dcterms:hasPart rdf:resource="#item_672"/>
        <dcterms:hasPart rdf:resource="#item_682"/>
        <dcterms:hasPart rdf:resource="#item_690"/>
        <dcterms:hasPart rdf:resource="urn:isbn:1062-922X"/>
        <dcterms:hasPart rdf:resource="#item_844"/>
    </z:Collection>
</rdf:RDF>
