<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:vcard="http://nwalsh.com/rdf/vCard#"
 xmlns:link="http://purl.org/rss/1.0/modules/link/"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/">
    <rdf:Description rdf:about="urn:isbn:978-157735835-0%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-157735835-0 (ISBN)</dc:identifier>
                <dc:title>AAAI - AAAI Conf. Artif. Intell.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>AAAI press</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ju</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deng</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1492"/>
        <dcterms:isReferencedBy rdf:resource="#item_2307"/>
        <dc:subject>Knowledge graphs</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Knowledge representation</dc:subject>
        <dc:subject>Domain knowledge</dc:subject>
        <dc:subject>Domain specific</dc:subject>
        <dc:subject>Domain-specific knowledge</dc:subject>
        <dc:subject>Knowledge incorporation</dc:subject>
        <dc:subject>Loading models</dc:subject>
        <dc:subject>Pre-training</dc:subject>
        <dc:subject>Representation model</dc:subject>
        <dc:title>K-BERT: Enabling language representation with knowledge graph</dc:title>
        <dcterms:abstract>Pre-trained language representation models, such as BERT, capture a general language representation from large-scale corpora, but lack domain-specific knowledge. When reading a domain text, experts make inferences with relevant knowledge. For machines to achieve this capability, we propose a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge. However, too much knowledge incorporation may divert the sentence from its correct meaning, which is called knowledge noise (KN) issue. To overcome KN, K-BERT introduces soft-position and visible matrix to limit the impact of knowledge. K-BERT can easily inject domain knowledge into the models by being equipped with a KG without pre-training by itself because it is capable of loading model parameters from the pre-trained BERT. Our investigation reveals promising results in twelve NLP tasks. Especially in domain-specific tasks (including finance, law, and medicine), K-BERT significantly outperforms BERT, which demonstrates that K-BERT is an excellent choice for solving the knowledge-driven problems that require experts. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106402604&amp;partnerID=40&amp;md5=f871a87e7bb104c4921c1497e39c8366</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: AAAI - AAAI Conf. Artif. Intell.</dc:description>
        <bib:pages>2901-2908</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>AAAI 2020 - 34th AAAI Conference on Artificial Intelligence</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1492">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 546; Correspondence Address: Q. Ju; Tencent Research, Beijing, China; email: damonju@tencent.com; P. Wang; Peking University, Beijing, China; email: pwang@pku.edu.cn; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2307">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-195073748-2%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-195073748-2 (ISBN)</dc:identifier>
                <dc:title>ACL - Annu. Meet. Assoc. Comput. Linguist., Proc. Conf.</dc:title>
                <dc:identifier>DOI 10.18653/v1/P19-1636</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Florence, Italy</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chalkidis</foaf:surname>
                        <foaf:givenName>I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fergadiotis</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Malakasiotis</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Androutsopoulos</foaf:surname>
                        <foaf:givenName>I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1493"/>
        <link:link rdf:resource="#item_2687"/>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Context sensitive</dc:subject>
        <dc:subject>Domain specific</dc:subject>
        <dc:subject>EU legislation</dc:subject>
        <dc:subject>Improve performance</dc:subject>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>Legal domains</dc:subject>
        <dc:subject>Multi-label text classification</dc:subject>
        <dc:subject>Neural classifiers</dc:subject>
        <dc:subject>State-of-the-art methods</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:title>Large-scale multi-label text classification on EU legislation</dc:title>
        <dcterms:abstract>We consider Large-Scale Multi-Label Text Classification (LMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR-LEX, annotated with ~4.3k EUROVOC labels, which is suitable for LMTC, few- and zero-shot learning. Experimenting with several neural classifiers, we show that BIGRUs with label-wise attention perform better than other current state of the art methods. Domain-specific WORD2VEC and context-sensitive ELMO embeddings further improve performance. We also find that considering only particular zones of the documents is sufficient. This allows us to bypass BERT's maximum text length limit and fine-tune BERT, obtaining the best results in all but zero-shot learning cases. © 2019 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <z:libraryCatalog>ACLWeb</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084087887&amp;partnerID=40&amp;md5=592be7232ecc0dcd1836d9b359a63daf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: ACL - Annu. Meet. Assoc. Comput. Linguist., Proc. Conf.</dc:description>
        <bib:pages>6314-6322</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1493">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 114; Conference name: 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019; Conference date: 28 July 2019 through 2 August 2019; Conference code: 159206&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2687">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2687/Chalkidis et al. - 2019 - Large-Scale Multi-Label Text Classification on EU Legislation.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://aclanthology.org/P19-1636.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:42:32</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-172816926-2%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-172816926-2 (ISBN)</dc:identifier>
                <dc:title>Proc Int Jt Conf Neural Networks</dc:title>
                <dc:identifier>DOI 10.1109/IJCNN48605.2020.9207528</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hong</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mo</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1495"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Neural networks</dc:subject>
        <dc:subject>Similar case</dc:subject>
        <dc:subject>Legal case</dc:subject>
        <dc:subject>Attention mechanism</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>Enhanced semantics</dc:subject>
        <dc:subject>Feature vectors</dc:subject>
        <dc:subject>Law intelligence</dc:subject>
        <dc:subject>Learning models</dc:subject>
        <dc:subject>Long-range dependencies</dc:subject>
        <dc:subject>Public dataset</dc:subject>
        <dc:subject>Semantic Web</dc:subject>
        <dc:subject>Similar case matching</dc:subject>
        <dc:subject>Text-matching</dc:subject>
        <dc:title>Legal Feature Enhanced Semantic Matching Network for Similar Case Matching</dc:title>
        <dcterms:abstract>Similar case matching (SCM) aims to determine whether legal case documents are similar or not. In fact, SCM is an extension of the semantic text matching. Various deep learning models are proposed to solve the semantic text matching problems. However, the main difference between the case documents may be subtle, and the length of documents can be quite long. Moreover, the case documents are written in structural format and contain plenty of legal terms. To address these challenges, we propose a novel model in this paper. Accordingly, the legal feature vector is introduced into the semantic text matching model, and BERT is adopted as the encoding layer to capture long-range dependencies in the case documents. We conduct several experiments to evaluate the performance of our proposed model. The results show that our model outperforms other existing methods on the public dataset CAIL2019-SCM. © 2020 IEEE.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093864460&amp;doi=10.1109%2fIJCNN48605.2020.9207528&amp;partnerID=40&amp;md5=e6ad15744d5ebe9321c6415398f56c7f</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc Int Jt Conf Neural Networks</dc:description>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the International Joint Conference on Neural Networks</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1495">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 26; Conference name: 2020 International Joint Conference on Neural Networks, IJCNN 2020; Conference date: 19 July 2020 through 24 July 2020; Conference code: 163566; CODEN: 85OFA&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:22128271%20(ISSN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>91</prism:volume>
                <dc:identifier>ISBN 22128271 (ISSN)</dc:identifier>
                <dc:title>Procedia CIRP</dc:title>
                <dc:identifier>DOI 10.1016/j.procir.2020.02.210</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Elsevier B.V.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Akay</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>S.-G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Mpofu K.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Butala P.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1494"/>
        <dcterms:isReferencedBy rdf:resource="#item_2323"/>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Representation model</dc:subject>
        <dc:subject>Axiomatic design</dc:subject>
        <dc:subject>functional independence</dc:subject>
        <dc:subject>Functional requirement</dc:subject>
        <dc:subject>language representation</dc:subject>
        <dc:subject>Learning languages</dc:subject>
        <dc:subject>Quantitative values</dc:subject>
        <dc:subject>Semantic similarity</dc:subject>
        <dc:subject>Subjective judgement</dc:subject>
        <dc:subject>Vector spaces</dc:subject>
        <dc:title>Measuring functional independence in design with deep-learning language representation models</dc:title>
        <dcterms:abstract>Measuring functional coupling in complex systems is an important task for good design practice, though historically it has been an art of subjective judgement. With the recent advancements in Deep Learning and Natural Language Processing, functional requirements (FRs) and design parameters (DPs), which are expressed as words and sentences, can be represented in a vector space. The sentence embedding model, BERT, was used in this paper to vectorize FRs and DPs, to calculate functional independence and to study how metrics for functional coupling measurement can be enhanced. It was found that semantic similarity among FRs and DPs, represented in vector space, could be used to compute quantitative values for metrics of functional independence. It was also found that design cases where coupling was unambiguous yielded the best results, while cases where laws of physics needed to define the FR-DP relationship did not transliterate well to the natural language used to express the FR-DP highlighted the limitations of the model in its current state. This study, however, demonstrates a great opportunity to develop a robust, fine-tuned design language representation model for accurately measuring functional independence as a part of our effort to enhance design intelligence. © 2017 The Authors. Published by Elsevier B.V.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091699704&amp;doi=10.1016%2fj.procir.2020.02.210&amp;partnerID=40&amp;md5=845e7642a8711cbb731c7390b7db795b</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Procedia CIRP</dc:description>
        <bib:pages>528-533</bib:pages>
        <bib:presentedAt>
           <bib:Conference><dc:title>Procedia CIRP</dc:title></bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1494">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 7; Correspondence Address: S.-G. Kim; Massachusetts Institute of Technology, Cambridge, United States; email: sangkim@mit.edu; Conference name: 30th CIRP Design on Design, CIRP Design 2020; Conference date: 5 May 2020 through 8 May 2020; Conference code: 162574&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2323">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:10450823%20(ISSN);%20978-099924116-5%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2021-January</prism:volume>
                <dc:identifier>ISBN 10450823 (ISSN); 978-099924116-5 (ISBN)</dc:identifier>
                <dc:title>IJCAI Int. Joint Conf. Artif. Intell.</dc:title>
                <dc:identifier>DOI 10.24963/ijcai.2020/484</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>International Joint Conferences on Artificial Intelligence</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shao</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mao</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ma</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Satoh</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ma</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Bessiere C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1496"/>
        <link:link rdf:resource="#item_2673"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Case retrieval</dc:subject>
        <dc:subject>Computational costs</dc:subject>
        <dc:subject>Keyword queries</dc:subject>
        <dc:subject>Large dataset</dc:subject>
        <dc:subject>Legal case</dc:subject>
        <dc:subject>Query processing</dc:subject>
        <dc:subject>Relevance judgment</dc:subject>
        <dc:subject>Semantic relationships</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Small scale</dc:subject>
        <dc:subject>Text retrieval</dc:subject>
        <dc:title>BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval</dc:title>
        <dcterms:abstract>Legal case retrieval is a specialized IR task that involves retrieving supporting cases given a query case. Compared with traditional ad-hoc text retrieval, the legal case retrieval task is more challenging since the query case is much longer and more complex than common keyword queries. Besides that, the definition of relevance between a query case and a supporting case is beyond general topical relevance and it is therefore difficult to construct a large-scale case retrieval dataset, especially one with accurate relevance judgments. To address these challenges, we propose BERT-PLI, a novel model that utilizes BERT to capture the semantic relationships at the paragraph-level and then infers the relevance between two cases by aggregating paragraph-level interactions. We fine-tune the BERT model with a relatively small-scale case law entailment dataset to adapt it to the legal scenario and employ a cascade framework to reduce the computational cost. We conduct extensive experiments on the benchmark of the relevant case retrieval task in COLIEE 2019. Experimental results demonstrate that our proposed method outperforms existing solutions. © 2020 Inst. Sci. inf., Univ. Defence in Belgrade. All rights reserved.</dcterms:abstract>
        <dc:date>2020/07/09</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <z:libraryCatalog>www.ijcai.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097347564&amp;partnerID=40&amp;md5=e81b06717bf65b9adfa866b758c2f4e0</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: IJCAI Int. Joint Conf. Artif. Intell.</dc:description>
        <bib:pages>3501-3507</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Twenty-Ninth International Joint Conference on Artificial Intelligence</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1496">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 117; Correspondence Address: Y. Liu; BNRist, DCST, Tsinghua University, Beijing, China; email: yiqunliu@tsinghua.edu.cn; Conference name: 29th International Joint Conference on Artificial Intelligence, IJCAI 2020; Conference code: 165342&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2673">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2673/Shao et al. - 2020 - BERT-PLI Modeling Paragraph-Level Interactions for Legal Case Retrieval.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.ijcai.org/proceedings/2020/0484.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:36:08</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-172818156-1%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-172818156-1 (ISBN)</dc:identifier>
                <dc:title>Proc. - IEEE Int. Conf. Knowl. Graph, ICKG</dc:title>
                <dc:identifier>DOI 10.1109/ICBK50248.2020.00086</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yao</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                   <foaf:Person><foaf:surname>Chen E.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Antoniou G.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Wu X.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Kumar V.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1497"/>
        <link:link rdf:resource="#item_2635"/>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>Legal texts</dc:subject>
        <dc:subject>Extraction</dc:subject>
        <dc:subject>Knowledge representation</dc:subject>
        <dc:subject>Case description</dc:subject>
        <dc:subject>Chinese characters</dc:subject>
        <dc:subject>Chinese legal text</dc:subject>
        <dc:subject>Data annotation</dc:subject>
        <dc:subject>Event dataset construction</dc:subject>
        <dc:subject>Event extraction</dc:subject>
        <dc:subject>Event trigger</dc:subject>
        <dc:subject>Information analysis</dc:subject>
        <dc:subject>Litigation visualization</dc:subject>
        <dc:subject>Role assignment</dc:subject>
        <dc:subject>WEB application</dc:subject>
        <dc:title>Event extraction for criminal legal text</dc:title>
        <dcterms:abstract>This paper concerns with the actual problems in the legal work. We apply event extraction technology to the case description part in the Chinese legal text. We define the event type, event argument and event argument role of the larceny case, and construct a larceny case event extraction dataset through data annotation. We divide event extraction into two steps: event trigger word and argument joint extraction and event argument role assignment. We use BERT to obtain Chinese character vectors, use the BiLSTM-CRF model for extraction at the first step, and combine additional features with the extraction results of the first step, then input them to the CRF model of the second step to obtain an improvement in extraction result. We display the extracted event information in time series to realize the litigation visualization. We format Chinese time expressions, sorts the event information in tine series, and develops a Web application to display the timeline of event information. © 2020 IEEE.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092526577&amp;doi=10.1109%2fICBK50248.2020.00086&amp;partnerID=40&amp;md5=57f32f4d93771c9310e9b10e1cb98e8a</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. - IEEE Int. Conf. Knowl. Graph, ICKG</dc:description>
        <bib:pages>573-580</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings - 11th IEEE International Conference on Knowledge Graph, ICKG 2020</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1497">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 11; Correspondence Address: J. Yao; East China Normal University, Shanghai, China; email: junjie.yao@cs.ecnu.edu.cn; Conference name: 11th IEEE International Conference on Knowledge Graph, ICKG 2020; Conference date: 9 August 2020 through 11 August 2020; Conference code: 163030&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2635">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2635/Li et al. - 2020 - Event extraction for criminal legal text.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=9194466&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:08:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-195073761-1%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-195073761-1 (ISBN)</dc:identifier>
                <dc:title>SIGDIAL - Annu. Meet. Spec. Interest Group Discourse Dial. - Proc. Conf.</dc:title>
                <dc:identifier>DOI 10.18653/v1/w19-5930</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alloatti</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Caro</foaf:surname>
                        <foaf:givenName>L.D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sportelli</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1498"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Text classification</dc:subject>
        <dc:subject>Automatic systems</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Real-life applications</dc:subject>
        <dc:subject>Restricted-domain</dc:subject>
        <dc:subject>Specific tasks</dc:subject>
        <dc:subject>Test corpus</dc:subject>
        <dc:subject>Training corpus</dc:subject>
        <dc:title>Real life application of a question answering system using bert language model</dc:title>
        <dcterms:abstract>Real life scenarios are often left untouched by the newest advances in research. They usually require the resolution of some specific task applied to a restricted domain, all the while providing small amounts of data to begin with. In this study we apply one of the newest innovations in Deep Learning to a task of text classification. The goal is to create a question answering system in Italian that provides information about a specific subject, e-invoicing and digital billing. Italy recently introduced a new legislation about e-invoicing and people have some legit doubts, therefore a large share of professionals could benefit from this tool. We gathered few pairs of question and answers; afterwards, we expanded the data, using it as a training corpus for BERT language model. Through a separate test corpus we evaluated the accuracy of the answer provided. Values show that the automatic system alone performs surprisingly well. The demo interface is hosted on Telegram, which makes the system immediately available to test. ©2019 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091584760&amp;doi=10.18653%2fv1%2fw19-5930&amp;partnerID=40&amp;md5=917a2b7cbb130185fdb9ea82a80eee11</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: SIGDIAL - Annu. Meet. Spec. Interest Group Discourse Dial. - Proc. Conf.</dc:description>
        <bib:pages>250-253</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>SIGDIAL 2019 - 20th Annual Meeting of the Special Interest Group Discourse Dialogue - Proceedings of the Conference</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1498">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 12; Conference name: 20th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL 2019; Conference date: 11 September 2019 through 13 September 2019; Conference code: 161400&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-172810858-2%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-172810858-2 (ISBN)</dc:identifier>
                <dc:title>Proc. - IEEE Int. Conf. Big Data, Big Data</dc:title>
                <dc:identifier>DOI 10.1109/BigData47090.2019.9006511</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yamakoshi</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Komamizu</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ogawa</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Toyama</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                   <foaf:Person><foaf:surname>Baru C.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Huan J.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Khan L.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Hu X.T.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Ak R.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Tian Y.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Barga R.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Zaniolo C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Lee K.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Ye Y.F.</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1499"/>
        <link:link rdf:resource="#item_2640"/>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>Big data</dc:subject>
        <dc:subject>Decision trees</dc:subject>
        <dc:subject>Random forests</dc:subject>
        <dc:subject>Prior knowledge</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>Common knowledge</dc:subject>
        <dc:subject>Conventional classifier</dc:subject>
        <dc:subject>Domain adaptation</dc:subject>
        <dc:subject>Japanese</dc:subject>
        <dc:subject>legal term</dc:subject>
        <dc:subject>term correction</dc:subject>
        <dc:subject>Training techniques</dc:subject>
        <dc:title>Japanese Mistakable Legal Term Correction using Infrequency-aware BERT Classifier</dc:title>
        <dcterms:abstract>We propose a method that assists legislative drafters in locating inappropriate legal terms in Japanese statutory sentences and suggests corrections. We focus on sets of mistakable legal terms whose usages are defined in legislation drafting rules. Our method predicts suitable legal terms using a classifier based on a BERT (Bidirectional Encoder Representations from Transformers) model. We apply three techniques in training the BERT classifier, specifically, preliminary domain adaptation, repetitive soft undersampling, and classifier unification. These techniques cope with two levels of infrequency: legal term-level infrequency that causes class imbalance and legal term set-level infrequency that causes underfitting. Concretely, preliminary domain adaptation improves overall performance by providing prior knowledge of statutory sentences, repetitive soft undersampling improves performance on infrequent legal terms without sacrificing performance on frequent legal terms, and classifier unification improves performance on infrequent legal term sets by sharing common knowledge among legal term sets. Our experiments show that our classifier outperforms conventional classifiers using Random Forest or a language model, and that all three training techniques contribute to performance improvement. © 2019 IEEE.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081324514&amp;doi=10.1109%2fBigData47090.2019.9006511&amp;partnerID=40&amp;md5=61faef3e5bfd6ae0e05bca12d3a0994c</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. - IEEE Int. Conf. Big Data, Big Data</dc:description>
        <bib:pages>4342-4351</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings - 2019 IEEE International Conference on Big Data, Big Data 2019</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1499">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 2; Conference name: 2019 IEEE International Conference on Big Data, Big Data 2019; Conference date: 9 December 2019 through 12 December 2019; Conference code: 157991&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2640">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2640/Yamakoshi et al. - 2019 - Japanese Mistakable Legal Term Correction using Infrequency-aware BERT Classifier.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=9006511&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:08:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076355086&amp;doi=10.7755%2fMFR.81.1.1&amp;partnerID=40&amp;md5=f0013419e7bf2aec8bcf52771a5664e9">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:00901830%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Spence</foaf:surname>
                        <foaf:givenName>B.C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1501"/>
        <dcterms:isReferencedBy rdf:resource="#item_2244"/>
        <dc:subject>anthropogenic effect</dc:subject>
        <dc:subject>Cali</dc:subject>
        <dc:subject>California</dc:subject>
        <dc:subject>Colombia</dc:subject>
        <dc:subject>conservation management</dc:subject>
        <dc:subject>conservation status</dc:subject>
        <dc:subject>endangered species</dc:subject>
        <dc:subject>freshwater environment</dc:subject>
        <dc:subject>human activity</dc:subject>
        <dc:subject>Jordan</dc:subject>
        <dc:subject>nomenclature</dc:subject>
        <dc:subject>Oncorhynchus</dc:subject>
        <dc:subject>Pisces</dc:subject>
        <dc:subject>salmonid</dc:subject>
        <dc:subject>species conservation</dc:subject>
        <dc:subject>taxonomy</dc:subject>
        <dc:subject>United States</dc:subject>
        <dc:subject>Valle del Cauca</dc:subject>
        <dc:subject>watershed</dc:subject>
        <dc:title>Interpreting early species range descriptions for Pacific salmon, Oncorhynchus spp., in coastal California watersheds: The historical context</dc:title>
        <dcterms:abstract>Scientists and managscientists and others have often taken these ers implementing endangered species laws es. Further confounding interpretation of early range descriptions at face value, often face the task of defining the histori-early reports is that the first systematic ex-without critically examining the underlying cal geographic ranges for threatened and plorations of coastal watersheds took place historical context. When Jordan and his endangered species. To do so, they com-well after significant anthropogenic damage contemporaries first began writing about monly turn to the writings of early bioloto salmon habitats had already occurred; the ranges of Pacific salmon, scientific ex-gists seeking accounts of species in regions thus, failure to detect species on these surploration of coastal watersheds of Cali-where they may have been extirpated as a veys does not necessarily indicate a species fornia was in its infancy. Additionally, the result of anthropogenic activities over the was absent, either at the time of the survey taxonomy and nomenclature of Pacific sallast 150–175 years. In the case of Pacific or in the years prior to significant human monids were in states of extreme disarray, salmon, Oncorhynchus spp., the writings disturbances. As a result, any single writ-with numerous putative species described of David Starr Jordan, Charles Henry Giling of Jordan’s and his colleagues between based on variations due to age, sex, and bert, John Otterbein Snyder, and other facthe late 1870’s and the early 1900’s is like-reproductive condition. ulty and staff at Stanford University during ly to contain species range information that Even after Jordan and Gilbert began the late 1800’s and early 1900’s have been is equivocal, if not demonstrably inaccuto resolve Pacific salmon taxonomy in the particularly influential, as these scientists rate. This is not to disparage Jordan and his 1880’s, confusion in nomenclature, exacerwere widely recognized as the leading aucontemporaries in any way or to diminish bated by a primitive understanding of Pacifthorities on west coast fishes and salmotheir extraordinary scientific achievements. ic salmon life histories, contributed to frenids in particular. However, scientists and managers need to quent misidentification of west coast salmoBecause of the tremendous achieve-be cognizant of these limitations when using nids and hence inaccurate descriptions of ments of these pioneering ichthyologists, historical writings to guide management of their historical freshwater spawning rang-endangered species. © 2019 National Marine Fisheries Service. All rights reserved.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076355086&amp;doi=10.7755%2fMFR.81.1.1&amp;partnerID=40&amp;md5=f0013419e7bf2aec8bcf52771a5664e9</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: National Marine Fisheries Service</dc:description>
        <bib:pages>1-39</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:00901830%20(ISSN)">
        <prism:volume>81</prism:volume>
        <dc:title>Marine Fisheries Review</dc:title>
        <dc:identifier>DOI 10.7755/MFR.81.1.1</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Mar. Fish. Rev.</dcterms:alternative>
        <dc:identifier>ISSN 00901830 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1501">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 1; Correspondence Address: B.C. Spence; Fisheries Ecology Division, Southwest Fisheries Science Center, National Marine Fisheries Service, NOAA, Santa Cruz, 110 Shaffer Road, 95060, United States; email: brian.spence@noaa.gov&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2244">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:03029743%20(ISSN);%20978-303032380-6%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>11856 LNAI</prism:volume>
                <dc:identifier>ISBN 03029743 (ISSN); 978-303032380-6 (ISBN)</dc:identifier>
                <dc:title>Lect. Notes Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-32381-3_36</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>Springer</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Duan</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ma</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cui</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huo</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hu</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                   <foaf:Person><foaf:surname>Sun M.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Liu Y.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Liu Z.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Huang X.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Ji H.</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1500"/>
        <dcterms:isReferencedBy rdf:resource="#item_2674"/>
        <dc:subject>Baseline models</dc:subject>
        <dc:subject>Benchmark datasets</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Element extraction</dc:subject>
        <dc:subject>Element type</dc:subject>
        <dc:subject>Reading comprehension</dc:subject>
        <dc:title>CJRC: A Reliable Human-Annotated Benchmark DataSet for Chinese Judicial Reading Comprehension</dc:title>
        <dcterms:abstract>We present a Chinese judicial reading comprehension (CJRC) dataset which contains approximately 10K documents and almost 50K questions with answers. The documents come from judgment documents and the questions are annotated by law experts. The CJRC dataset can help researchers extract elements by reading comprehension technology. Element extraction is an important task in the legal field. However, it is difficult to predefine the element types completely due to the diversity of document types and causes of action. By contrast, machine reading comprehension technology can quickly extract elements by answering various questions from the long document. We build two strong baseline models based on BERT and BiDAF. The experimental results show that there is enough space for improvement compared to human annotators. © 2019, Springer Nature Switzerland AG.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075712647&amp;doi=10.1007%2f978-3-030-32381-3_36&amp;partnerID=40&amp;md5=7e3c44b5e008fe55ba1e7d93d44bc4c4</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Lect. Notes Comput. Sci.</dc:description>
        <bib:pages>439-451</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1500">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 52; Correspondence Address: X. Duan; Joint Laboratory of HIT and iFLYTEK (HFL), iFLYTEK Research, Beijing, China; email: xyduan@iflytek.com; Conference name: 18th China National Conference on Computational Linguistics, CCL 2019; Conference date: 18 October 2019 through 20 October 2019; Conference code: 233289&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2674">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser de acesso pago&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048523859&amp;doi=10.1016%2fj.landusepol.2018.05.002&amp;partnerID=40&amp;md5=de03f09b1dd5f5f967aabde959d56b02">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:02648377%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Linkous</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Skuzinski</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1502"/>
        <dc:subject>decision making</dc:subject>
        <dc:subject>United States</dc:subject>
        <dc:subject>dispute resolution</dc:subject>
        <dc:subject>Dispute resolution</dc:subject>
        <dc:subject>Florida</dc:subject>
        <dc:subject>Florida [United States]</dc:subject>
        <dc:subject>Institutional analysis and development</dc:subject>
        <dc:subject>institutional development</dc:subject>
        <dc:subject>Land use law</dc:subject>
        <dc:subject>land use planning</dc:subject>
        <dc:subject>legislation</dc:subject>
        <dc:subject>local government</dc:subject>
        <dc:subject>property rights</dc:subject>
        <dc:subject>Property rights</dc:subject>
        <dc:subject>state</dc:subject>
        <dc:title>Land use decision-making in the wake of state property rights legislation: Examining the institutional response to Florida's Harris Act</dc:title>
        <dcterms:abstract>Land use scholars hypothesize that state property rights legislation—adopted by more than half of U.S. states as a way to buttress the protections of landowners against uncompensated regulatory takings—negatively impacts the ability of local governments to regulate land use. This theorized impact can happen in two ways. First, compensation provisions may “chill” land use regulation due to increased risk of liability from adverse adjudication. Second, settlement and dispute resolution processes may limit public participation and abrogate decisions made in the public interest. However, there is little empirical or theoretical treatment of these concerns. To address this gap, we examine local land use decision-making in Florida in the more than two decades since the 1995 enactment of the Bert J. Harris, Jr. Private Property Rights Act [Act], one of the strongest state property rights laws in the United States. We use the Institutional Analysis and Development (IAD) framework to understand the system of rules and norms that operate within and between institutional actors in Florida's land use arena and animate decisions related to the Act. Drawing on key informant interviews and public documents, we show how contextual conditions and institutions mediate the impact of state private property laws. In Florida, institutions and transactions costs limit litigation under the Act—and consequently mitigate the Act's chilling effect—although unevenly depending on local context. The Act's compensation provision does little to reconfigure institutional arrangements and outcomes in property rights disputes; however, settlement and dispute resolution processes triggered by the Act effectively resolve local process and political challenges. Our findings suggest that dispute resolution is a more impactful and socially optimal approach to state property rights laws compared to compensation, and can enhance land use decision-making and outcomes for planning, the public interest, and landowners. © 2018 Elsevier Ltd</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048523859&amp;doi=10.1016%2fj.landusepol.2018.05.002&amp;partnerID=40&amp;md5=de03f09b1dd5f5f967aabde959d56b02</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier Ltd</dc:description>
        <bib:pages>603-612</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:02648377%20(ISSN)">
        <prism:volume>77</prism:volume>
        <dc:title>Land Use Policy</dc:title>
        <dc:identifier>DOI 10.1016/j.landusepol.2018.05.002</dc:identifier>
        <dcterms:alternative>Land Use Policy</dcterms:alternative>
        <dc:identifier>ISSN 02648377 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1502">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 7; Correspondence Address: E. Linkous; University of South Florida, School of Public Affairs, Tampa, 4202 East Fowler Avenue, SOC 107, 33620, United States; email: elinkous@usf.edu&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183596144&amp;doi=10.1016%2fj.eswa.2023.122666&amp;partnerID=40&amp;md5=69e705fa3ff7e537e478ae12ba9e96d6">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:09574174%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Islam</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Elmekki</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Elsebai</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bentahar</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Drawel</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rjoub</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pedrycz</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1504"/>
        <dcterms:isReferencedBy rdf:resource="#item_2219"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Natural language processing (NLP)</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Internet of things</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>Deep neural networks</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Long short-term memory</dc:subject>
        <dc:subject>Computer vision</dc:subject>
        <dc:subject>Computer vision (CV)</dc:subject>
        <dc:subject>Learning tasks</dc:subject>
        <dc:subject>Multi-modality</dc:subject>
        <dc:subject>Self-attention</dc:subject>
        <dc:subject>Speech processing</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:title>A comprehensive survey on applications of transformers for deep learning tasks</dc:title>
        <dcterms:abstract>Transformers are Deep Neural Networks (DNN) that utilize a self-attention mechanism to capture contextual relationships within sequential data. Unlike traditional neural networks and variants of Recurrent Neural Networks (RNNs), such as Long Short-Term Memory (LSTM), Transformer models excel at managing long dependencies among input sequence elements and facilitate parallel processing. Consequently, Transformer-based models have garnered significant attention from researchers in the field of artificial intelligence. This is due to their tremendous potential and impressive accomplishments, which extend beyond Natural Language Processing (NLP) tasks to encompass various domains, including Computer Vision (CV), audio and speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have been published, spotlighting the Transformer's contributions in specific fields, architectural disparities, or performance assessments, there remains a notable absence of a comprehensive survey paper that encompasses its major applications across diverse domains. Therefore, this paper addresses this gap by conducting an extensive survey of proposed Transformer models spanning from 2017 to 2022. Our survey encompasses the identification of the top five application domains for Transformer-based models, namely: NLP, CV, multi-modality, audio and speech processing, and signal processing. We analyze the influence of highly impactful Transformer-based models within these domains and subsequently categorize them according to their respective tasks, employing a novel taxonomy. Our primary objective is to illuminate the existing potential and future prospects of Transformers for researchers who are passionate about this area, thereby contributing to a more comprehensive understanding of this groundbreaking technology. © 2023 Elsevier Ltd</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183596144&amp;doi=10.1016%2fj.eswa.2023.122666&amp;partnerID=40&amp;md5=69e705fa3ff7e537e478ae12ba9e96d6</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier Ltd</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:09574174%20(ISSN)">
        <prism:volume>241</prism:volume>
        <dc:title>Expert Systems with Applications</dc:title>
        <dc:identifier>DOI 10.1016/j.eswa.2023.122666</dc:identifier>
        <dcterms:alternative>Expert Sys Appl</dcterms:alternative>
        <dc:identifier>ISSN 09574174 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1504">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 61; Correspondence Address: J. Bentahar; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, Canada; email: jamal.bentahar@concordia.ca; CODEN: ESAPE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2219">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185390721&amp;doi=10.1007%2fs10916-024-02045-3&amp;partnerID=40&amp;md5=73be66b342b8dd2c6648e3b7efefb9c9">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:01485598%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cascella</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Semeraro</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Montomoli</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bellini</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Piazza</foaf:surname>
                        <foaf:givenName>O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bignami</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1503"/>
        <dcterms:isReferencedBy rdf:resource="#item_2385"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>Artificial Intelligence</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>Generative AI</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>Humans</dc:subject>
        <dc:subject>Review</dc:subject>
        <dc:subject>documentation</dc:subject>
        <dc:subject>Chatbot</dc:subject>
        <dc:subject>health care personnel</dc:subject>
        <dc:subject>interpersonal communication</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>language</dc:subject>
        <dc:subject>Language</dc:subject>
        <dc:subject>Communication</dc:subject>
        <dc:subject>Documentation</dc:subject>
        <dc:subject>biomedicine</dc:subject>
        <dc:subject>Clinical Decision Support</dc:subject>
        <dc:subject>clinical decision support system</dc:subject>
        <dc:subject>educational status</dc:subject>
        <dc:subject>Educational Status</dc:subject>
        <dc:subject>Electric Power Supplies</dc:subject>
        <dc:subject>health care</dc:subject>
        <dc:subject>large language model</dc:subject>
        <dc:subject>Large Language Models</dc:subject>
        <dc:subject>medical literature</dc:subject>
        <dc:subject>power supply</dc:subject>
        <dc:title>The Breakthrough of Large Language Models Release for Medical Applications: 1-Year Timeline and Perspectives</dc:title>
        <dcterms:abstract>Within the domain of Natural Language Processing (NLP), Large Language Models (LLMs) represent sophisticated models engineered to comprehend, generate, and manipulate text resembling human language on an extensive scale. They are transformer-based deep learning architectures, obtained through the scaling of model size, pretraining of corpora, and computational resources. The potential healthcare applications of these models primarily involve chatbots and interaction systems for clinical documentation management, and medical literature summarization (Biomedical NLP). The challenge in this field lies in the research for applications in diagnostic and clinical decision support, as well as patient triage. Therefore, LLMs can be used for multiple tasks within patient care, research, and education. Throughout 2023, there has been an escalation in the release of LLMs, some of which are applicable in the healthcare domain. This remarkable output is largely the effect of the customization of pre-trained models for applications like chatbots, virtual assistants, or any system requiring human-like conversational engagement. As healthcare professionals, we recognize the imperative to stay at the forefront of knowledge. However, keeping abreast of the rapid evolution of this technology is practically unattainable, and, above all, understanding its potential applications and limitations remains a subject of ongoing debate. Consequently, this article aims to provide a succinct overview of the recently released LLMs, emphasizing their potential use in the field of medicine. Perspectives for a more extensive range of safe and effective applications are also discussed. The upcoming evolutionary leap involves the transition from an AI-powered model primarily designed for answering medical questions to a more versatile and practical tool for healthcare providers such as generalist biomedical AI systems for multimodal-based calibrated decision-making processes. On the other hand, the development of more accurate virtual clinical partners could enhance patient engagement, offering personalized support, and improving chronic disease management. © The Author(s) 2024.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185390721&amp;doi=10.1007%2fs10916-024-02045-3&amp;partnerID=40&amp;md5=73be66b342b8dd2c6648e3b7efefb9c9</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:01485598%20(ISSN)">
        <prism:volume>48</prism:volume>
        <dc:title>Journal of Medical Systems</dc:title>
        <dc:identifier>DOI 10.1007/s10916-024-02045-3</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>J. Med. Syst.</dcterms:alternative>
        <dc:identifier>ISSN 01485598 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1503">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 27; Correspondence Address: V. Bellini; Anesthesiology, Critical Care and Pain Medicine Division, Department of Medicine and Surgery, University of Parma, Parma, Viale Gramsci 14, 43126, Italy; email: valentina.bellini@unipr.it; CODEN: JMSYD&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2385">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187454083&amp;partnerID=40&amp;md5=c8d81687d4e80f7949f44e46271a2a0f">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:21476799%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Godi</foaf:surname>
                        <foaf:givenName>R.K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Basvant</foaf:surname>
                        <foaf:givenName>M.S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deepak</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Srivastava</foaf:surname>
                        <foaf:givenName>A.P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kumar</foaf:surname>
                        <foaf:givenName>T.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sankhyan</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shrivastava</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1506"/>
        <dcterms:isReferencedBy rdf:resource="#item_2374"/>
        <dc:subject>NLP</dc:subject>
        <dc:subject>Machine learning</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Big Data</dc:subject>
        <dc:subject>Sentiment analysis</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>FastText</dc:subject>
        <dc:subject>GloVe</dc:subject>
        <dc:subject>hybridclassifiers</dc:subject>
        <dc:subject>Omicron</dc:subject>
        <dc:subject>RoBERTa</dc:subject>
        <dc:subject>TextBlob</dc:subject>
        <dc:subject>TF-IDF</dc:subject>
        <dc:subject>Twitter analysis</dc:subject>
        <dc:subject>Word2Vec</dc:subject>
        <dc:title>Sentiment Analysis on Omicron Tweets Using Hybrid Classifiers with Multiple Feature ExtractionTechniques and Transformer Based Models</dc:title>
        <dcterms:abstract>Since the beginning of Covid-19, the world has been in a dilemma to cope up with its effects. With time the coronavirus has evolved into variants that caused a lot of destruction to human race. One such variant is “Omicron”. This variant made its presence in many countries throughout the world. The government is left in a straining situation to curb the spread of this variant and to stop the evolution of coronavirus. Though the strict precautions were exercised, the evolution was unstoppable. To understand the thoughts and feelings of the public, twitter can be considered as one of the best platforms for sentiment analysis. Analyzing the sentiments of people across the continents is horridly difficult but with the way technology has been making advancement in the world, analyzing has become a quiet easy job. In the existing studies on Covid-19, various word embedding techniques with machine learning and deeplearning classifiers has been used for the analysis. Language based models have proven to achieve higher accuracy forsentiment analysis. Amidst these hybrid classifiers, have performed tremendously good. In the proposed work, seven Machine Learning hybrid classifiers are compared with four single classifiers using TF-IDF and Word2Vec. A proposedDeep Learning hybrid classifier is compared with two single classifiers using GloVe and FastText. Furthermore, language models like BERT and RoBERTa are employed in an effort to boost validation outcomes upto 93.39% and 93.47%. © 2024, Auricle Global Society of Education and Research. All rights reserved.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187454083&amp;partnerID=40&amp;md5=c8d81687d4e80f7949f44e46271a2a0f</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Auricle Global Society of Education and Research</dc:description>
        <bib:pages>257-275</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:21476799%20(ISSN)">
        <prism:volume>12</prism:volume>
        <dc:title>International Journal of Intelligent Systems and Applications in Engineering</dc:title>
        <prism:number>15s</prism:number>
        <dcterms:alternative>Internat. J. Intel. Syst. Appl. Eng.</dcterms:alternative>
        <dc:identifier>ISSN 21476799 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1506">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 25&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2374">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153368140&amp;doi=10.1109%2fTNNLS.2022.3227717&amp;partnerID=40&amp;md5=324ae9b15179fd9baa63c3cc4a1146da">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2162237X%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hou</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yuan</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shi</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fan</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1507"/>
        <dcterms:isReferencedBy rdf:resource="#item_2220"/>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>Computational modelling</dc:subject>
        <dc:subject>Decoding</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>investment</dc:subject>
        <dc:subject>article</dc:subject>
        <dc:subject>Deep neural networks</dc:subject>
        <dc:subject>Benchmarking</dc:subject>
        <dc:subject>computer vision</dc:subject>
        <dc:subject>Network architecture</dc:subject>
        <dc:subject>human experiment</dc:subject>
        <dc:subject>Pre-training</dc:subject>
        <dc:subject>taxonomy</dc:subject>
        <dc:subject>Computer vision</dc:subject>
        <dc:subject>Classification</dc:subject>
        <dc:subject>computer vision (CV)</dc:subject>
        <dc:subject>Convolution</dc:subject>
        <dc:subject>detection</dc:subject>
        <dc:subject>Detection</dc:subject>
        <dc:subject>embedding</dc:subject>
        <dc:subject>Image segmentation</dc:subject>
        <dc:subject>motivation</dc:subject>
        <dc:subject>point clouds</dc:subject>
        <dc:subject>Point-clouds</dc:subject>
        <dc:subject>segmentation</dc:subject>
        <dc:subject>Segmentation</dc:subject>
        <dc:subject>self-supervision</dc:subject>
        <dc:subject>Self-supervision</dc:subject>
        <dc:subject>Taxonomies</dc:subject>
        <dc:subject>vision</dc:subject>
        <dc:subject>Visual languages</dc:subject>
        <dc:subject>visual Transformer</dc:subject>
        <dc:subject>Visual transformer</dc:subject>
        <dc:subject>visual-linguistic pretraining</dc:subject>
        <dc:subject>Visual-linguistic pretraining</dc:subject>
        <dc:title>A Survey of Visual Transformers</dc:title>
        <dcterms:abstract>Transformer, an attention-based encoder-decoder model, has already revolutionized the field of natural language processing (NLP). Inspired by such significant achievements, some pioneering works have recently been done on employing Transformer-liked architectures in the computer vision (CV) field, which have demonstrated their effectiveness on three fundamental CV tasks (classification, detection, and segmentation) as well as multiple sensory data stream (images, point clouds, and vision-language data). Because of their competitive modeling capabilities, the visual Transformers have achieved impressive performance improvements over multiple benchmarks as compared with modern convolution neural networks (CNNs). In this survey, we have reviewed over 100 of different visual Transformers comprehensively according to three fundamental CV tasks and different data stream types, where taxonomy is proposed to organize the representative methods according to their motivations, structures, and application scenarios. Because of their differences on training settings and dedicated vision tasks, we have also evaluated and compared all these existing visual Transformers under different configurations. Furthermore, we have revealed a series of essential but unexploited aspects that may empower such visual Transformers to stand out from numerous architectures, e.g., slack high-level semantic embeddings to bridge the gap between the visual Transformers and the sequential ones. Finally, two promising research directions are suggested for future investment. We will continue to update the latest articles and their released source codes at https://github.com/liuyang-ict/awesome-visual-transformers.  © 2012 IEEE.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153368140&amp;doi=10.1109%2fTNNLS.2022.3227717&amp;partnerID=40&amp;md5=324ae9b15179fd9baa63c3cc4a1146da</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Institute of Electrical and Electronics Engineers Inc.</dc:description>
        <bib:pages>7478-7498</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2162237X%20(ISSN)">
        <prism:volume>35</prism:volume>
        <dc:title>IEEE Transactions on Neural Networks and Learning Systems</dc:title>
        <dc:identifier>DOI 10.1109/TNNLS.2022.3227717</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>IEEE Trans. Neural Networks Learn. Sys.</dcterms:alternative>
        <dc:identifier>ISSN 2162237X (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1507">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 140; Correspondence Address: Y. Zhang; Lenovo Research, AI Lab, Beijing, 100000, China; email: zhangyang20@lenovo.com; Z. Shi; Lenovo Research, AI Lab, Beijing, 100000, China; email: shizc2@lenovo.com&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2220">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178660816&amp;doi=10.1016%2fj.neucom.2023.127063&amp;partnerID=40&amp;md5=ced44c7c3eb82b767e856bdac9110241">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:09252312%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Su</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ahmed</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pan</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bo</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1505"/>
        <dcterms:isReferencedBy rdf:resource="#item_2371"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>female</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>male</dc:subject>
        <dc:subject>article</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Pre-trained language model</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Embeddings</dc:subject>
        <dc:subject>Pre-training</dc:subject>
        <dc:subject>benchmarking</dc:subject>
        <dc:subject>Encoding (symbols)</dc:subject>
        <dc:subject>Information encoding</dc:subject>
        <dc:subject>language model</dc:subject>
        <dc:subject>Position information</dc:subject>
        <dc:subject>Position information encoding</dc:subject>
        <dc:subject>Pre-trained language models</dc:subject>
        <dc:subject>Rope</dc:subject>
        <dc:subject>rotation</dc:subject>
        <dc:subject>Signal encoding</dc:subject>
        <dc:subject>training</dc:subject>
        <dc:title>RoFormer: Enhanced transformer with Rotary Position Embedding</dc:title>
        <dcterms:abstract>Position encoding has recently been shown to be effective in transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding (RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in the self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: https://huggingface.co/docs/transformers/model_doc/roformer. © 2023 Elsevier B.V.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178660816&amp;doi=10.1016%2fj.neucom.2023.127063&amp;partnerID=40&amp;md5=ced44c7c3eb82b767e856bdac9110241</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier B.V.</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:09252312%20(ISSN)">
        <prism:volume>568</prism:volume>
        <dc:title>Neurocomputing</dc:title>
        <dc:identifier>DOI 10.1016/j.neucom.2023.127063</dc:identifier>
        <dcterms:alternative>Neurocomputing</dcterms:alternative>
        <dc:identifier>ISSN 09252312 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1505">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 203; Correspondence Address: M. Ahmed; Zhuiyi Technology Co., Ltd. Shenzhen, China; email: murtadha.alrahbi@gmail.com; CODEN: NRCGE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2371">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183173863&amp;doi=10.1016%2fj.cmpb.2024.108013&amp;partnerID=40&amp;md5=1e0ec823297dd19c5125c234f2ec16a4">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:01692607%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dada</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Puladi</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kleesiek</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Egger</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1509"/>
        <dcterms:isReferencedBy rdf:resource="#item_2276"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>NLP</dc:subject>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>Artificial Intelligence</dc:subject>
        <dc:subject>classification</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>Healthcare</dc:subject>
        <dc:subject>medicine</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>Humans</dc:subject>
        <dc:subject>Review</dc:subject>
        <dc:subject>workflow</dc:subject>
        <dc:subject>medical education</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>physician</dc:subject>
        <dc:subject>systematic review</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>long short term memory network</dc:subject>
        <dc:subject>medical research</dc:subject>
        <dc:subject>health care personnel</dc:subject>
        <dc:subject>publication</dc:subject>
        <dc:subject>Medical applications</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Binary alloys</dc:subject>
        <dc:subject>Potassium alloys</dc:subject>
        <dc:subject>Uranium alloys</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>taxonomy</dc:subject>
        <dc:subject>large language model</dc:subject>
        <dc:subject>Taxonomies</dc:subject>
        <dc:subject>Bard</dc:subject>
        <dc:subject>clinical decision making</dc:subject>
        <dc:subject>consultation</dc:subject>
        <dc:subject>Databases, Factual</dc:subject>
        <dc:subject>factual database</dc:subject>
        <dc:subject>Health care</dc:subject>
        <dc:subject>LLaMA</dc:subject>
        <dc:subject>LLM</dc:subject>
        <dc:subject>Medline</dc:subject>
        <dc:subject>Openai</dc:subject>
        <dc:subject>OpenAI</dc:subject>
        <dc:subject>Physicians</dc:subject>
        <dc:subject>PubMed</dc:subject>
        <dc:subject>recurrent neural network</dc:subject>
        <dc:subject>Taxonomy</dc:subject>
        <dc:title>ChatGPT in healthcare: A taxonomy and systematic review</dc:title>
        <dcterms:abstract>The recent release of ChatGPT, a chat bot research project/product of natural language processing (NLP) by OpenAI, stirs up a sensation among both the general public and medical professionals, amassing a phenomenally large user base in a short time. This is a typical example of the ‘productization’ of cutting-edge technologies, which allows the general public without a technical background to gain firsthand experience in artificial intelligence (AI), similar to the AI hype created by AlphaGo (DeepMind Technologies, UK) and self-driving cars (Google, Tesla, etc.). However, it is crucial, especially for healthcare researchers, to remain prudent amidst the hype. This work provides a systematic review of existing publications on the use of ChatGPT in healthcare, elucidating the ‘status quo’ of ChatGPT in medical applications, for general readers, healthcare professionals as well as NLP scientists. The large biomedical literature database PubMed is used to retrieve published works on this topic using the keyword ‘ChatGPT’. An inclusion criterion and a taxonomy are further proposed to filter the search results and categorize the selected publications, respectively. It is found through the review that the current release of ChatGPT has achieved only moderate or ‘passing’ performance in a variety of tests, and is unreliable for actual clinical deployment, since it is not intended for clinical applications by design. We conclude that specialized NLP models trained on (bio)medical datasets still represent the right direction to pursue for critical clinical applications. © 2024 The Author(s)</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183173863&amp;doi=10.1016%2fj.cmpb.2024.108013&amp;partnerID=40&amp;md5=1e0ec823297dd19c5125c234f2ec16a4</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier Ireland Ltd</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:01692607%20(ISSN)">
        <prism:volume>245</prism:volume>
        <dc:title>Computer Methods and Programs in Biomedicine</dc:title>
        <dc:identifier>DOI 10.1016/j.cmpb.2024.108013</dc:identifier>
        <dcterms:alternative>Comput. Methods Programs Biomed.</dcterms:alternative>
        <dc:identifier>ISSN 01692607 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1509">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 77; Correspondence Address: J. Egger; Institute for Artificial Intelligence in Medicine, University Hospital Essen (AöR), Essen, Girardetstraße, 45131, Germany; email: jan.egger@uk-essen.de; CODEN: CMPBE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2276">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184808429&amp;doi=10.1109%2fACCESS.2024.3363469&amp;partnerID=40&amp;md5=2e93755707d1b8ec5af756a0d22ad2b7">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:21693536%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ferrag</foaf:surname>
                        <foaf:givenName>M.A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ndhlovu</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tihanyi</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cordeiro</foaf:surname>
                        <foaf:givenName>L.C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Debbah</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lestable</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Thandi</foaf:surname>
                        <foaf:givenName>N.S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1510"/>
        <dcterms:isReferencedBy rdf:resource="#item_2370"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Generative AI</dc:subject>
        <dc:subject>large language models</dc:subject>
        <dc:subject>generative AI</dc:subject>
        <dc:subject>Data privacy</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Internet of things</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Large language model</dc:subject>
        <dc:subject>Network architecture</dc:subject>
        <dc:subject>Long short-term memory</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>Encoding (symbols)</dc:subject>
        <dc:subject>Bidirectional encoder representation from transformer</dc:subject>
        <dc:subject>Computer architecture</dc:subject>
        <dc:subject>Cybe threat detection</dc:subject>
        <dc:subject>Cyber threat detection</dc:subject>
        <dc:subject>Cyber threats</dc:subject>
        <dc:subject>Cybersecurity</dc:subject>
        <dc:subject>Encodings</dc:subject>
        <dc:subject>IoT network</dc:subject>
        <dc:subject>IoT networks</dc:subject>
        <dc:subject>Network coding</dc:subject>
        <dc:subject>Network security</dc:subject>
        <dc:subject>Threat assessment</dc:subject>
        <dc:subject>Threat detection</dc:subject>
        <dc:title>Revolutionizing Cyber Threat Detection with Large Language Models: A Privacy-Preserving BERT-Based Lightweight Model for IoT/IIoT Devices</dc:title>
        <dcterms:abstract>The field of Natural Language Processing (NLP) is currently undergoing a revolutionary transformation driven by the power of pre-trained Large Language Models (LLMs) based on groundbreaking Transformer architectures. As the frequency and diversity of cybersecurity attacks continue to rise, the importance of incident detection has significantly increased. IoT devices are expanding rapidly, resulting in a growing need for efficient techniques to autonomously identify network-based attacks in IoT networks with both high precision and minimal computational requirements. This paper presents SecurityBERT, a novel architecture that leverages the Bidirectional Encoder Representations from Transformers (BERT) model for cyber threat detection in IoT networks. During the training of SecurityBERT, we incorporated a novel privacy-preserving encoding technique called Privacy-Preserving Fixed-Length Encoding (PPFLE). We effectively represented network traffic data in a structured format by combining PPFLE with the Byte-level Byte-Pair Encoder (BBPE) Tokenizer. Our research demonstrates that SecurityBERT outperforms traditional Machine Learning (ML) and Deep Learning (DL) methods, such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), in cyber threat detection. Employing the Edge-IIoTset cybersecurity dataset, our experimental analysis shows that SecurityBERT achieved an impressive 98.2% overall accuracy in identifying fourteen distinct attack types, surpassing previous records set by hybrid solutions such as GAN-Transformer-based architectures and CNN-LSTM models. With an inference time of less than 0.15 seconds on an average CPU and a compact model size of just 16.7MB, SecurityBERT is ideally suited for real-life traffic analysis and a suitable choice for deployment on resource-constrained IoT devices.  © 2013 IEEE.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184808429&amp;doi=10.1109%2fACCESS.2024.3363469&amp;partnerID=40&amp;md5=2e93755707d1b8ec5af756a0d22ad2b7</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Institute of Electrical and Electronics Engineers Inc.</dc:description>
        <bib:pages>23733-23750</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:21693536%20(ISSN)">
        <prism:volume>12</prism:volume>
        <dc:title>IEEE Access</dc:title>
        <dc:identifier>DOI 10.1109/ACCESS.2024.3363469</dc:identifier>
        <dcterms:alternative>IEEE Access</dcterms:alternative>
        <dc:identifier>ISSN 21693536 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1510">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 22; Correspondence Address: M.A. Ferrag; Technology Innovation Institute, Abu Dhabi, United Arab Emirates; email: Mohamed.Ferrag@tii.ae&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2370">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189075847&amp;doi=10.1016%2fj.jjimei.2024.100232&amp;partnerID=40&amp;md5=09bc7bfe6af34a1a69f57acaef237b53">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:26670968%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gupta</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nair</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mishra</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ibrahim</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bhardwaj</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1511"/>
        <dcterms:isReferencedBy rdf:resource="#item_2261"/>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>Generative AI</dc:subject>
        <dc:subject>Chatbots</dc:subject>
        <dc:subject>Adoption</dc:subject>
        <dc:title>Adoption and impacts of generative artificial intelligence: Theoretical underpinnings and research agenda</dc:title>
        <dcterms:abstract>Large language models (LLMs) have received considerable interest in the field of natural language processing (NLP) owing to their remarkable ability to generate clear, consistent, and contextually relevant materials. Among the numerous LLMs, ChatGPT (Generative Pre-trained Transformer for Chatbots) is emerging as a prominent prospective tool for developing conversational agents such as chatbots. However, there is a need for a clear conceptual understanding of ChatGPT's potential implications for the industry and its role in marketing. This study explores the adoption of ChatGPT in marketing and examines theories that may influence its adoption by marketers and consumers, as well as its implications for marketers. This study discusses how ChatGPT may allow for more personalized and engaging content, better customer experience, and improved ROI. However, adoption also brings challenges, including ethical considerations and the need for new skill development. This study also discusses future research opportunities for the adoption of ChatGPT and other generative artificial intelligence technologies in marketing. The goal is to provide insights for organizations that consider implementing these technologies, and to contribute to the literature on the adoption of Artificial Intelligence (AI) and the use of Generative AI in marketing. © 2024</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189075847&amp;doi=10.1016%2fj.jjimei.2024.100232&amp;partnerID=40&amp;md5=09bc7bfe6af34a1a69f57acaef237b53</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier B.V.</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:26670968%20(ISSN)">
        <prism:volume>4</prism:volume>
        <dc:title>International Journal of Information Management Data Insights</dc:title>
        <dc:identifier>DOI 10.1016/j.jjimei.2024.100232</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Int. J. Inf. Manag. Data Insights</dcterms:alternative>
        <dc:identifier>ISSN 26670968 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1511">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 27; Correspondence Address: K. Nair; College of Business, Abu Dhabi University, Abu Dhabi, United Arab Emirates; email: kiran.nair@adu.ac.ae&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2261">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182288843&amp;doi=10.2196%2f48996&amp;partnerID=40&amp;md5=bcefbc7b4792530573f06a8868a90229">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:14388871%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gupta</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deng</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Park</foaf:surname>
                        <foaf:givenName>Y.-J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Paget</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Naugler</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1508"/>
        <dcterms:isReferencedBy rdf:resource="#item_2270"/>
        <dc:subject>NLP</dc:subject>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>GPT-4</dc:subject>
        <dc:subject>classification</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>GPT</dc:subject>
        <dc:subject>large language models</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>Humans</dc:subject>
        <dc:subject>workflow</dc:subject>
        <dc:subject>Article</dc:subject>
        <dc:subject>systematic review</dc:subject>
        <dc:subject>medical research</dc:subject>
        <dc:subject>language</dc:subject>
        <dc:subject>Language</dc:subject>
        <dc:subject>large language model</dc:subject>
        <dc:subject>language model</dc:subject>
        <dc:subject>LLM</dc:subject>
        <dc:subject>abstract screening</dc:subject>
        <dc:subject>Biomedical Research</dc:subject>
        <dc:subject>Chat GPT</dc:subject>
        <dc:subject>consensus</dc:subject>
        <dc:subject>Consensus</dc:subject>
        <dc:subject>data analysis</dc:subject>
        <dc:subject>Data Analysis</dc:subject>
        <dc:subject>extract</dc:subject>
        <dc:subject>extraction</dc:subject>
        <dc:subject>free text</dc:subject>
        <dc:subject>nonopiod analgesia</dc:subject>
        <dc:subject>prevalence</dc:subject>
        <dc:subject>problem solving</dc:subject>
        <dc:subject>Problem Solving</dc:subject>
        <dc:subject>review methodology</dc:subject>
        <dc:subject>review methods</dc:subject>
        <dc:subject>screening</dc:subject>
        <dc:subject>sensitivity analysis</dc:subject>
        <dc:subject>systematic</dc:subject>
        <dc:subject>unstructured data</dc:subject>
        <dc:title>Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study</dc:title>
        <dcterms:abstract>Background: The systematic review of clinical research papers is a labor-intensive and time-consuming process that often involves the screening of thousands of titles and abstracts. The accuracy and efficiency of this process are critical for the quality of the review and subsequent health care decisions. Traditional methods rely heavily on human reviewers, often requiring a significant investment of time and resources. Objective: This study aims to assess the performance of the OpenAI generative pretrained transformer (GPT) and GPT-4 application programming interfaces (APIs) in accurately and efficiently identifying relevant titles and abstracts from real-world clinical review data sets and comparing their performance against ground truth labeling by 2 independent human reviewers. Methods: We introduce a novel workflow using the Chat GPT and GPT-4 APIs for screening titles and abstracts in clinical reviews. A Python script was created to make calls to the API with the screening criteria in natural language and a corpus of title and abstract data sets filtered by a minimum of 2 human reviewers. We compared the performance of our model against human-reviewed papers across 6 review papers, screening over 24,000 titles and abstracts. Results: Our results show an accuracy of 0.91, a macro F1-score of 0.60, a sensitivity of excluded papers of 0.91, and a sensitivity of included papers of 0.76. The interrater variability between 2 independent human screeners was κ=0.46, and the prevalence and bias-adjusted κ between our proposed methods and the consensus-based human decisions was κ=0.96. On a randomly selected subset of papers, the GPT models demonstrated the ability to provide reasoning for their decisions and corrected their initial decisions upon being asked to explain their reasoning for incorrect classifications. Conclusions: Large language models have the potential to streamline the clinical review process, save valuable time and effort for researchers, and contribute to the overall quality of clinical reviews. By prioritizing the workflow and acting as an aid rather than a replacement for researchers and reviewers, models such as GPT-4 can enhance efficiency and lead to more accurate and reliable conclusions in medical research. © 2024 Journal of Medical Internet Research. All rights reserved.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182288843&amp;doi=10.2196%2f48996&amp;partnerID=40&amp;md5=bcefbc7b4792530573f06a8868a90229</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: JMIR Publications Inc.</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:14388871%20(ISSN)">
        <prism:volume>26</prism:volume>
        <dc:title>Journal of Medical Internet Research</dc:title>
        <dc:identifier>DOI 10.2196/48996</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>J. Med. Internet Res.</dcterms:alternative>
        <dc:identifier>ISSN 14388871 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1508">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 21; Correspondence Address: E. Guo; Cumming School of Medicine, University of Calgary, Calgary, 3330 University Dr NW, T2N 1N4, Canada; email: eddie.guo@ucalgary.ca&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2270">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185128981&amp;doi=10.1109%2fACCESS.2024.3365742&amp;partnerID=40&amp;md5=2b8272d893e7d9bb9ee3ee8fc17356ed">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>12</prism:volume>
                <dc:title>IEEE Access</dc:title>
                <dc:identifier>DOI 10.1109/ACCESS.2024.3365742</dc:identifier>
                <dcterms:alternative>IEEE Access</dcterms:alternative>
                <dc:identifier>ISSN 21693536 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Raiaan</foaf:surname>
                        <foaf:givenName>M.A.K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mukta</foaf:surname>
                        <foaf:givenName>M.S.H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fatema</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fahad</foaf:surname>
                        <foaf:givenName>N.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sakib</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mim</foaf:surname>
                        <foaf:givenName>M.M.J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ahmad</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ali</foaf:surname>
                        <foaf:givenName>M.E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Azam</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1512"/>
        <link:link rdf:resource="#item_2598"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>Evolution</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Large language model</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>natural language processing (NLP)</dc:subject>
        <dc:subject>taxonomy</dc:subject>
        <dc:subject>Taxonomies</dc:subject>
        <dc:subject>application</dc:subject>
        <dc:subject>Cognition</dc:subject>
        <dc:subject>Job analysis</dc:subject>
        <dc:subject>Large language models (LLM)</dc:subject>
        <dc:subject>pre-trained models</dc:subject>
        <dc:subject>Pretrained model</dc:subject>
        <dc:subject>Question answering (information retrieval)</dc:subject>
        <dc:subject>transformer</dc:subject>
        <dc:title>A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges</dc:title>
        <dcterms:abstract>Large Language Models (LLMs) recently demonstrated extraordinary capability in various natural language processing (NLP) tasks including language translation, text generation, question answering, etc. Moreover, LLMs are new and essential part of computerized language processing, having the ability to understand complex verbal patterns and generate coherent and appropriate replies in a given context. Though this success of LLMs has prompted a substantial increase in research contributions, rapid growth has made it difficult to understand the overall impact of these improvements. Since a plethora of research on LLMs have been appeared within a short time, it is quite impossible to track all of these and get an overview of the current state of research in this area. Consequently, the research community would benefit from a short but thorough review of the recent changes in this area. This article thoroughly overviews LLMs, including their history, architectures, transformers, resources, training methods, applications, impacts, challenges, etc. This paper begins by discussing the fundamental concepts of LLMs with its traditional pipeline of the LLMs training phase. Then the paper provides an overview of the existing works, the history of LLMs, their evolution over time, the architecture of transformers in LLMs, the different resources of LLMs, and the different training methods that have been used to train them. The paper also demonstrates the datasets utilized in the studies. After that, the paper discusses the wide range of applications of LLMs, including biomedical and healthcare, education, social, business, and agriculture. The study also illustrates how LLMs create an impact on society and shape the future of AI and how they can be used to solve real-world problems. Finally, the paper also explores open issues and challenges to deploy LLMs in real-world scenario. Our review paper aims to help practitioners, researchers, and experts thoroughly understand the evolution of LLMs, pre-trained architectures, applications, challenges, and future goals.  © 2013 IEEE.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185128981&amp;doi=10.1109%2fACCESS.2024.3365742&amp;partnerID=40&amp;md5=2b8272d893e7d9bb9ee3ee8fc17356ed</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Institute of Electrical and Electronics Engineers Inc.</dc:description>
        <bib:pages>26839-26874</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1512">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 67; Correspondence Address: M.S.H. Mukta; Lappeenranta-Lahti University of Technology, LUT School of Engineering Sciences, Lappeenranta, 53850, Finland; email: Saddam.Mukta@lut.fi&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2598">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2598/Raiaan et al. - 2024 - A Review on Large Language Models Architectures, Applications, Taxonomies, Open Issues and Challeng.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=10433480&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:01:33</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153204653&amp;doi=10.5114%2fBIOLSPORT.2023.125623&amp;partnerID=40&amp;md5=72f951f36288364372691a144dec3114">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0860021X%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dergaa</foaf:surname>
                        <foaf:givenName>I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chamari</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zmijewski</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saad</foaf:surname>
                        <foaf:givenName>H.B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1513"/>
        <dc:subject>NLP</dc:subject>
        <dc:subject>Artificial Intelligence</dc:subject>
        <dc:subject>Machine Learning</dc:subject>
        <dc:subject>Chatbot</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Deep Learning</dc:subject>
        <dc:subject>LLaMA</dc:subject>
        <dc:subject>LLM</dc:subject>
        <dc:subject>Google Bard</dc:subject>
        <dc:subject>Higher Education</dc:subject>
        <dc:subject>NLM</dc:subject>
        <dc:subject>Paperpal</dc:subject>
        <dc:subject>Peer Review</dc:subject>
        <dc:subject>QuillBot</dc:subject>
        <dc:subject>Rayyan</dc:subject>
        <dc:subject>Research</dc:subject>
        <dc:subject>Sports Medicine</dc:subject>
        <dc:title>From human writing to artificial intelligence generated text: examining the prospects and potential threats of ChatGPT in academic writing</dc:title>
        <dcterms:abstract>Natural language processing (NLP) has been studied in computing for decades. Recent technological advancements have led to the development of sophisticated artificial intelligence (AI) models, such as Chat Generative Pre-trained Transformer (ChatGPT). These models can perform a range of language tasks and generate human-like responses, which offers exciting prospects for academic efficiency. This manuscript aims at (i) exploring the potential benefits and threats of ChatGPT and other NLP technologies in academic writing and research publications; (ii) highlights the ethical considerations involved in using these tools, and (iii) consider the impact they may have on the authenticity and credibility of academic work. This study involved a literature review of relevant scholarly articles published in peer-reviewed journals indexed in Scopus as quartile 1. The search used keywords such as “ChatGPT,” “AI-generated text,” “academic writing,” and “natural language processing.” The analysis was carried out using a quasi-qualitative approach, which involved reading and critically evaluating the sources and identifying relevant data to support the research questions. The study found that ChatGPT and other NLP technologies have the potential to enhance academic writing and research efficiency. However, their use also raises concerns about the impact on the authenticity and credibility of academic work. The study highlights the need for comprehensive discussions on the potential use, threats, and limitations of these tools, emphasizing the importance of ethical and academic principles, with human intelligence and critical thinking at the forefront of the research process. This study highlights the need for comprehensive debates and ethical considerations involved in their use. The study also recommends that academics exercise caution when using these tools and ensure transparency in their use, emphasizing the importance of human intelligence and critical thinking in academic work. © 2023 Institute of Sport. All rights reserved.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153204653&amp;doi=10.5114%2fBIOLSPORT.2023.125623&amp;partnerID=40&amp;md5=72f951f36288364372691a144dec3114</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Institute of Sport</dc:description>
        <bib:pages>615-622</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0860021X%20(ISSN)">
        <prism:volume>40</prism:volume>
        <dc:title>Biology of Sport</dc:title>
        <dc:identifier>DOI 10.5114/BIOLSPORT.2023.125623</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>Biol. Sport</dcterms:alternative>
        <dc:identifier>ISSN 0860021X (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1513">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 217; Correspondence Address: H.B. Saad; University of Sousse, Farhat HACHED Hospital, Research Laboratory LR12SP09 «Heart Failure», Sousse, Tunisia; email: helmi.bensaad@rns.tn&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162121706&amp;doi=10.1016%2fj.inffus.2023.101861&amp;partnerID=40&amp;md5=50aafcd73742a897267f9e0ea3796df5">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:15662535%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kocoń</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cichecki</foaf:surname>
                        <foaf:givenName>I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kaszyca</foaf:surname>
                        <foaf:givenName>O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kochanek</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Szydło</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Baran</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bielaniewicz</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gruza</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Janz</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kanclerz</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kocoń</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koptyra</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mieleszczenko-Kowszewicz</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Miłkowski</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oleksy</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Piasecki</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Radliński</foaf:surname>
                        <foaf:givenName>Ł.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wojtasik</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Woźniak</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kazienko</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1514"/>
        <dcterms:isReferencedBy rdf:resource="#item_2277"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Natural language processing (NLP)</dc:subject>
        <dc:subject>GPT-4</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>State of the art</dc:subject>
        <dc:subject>Quality control</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Large language model</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Zero-shot learning</dc:subject>
        <dc:subject>Speech recognition</dc:subject>
        <dc:subject>Emotion recognition</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Text classification</dc:subject>
        <dc:subject>Sentiment analysis</dc:subject>
        <dc:subject>Art analysis</dc:subject>
        <dc:subject>Chat generative pre-trained transformer</dc:subject>
        <dc:subject>Emotion Recognition</dc:subject>
        <dc:subject>Humor detection</dc:subject>
        <dc:subject>Language inference</dc:subject>
        <dc:subject>Model personalization</dc:subject>
        <dc:subject>Natural language inference</dc:subject>
        <dc:subject>Natural language inference (NLI)</dc:subject>
        <dc:subject>Offensive content</dc:subject>
        <dc:subject>Personalizations</dc:subject>
        <dc:subject>Petroleum reservoir evaluation</dc:subject>
        <dc:subject>Pragmatic natural language processing task</dc:subject>
        <dc:subject>Pragmatic NLP tasks</dc:subject>
        <dc:subject>Prompting</dc:subject>
        <dc:subject>Question answering (QA)</dc:subject>
        <dc:subject>Semantic natural language processing task</dc:subject>
        <dc:subject>Semantic NLP tasks</dc:subject>
        <dc:subject>SOTA analysis</dc:subject>
        <dc:subject>Stance detection</dc:subject>
        <dc:subject>State-of-the-art analyse</dc:subject>
        <dc:subject>Subjective natural language processing task</dc:subject>
        <dc:subject>Subjective NLP tasks</dc:subject>
        <dc:subject>Word Sense Disambiguation</dc:subject>
        <dc:subject>Word sense disambiguation (WSD)</dc:subject>
        <dc:title>ChatGPT: Jack of all trades, master of none</dc:title>
        <dcterms:abstract>OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. The first contact with the chatbot reveals its ability to provide detailed and precise answers in various areas. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quality of the ChatGPT model was about 25% for zero-shot and few-shot evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed that the more difficult the task (lower SOTA performance), the higher the ChatGPT loss. It especially refers to pragmatic NLP problems like emotion recognition. We also tested the ability to personalize ChatGPT responses for selected subjective tasks via Random Contextual Few-Shot Personalization, and we obtained significantly better user-based predictions. Additional qualitative analysis revealed a ChatGPT bias, most likely due to the rules imposed on human trainers by OpenAI. Our results provide the basis for a fundamental discussion of whether the high quality of recent predictive NLP models can indicate a tool's usefulness to society and how the learning and validation procedures for such systems should be established. © 2023 The Author(s)</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162121706&amp;doi=10.1016%2fj.inffus.2023.101861&amp;partnerID=40&amp;md5=50aafcd73742a897267f9e0ea3796df5</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier B.V.</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:15662535%20(ISSN)">
        <prism:volume>99</prism:volume>
        <dc:title>Information Fusion</dc:title>
        <dc:identifier>DOI 10.1016/j.inffus.2023.101861</dc:identifier>
        <dcterms:alternative>Inf. Fusion</dcterms:alternative>
        <dc:identifier>ISSN 15662535 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1514">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 260; Correspondence Address: J. Kocoń; Department of Artificial Intelligence, Wrocław University of Science and Technology, Wrocław, Wyb. Wyspiańskiego 27, 50-370, Poland; email: jan.kocon@pwr.edu.pl&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2277">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148608599&amp;doi=10.37074%2fjalt.2023.6.1.9&amp;partnerID=40&amp;md5=5aed8395fdc3cde275b039226634df1f">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2591801X%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rudolph</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tan</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tan</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1515"/>
        <dc:subject>Artificial Intelligence (AI)</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>higher education</dc:subject>
        <dc:subject>natural language processing (NLP)</dc:subject>
        <dc:subject>Artificial Intelligence in Education (AIEd)</dc:subject>
        <dc:subject>assessment</dc:subject>
        <dc:subject>Generative Pre-trained Transformer 3 (GPT-3)</dc:subject>
        <dc:subject>learning &amp; teaching</dc:subject>
        <dc:title>ChatGPT: Bullshit spewer or the end of traditional assessments in higher education?</dc:title>
        <dcterms:abstract>ChatGPT is the world’s most advanced chatbot thus far. Unlike other chatbots, it can create impressive prose within seconds, and it has created much hype and doomsday predictions when it comes to student assessment in higher education and a host of other matters. ChatGPT is a state-of-the-art language model (a variant of OpenAI’s Generative Pretrained Transformer (GPT) language model) designed to generate text that can be indistinguishable from text written by humans. It can engage in conversation with users in a seemingly natural and intuitive way. In this article, we briefly tell the story of OpenAI, the organisation behind ChatGPT. We highlight the fundamental change from a not-for-profit organisation to a commercial business model. In terms of our methods, we conducted an extensive literature review and experimented with this artificial intelligence (AI) software. Our literature review shows our review to be amongst the first peer-reviewed academic journal articles to explore ChatGPT and its relevance for higher education (especially assessment, learning and teaching). After a description of ChatGPT’s functionality and a summary of its strengths and limitations, we focus on the technology’s implications for higher education and discuss what is the future of learning, teaching and assessment in higher education in the context of AI chatbots such as ChatGPT. We position ChatGPT in the context of current Artificial Intelligence in Education (AIEd) research, discuss student-facing, teacher-facing and system-facing applications, and analyse opportunities and threats. We conclude the article with recommendations for students, teachers and higher education institutions. Many of them focus on assessment. © 2023. Jürgen Rudolph, Samson Tan and Shannon Tan.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148608599&amp;doi=10.37074%2fjalt.2023.6.1.9&amp;partnerID=40&amp;md5=5aed8395fdc3cde275b039226634df1f</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Kaplan Singapore</dc:description>
        <bib:pages>342-363</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2591801X%20(ISSN)">
        <prism:volume>6</prism:volume>
        <dc:title>Journal of Applied Learning and Teaching</dc:title>
        <dc:identifier>DOI 10.37074/jalt.2023.6.1.9</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>J. Appl. Learn. Teach.</dcterms:alternative>
        <dc:identifier>ISSN 2591801X (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1515">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 646&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148992575&amp;doi=10.2196%2f45312&amp;partnerID=40&amp;md5=a70a9b11b70f9182a6b1a9ed24dc5122">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:23693762%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gilson</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Safranek</foaf:surname>
                        <foaf:givenName>C.W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Socrates</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chi</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Taylor</foaf:surname>
                        <foaf:givenName>R.A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chartash</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1518"/>
        <dcterms:isReferencedBy rdf:resource="#item_2297"/>
        <dc:subject>NLP</dc:subject>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>generative pre-trained transformer</dc:subject>
        <dc:subject>GPT</dc:subject>
        <dc:subject>chatbot</dc:subject>
        <dc:subject>medical education</dc:subject>
        <dc:subject>conversational agent</dc:subject>
        <dc:subject>education technology</dc:subject>
        <dc:subject>MedQA</dc:subject>
        <dc:title>How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment</dc:title>
        <dcterms:abstract>Background: Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input. Objective: This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination Step 1 and Step 2 exams, as well as to analyze responses for user interpretability. Methods: We used 2 sets of multiple-choice questions to evaluate ChatGPT’s performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT’s performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results: Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT’s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P&lt;.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions: ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT’s capacity to provide logic and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as an interactive medical education tool to support learning. ©Aidan Gilson, Conrad W Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Andrew Taylor, David Chartash.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148992575&amp;doi=10.2196%2f45312&amp;partnerID=40&amp;md5=a70a9b11b70f9182a6b1a9ed24dc5122</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: JMIR Publications Inc.</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:23693762%20(ISSN)">
        <prism:volume>9</prism:volume>
        <dc:title>JMIR Medical Education</dc:title>
        <dc:identifier>DOI 10.2196/45312</dc:identifier>
        <dcterms:alternative>JMIR Med. Educ.</dcterms:alternative>
        <dc:identifier>ISSN 23693762 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1518">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 872; Correspondence Address: D. Chartash; Section for Biomedical Informatics and Data Science Yale University School of Medicine, New Haven, 300 George Street Suite 501, 06511, United States; email: david.chartash@yale.edu&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2297">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146493821&amp;doi=10.1145%2f3530811&amp;partnerID=40&amp;md5=7656b9c032dfd94cccd4de8d7714907a">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:03600300%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tay</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dehghani</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bahri</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Metzler</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1519"/>
        <dcterms:isReferencedBy rdf:resource="#item_2225"/>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>Transformers</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Learning algorithms</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>Neural-networks</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Reinforcement learning</dc:subject>
        <dc:subject>Network architecture</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>attention</dc:subject>
        <dc:subject>Attention</dc:subject>
        <dc:subject>Computational efficiency</dc:subject>
        <dc:subject>Language learning</dc:subject>
        <dc:subject>Modeling architecture</dc:subject>
        <dc:subject>neural networks</dc:subject>
        <dc:subject>Reinforcement learnings</dc:subject>
        <dc:subject>Vision learning</dc:subject>
        <dc:title>Efficient Transformers: A Survey</dc:title>
        <dcterms:abstract>Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of &quot;X-former&quot;models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored &quot;X-former&quot;models, providing an organized and comprehensive overview of existing work and models across multiple domains.  © 2022 Copyright held by the owner/author(s).</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146493821&amp;doi=10.1145%2f3530811&amp;partnerID=40&amp;md5=7656b9c032dfd94cccd4de8d7714907a</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Association for Computing Machinery</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:03600300%20(ISSN)">
        <prism:volume>55</prism:volume>
        <dc:title>ACM Computing Surveys</dc:title>
        <dc:identifier>DOI 10.1145/3530811</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>ACM Comput Surv</dcterms:alternative>
        <dc:identifier>ISSN 03600300 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1519">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 332; CODEN: ACSUE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2225">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125362171&amp;doi=10.1109%2fTPAMI.2022.3152247&amp;partnerID=40&amp;md5=e6f58af2558fe73908eed19f4a759bff">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:01628828%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xiao</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tao</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1522"/>
        <dcterms:isReferencedBy rdf:resource="#item_2222"/>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Object detection</dc:subject>
        <dc:subject>Computational modelling</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>Deep neural networks</dc:subject>
        <dc:subject>Recurrent neural networks</dc:subject>
        <dc:subject>Computer vision</dc:subject>
        <dc:subject>Self-attention</dc:subject>
        <dc:subject>Signal encoding</dc:subject>
        <dc:subject>Encodings</dc:subject>
        <dc:subject>transformer</dc:subject>
        <dc:subject>high-level vision</dc:subject>
        <dc:subject>High-level visions</dc:subject>
        <dc:subject>low-level vision</dc:subject>
        <dc:subject>Low-level vision</dc:subject>
        <dc:subject>Objects detection</dc:subject>
        <dc:subject>self-Attention</dc:subject>
        <dc:subject>video</dc:subject>
        <dc:subject>Video</dc:subject>
        <dc:subject>Video signal processing</dc:subject>
        <dc:title>A Survey on Vision Transformer</dc:title>
        <dcterms:abstract>Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-Attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-Attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.  © 1979-2012 IEEE.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125362171&amp;doi=10.1109%2fTPAMI.2022.3152247&amp;partnerID=40&amp;md5=e6f58af2558fe73908eed19f4a759bff</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: IEEE Computer Society</dc:description>
        <bib:pages>87-110</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:01628828%20(ISSN)">
        <prism:volume>45</prism:volume>
        <dc:title>IEEE Transactions on Pattern Analysis and Machine Intelligence</dc:title>
        <dc:identifier>DOI 10.1109/TPAMI.2022.3152247</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>IEEE Trans Pattern Anal Mach Intell</dcterms:alternative>
        <dc:identifier>ISSN 01628828 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1522">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 1557; Correspondence Address: Y. Wang; Huawei Noah's Ark Lab, Beijing, 100084, China; email: wangyunhe@pku.edu.cn; D. Tao; University of Sydney, School of Computer Science, Faculty of Engineering, Darlington, 2008, Australia; email: dacheng.tao@sydney.edu.au; CODEN: ITPID&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2222">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139426381&amp;doi=10.1109%2fTPAMI.2022.3206148&amp;partnerID=40&amp;md5=390425173748f3cd12ddf88f7daabf2b">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>45</prism:volume>
                <dc:title>IEEE Transactions on Pattern Analysis and Machine Intelligence</dc:title>
                <dc:identifier>DOI 10.1109/TPAMI.2022.3206148</dc:identifier>
                <prism:number>4</prism:number>
                <dcterms:alternative>IEEE Trans Pattern Anal Mach Intell</dcterms:alternative>
                <dc:identifier>ISSN 01628828 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Touvron</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bojanowski</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Caron</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cord</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>El-Nouby</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grave</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Izacard</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Joulin</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Synnaeve</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Verbeek</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jegou</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1516"/>
        <dcterms:isReferencedBy rdf:resource="#item_2365"/>
        <dc:subject>NLP</dc:subject>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Economic and social effects</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Computer aided language translation</dc:subject>
        <dc:subject>Machine translation</dc:subject>
        <dc:subject>Machine translations</dc:subject>
        <dc:subject>Decoding</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>article</dc:subject>
        <dc:subject>Simple++</dc:subject>
        <dc:subject>Benchmarking</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Network architecture</dc:subject>
        <dc:subject>Computer vision</dc:subject>
        <dc:subject>Signal encoding</dc:subject>
        <dc:subject>Computer architecture</dc:subject>
        <dc:subject>computer-vision</dc:subject>
        <dc:subject>Distillation</dc:subject>
        <dc:subject>Feed-forward network</dc:subject>
        <dc:subject>Image classification</dc:subject>
        <dc:subject>Image patches</dc:subject>
        <dc:subject>Images classification</dc:subject>
        <dc:subject>Knowledge engineering</dc:subject>
        <dc:subject>Multi-layer perceptron</dc:subject>
        <dc:subject>Multilayer neural networks</dc:subject>
        <dc:subject>Multilayers perceptrons</dc:subject>
        <dc:subject>Network layers</dc:subject>
        <dc:subject>Two-layer</dc:subject>
        <dc:title>ResMLP: Feedforward Networks for Image Classification with Data-Efficient Training</dc:title>
        <dcterms:abstract>We present ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity trade-offs on ImageNet. We also train ResMLP models in a self-supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre-trained models and our code based on the Timm library.  © 1979-2012 IEEE.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139426381&amp;doi=10.1109%2fTPAMI.2022.3206148&amp;partnerID=40&amp;md5=390425173748f3cd12ddf88f7daabf2b</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: IEEE Computer Society</dc:description>
        <bib:pages>5314-5321</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1516">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 246; Correspondence Address: H. Touvron; Facebook AI Research, Paris, 75004, France; email: htouvron@fb.com; CODEN: ITPID&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2365">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160864044&amp;doi=10.1016%2fj.tbench.2023.100105&amp;partnerID=40&amp;md5=7ea96df397ecb98fe442739d9c87827e">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:27724859%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Javaid</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Haleem</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>R.P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1517"/>
        <dcterms:isReferencedBy rdf:resource="#item_2275"/>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>Healthcare</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Large corpora</dc:subject>
        <dc:subject>Sentiment analysis</dc:subject>
        <dc:subject>Health care</dc:subject>
        <dc:subject>Applications</dc:subject>
        <dc:subject>Education</dc:subject>
        <dc:subject>Emerging stages</dc:subject>
        <dc:subject>Healthcare services</dc:subject>
        <dc:subject>Learning</dc:subject>
        <dc:subject>Limitation</dc:subject>
        <dc:subject>Limitations</dc:subject>
        <dc:subject>Text data</dc:subject>
        <dc:subject>Treatment</dc:subject>
        <dc:title>ChatGPT for healthcare services: An emerging stage for an innovative perspective</dc:title>
        <dcterms:abstract>Generative Pretrained Transformer, often known as GPT, is an innovative kind of Artificial Intelligence (AI) which can produce writing that seems to have been written by a person. OpenAI created this AI language model called ChatGPT. It is built using the GPT architecture and is trained on a large corpus of text data to respond to natural language inquiries that resemble a person's requirements. This technology has lots of applications in healthcare. The need for accurate and current data is one of the major obstacles to adopting ChatGPT in healthcare. GPT must have access to precise and up-to-date medical data to provide trustworthy suggestions and treatment options. It might be accomplished by ensuring that the data used by GPT is received from reliable sources and that the data is updated regularly. Since sensitive medical information would be involved, it will also be crucial to consider privacy and security issues while utilising GPT in the healthcare industry. This paper briefs about ChatGPT and its need for healthcare, its significant Work Flow Dimensions and typical features of ChatGPT for the Healthcare domain. Finally, it identified and discussed significant applications of ChatGPT for healthcare. ChatGPT can comprehend the conversational context and provide contextually appropriate replies. Its effectiveness as a conversational AI tool makes it useful for chatbots, virtual assistants, and other applications. However, we see many limitations in medical ethics, data interpretation, accountability and other issues related to the privacy. Regarding specialised tasks like text creation, language translation, text categorisation, text summarisation, and creating conversation systems, ChatGPT has been pre-trained on a large corpus of text data, and somewhat satisfactory results can be expected. Moreover, it can also be utilised for various Natural Language Processing (NLP) activities, including sentiment analysis, part-of-speech tagging, and named entity identification. © 2023 The Authors</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160864044&amp;doi=10.1016%2fj.tbench.2023.100105&amp;partnerID=40&amp;md5=7ea96df397ecb98fe442739d9c87827e</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier B.V.</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:27724859%20(ISSN)">
        <prism:volume>3</prism:volume>
        <dc:title>BenchCouncil Transactions on Benchmarks, Standards and Evaluations</dc:title>
        <dc:identifier>DOI 10.1016/j.tbench.2023.100105</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>BenchCounc. Trans. Benchmarks, Stand. Evaluation</dcterms:alternative>
        <dc:identifier>ISSN 27724859 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1517">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 196; Correspondence Address: M. Javaid; Department of Mechanical Engineering, Jamia Millia Islamia, New Delhi, India; email: mjavaid@jmi.ac.in&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2275">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163727625&amp;doi=10.3390%2ffi15060192&amp;partnerID=40&amp;md5=983680a889ad15454028f841da828a9f">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:19995903%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roumeliotis</foaf:surname>
                        <foaf:givenName>K.I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tselikas</foaf:surname>
                        <foaf:givenName>N.D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1521"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>GPT-4</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>generative pre-trained transformer</dc:subject>
        <dc:subject>Learning algorithms</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Reinforcement learning</dc:subject>
        <dc:subject>Reinforcement learnings</dc:subject>
        <dc:subject>ChatGPT review</dc:subject>
        <dc:subject>Generative pre-trained transformer</dc:subject>
        <dc:subject>Literature reviews</dc:subject>
        <dc:subject>Training process</dc:subject>
        <dc:title>ChatGPT and Open-AI Models: A Preliminary Review</dc:title>
        <dcterms:abstract>According to numerous reports, ChatGPT represents a significant breakthrough in the field of artificial intelligence. ChatGPT is a pre-trained AI model designed to engage in natural language conversations, utilizing sophisticated techniques from Natural Language Processing (NLP), Supervised Learning, and Reinforcement Learning to comprehend and generate text comparable to human-generated text. This article provides an overview of the training process and fundamental functionality of ChatGPT, accompanied by a preliminary review of the relevant literature. Notably, this article presents the first comprehensive literature review of this technology at the time of publication, aiming to aggregate all the available pertinent articles to facilitate further developments in the field. Ultimately, the authors aim to offer an appraisal of the technology’s potential implications on existing knowledge and technology, along with potential challenges that must be addressed. © 2023 by the authors.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163727625&amp;doi=10.3390%2ffi15060192&amp;partnerID=40&amp;md5=983680a889ad15454028f841da828a9f</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: MDPI</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:19995903%20(ISSN)">
        <prism:volume>15</prism:volume>
        <dc:title>Future Internet</dc:title>
        <dc:identifier>DOI 10.3390/fi15060192</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>Future Internet</dcterms:alternative>
        <dc:identifier>ISSN 19995903 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1521">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 197; Correspondence Address: N.D. Tselikas; Department of Informatics and Telecommunications, University of Peloponnese, Tripoli, 221 00, Greece; email: ntsel@uop.gr&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164010801&amp;partnerID=40&amp;md5=05bb5fc2067af827d1772808b404290b">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>11</prism:volume>
                <dc:title>International Journal of Intelligent Systems and Applications in Engineering</dc:title>
                <prism:number>7s</prism:number>
                <dcterms:alternative>Internat. J. Intel. Syst. Appl. Eng.</dcterms:alternative>
                <dc:identifier>ISSN 21476799 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khetani</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gandhi</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bhattacharya</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ajani</foaf:surname>
                        <foaf:givenName>S.N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Limkar</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1520"/>
        <dcterms:isReferencedBy rdf:resource="#item_2281"/>
        <dc:subject>interconnected</dc:subject>
        <dc:subject>opportunities</dc:subject>
        <dc:subject>potential</dc:subject>
        <dc:subject>provides</dc:subject>
        <dc:subject>transferability</dc:subject>
        <dc:title>Cross-Domain Analysis of ML and DL: Evaluating their Impact in Diverse Domains</dc:title>
        <dcterms:abstract>Deep Learning (DL) and Machine Learning (ML) techniques have been widely used in recent years to develop new and innovative products and services in various industries. These techniques have the potential to transform the way people think about and use technology. They have the capability to perform complex tasks and make accurate predictions. The efficiency of DL and ML algorithms has been studied in various domains, leading to significant progress in several applications. As technology and the domains become more interconnected, it is important to explore their effects on different sectors. One of the most important factors that can be considered when it comes to analyzing the generalizability and transferability of these techniques is cross-domain analysis. This allows us to identify the potential of these techniques to solve various problems. Cross-domain analysis is beneficial for several reasons. It allows us to identify ML and DL algorithms' limitations and strengths and transfer knowledge between them, which can help speed up the development of new solutions and decrease the time and effort involved in the process. If ML algorithm is able to perform high-accuracy in healthcare, it can provide valuable insights for the detection of financial fraud. For several reasons, cross-domain analysis is essential for the design and implementation of DL and ML algorithms. It helps in identifying the specific requirements and challenges of the given domain, and it enables the optimization of existing frameworks. The objectives and characteristics of each domain dictate the need for specific modifications or upgrades. This study aims to analyze the effects of DL and ML algorithms on different sectors, such as healthcare, financial services, and network security. It will examine the suitability and performance of different ML and DL algorithms in these domains. The findings of this research will allow us to gain a deeper understanding of their potential to address specific applications. The study covers the effects of DL and ML algorithms on different sectors, such as healthcare, NLP, financial services, and network security. It performs a comprehensive analysis of the different algorithms in these areas, including Gradient Boosting Machines (GBM), Logistic Regression (LR), Random Forest (RF), and Support Vector Machine (SVM). DL algorithms, such as Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU) and Transformer are also evaluated for their suitability and performance. This research offers actionable insights to practitioners and researchers, guiding them in picking suitable algorithms for specific applications, ultimately serving the goals of network security, healthcare, financial services, and NLP. The findings of this study will contribute to the increasing number of people who know about the applications of DL and DL algorithms. It will also help practitioners and researchers use these tools effectively in various fields. The study's cross-domain analysis also provides opportunities to enhance and transfer knowledge. © 2023, Ismail Saritas. All rights reserved.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164010801&amp;partnerID=40&amp;md5=05bb5fc2067af827d1772808b404290b</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Ismail Saritas</dc:description>
        <bib:pages>253-262</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1520">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 163&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2281">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138449494&amp;doi=10.1109%2fTPAMI.2021.3095381&amp;partnerID=40&amp;md5=8fd0be18cacc69f9dde8b1485e306e22">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>44</prism:volume>
                <dc:title>IEEE Transactions on Pattern Analysis and Machine Intelligence</dc:title>
                <dc:identifier>DOI 10.1109/TPAMI.2021.3095381</dc:identifier>
                <prism:number>10</prism:number>
                <dcterms:alternative>IEEE Trans Pattern Anal Mach Intell</dcterms:alternative>
                <dc:identifier>ISSN 01628828 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Elnaggar</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Heinzinger</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dallago</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rehawi</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jones</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gibbs</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Feher</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Angerer</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Steinegger</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bhowmik</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rost</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1524"/>
        <dcterms:isReferencedBy rdf:resource="#item_2346"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Algorithms</dc:subject>
        <dc:subject>Search engines</dc:subject>
        <dc:subject>algorithm</dc:subject>
        <dc:subject>Forecasting</dc:subject>
        <dc:subject>procedures</dc:subject>
        <dc:subject>Computational modelling</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Machine-learning</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Long short-term memory</dc:subject>
        <dc:subject>Modeling languages</dc:subject>
        <dc:subject>chemistry</dc:subject>
        <dc:subject>Amino acids</dc:subject>
        <dc:subject>Amino-acids</dc:subject>
        <dc:subject>biology</dc:subject>
        <dc:subject>Computational biology</dc:subject>
        <dc:subject>Computational Biology</dc:subject>
        <dc:subject>high performance computing</dc:subject>
        <dc:subject>High performance computing</dc:subject>
        <dc:subject>language modeling</dc:subject>
        <dc:subject>Performance computing</dc:subject>
        <dc:subject>Program processors</dc:subject>
        <dc:subject>protein</dc:subject>
        <dc:subject>Proteins</dc:subject>
        <dc:subject>Supercomputers</dc:subject>
        <dc:subject>supervised machine learning</dc:subject>
        <dc:subject>Supervised Machine Learning</dc:subject>
        <dc:subject>Three dimensional displays</dc:subject>
        <dc:subject>Three-dimensional display</dc:subject>
        <dc:title>ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning</dc:title>
        <dcterms:abstract>Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models (LMs) taken from Natural Language Processing (NLP). These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The protein LMs (pLMs) were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw pLM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks: (1) a per-residue (per-token) prediction of protein secondary structure (3-state accuracy Q3=81%-87%); (2) per-protein (pooling) predictions of protein sub-cellular location (ten-state accuracy: Q10=81%) and membrane versus water-soluble (2-state accuracy Q2=91%). For secondary structure, the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without multiple sequence alignments (MSAs) or evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that pLMs learned some of the grammar of the language of life. All our models are available through https://github.com/agemagician/ProtTrans.  © 1979-2012 IEEE.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138449494&amp;doi=10.1109%2fTPAMI.2021.3095381&amp;partnerID=40&amp;md5=8fd0be18cacc69f9dde8b1485e306e22</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: IEEE Computer Society</dc:description>
        <bib:pages>7112-7127</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1524">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 578; Correspondence Address: A. Elnaggar; Technical University of Munich (TUM), Department of Informatics, Bioinformatics &amp; Computational Biology - i12, Munich, Garching, 85748, Germany; email: ahmed.elnaggar@tum.de; CODEN: ITPID&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2346">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143257592&amp;partnerID=40&amp;md5=ef2617c31531791ec03b2a1817a283ed">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Proc. Conf. Empir. Methods Nat. Lang. Process., EMNLP</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mishra</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alipoormolabashi</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kordi</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mirzaei</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Arunkumar</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ashok</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dhanasekaran</foaf:surname>
                        <foaf:givenName>A.S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Naik</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stap</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pathak</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Karamanolakis</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lai</foaf:surname>
                        <foaf:givenName>H.G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Purohit</foaf:surname>
                        <foaf:givenName>I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mondal</foaf:surname>
                        <foaf:givenName>I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Anderson</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kuznia</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Doshi</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Patel</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pal</foaf:surname>
                        <foaf:givenName>K.K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Moradshahi</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Parmar</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Purohit</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Varshney</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kaza</foaf:surname>
                        <foaf:givenName>P.R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Verma</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Puri</foaf:surname>
                        <foaf:givenName>R.S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Karia</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sampat</foaf:surname>
                        <foaf:givenName>S.K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Doshi</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mishra</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reddy</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Patro</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dixit</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Baral</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Choi</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Smith</foaf:surname>
                        <foaf:givenName>N.A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hajishirzi</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khashabi</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Goldberg Y.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Kozareva Z.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Zhang Y.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1525"/>
        <dcterms:isReferencedBy rdf:resource="#item_2379"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Model size</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Generalisation</dc:subject>
        <dc:subject>In contexts</dc:subject>
        <dc:subject>Infilling</dc:subject>
        <dc:subject>Orders of magnitude</dc:subject>
        <dc:subject>Scaling parameter</dc:subject>
        <dc:subject>Task type</dc:subject>
        <dc:subject>Training model</dc:subject>
        <dc:title>SUPER-NATURALINSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks</dc:title>
        <dcterms:abstract>How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce SUPER-NATURALINSTRUCTIONS, a benchmark of 1, 616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions-training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-INSTRUCT, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-INSTRUCT outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models. © 2022 Association for Computational Linguistics.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143257592&amp;partnerID=40&amp;md5=ef2617c31531791ec03b2a1817a283ed</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Conf. Empir. Methods Nat. Lang. Process., EMNLP</dc:description>
        <bib:pages>5085-5109</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1525">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 288; Conference name: 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022; Conference date: 7 December 2022 through 11 December 2022; Conference code: 186895&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2379">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105766084&amp;doi=10.1145%2f3440755&amp;partnerID=40&amp;md5=061fb1f65175c86430f220f9f26956e8">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>54</prism:volume>
                <dc:title>ACM Computing Surveys</dc:title>
                <dc:identifier>DOI 10.1145/3440755</dc:identifier>
                <prism:number>2</prism:number>
                <dcterms:alternative>ACM Comput Surv</dcterms:alternative>
                <dc:identifier>ISSN 03600300 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chandrasekaran</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mago</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1527"/>
        <dcterms:isReferencedBy rdf:resource="#item_2226"/>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Deep neural networks</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Surveys</dc:subject>
        <dc:subject>Embeddings</dc:subject>
        <dc:subject>Knowledge based systems</dc:subject>
        <dc:subject>Semantic similarity</dc:subject>
        <dc:subject>Text data</dc:subject>
        <dc:subject>corpus-based methods</dc:subject>
        <dc:subject>Corpus-based methods</dc:subject>
        <dc:subject>knowledge-based methods</dc:subject>
        <dc:subject>Knowledge-based methods</dc:subject>
        <dc:subject>linguistics</dc:subject>
        <dc:subject>Research problems</dc:subject>
        <dc:subject>supervised and unsupervised methods</dc:subject>
        <dc:subject>Supervised methods</dc:subject>
        <dc:subject>Unsupervised method</dc:subject>
        <dc:subject>Word embedding</dc:subject>
        <dc:subject>word embeddings</dc:subject>
        <dc:title>Evolution of Semantic Similarity-A Survey</dc:title>
        <dcterms:abstract>Estimating the semantic similarity between text data is one of the challenging and open research problems in the field of Natural Language Processing (NLP). The versatility of natural language makes it difficult to define rule-based methods for determining semantic similarity measures. To address this issue, various semantic similarity methods have been proposed over the years. This survey article traces the evolution of such methods beginning from traditional NLP techniques such as kernel-based methods to the most recent research work on transformer-based models, categorizing them based on their underlying principles as knowledge-based, corpus-based, deep neural network-based methods, and hybrid methods. Discussing the strengths and weaknesses of each method, this survey provides a comprehensive view of existing systems in place for new researchers to experiment and develop innovative ideas to address the issue of semantic similarity. © 2021 ACM.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105766084&amp;doi=10.1145%2f3440755&amp;partnerID=40&amp;md5=061fb1f65175c86430f220f9f26956e8</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Association for Computing Machinery</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_1527">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 197; CODEN: ACSUE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2226">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:26403498%20(ISSN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>162</prism:volume>
                <dc:identifier>ISBN 26403498 (ISSN)</dc:identifier>
                <dc:title>Proc. Mach. Learn. Res.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>ML Research Press</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Baevski</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hsu</foaf:surname>
                        <foaf:givenName>W.-N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Babu</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gu</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Auli</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Chaudhuri K.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Jegelka S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Song L.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Szepesvari C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Niu G.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Sabato S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1523"/>
        <dcterms:isReferencedBy rdf:resource="#item_2284"/>
        <dc:subject>Learning algorithms</dc:subject>
        <dc:subject>Supervised learning</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Benchmarking</dc:subject>
        <dc:subject>State-of-the-art performance</dc:subject>
        <dc:subject>Speech recognition</dc:subject>
        <dc:subject>Natural language understanding</dc:subject>
        <dc:subject>Distillation</dc:subject>
        <dc:subject>Images classification</dc:subject>
        <dc:subject>Competitive performance</dc:subject>
        <dc:subject>Human speech</dc:subject>
        <dc:subject>Input datas</dc:subject>
        <dc:subject>Learning methods</dc:subject>
        <dc:title>data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language</dc:title>
        <dcterms:abstract>While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches. Models and code are available at www.github.com/pytorch/fairseq/tree/master/examples/data2vec. Copyright © 2022 by the author(s)</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163088369&amp;partnerID=40&amp;md5=4744ac75d7278d4a1451463931be669e</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Mach. Learn. Res.</dc:description>
        <bib:pages>1298-1312</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>Proceedings of Machine Learning Research</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1523">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 233; Correspondence Address: A. Baevski; Meta AI, United States; email: abaevski@fb.com; M. Auli; Meta AI, United States; email: michaelauli@fb.com; Conference name: 39th International Conference on Machine Learning, ICML 2022; Conference date: 17 July 2022 through 23 July 2022; Conference code: 189002&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2284">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146648064&amp;doi=10.1145%2f3505244&amp;partnerID=40&amp;md5=c47712d45818aab5311b5c5e74fe07ed">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>54</prism:volume>
                <dc:title>ACM Computing Surveys</dc:title>
                <dc:identifier>DOI 10.1145/3505244</dc:identifier>
                <prism:number>10</prism:number>
                <dcterms:alternative>ACM Comput Surv</dcterms:alternative>
                <dc:identifier>ISSN 03600300 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khan</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Naseer</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hayat</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zamir</foaf:surname>
                        <foaf:givenName>S.W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khan</foaf:surname>
                        <foaf:givenName>F.S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shah</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1526"/>
        <dcterms:isReferencedBy rdf:resource="#item_2230"/>
        <dc:subject>deep neural networks</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Object detection</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>Deep neural networks</dc:subject>
        <dc:subject>Three dimensional computer graphics</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Image enhancement</dc:subject>
        <dc:subject>Large dataset</dc:subject>
        <dc:subject>Recurrent neural networks</dc:subject>
        <dc:subject>Computer vision</dc:subject>
        <dc:subject>Self-attention</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Convolution</dc:subject>
        <dc:subject>Image segmentation</dc:subject>
        <dc:subject>self-supervision</dc:subject>
        <dc:subject>Self-supervision</dc:subject>
        <dc:subject>Network coding</dc:subject>
        <dc:subject>Video signal processing</dc:subject>
        <dc:subject>Bidirectional encoder</dc:subject>
        <dc:subject>bidirectional encoders</dc:subject>
        <dc:subject>Computer vision problems</dc:subject>
        <dc:subject>convolutional networks</dc:subject>
        <dc:subject>Convolutional networks</dc:subject>
        <dc:subject>literature survey</dc:subject>
        <dc:subject>Literature survey</dc:subject>
        <dc:subject>Object recognition</dc:subject>
        <dc:subject>transformers</dc:subject>
        <dc:subject>Vision communities</dc:subject>
        <dc:title>Transformers in Vision: A Survey</dc:title>
        <dcterms:abstract>Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.  © 2022 Association for Computing Machinery.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146648064&amp;doi=10.1145%2f3505244&amp;partnerID=40&amp;md5=c47712d45818aab5311b5c5e74fe07ed</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Association for Computing Machinery</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_1526">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 1231; CODEN: ACSUE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2230">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:03029743%20(ISSN);%20978-303108998-5%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>12962 LNCS</prism:volume>
                <dc:identifier>ISBN 03029743 (ISSN); 978-303108998-5 (ISBN)</dc:identifier>
                <dc:title>Lect. Notes Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-031-08999-2_22</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Springer Science and Business Media Deutschland GmbH</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hatamizadeh</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nath</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roth</foaf:surname>
                        <foaf:givenName>H.R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Crimi A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Bakas S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1529"/>
        <dcterms:isReferencedBy rdf:resource="#item_2380"/>
        <dc:subject>Convolutional neural networks</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Convolutional neural network</dc:subject>
        <dc:subject>Diagnosis</dc:subject>
        <dc:subject>Benchmarking</dc:subject>
        <dc:subject>Brain</dc:subject>
        <dc:subject>Network architecture</dc:subject>
        <dc:subject>Convolution</dc:subject>
        <dc:subject>Image segmentation</dc:subject>
        <dc:subject>Signal encoding</dc:subject>
        <dc:subject>Brain tumor segmentation</dc:subject>
        <dc:subject>Brain tumors</dc:subject>
        <dc:subject>BRATS</dc:subject>
        <dc:subject>Images segmentations</dc:subject>
        <dc:subject>Magnetic resonance imaging</dc:subject>
        <dc:subject>Medical imaging</dc:subject>
        <dc:subject>Semantic segmentation</dc:subject>
        <dc:subject>Semantic Segmentation</dc:subject>
        <dc:subject>Swin transformer</dc:subject>
        <dc:subject>Swin UNEt transformer</dc:subject>
        <dc:subject>Swin UNETR</dc:subject>
        <dc:subject>Tumors</dc:subject>
        <dc:subject>UNETR</dc:subject>
        <dc:subject>Vision transformer</dc:subject>
        <dc:title>Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images</dc:title>
        <dcterms:abstract>Semantic segmentation of brain tumors is a fundamental medical image analysis task involving multiple MRI imaging modalities that can assist clinicians in diagnosing the patient and successively studying the progression of the malignant entity. In recent years, Fully Convolutional Neural Networks (FCNNs) approaches have become the de facto standard for 3D medical image segmentation. The popular “U-shaped” network architecture has achieved state-of-the-art performance benchmarks on different 2D and 3D semantic segmentation tasks and across various imaging modalities. However, due to the limited kernel size of convolution layers in FCNNs, their performance of modeling long-range information is sub-optimal, and this can lead to deficiencies in the segmentation of tumors with variable sizes. On the other hand, transformer models have demonstrated excellent capabilities in capturing such long-range information in multiple domains, including natural language processing and computer vision. Inspired by the success of vision transformers and their variants, we propose a novel segmentation model termed Swin UNEt TRansformers (Swin UNETR). Specifically, the task of 3D brain tumor semantic segmentation is reformulated as a sequence to sequence prediction problem wherein multi-modal input data is projected into a 1D sequence of embedding and used as an input to a hierarchical Swin transformer as the encoder. The swin transformer encoder extracts features at five different resolutions by utilizing shifted windows for computing self-attention and is connected to an FCNN-based decoder at each resolution via skip connections. We have participated in BraTS 2021 segmentation challenge, and our proposed model ranks among the top-performing approaches in the validation phase. Code: https://monai.io/research/swin-unetr. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135046823&amp;doi=10.1007%2f978-3-031-08999-2_22&amp;partnerID=40&amp;md5=74a3350816f4cbb29dfe20c6181f0e21</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Lect. Notes Comput. Sci.</dc:description>
        <bib:pages>272-284</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1529">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 483; Correspondence Address: A. Hatamizadeh; NVIDIA, Santa Clara, United States; email: ahatamizadeh@nvidia.com; Conference name: 7th International Brain Lesion Workshop, BrainLes 2021, held in conjunction with the Medical Image Computing and Computer Assisted Intervention, MICCAI 2021; Conference date: 27 September 2021 through 27 September 2021; Conference code: 280639&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2380">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112567461&amp;doi=10.1016%2fj.neucom.2021.05.103&amp;partnerID=40&amp;md5=3fb8d718445a6b4b81331f75cc483edb">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>470</prism:volume>
                <dc:title>Neurocomputing</dc:title>
                <dc:identifier>DOI 10.1016/j.neucom.2021.05.103</dc:identifier>
                <dcterms:alternative>Neurocomputing</dcterms:alternative>
                <dc:identifier>ISSN 09252312 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lauriola</foaf:surname>
                        <foaf:givenName>I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lavelli</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Aiolli</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1528"/>
        <dcterms:isReferencedBy rdf:resource="#item_2242"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>Software</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>article</dc:subject>
        <dc:subject>software</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Deep Learning</dc:subject>
        <dc:subject>human experiment</dc:subject>
        <dc:subject>Language modeling</dc:subject>
        <dc:subject>Language Models</dc:subject>
        <dc:subject>Modelling tools</dc:subject>
        <dc:subject>Processing model</dc:subject>
        <dc:subject>Processing technique</dc:subject>
        <dc:subject>Processing tools</dc:subject>
        <dc:title>An introduction to Deep Learning in Natural Language Processing: Models, techniques, and tools</dc:title>
        <dcterms:abstract>Natural Language Processing (NLP) is a branch of artificial intelligence that involves the design and implementation of systems and algorithms able to interact through human language. Thanks to the recent advances of deep learning, NLP applications have received an unprecedented boost in performance. In this paper, we present a survey of the application of deep learning techniques in NLP, with a focus on the various tasks where deep learning is demonstrating stronger impact. Additionally, we explore, describe, and revise the main resources in NLP research, including software, hardware, and popular corpora. Finally, we emphasize the main limits of deep learning in NLP and current research directions. © 2021 Elsevier B.V.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112567461&amp;doi=10.1016%2fj.neucom.2021.05.103&amp;partnerID=40&amp;md5=3fb8d718445a6b4b81331f75cc483edb</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier B.V.</dc:description>
        <bib:pages>443-456</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1528">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 333; Correspondence Address: A. Lavelli; Fondazione Bruno Kessler, Italy; email: lavelli@fbk.eu; CODEN: NRCGE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2242">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-166540915-5%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-166540915-5 (ISBN)</dc:identifier>
                <dc:title>Proc. - IEEE/CVF Winter Conf. Appl. Comput. Vis., WACV</dc:title>
                <dc:identifier>DOI 10.1109/WACV51458.2022.00181</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hatamizadeh</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nath</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Myronenko</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Landman</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roth</foaf:surname>
                        <foaf:givenName>H.R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1532"/>
        <dcterms:isReferencedBy rdf:resource="#item_2236"/>
        <dc:subject>Convolutional neural networks</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Decoding</dc:subject>
        <dc:subject>Convolutional neural network</dc:subject>
        <dc:subject>Benchmarking</dc:subject>
        <dc:subject>Convolution</dc:subject>
        <dc:subject>Signal encoding</dc:subject>
        <dc:subject>Medical imaging</dc:subject>
        <dc:subject>Semantic Segmentation</dc:subject>
        <dc:subject>3D medical image</dc:subject>
        <dc:subject>Bioinformatics</dc:subject>
        <dc:subject>Global feature</dc:subject>
        <dc:subject>Local feature</dc:subject>
        <dc:subject>Long-range spatials</dc:subject>
        <dc:subject>Medical image segmentation</dc:subject>
        <dc:subject>Medical imaging/imaging for bioinformatic/biological and cell microscopy</dc:subject>
        <dc:subject>Medical Imaging/Imaging for Bioinformatics/Biological and Cell Microscopy</dc:subject>
        <dc:subject>Sequence learning</dc:subject>
        <dc:subject>Spatial dependencies</dc:subject>
        <dc:subject>Volumetric 3D</dc:subject>
        <dc:title>UNETR: Transformers for 3D Medical Image Segmentation</dc:title>
        <dcterms:abstract>Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful &quot;U-shaped&quot;network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard.  © 2022 IEEE.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123975952&amp;doi=10.1109%2fWACV51458.2022.00181&amp;partnerID=40&amp;md5=73e6eebab66e62fac3302882ec364d57</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. - IEEE/CVF Winter Conf. Appl. Comput. Vis., WACV</dc:description>
        <bib:pages>1748-1758</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1532">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 1052; Conference name: 22nd IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022; Conference date: 4 January 2022 through 8 January 2022; Conference code: 177326&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2236">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125517450&amp;doi=10.1038%2fs41587-022-01226-0&amp;partnerID=40&amp;md5=df48de3642b26da7f97d37f2b290d129">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:10870156%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ma</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xia</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tong</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ye</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Feng</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1530"/>
        <dcterms:isReferencedBy rdf:resource="#item_2298"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>data mining</dc:subject>
        <dc:subject>Learning algorithms</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>Humans</dc:subject>
        <dc:subject>Article</dc:subject>
        <dc:subject>controlled study</dc:subject>
        <dc:subject>prediction</dc:subject>
        <dc:subject>artificial neural network</dc:subject>
        <dc:subject>drug efficacy</dc:subject>
        <dc:subject>long short term memory network</dc:subject>
        <dc:subject>Deep Learning</dc:subject>
        <dc:subject>Neural network model</dc:subject>
        <dc:subject>Animals</dc:subject>
        <dc:subject>Long short-term memory</dc:subject>
        <dc:subject>chemistry</dc:subject>
        <dc:subject>Acinetobacter baumannii</dc:subject>
        <dc:subject>Adenosine Monophosphate</dc:subject>
        <dc:subject>adenosine phosphate</dc:subject>
        <dc:subject>amikacin</dc:subject>
        <dc:subject>amino acid sequence</dc:subject>
        <dc:subject>amoxicillin plus clavulanic acid</dc:subject>
        <dc:subject>ampicillin</dc:subject>
        <dc:subject>animal</dc:subject>
        <dc:subject>animal experiment</dc:subject>
        <dc:subject>animal model</dc:subject>
        <dc:subject>animal tissue</dc:subject>
        <dc:subject>Anti-Bacterial Agents</dc:subject>
        <dc:subject>Anti-microbial activity</dc:subject>
        <dc:subject>antibacterial activity</dc:subject>
        <dc:subject>antibiotic resistance</dc:subject>
        <dc:subject>antiinfective agent</dc:subject>
        <dc:subject>Antimicrobial peptide</dc:subject>
        <dc:subject>antimicrobial peptide 1043</dc:subject>
        <dc:subject>antimicrobial peptide 1655</dc:subject>
        <dc:subject>antimicrobial peptide 2041</dc:subject>
        <dc:subject>antimicrobial peptide 240</dc:subject>
        <dc:subject>antimicrobial peptide 250</dc:subject>
        <dc:subject>antimicrobial peptide 518</dc:subject>
        <dc:subject>antimicrobial peptide 575</dc:subject>
        <dc:subject>antimicrobial peptide 593</dc:subject>
        <dc:subject>antimicrobial peptide 660</dc:subject>
        <dc:subject>antimicrobial peptide 67</dc:subject>
        <dc:subject>antimicrobial peptide 69</dc:subject>
        <dc:subject>Antimicrobial Peptides</dc:subject>
        <dc:subject>attention layer</dc:subject>
        <dc:subject>aztreonam</dc:subject>
        <dc:subject>Bacillus subtilis</dc:subject>
        <dc:subject>bacterial load</dc:subject>
        <dc:subject>beta defensin 2</dc:subject>
        <dc:subject>bidirectional encoder representations from transformers model</dc:subject>
        <dc:subject>cathelicidin bf</dc:subject>
        <dc:subject>CC50 (cytotoxic concentration)</dc:subject>
        <dc:subject>cefalexin</dc:subject>
        <dc:subject>cefazolin</dc:subject>
        <dc:subject>cefepime</dc:subject>
        <dc:subject>cefoperazone plus sulbactam</dc:subject>
        <dc:subject>cefotaxime</dc:subject>
        <dc:subject>cefoxitin</dc:subject>
        <dc:subject>ceftazidime</dc:subject>
        <dc:subject>ceftriaxone</dc:subject>
        <dc:subject>cefuroxime</dc:subject>
        <dc:subject>chloramphenicol</dc:subject>
        <dc:subject>ciprofloxacin</dc:subject>
        <dc:subject>Computational predictions</dc:subject>
        <dc:subject>defensin np 1</dc:subject>
        <dc:subject>drug cytotoxicity</dc:subject>
        <dc:subject>drug identification</dc:subject>
        <dc:subject>drug mechanism</dc:subject>
        <dc:subject>drug potency</dc:subject>
        <dc:subject>Enterobacter cloacae</dc:subject>
        <dc:subject>ertapenem</dc:subject>
        <dc:subject>erythrocyte</dc:subject>
        <dc:subject>Escherichia coli</dc:subject>
        <dc:subject>Gastrointestinal Microbiome</dc:subject>
        <dc:subject>gentamicin</dc:subject>
        <dc:subject>HCT 116 cell line</dc:subject>
        <dc:subject>human cell</dc:subject>
        <dc:subject>Human guts</dc:subject>
        <dc:subject>IC50</dc:subject>
        <dc:subject>imipenem</dc:subject>
        <dc:subject>intermethod comparison</dc:subject>
        <dc:subject>intestine flora</dc:subject>
        <dc:subject>Klebsiella pneumoniae</dc:subject>
        <dc:subject>Klebsiella pneumoniae infection</dc:subject>
        <dc:subject>levofloxacin</dc:subject>
        <dc:subject>lung infection</dc:subject>
        <dc:subject>magainin 2</dc:subject>
        <dc:subject>mastoparan like peptide 12c precursor</dc:subject>
        <dc:subject>meropenem</dc:subject>
        <dc:subject>metagenome</dc:subject>
        <dc:subject>metagenomics</dc:subject>
        <dc:subject>Mice</dc:subject>
        <dc:subject>Microbiome</dc:subject>
        <dc:subject>Microorganisms</dc:subject>
        <dc:subject>minimum inhibitory concentration</dc:subject>
        <dc:subject>minocycline</dc:subject>
        <dc:subject>mouse</dc:subject>
        <dc:subject>multidrug resistant Gram negative bacterium</dc:subject>
        <dc:subject>nalidixic acid</dc:subject>
        <dc:subject>Needleman Wunsch algorithm</dc:subject>
        <dc:subject>nonhuman</dc:subject>
        <dc:subject>peptide</dc:subject>
        <dc:subject>Peptide identification</dc:subject>
        <dc:subject>Peptides</dc:subject>
        <dc:subject>piperacillin plus tazobactam</dc:subject>
        <dc:subject>polymyxin B</dc:subject>
        <dc:subject>polypeptide antibiotic agent</dc:subject>
        <dc:subject>positivity rate</dc:subject>
        <dc:subject>proof of concept</dc:subject>
        <dc:subject>Pseudomonas aeruginosa</dc:subject>
        <dc:subject>sequence homology</dc:subject>
        <dc:subject>Sequence homology</dc:subject>
        <dc:subject>Staphylococcus aureus</dc:subject>
        <dc:subject>sulfamethoxazole</dc:subject>
        <dc:subject>Synthesised</dc:subject>
        <dc:subject>tetracycline</dc:subject>
        <dc:subject>ticarcillin</dc:subject>
        <dc:subject>tigecycline</dc:subject>
        <dc:subject>tobramycin</dc:subject>
        <dc:subject>Training sets</dc:subject>
        <dc:subject>trimethoprim</dc:subject>
        <dc:subject>unclassified drug</dc:subject>
        <dc:title>Identification of antimicrobial peptides from the human gut microbiome using deep learning</dc:title>
        <dcterms:abstract>The human gut microbiome encodes a large variety of antimicrobial peptides (AMPs), but the short lengths of AMPs pose a challenge for computational prediction. Here we combined multiple natural language processing neural network models, including LSTM, Attention and BERT, to form a unified pipeline for candidate AMP identification from human gut microbiome data. Of 2,349 sequences identified as candidate AMPs, 216 were chemically synthesized, with 181 showing antimicrobial activity (a positive rate of &gt;83%). Most of these peptides have less than 40% sequence homology to AMPs in the training set. Further characterization of the 11 most potent AMPs showed high efficacy against antibiotic-resistant, Gram-negative pathogens and demonstrated significant efficacy in lowering bacterial load by more than tenfold against a mouse model of bacterial lung infection. Our study showcases the potential of machine learning approaches for mining functional peptides from metagenome data and accelerating the discovery of promising AMP candidate molecules for in-depth investigations. © 2022, The Author(s), under exclusive licence to Springer Nature America, Inc.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125517450&amp;doi=10.1038%2fs41587-022-01226-0&amp;partnerID=40&amp;md5=df48de3642b26da7f97d37f2b290d129</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Nature Research</dc:description>
        <bib:pages>921-931</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:10870156%20(ISSN)">
        <prism:volume>40</prism:volume>
        <dc:title>Nature Biotechnology</dc:title>
        <dc:identifier>DOI 10.1038/s41587-022-01226-0</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>Nat. Biotechnol.</dcterms:alternative>
        <dc:identifier>ISSN 10870156 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1530">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 210; Correspondence Address: J. Wang; CAS Key Laboratory of Pathogenic Microbiology and Immunology, Institute of Microbiology, Chinese Academy of Sciences, Beijing, China; email: junwang@im.ac.cn; Y. Chen; University of Chinese Academy of Sciences, Beijing, China; email: chenyihua@im.ac.cn; CODEN: NABIF&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2298">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:03029743%20(ISSN);%20978-303120085-4%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>13662 LNCS</prism:volume>
                <dc:identifier>ISBN 03029743 (ISSN); 978-303120085-4 (ISBN)</dc:identifier>
                <dc:title>Lect. Notes Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-031-20086-1_35</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Springer Science and Business Media Deutschland GmbH</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tay</foaf:surname>
                        <foaf:givenName>F.E.H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yuan</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Avidan S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Brostow G.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Cissé M.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Farinella G.M.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Hassner T.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1531"/>
        <dcterms:isReferencedBy rdf:resource="#item_2320"/>
        <dc:subject>Learning algorithms</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Supervised learning</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Property</dc:subject>
        <dc:subject>Pre-training</dc:subject>
        <dc:subject>Point-clouds</dc:subject>
        <dc:subject>Computer architecture</dc:subject>
        <dc:subject>Asymmetric design</dc:subject>
        <dc:subject>Auto encoders</dc:subject>
        <dc:subject>Information density</dc:subject>
        <dc:subject>Learn+</dc:subject>
        <dc:subject>Location information</dc:subject>
        <dc:title>Masked Autoencoders for Point Cloud Self-supervised Learning</dc:title>
        <dcterms:abstract>As a promising scheme of self-supervised learning, masked autoencoding has significantly advanced natural language processing and computer vision. Inspired by this, we propose a neat scheme of masked autoencoders for point cloud self-supervised learning, addressing the challenges posed by point cloud’s properties, including leakage of location information and uneven information density. Concretely, we divide the input point cloud into irregular point patches and randomly mask them at a high ratio. Then, a standard Transformer based autoencoder, with an asymmetric design and a shifting mask tokens operation, learns high-level latent features from unmasked point patches, aiming to reconstruct the masked point patches. Extensive experiments show that our approach is efficient during pre-training and generalizes well on various downstream tasks. The pre-trained models achieve 85.18% accuracy on ScanObjectNN and 94.04% accuracy on ModelNet40, outperforming all the other self-supervised learning methods. We show with our scheme, a simple architecture entirely based on standard Transformers can surpass dedicated Transformer models from supervised learning. Our approach also advances state-of-the-art accuracies by 1.5%–2.3% in the few-shot classification. Furthermore, our work inspires the feasibility of applying unified architectures from languages and images to the point cloud. Codes are available at https://github.com/Pang-Yatian/Point-MAE. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142748029&amp;doi=10.1007%2f978-3-031-20086-1_35&amp;partnerID=40&amp;md5=3fe4f29b01fd9342559771ae0d15a6b9</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Lect. Notes Comput. Sci.</dc:description>
        <bib:pages>604-621</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1531">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 154; Correspondence Address: L. Yuan; School of Electronic and Computer Engineering, Peking University, Beijing, China; email: yuanli-ece@pku.edu.cn; Conference name: 17th European Conference on Computer Vision, ECCV 2022; Conference date: 23 October 2022 through 27 October 2022; Conference code: 285469&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2320">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-195408546-6%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-195408546-6 (ISBN)</dc:identifier>
                <dc:title>NAACL-HLT - Conf. N. Am. Chapter Assoc. Comput. Linguist.: Hum. Lang. Technol., Proc. Conf.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xue</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Constant</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roberts</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kale</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Al-Rfou</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Siddhant</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barua</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Raffel</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1536"/>
        <dcterms:isReferencedBy rdf:resource="#item_2329"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>State of the art</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Translation (languages)</dc:subject>
        <dc:subject>Simple++</dc:subject>
        <dc:subject>Benchmarking</dc:subject>
        <dc:subject>State-of-the-art performance</dc:subject>
        <dc:subject>Zero-shot learning</dc:subject>
        <dc:subject>English languages</dc:subject>
        <dc:subject>Generative model</dc:subject>
        <dc:subject>Text format</dc:subject>
        <dc:title>mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer</dc:title>
        <dcterms:abstract>The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent “accidental translation” in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available. © 2021 Association for Computational Linguistics.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121486920&amp;partnerID=40&amp;md5=690890eb359a684d518607934c0a7da5</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: NAACL-HLT - Conf. N. Am. Chapter Assoc. Comput. Linguist.: Hum. Lang. Technol., Proc. Conf.</dc:description>
        <bib:pages>483-498</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>NAACL-HLT 2021 - 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1536">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 1070; Correspondence Address: L. Xue; Google Research, United States; email: lintingx@google.com; N. Constant; Google Research, United States; email: nconstant@google.com; A. Roberts; Google Research, United States; email: adarob@google.com; C. Raffel; Google Research, United States; email: craffel@google.com; Conference name: 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021; Conference date: 6 June 2021 through 11 June 2021; Conference code: 182055&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2329">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-195408552-7%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-195408552-7 (ISBN)</dc:identifier>
                <dc:title>ACL-IJCNLP - Annu. Meet. Assoc. Comput. Linguist. Int. Jt. Conf. Nat. Lang. Process., Proc. Conf.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Giorgi</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nitski</foaf:surname>
                        <foaf:givenName>O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bader</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1537"/>
        <dcterms:isReferencedBy rdf:resource="#item_2285"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Labeled data</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Down-stream</dc:subject>
        <dc:subject>Embeddings</dc:subject>
        <dc:subject>Pre-training</dc:subject>
        <dc:subject>Clusterings</dc:subject>
        <dc:subject>Labeled training data</dc:subject>
        <dc:subject>Metric learning</dc:subject>
        <dc:subject>Text corpora</dc:subject>
        <dc:subject>Textual representation</dc:subject>
        <dc:title>DeCLUTR: Deep contrastive learning for unsupervised textual representations</dc:title>
        <dcterms:abstract>Sentence embeddings are an important component of many natural language processing (NLP) systems. Like word embeddings, sentence embeddings are typically learned on large text corpora and then transferred to various downstream tasks, such as clustering and retrieval. Unlike word embeddings, the highest performing solutions for learning sentence embeddings require labelled data, limiting their usefulness to languages and domains where labelled data is abundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. Inspired by recent advances in deep metric learning (DML), we carefully design a self-supervised objective for learning universal sentence embeddings that does not require labelled training data. When used to extend the pretraining of transformer-based language models, our approach closes the performance gap between unsupervised and supervised pretraining for universal sentence encoders. Importantly, our experiments suggest that the quality of the learned embeddings scale with both the number of trainable parameters and the amount of unlabelled training data. Our code and pretrained models are publicly available and can be easily adapted to new domains or used to embed unseen text. © 2021 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115425635&amp;partnerID=40&amp;md5=61c5189e9557b22ce789930c2e33e8fa</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: ACL-IJCNLP - Annu. Meet. Assoc. Comput. Linguist. Int. Jt. Conf. Nat. Lang. Process., Proc. Conf.</dc:description>
        <bib:pages>879-895</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>ACL-IJCNLP 2021 - 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the Conference</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1537">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 254; Correspondence Address: B. Wang; Department of Computer Science, University of Toronto, Canada; email: bowang@vectorinstitute.ai; G. Bader; Department of Computer Science, University of Toronto, Canada; email: gary.bader@mail.utoronto.ca; Conference name: Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL-IJCNLP 2021; Conference date: 1 August 2021 through 6 August 2021; Conference code: 173030&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2285">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100710769&amp;doi=10.1007%2fs10462-021-09958-2&amp;partnerID=40&amp;md5=05ab3c384057f2825868bae6e14e843b">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:02692821%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Acheampong</foaf:surname>
                        <foaf:givenName>F.A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nunoo-Mensah</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1533"/>
        <dcterms:isReferencedBy rdf:resource="#item_2245"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Emotion detection</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Transformers</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Emotion recognition</dc:subject>
        <dc:subject>Sentiment analysis</dc:subject>
        <dc:subject>Contextual information</dc:subject>
        <dc:subject>Cross-lingual</dc:subject>
        <dc:subject>Future research directions</dc:subject>
        <dc:subject>Text-based emotion detection</dc:subject>
        <dc:subject>Transformer models</dc:subject>
        <dc:title>Transformer models for text-based emotion detection: a review of BERT-based approaches</dc:title>
        <dcterms:abstract>We cannot overemphasize the essence of contextual information in most natural language processing (NLP) applications. The extraction of context yields significant improvements in many NLP tasks, including emotion recognition from texts. The paper discusses transformer-based models for NLP tasks. It highlights the pros and cons of the identified models. The models discussed include the Generative Pre-training (GPT) and its variants, Transformer-XL, Cross-lingual Language Models (XLM), and the Bidirectional Encoder Representations from Transformers (BERT). Considering BERT’s strength and popularity in text-based emotion detection, the paper discusses recent works in which researchers proposed various BERT-based models. The survey presents its contributions, results, limitations, and datasets used. We have also provided future research directions to encourage research in text-based emotion detection using these models. © 2021, The Author(s), under exclusive licence to Springer Nature B.V. part of Springer Nature.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100710769&amp;doi=10.1007%2fs10462-021-09958-2&amp;partnerID=40&amp;md5=05ab3c384057f2825868bae6e14e843b</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer Science and Business Media B.V.</dc:description>
        <bib:pages>5789-5829</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:02692821%20(ISSN)">
        <prism:volume>54</prism:volume>
        <dc:title>Artificial Intelligence Review</dc:title>
        <dc:identifier>DOI 10.1007/s10462-021-09958-2</dc:identifier>
        <prism:number>8</prism:number>
        <dcterms:alternative>Artif Intell Rev</dcterms:alternative>
        <dc:identifier>ISSN 02692821 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1533">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 304; Correspondence Address: H. Nunoo-Mensah; Connected Devices Lab, Department of Computer Engineering, Kwame Nkrumah University of Science and Technology, Kumasi, Ghana; email: hnunoo-mensah@knust.edu.gh; CODEN: AIRVE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2245">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:10495258%20(ISSN);%20978-171384539-3%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>11</prism:volume>
                <dc:identifier>ISBN 10495258 (ISSN); 978-171384539-3 (ISBN)</dc:identifier>
                <dc:title>Adv. neural inf. proces. syst.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Neural information processing systems foundation</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dai</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>So</foaf:surname>
                        <foaf:givenName>D.R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Le</foaf:surname>
                        <foaf:givenName>Q.V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Ranzato M.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Beygelzimer A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Dauphin Y.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Liang P.S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Wortman Vaughan J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1534"/>
        <dcterms:isReferencedBy rdf:resource="#item_2339"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Down-stream</dc:subject>
        <dc:subject>Network architecture</dc:subject>
        <dc:subject>Pre-training</dc:subject>
        <dc:subject>Architectural innovation</dc:subject>
        <dc:subject>Simple networks</dc:subject>
        <dc:subject>Vision applications</dc:subject>
        <dc:title>Pay Attention to MLPs</dc:title>
        <dcterms:abstract>Transformers [1] have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple network architecture, gMLP, based on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream NLP tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute. © 2021 Neural information processing systems foundation. All rights reserved.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131898296&amp;partnerID=40&amp;md5=4cb97c8e30cc45196420b09ac44fccf9</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Adv. neural inf. proces. syst.</dc:description>
        <bib:pages>9204-9215</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Advances in Neural Information Processing Systems</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1534">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 357; Conference name: 35th Conference on Neural Information Processing Systems, NeurIPS 2021; Conference date: 6 December 2021 through 14 December 2021; Conference code: 179642&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2339">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:02705257%20(ISSN);%20978-073811319-7%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 02705257 (ISSN); 978-073811319-7 (ISBN)</dc:identifier>
                <dc:title>Proc Int Conf Software Eng</dc:title>
                <dc:identifier>DOI 10.1109/ICSE43902.2021.00041</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>IEEE Computer Society</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mastropaolo</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scalabrino</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cooper</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nader Palacio</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Poshyvanyk</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oliveto</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bavota</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1535"/>
        <dcterms:isReferencedBy rdf:resource="#item_2378"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Performance</dc:subject>
        <dc:subject>State-of-the-art performance</dc:subject>
        <dc:subject>Sentence classifications</dc:subject>
        <dc:subject>Deep Learning</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Large dataset</dc:subject>
        <dc:subject>Specific tasks</dc:subject>
        <dc:subject>Bug-fixing</dc:subject>
        <dc:subject>Empirical software engineering</dc:subject>
        <dc:subject>Empirical Software Engineering</dc:subject>
        <dc:subject>Engineering community</dc:subject>
        <dc:subject>Language translation</dc:subject>
        <dc:subject>Learning techniques</dc:subject>
        <dc:title>Studying the usage of text-to-text transfer transformer to support code-related tasks</dc:title>
        <dcterms:abstract>Deep learning (DL) techniques are gaining more and more attention in the software engineering community. They have been used to support several code-related tasks, such as automatic bug fixing and code comments generation. Recent studies in the Natural Language Processing (NLP) field have shown that the Text-To-Text Transfer Transformer (T5) architecture can achieve state-of-the-art performance for a variety of NLP tasks. The basic idea behind T5 is to first pre-train a model on a large and generic dataset using a self-supervised task (e.g., filling masked words in sentences). Once the model is pre-trained, it is fine-tuned on smaller and specialized datasets, each one related to a specific task (e.g., language translation, sentence classification). In this paper, we empirically investigate how the T5 model performs when pre-trained and fine-tuned to support code-related tasks. We pre-train a T5 model on a dataset composed of natural language English text and source code. Then, we fine-tune such a model by reusing datasets used in four previous works that used DL techniques to: (i) fix bugs, (ii) inject code mutants, (iii) generate assert statements, and (iv) generate code comments. We compared the performance of this single model with the results reported in the four original papers proposing DL-based solutions for those four tasks. We show that our T5 model, exploiting additional data for the self-supervised pre-training phase, can achieve performance improvements over the four baselines.  © 2021 IEEE.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113421369&amp;doi=10.1109%2fICSE43902.2021.00041&amp;partnerID=40&amp;md5=8a53279ede6493be3e9eb948dc66a86d</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc Int Conf Software Eng</dc:description>
        <bib:pages>336-347</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings - International Conference on Software Engineering</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1535">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 151; Conference name: 43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021; Conference date: 22 May 2021 through 30 May 2021; Conference code: 170992; CODEN: PCSED&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2378">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-195591709-4%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-195591709-4 (ISBN)</dc:identifier>
                <dc:title>EMNLP - Conf. Empir. Methods Nat. Lang. Process., Proc.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Joty</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hoi</foaf:surname>
                        <foaf:givenName>S.C.H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1539"/>
        <dcterms:isReferencedBy rdf:resource="#item_2278"/>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Decoding</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Unified framework</dc:subject>
        <dc:subject>'current</dc:subject>
        <dc:subject>Pre-training</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Signal encoding</dc:subject>
        <dc:subject>Code defects</dc:subject>
        <dc:subject>Code semantics</dc:subject>
        <dc:subject>Code understanding</dc:subject>
        <dc:subject>Codegeneration</dc:subject>
        <dc:subject>Encoder-decoder</dc:subject>
        <dc:title>CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation</dc:title>
        <dcterms:abstract>Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5. © 2021 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125780341&amp;partnerID=40&amp;md5=e737f03414b7cc364c8ce3da26c33ea4</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: EMNLP - Conf. Empir. Methods Nat. Lang. Process., Proc.</dc:description>
        <bib:pages>8696-8708</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>EMNLP 2021 - 2021 Conference on Empirical Methods in Natural Language Processing, Proceedings</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1539">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 467; Conference name: 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021; Conference date: 7 November 2021 through 11 November 2021; Conference code: 177530&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2278">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:15505499%20(ISSN);%20978-166542812-5%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 15505499 (ISSN); 978-166542812-5 (ISBN)</dc:identifier>
                <dc:title>Proc IEEE Int Conf Comput Vision</dc:title>
                <dc:identifier>DOI 10.1109/ICCV48922.2021.00062</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yuan</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1538"/>
        <dcterms:isReferencedBy rdf:resource="#item_2302"/>
        <dc:subject>Convolutional neural networks</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Training data</dc:subject>
        <dc:subject>Convolutional neural network</dc:subject>
        <dc:subject>Performance</dc:subject>
        <dc:subject>Image enhancement</dc:subject>
        <dc:subject>Network architecture</dc:subject>
        <dc:subject>Long-range dependencies</dc:subject>
        <dc:subject>Computer vision</dc:subject>
        <dc:subject>Convolution</dc:subject>
        <dc:subject>Feed forward</dc:subject>
        <dc:subject>Image transformers</dc:subject>
        <dc:subject>Input image</dc:subject>
        <dc:subject>Large amounts</dc:subject>
        <dc:subject>Low-level features</dc:subject>
        <dc:subject>Personnel training</dc:subject>
        <dc:subject>Tokenization</dc:subject>
        <dc:title>Incorporating Convolution Designs into Visual Transformers</dc:title>
        <dcterms:abstract>Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new Convolution-enhanced image Transformer (CeiT) which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: 1) instead of the straightforward tokenization from raw input images, we design an Image-to-Tokens (I2T) module that extracts patches from generated low-level features; 2) the feed-froward network in each encoder block is replaced with a Locally-enhanced Feed-Forward (LeFF) layer that promotes the correlation among neighboring tokens in the spatial dimension; 3) a Layer-wise Class token Attention (LCA) is attached at the top of the Transformer that utilizes the multi-level representations. Experimental results on ImageNet and seven downstream tasks show the effectiveness and generalization ability of CeiT compared with previous Transformers and state-of-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models demonstrate better convergence with 3× fewer training iterations, which can reduce the training cost significantly. © 2021 IEEE</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119124402&amp;doi=10.1109%2fICCV48922.2021.00062&amp;partnerID=40&amp;md5=03f5fa274fa808bd17f6316d7e963102</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc IEEE Int Conf Comput Vision</dc:description>
        <bib:pages>559-568</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the IEEE International Conference on Computer Vision</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1538">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 413; Conference name: 18th IEEE/CVF International Conference on Computer Vision, ICCV 2021; Conference date: 11 October 2021 through 17 October 2021; Conference code: 177652; CODEN: PICVE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2302">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118623180&amp;doi=10.1109%2fTASLP.2021.3124365&amp;partnerID=40&amp;md5=e16f106b076431ca01cef0afc415db10">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:23299290%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cui</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Che</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qin</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1540"/>
        <dcterms:isReferencedBy rdf:resource="#item_2342"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Computational modelling</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Pre-trained language model</dc:subject>
        <dc:subject>Performance</dc:subject>
        <dc:subject>Predictive models</dc:subject>
        <dc:subject>Modeling languages</dc:subject>
        <dc:subject>Pre-training</dc:subject>
        <dc:subject>Job analysis</dc:subject>
        <dc:subject>Adaptation models</dc:subject>
        <dc:subject>Bit error rate</dc:subject>
        <dc:subject>Bit-error rate</dc:subject>
        <dc:subject>Representation learning</dc:subject>
        <dc:title>Pre-Training with Whole Word Masking for Chinese BERT</dc:title>
        <dcterms:abstract>Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and its consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we aim to first introduce the whole word masking (wwm) strategy for Chinese BERT, along with a series of Chinese pre-trained language models. Then we also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways. Especially, we propose a new masking strategy called MLM as correction (Mac). To demonstrate the effectiveness of these models, we create a series of Chinese pre-trained language models as our baselines, including BERT, RoBERTa, ELECTRA, RBT, etc. We carried out extensive experiments on ten Chinese NLP tasks to evaluate the created Chinese pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research. We open-source our pre-trained language models for further facilitating our research community.1  © 2014 IEEE.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118623180&amp;doi=10.1109%2fTASLP.2021.3124365&amp;partnerID=40&amp;md5=e16f106b076431ca01cef0afc415db10</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Institute of Electrical and Electronics Engineers Inc.</dc:description>
        <bib:pages>3504-3514</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:23299290%20(ISSN)">
        <prism:volume>29</prism:volume>
        <dc:title>IEEE/ACM Transactions on Audio Speech and Language Processing</dc:title>
        <dc:identifier>DOI 10.1109/TASLP.2021.3124365</dc:identifier>
        <dcterms:alternative>IEEE ACM Trans. Audio Speech Lang. Process.</dcterms:alternative>
        <dc:identifier>ISSN 23299290 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1540">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 798; Correspondence Address: T. Liu; Harbin Institute of Technology, Harbin, 150001, China; email: tliu@hit.edu.cn&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2342">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104757563&amp;doi=10.1088%2f2632-2153%2fabc81d&amp;partnerID=40&amp;md5=bf47e8de0b59a22eba66365e9fa69ee3">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:26322153%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schwaller</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vaucher</foaf:surname>
                        <foaf:givenName>A.C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Laino</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reymond</foaf:surname>
                        <foaf:givenName>J.-L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1541"/>
        <dcterms:isReferencedBy rdf:resource="#item_2343"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Forecasting</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Predictive analytics</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Signal encoding</dc:subject>
        <dc:subject>Chemical descriptors</dc:subject>
        <dc:subject>Chemical reactions</dc:subject>
        <dc:subject>High throughput experiments</dc:subject>
        <dc:subject>Molecular fingerprint</dc:subject>
        <dc:subject>Prediction performance</dc:subject>
        <dc:subject>Reaction conversion</dc:subject>
        <dc:subject>Reaction prediction</dc:subject>
        <dc:subject>Reaction rates</dc:subject>
        <dc:subject>Synthesis (chemical)</dc:subject>
        <dc:subject>Yield prediction</dc:subject>
        <dc:title>Prediction of chemical reaction yields using deep learning</dc:title>
        <dcterms:abstract>Artificial intelligence is driving one of the most important revolutions in organic chemistry. Multiple platforms, including tools for reaction prediction and synthesis planning based on machine learning, have successfully become part of the organic chemists’ daily laboratory, assisting in domain-specific synthetic problems. Unlike reaction prediction and retrosynthetic models, the prediction of reaction yields has received less attention in spite of the enormous potential of accurately predicting reaction conversion rates. Reaction yields models, describing the percentage of the reactants converted to the desired products, could guide chemists and help them select high-yielding reactions and score synthesis routes, reducing the number of attempts. So far, yield predictions have been predominantly performed for high-throughput experiments using a categorical (one-hot) encoding of reactants, concatenated molecular fingerprints, or computed chemical descriptors. Here, we extend the application of natural language processing architectures to predict reaction properties given a text-based representation of the reaction, using an encoder transformer model combined with a regression layer. We demonstrate outstanding prediction performance on two high-throughput experiment reactions sets. An analysis of the yields reported in the open-source USPTO data set shows that their distribution differs depending on the mass scale, limiting the data set applicability in reaction yields predictions. © 2021 The Author(s). Published by IOP Publishing Ltd</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104757563&amp;doi=10.1088%2f2632-2153%2fabc81d&amp;partnerID=40&amp;md5=bf47e8de0b59a22eba66365e9fa69ee3</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: IOP Publishing Ltd</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:26322153%20(ISSN)">
        <prism:volume>2</prism:volume>
        <dc:title>Machine Learning: Science and Technology</dc:title>
        <dc:identifier>DOI 10.1088/2632-2153/abc81d</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Mach. Learn.: Sci. Technol.</dcterms:alternative>
        <dc:identifier>ISSN 26322153 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1541">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 187; Correspondence Address: P. Schwaller; IBM Research—Europe, Rüschlikon, Säumerstrasse 4, 8803, Switzerland; email: phs@zurich.ibm.com&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2343">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103692698&amp;doi=10.1016%2fj.csbj.2021.03.022&amp;partnerID=40&amp;md5=0c846d5dfe80765b2cc01f4a16474c5a">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:20010370%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ofer</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brandes</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Linial</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1542"/>
        <dcterms:isReferencedBy rdf:resource="#item_2387"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>algorithm</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>priority journal</dc:subject>
        <dc:subject>Review</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Transformer</dc:subject>
        <dc:subject>prediction</dc:subject>
        <dc:subject>Neural networks</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>artificial neural network</dc:subject>
        <dc:subject>Embeddings</dc:subject>
        <dc:subject>Modeling languages</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>protein</dc:subject>
        <dc:subject>Proteins</dc:subject>
        <dc:subject>Word embedding</dc:subject>
        <dc:subject>Bioinformatics</dc:subject>
        <dc:subject>amino acid sequence</dc:subject>
        <dc:subject>nonhuman</dc:subject>
        <dc:subject>Tokenization</dc:subject>
        <dc:subject>Artificial neural networks</dc:subject>
        <dc:subject>atom</dc:subject>
        <dc:subject>Bag of words</dc:subject>
        <dc:subject>contextualized embedding</dc:subject>
        <dc:subject>Contextualized embedding</dc:subject>
        <dc:subject>Language models</dc:subject>
        <dc:subject>protein analysis</dc:subject>
        <dc:subject>proteomics</dc:subject>
        <dc:subject>sequence analysis</dc:subject>
        <dc:subject>word embedding</dc:subject>
        <dc:subject>Word2vec</dc:subject>
        <dc:title>The language of proteins: NLP, machine learning &amp; protein sequences</dc:title>
        <dcterms:abstract>Natural language processing (NLP) is a field of computer science concerned with automated text and language analysis. In recent years, following a series of breakthroughs in deep and machine learning, NLP methods have shown overwhelming progress. Here, we review the success, promise and pitfalls of applying NLP algorithms to the study of proteins. Proteins, which can be represented as strings of amino-acid letters, are a natural fit to many NLP methods. We explore the conceptual similarities and differences between proteins and language, and review a range of protein-related tasks amenable to machine learning. We present methods for encoding the information of proteins as text and analyzing it with NLP methods, reviewing classic concepts such as bag-of-words, k-mers/n-grams and text search, as well as modern techniques such as word embedding, contextualized embedding, deep learning and neural language models. In particular, we focus on recent innovations such as masked language modeling, self-supervised learning and attention-based models. Finally, we discuss trends and challenges in the intersection of NLP and protein research. © 2021 The Author(s)</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103692698&amp;doi=10.1016%2fj.csbj.2021.03.022&amp;partnerID=40&amp;md5=0c846d5dfe80765b2cc01f4a16474c5a</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier B.V.</dc:description>
        <bib:pages>1750-1758</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:20010370%20(ISSN)">
        <prism:volume>19</prism:volume>
        <dc:title>Computational and Structural Biotechnology Journal</dc:title>
        <dc:identifier>DOI 10.1016/j.csbj.2021.03.022</dc:identifier>
        <dcterms:alternative>Comput. Struct. Biotechnol. J.</dcterms:alternative>
        <dc:identifier>ISSN 20010370 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1542">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 186; Correspondence Address: N. Brandes; The Rachel and Selim Benin School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem, Israel; email: nadav.brandes@mail.huji.ac.il&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2387">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:10495258%20(ISSN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2020-December</prism:volume>
                <dc:identifier>ISBN 10495258 (ISSN)</dc:identifier>
                <dc:title>Adv. neural inf. proces. syst.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Neural information processing systems foundation</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wei</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dong</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bao</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1543"/>
        <dcterms:isReferencedBy rdf:resource="#item_2325"/>
        <dc:subject>State of the art</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Real-life applications</dc:subject>
        <dc:subject>Distillation</dc:subject>
        <dc:subject>Transformer models</dc:subject>
        <dc:subject>Capacity constraints</dc:subject>
        <dc:subject>Distilleries</dc:subject>
        <dc:subject>Effective approaches</dc:subject>
        <dc:subject>Student Models</dc:subject>
        <dc:subject>Students</dc:subject>
        <dc:subject>Transformer parameters</dc:subject>
        <dc:title>MINILM: Deep self-attention distillation for task-agnostic compression of pre-trained transformers</dc:title>
        <dcterms:abstract>Pre-trained language models (e.g., BERT [12] and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer [42] based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant [26] also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model2 outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models. © 2020 Neural information processing systems foundation. All rights reserved.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101000449&amp;partnerID=40&amp;md5=8b792cd5c3a9e5570b7b34d683aec21c</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Adv. neural inf. proces. syst.</dc:description>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Advances in Neural Information Processing Systems</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1543">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 522; Correspondence Address: F. Wei; Microsoft Research; email: fuwei@microsoft.com; Conference name: 34th Conference on Neural Information Processing Systems, NeurIPS 2020; Conference date: 6 December 2020 through 12 December 2020; Conference code: 169463&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2325">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097333449&amp;partnerID=40&amp;md5=45a008708bb4033f27b1b4f818cc9724">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2020-December</prism:volume>
                <dc:identifier>ISBN 10495258 (ISSN)</dc:identifier>
                <dc:title>Adv. neural inf. proces. syst.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Neural information processing systems foundation</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zaheer</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guruganesh</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dubey</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ainslie</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alberti</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ontanon</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pham</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ravula</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ahmed</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1544"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Learning models</dc:subject>
        <dc:subject>Attention mechanisms</dc:subject>
        <dc:subject>Attention model</dc:subject>
        <dc:subject>Novel applications</dc:subject>
        <dc:subject>Sequence lengths</dc:subject>
        <dc:subject>Turing-complete</dc:subject>
        <dc:subject>Universal approximators</dc:subject>
        <dc:title>Big bird: Transformers for longer sequences</dc:title>
        <dcterms:abstract>Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BIGBIRD, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BIGBIRD is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BIGBIRD drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data. © 2020 Neural information processing systems foundation. All rights reserved.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097333449&amp;partnerID=40&amp;md5=45a008708bb4033f27b1b4f818cc9724</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Adv. neural inf. proces. syst.</dc:description>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Advances in Neural Information Processing Systems</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1544">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 996; Conference name: 34th Conference on Neural Information Processing Systems, NeurIPS 2020; Conference date: 6 December 2020 through 12 December 2020; Conference code: 169463&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080840963&amp;doi=10.1093%2fbioinformatics%2fbtz682&amp;partnerID=40&amp;md5=b8f287a6d1ca7da47309e2d4303e74e4">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:13674803%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yoon</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>So</foaf:surname>
                        <foaf:givenName>C.H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kang</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1547"/>
        <dcterms:isReferencedBy rdf:resource="#item_2272"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>Software</dc:subject>
        <dc:subject>data mining</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>article</dc:subject>
        <dc:subject>software</dc:subject>
        <dc:subject>mining</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>language</dc:subject>
        <dc:subject>Language</dc:subject>
        <dc:subject>human experiment</dc:subject>
        <dc:subject>Data Mining</dc:subject>
        <dc:title>BioBERT: A pre-trained biomedical language representation model for biomedical text mining</dc:title>
        <dcterms:abstract>Motivation: Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. Results: We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. © 2020 Oxford University Press. All rights reserved.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080840963&amp;doi=10.1093%2fbioinformatics%2fbtz682&amp;partnerID=40&amp;md5=b8f287a6d1ca7da47309e2d4303e74e4</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Oxford University Press</dc:description>
        <bib:pages>1234-1240</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:13674803%20(ISSN)">
        <prism:volume>36</prism:volume>
        <dc:title>Bioinformatics</dc:title>
        <dc:identifier>DOI 10.1093/bioinformatics/btz682</dc:identifier>
        <prism:number>4</prism:number>
        <dcterms:alternative>Bioinformatics</dcterms:alternative>
        <dc:identifier>ISSN 13674803 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1547">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 3495; Correspondence Address: J. Kang; Department of Computer Science and Engineering, Korea University, Seoul, 02841, South Korea; email: kangj@korea.ac.kr; CODEN: BOINF&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2272">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-195214890-3%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-195214890-3 (ISBN)</dc:identifier>
                <dc:title>Findings Assoc. Comp. Linguist. Findings ACL: EMNLP</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cui</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Che</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qin</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hu</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1546"/>
        <dcterms:isReferencedBy rdf:resource="#item_2369"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Simple++</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Performance</dc:subject>
        <dc:subject>State-of-the-art performance</dc:subject>
        <dc:subject>Chinese natural language processing</dc:subject>
        <dc:subject>Non-English languages</dc:subject>
        <dc:title>Revisiting pre-trained models for Chinese natural language processing</dc:title>
        <dcterms:abstract>Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we target on revisiting Chinese pre-trained language models to examine their effectiveness in a non-English language and release the Chinese pre-trained language model series to the community. We also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways, especially the masking strategy that adopts MLM as correction (Mac). We carried out extensive experiments on eight Chinese NLP tasks to revisit the existing pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research. © 2020 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118125136&amp;partnerID=40&amp;md5=8add7b76dc76071b3ab08c1a3af488bc</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Findings Assoc. Comp. Linguist. Findings ACL: EMNLP</dc:description>
        <bib:pages>657-668</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1546">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 453; Conference name: Findings of the Association for Computational Linguistics, ACL 2020: EMNLP 2020; Conference date: 16 November 2020 through 20 November 2020; Conference code: 172733&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2369">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-171382112-0%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>PartF168147-15</prism:volume>
                <dc:identifier>ISBN 978-171382112-0 (ISBN)</dc:identifier>
                <dc:title>Int. Conf. Machin. Learn., ICML</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>International Machine Learning Society (IMLS)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saleh</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>P.J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Daume H.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Singh A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1545"/>
        <dcterms:isReferencedBy rdf:resource="#item_2340"/>
        <dc:subject>Supervised learning</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>State of the art</dc:subject>
        <dc:subject>State-of-the-art performance</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Text summarization</dc:subject>
        <dc:subject>Human evaluation</dc:subject>
        <dc:subject>Human performance</dc:subject>
        <dc:subject>Multiple data sets</dc:subject>
        <dc:subject>Output sequences</dc:subject>
        <dc:subject>Systematic evaluation</dc:subject>
        <dc:title>PEGASUS: Pre-Training with extracted gap-sentences for abstractive summarization</dc:title>
        <dcterms:abstract>Recent work pre-Training Transformers with self-supervised objectives on large text corpora has shown great success when fine-Tuned on downstream NLP tasks including text summarization. However, pre-Training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-Training large Transformer-based encoder-decoder models on massive text corpora with a new selfsupervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-The-Art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-The-Art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets. © 2020 by the Authors All rights reserved.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105298429&amp;partnerID=40&amp;md5=0dd4c4265971fdbd66c48aa4eea5b366</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Int. Conf. Machin. Learn., ICML</dc:description>
        <bib:pages>11265-11276</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>37th International Conference on Machine Learning, ICML 2020</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1545">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 682; Correspondence Address: J. Zhang; Data Science Institute, Imperial College London, London, United Kingdom; email: jingqing.zhang15@imperial.ac.uk; Conference name: 37th International Conference on Machine Learning, ICML 2020; Conference date: 13 July 2020 through 18 July 2020; Conference code: 168147&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2340">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:0736587X%20(ISSN);%20978-195214825-5%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 0736587X (ISSN); 978-195214825-5 (ISBN)</dc:identifier>
                <dc:title>Proc. Annu. Meet. Assoc. Comput Linguist.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rahman</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hasan</foaf:surname>
                        <foaf:givenName>M.K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zadeh</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mao</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Morency</foaf:surname>
                        <foaf:givenName>L.-P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hoque</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1548"/>
        <dcterms:isReferencedBy rdf:resource="#item_2305"/>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Performance</dc:subject>
        <dc:subject>State-of-the-art performance</dc:subject>
        <dc:subject>Fine tuning</dc:subject>
        <dc:subject>Sentiment analysis</dc:subject>
        <dc:subject>Word representations</dc:subject>
        <dc:subject>Modeling languages</dc:subject>
        <dc:subject>Contextual modeling</dc:subject>
        <dc:subject>Contextual words</dc:subject>
        <dc:subject>Modal analysis</dc:subject>
        <dc:subject>Multi-modal</dc:subject>
        <dc:subject>Multi-modal information</dc:subject>
        <dc:subject>Multiple disciplines</dc:subject>
        <dc:title>Integrating multimodal information in large pretrained transformers</dc:title>
        <dcterms:abstract>Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. While finetuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication). Pre-trained models don't have the necessary components to accept two extra modalities of vision and acoustic. In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning. It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities. In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only finetuning of BERT and XLNet. On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community. © 2020 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114652770&amp;partnerID=40&amp;md5=fe44f7e01e600139f9319ca90beb6b28</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Annu. Meet. Assoc. Comput Linguist.</dc:description>
        <bib:pages>2359-2369</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the Annual Meeting of the Association for Computational Linguistics</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1548">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 389; Conference name: 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020; Conference date: 5 July 2020 through 10 July 2020; Conference code: 172533&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2305">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092733644&amp;partnerID=40&amp;md5=df1a6dc84cf71b099f2476907f7c8e17">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:15324435%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Raffel</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shazeer</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roberts</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Narang</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Matena</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>P.J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1549"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Transfer learning</dc:subject>
        <dc:subject>Unified framework</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Language understanding</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Text classification</dc:subject>
        <dc:subject>Multi-task learning</dc:subject>
        <dc:subject>Learning techniques</dc:subject>
        <dc:subject>Attentionbased models</dc:subject>
        <dc:subject>Language problems</dc:subject>
        <dc:subject>Systematic study</dc:subject>
        <dc:title>Exploring the limits of transfer learning with a unified text-to-text transformer</dc:title>
        <dcterms:abstract>Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new &quot;Colossal Clean Crawled Corpus&quot;, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code. ©2020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092733644&amp;partnerID=40&amp;md5=df1a6dc84cf71b099f2476907f7c8e17</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Microtome Publishing</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:15324435%20(ISSN)">
        <prism:volume>21</prism:volume>
        <dc:title>Journal of Machine Learning Research</dc:title>
        <dcterms:alternative>J. Mach. Learn. Res.</dcterms:alternative>
        <dc:identifier>ISSN 15324435 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1549">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 9121; Correspondence Address: C. Raffel; Google, Mountain View, 94043, United States; email: craffel@gmail.com&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150638039&amp;partnerID=40&amp;md5=f52cb8e8b59b88a0fb25f5e00ea3257e">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>Int. Conf. Learn. Represent., ICLR</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>International Conference on Learning Representations, ICLR</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Su</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhu</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wei</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dai</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1551"/>
        <dcterms:isReferencedBy rdf:resource="#item_2234"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Simple++</dc:subject>
        <dc:subject>Down-stream</dc:subject>
        <dc:subject>Linguistics</dc:subject>
        <dc:subject>Pre-training</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Image segmentation</dc:subject>
        <dc:subject>Input image</dc:subject>
        <dc:subject>Empirical analysis</dc:subject>
        <dc:subject>Generic representation</dc:subject>
        <dc:subject>Linguistic representations</dc:subject>
        <dc:subject>Region-of-interest</dc:subject>
        <dc:subject>Regions of interest</dc:subject>
        <dc:title>VL-BERT: PRE-TRAINING OF GENERIC VISUAL-LINGUISTIC REPRESENTATIONS</dc:title>
        <dcterms:abstract>We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and benefit the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark. Code is released at https://github.com/jackroos/VL-BERT. © 2020 8th International Conference on Learning Representations, ICLR 2020. All rights reserved.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150638039&amp;partnerID=40&amp;md5=f52cb8e8b59b88a0fb25f5e00ea3257e</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Int. Conf. Learn. Represent., ICLR</dc:description>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>8th International Conference on Learning Representations, ICLR 2020</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1551">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 438; Correspondence Address: J. Dai; Microsoft Research Asia; email: jifdai@microsoft.com; Conference name: 8th International Conference on Learning Representations, ICLR 2020; Conference code: 186995&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2234">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-195214862-0%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-195214862-0 (ISBN)</dc:identifier>
                <dc:title>EMNLP - Conf. Empir. Methods in Nat. Lang. Process., Proc. Syst. Demonstr.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Morris</foaf:surname>
                        <foaf:givenName>J.X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lifland</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yoo</foaf:surname>
                        <foaf:givenName>J.Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grigsby</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jin</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                   <foaf:Person><foaf:surname>Liu Q.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Schlangen D.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1552"/>
        <dcterms:isReferencedBy rdf:resource="#item_2384"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Data augmentation</dc:subject>
        <dc:subject>Goal functions</dc:subject>
        <dc:subject>Line of codes</dc:subject>
        <dc:subject>Model robustness</dc:subject>
        <dc:subject>Modeling accuracy</dc:subject>
        <dc:subject>Modeling performance</dc:subject>
        <dc:subject>Modular designs</dc:subject>
        <dc:subject>Search method</dc:subject>
        <dc:subject>Training modules</dc:subject>
        <dc:title>TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP</dc:title>
        <dcterms:abstract>While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack’s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness. TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack. © 2020 Association for Computational Linguistics.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150452032&amp;partnerID=40&amp;md5=c32bd9e1db2c93918c6167e9a7cb5e0f</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: EMNLP - Conf. Empir. Methods in Nat. Lang. Process., Proc. Syst. Demonstr.</dc:description>
        <bib:pages>119-126</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>EMNLP 2020 - Conference on Empirical Methods in Natural Language Processing, Proceedings of Systems Demonstrations</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1552">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 321; Conference name: 2020 System Demonstrations of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020; Conference date: 16 November 2020 through 20 November 2020; Conference code: 192531&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2384">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098839172&amp;doi=10.1162%2ftacl_a_00349&amp;partnerID=40&amp;md5=c14ea3cd386b168b85cd64948cf50d91">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2307387X%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rogers</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kovaleva</foaf:surname>
                        <foaf:givenName>O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rumshisky</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1550"/>
        <dcterms:isReferencedBy rdf:resource="#item_2240"/>
        <dc:subject>State of the art</dc:subject>
        <dc:subject>'current</dc:subject>
        <dc:subject>Learn+</dc:subject>
        <dc:subject>Overparameterization</dc:subject>
        <dc:subject>States of knowledge</dc:subject>
        <dc:title>A primer in bertology: What we know about how bert works</dc:title>
        <dcterms:abstract>Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research. © 2020 Association for Computational Linguistics.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098839172&amp;doi=10.1162%2ftacl_a_00349&amp;partnerID=40&amp;md5=c14ea3cd386b168b85cd64948cf50d91</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: MIT Press Journals</dc:description>
        <bib:pages>842-866</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2307387X%20(ISSN)">
        <prism:volume>8</prism:volume>
        <dc:title>Transactions of the Association for Computational Linguistics</dc:title>
        <dc:identifier>DOI 10.1162/tacl_a_00349</dc:identifier>
        <dcterms:alternative>Trans. Assoc. Comput. Linguist.</dcterms:alternative>
        <dc:identifier>ISSN 2307387X (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1550">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 815&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2240">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-195073790-1%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-195073790-1 (ISBN)</dc:identifier>
                <dc:title>EMNLP-IJCNLP - Conf. Empir. Methods Nat. Lang. Process. Int. Jt. Conf. Nat. Lang. Process., Proc. Conf.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computational Linguistics</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ghazvininejad</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levy</foaf:surname>
                        <foaf:givenName>O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zettlemoyer</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1554"/>
        <dcterms:isReferencedBy rdf:resource="#item_2319"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Forecasting</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Computer aided language translation</dc:subject>
        <dc:subject>State-of-the-art performance</dc:subject>
        <dc:subject>Modeling languages</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Auto-regressive</dc:subject>
        <dc:subject>Iterative decoding</dc:subject>
        <dc:subject>Least confidents</dc:subject>
        <dc:subject>Machine translation systems</dc:subject>
        <dc:subject>Number of iterations</dc:subject>
        <dc:subject>Parallel decoding</dc:subject>
        <dc:subject>Translation models</dc:subject>
        <dc:title>Mask-predict: Parallel decoding of conditional masked language models</dc:title>
        <dcterms:abstract>Most machine translation systems generate text autoregressively from left to right. We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation. This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about. By applying this strategy for a constant number of iterations, our model improves state-of-the-art performance levels for non-autoregressive and parallel decoding translation models by over 4 BLEU on average. It is also able to reach within about 1 BLEU point of a typical left-to-right transformer model, while decoding significantly faster.1. © 2019 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084305691&amp;partnerID=40&amp;md5=b69f7acba8e53f4e37c94a61d3881d34</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: EMNLP-IJCNLP - Conf. Empir. Methods Nat. Lang. Process. Int. Jt. Conf. Nat. Lang. Process., Proc. Conf.</dc:description>
        <bib:pages>6112-6121</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1554">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 344; Conference name: 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019; Conference date: 3 November 2019 through 7 November 2019; Conference code: 159367&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2319">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-151088698-8%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2019-June</prism:volume>
                <dc:identifier>ISBN 978-151088698-8 (ISBN)</dc:identifier>
                <dc:title>Int. Conf. Mach. Learn., ICML</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>International Machine Learning Society (IMLS)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Houlsby</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Giurgiu</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jastrzçbski</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Morrone</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>de Laroussilhe</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gesmundo</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Attariyan</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gelly</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1553"/>
        <dcterms:isReferencedBy rdf:resource="#item_2338"/>
        <dc:subject>Machine learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Transfer learning</dc:subject>
        <dc:subject>State-of-the-art performance</dc:subject>
        <dc:subject>Fine tuning</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Text classification</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Glues</dc:subject>
        <dc:subject>Gluing</dc:subject>
        <dc:subject>Parameter sharing</dc:subject>
        <dc:subject>Transfer mechanisms</dc:subject>
        <dc:title>Parameter-efficient transfer learning for NLP</dc:title>
        <dcterms:abstract>Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task. Copyright 2019 by the authors.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074716916&amp;partnerID=40&amp;md5=7da55455678b47674c4a36eccdc5aa5b</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Int. Conf. Mach. Learn., ICML</dc:description>
        <bib:pages>4944-4953</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>36th International Conference on Machine Learning, ICML 2019</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1553">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 480; Correspondence Address: N. Houlsby; Google Research; email: neilhoulsby@google.com; Conference name: 36th International Conference on Machine Learning, ICML 2019; Conference date: 9 June 2019 through 15 June 2019; Conference code: 156104&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2338">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086462845&amp;partnerID=40&amp;md5=1e2793f91021f33cad78362b32f5473d">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>32</prism:volume>
                <dc:identifier>ISBN 10495258 (ISSN)</dc:identifier>
                <dc:title>Adv. neural inf. proces. syst.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Neural information processing systems foundation</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Michel</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levy</foaf:surname>
                        <foaf:givenName>O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Neubig</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1555"/>
        <dcterms:isReferencedBy rdf:resource="#item_2269"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Statistical methods</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Attention mechanisms</dc:subject>
        <dc:subject>Accuracy Improvement</dc:subject>
        <dc:subject>Driving forces</dc:subject>
        <dc:subject>Greedy algorithms</dc:subject>
        <dc:subject>Memory efficiency</dc:subject>
        <dc:subject>Neural models</dc:subject>
        <dc:subject>Weighted averages</dc:subject>
        <dc:title>Are sixteen heads really better than one?</dc:title>
        <dcterms:abstract>Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art natural language processing (NLP) models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention “head” potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention1,. © 2019 Neural information processing systems foundation. All rights reserved.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086462845&amp;partnerID=40&amp;md5=1e2793f91021f33cad78362b32f5473d</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Adv. neural inf. proces. syst.</dc:description>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Advances in Neural Information Processing Systems</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1555">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 573; Conference name: 33rd Annual Conference on Neural Information Processing Systems, NeurIPS 2019; Conference date: 8 December 2019 through 14 December 2019; Conference code: 161263&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2269">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-195073713-0%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1</prism:volume>
                <dc:identifier>ISBN 978-195073713-0 (ISBN)</dc:identifier>
                <dc:title>NAACL HLT - Conf. N. Am. Chapter Assoc. Comput. Linguistics: Hum. Lang. Technol. - Proc. Conf.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qiu</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shao</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xue</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1556"/>
        <dcterms:isReferencedBy rdf:resource="#item_2377"/>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Training data</dc:subject>
        <dc:subject>Topology</dc:subject>
        <dc:subject>Long-range dependencies</dc:subject>
        <dc:subject>Adjacent nodes</dc:subject>
        <dc:subject>Connected structures</dc:subject>
        <dc:subject>Local compositions</dc:subject>
        <dc:subject>Model complexity</dc:subject>
        <dc:subject>Sparsification</dc:subject>
        <dc:subject>Star-shaped</dc:subject>
        <dc:subject>Stars</dc:subject>
        <dc:title>Star-transformer</dc:title>
        <dcterms:abstract>Although Transformer has achieved great successes on many NLP tasks, its heavy structure with fully-connected attention connections leads to dependencies on large training data. In this paper, we present Star-Transformer, a lightweight alternative by careful sparsification. To reduce model complexity, we replace the fully-connected structure with a star-shaped topology, in which every two non-adjacent nodes are connected through a shared relay node. Thus, complexity is reduced from quadratic to linear, while preserving the capacity to capture both local composition and long-range dependency. The experiments on four tasks (22 datasets) show that Star-Transformer achieved significant improvements against the standard Transformer for the modestly sized datasets. © 2019 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084287078&amp;partnerID=40&amp;md5=c8ebd55a81c9c9799aa985efba72a2be</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: NAACL HLT - Conf. N. Am. Chapter Assoc. Comput. Linguistics: Hum. Lang. Technol. - Proc. Conf.</dc:description>
        <bib:pages>1315-1325</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1556">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 145; Correspondence Address: X. Qiu; Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, China; email: xpqiu@fudan.edu.cn; Conference name: 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2019; Conference date: 2 June 2019 through 7 June 2019; Conference code: 159851&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2377">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076566125&amp;partnerID=40&amp;md5=12ae41add678ed22ea1b3ca38ee405e8">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1</prism:volume>
                <dc:identifier>ISBN 978-195073713-0 (ISBN)</dc:identifier>
                <dc:title>NAACL HLT - Conf. N. Am. Chapter Assoc. Comput. Linguistics: Hum. Lang. Technol. - Proc. Conf.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>N.F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gardner</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Belinkov</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Peters</foaf:surname>
                        <foaf:givenName>M.E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Smith</foaf:surname>
                        <foaf:givenName>N.A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1557"/>
        <dcterms:isReferencedBy rdf:resource="#item_2316"/>
        <dc:subject>State of the art</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Recurrent neural network (RNNs)</dc:subject>
        <dc:subject>Recurrent neural networks</dc:subject>
        <dc:subject>Multilayer neural networks</dc:subject>
        <dc:subject>Contextual words</dc:subject>
        <dc:subject>Fine grained</dc:subject>
        <dc:subject>Linguistic knowledge</dc:subject>
        <dc:subject>Monotonic trend</dc:subject>
        <dc:subject>Task-specific models</dc:subject>
        <dc:title>Linguistic knowledge and transferability of contextual representations</dc:title>
        <dcterms:abstract>Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results. © 2019 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076566125&amp;partnerID=40&amp;md5=12ae41add678ed22ea1b3ca38ee405e8</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: NAACL HLT - Conf. N. Am. Chapter Assoc. Comput. Linguistics: Hum. Lang. Technol. - Proc. Conf.</dc:description>
        <bib:pages>1073-1094</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1557">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 420; Conference name: 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2019; Conference date: 2 June 2019 through 7 June 2019; Conference code: 159851&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2316">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:10636919%20(ISSN);%20978-172813293-8%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2019-June</prism:volume>
                <dc:identifier>ISBN 10636919 (ISSN); 978-172813293-8 (ISBN)</dc:identifier>
                <dc:title>Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit</dc:title>
                <dc:identifier>DOI 10.1109/CVPR.2019.00344</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>IEEE Computer Society</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ni</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1559"/>
        <dcterms:isReferencedBy rdf:resource="#item_2327"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>State-of-the-art performance</dc:subject>
        <dc:subject>Deep Learning</dc:subject>
        <dc:subject>Computer vision</dc:subject>
        <dc:subject>Novel applications</dc:subject>
        <dc:subject>3D from Multiview and Sensors</dc:subject>
        <dc:subject>Camera streams</dc:subject>
        <dc:subject>Computation costs</dc:subject>
        <dc:subject>Effectiveness and efficiencies</dc:subject>
        <dc:subject>Multi-views</dc:subject>
        <dc:subject>Point sampling</dc:subject>
        <dc:subject>Set theory</dc:subject>
        <dc:subject>Training phase</dc:subject>
        <dc:title>Modeling point clouds with self-attention and gumbel subset sampling</dc:title>
        <dcterms:abstract>Geometric deep learning is increasingly important thanks to the popularity of 3D sensors. Inspired by the recent advances in NLP domain, the self-attention transformer is introduced to consume the point clouds. We develop Point Attention Transformers (PATs), using a parameter-efficient Group Shuffle Attention (GSA) to replace the costly Multi-Head Attention. We demonstrate its ability to process size-varying inputs, and prove its permutation equivariance. Besides, prior work uses heuristics dependence on the input data (e.g., Furthest Point Sampling) to hierarchically select subsets of input points. Thereby, we for the first time propose an end-to-end learnable and task-agnostic sampling operation, named Gumbel Subset Sampling (GSS), to select a representative subset of input points. Equipped with Gumbel-Softmax, it produces a 'soft' continuous subset in training phase, and a 'hard' discrete subset in test phase. By selecting representative subsets in a hierarchical fashion, the networks learn a stronger representation of the input sets with lower computation cost. Experiments on classification and segmentation benchmarks show the effectiveness and efficiency of our methods. Furthermore, we propose a novel application, to process event camera stream as point clouds, and achieve a state-of-the-art performance on DVS128 Gesture Dataset. © 2019 IEEE.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075823682&amp;doi=10.1109%2fCVPR.2019.00344&amp;partnerID=40&amp;md5=1ebd8bcff5e62d7d8f2160de9339fbf1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit</dc:description>
        <bib:pages>3318-3327</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1559">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 323; Correspondence Address: B. Ni; Shanghai Jiao Tong University, China; email: nibingbing@sjtu.edu.cn; Conference name: 32nd IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2019; Conference date: 16 June 2019 through 20 June 2019; Conference code: 156730; CODEN: PIVRE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2327">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-166542418-9%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-166542418-9 (ISBN)</dc:identifier>
                <dc:title>Proc. - Workshop Energy Effic. Mach. Learn. Cogn. Comput., EMC2-NIPS</dc:title>
                <dc:identifier>DOI 10.1109/EMC2-NIPS53020.2019.00016</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zafrir</foaf:surname>
                        <foaf:givenName>O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Boudoukh</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Izsak</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wasserblat</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1558"/>
        <dcterms:isReferencedBy rdf:resource="#item_2348"/>
        <dc:subject>Machine learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>transformers</dc:subject>
        <dc:subject>Transformer models</dc:subject>
        <dc:subject>Large amounts</dc:subject>
        <dc:subject>Accuracy loss</dc:subject>
        <dc:subject>bert</dc:subject>
        <dc:subject>Energy efficiency</dc:subject>
        <dc:subject>Integer programming</dc:subject>
        <dc:subject>language-modeling</dc:subject>
        <dc:subject>nlp</dc:subject>
        <dc:subject>Power resources</dc:subject>
        <dc:subject>Production environments</dc:subject>
        <dc:subject>quantization</dc:subject>
        <dc:subject>quantization-aware-training</dc:subject>
        <dc:subject>Quantized models</dc:subject>
        <dc:title>Q8BERT: Quantized 8Bit BERT</dc:title>
        <dcterms:abstract>Recently, pre-trained Transformer [1] based language models such as BERT [2] and GPT [3], have shown great improvement in many Natural Language Processing (NLP) tasks. However, these models contain a large amount of parameters. The emergence of even larger and more accurate models such as GPT2 [4] and Megatron11http://github.com/NVIDIA/Megatron-LM, suggest a trend of large pre-trained Transformer models. However, using these large models in production environments is a complex task requiring a large amount of compute, memory and power resources. In this work we show how to perform quantization-aware training during the fine-tuning phase of BERT in order to compress BERT by 4x with minimal accuracy loss. Furthermore, the produced quantized model can accelerate inference speed if it is optimized for 8bit Integer supporting hardware.  © 2019 IEEE.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095066609&amp;doi=10.1109%2fEMC2-NIPS53020.2019.00016&amp;partnerID=40&amp;md5=672a3896c47e1697636d933bf57f0180</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. - Workshop Energy Effic. Mach. Learn. Cogn. Comput., EMC2-NIPS</dc:description>
        <bib:pages>36-39</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings - 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing, EMC2-NIPS 2019</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1558">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 215; Conference name: 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing, EMC2-NIPS 2019; Conference code: 171033&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2348">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073146579&amp;doi=10.1016%2fj.ijmedinf.2019.103985&amp;partnerID=40&amp;md5=b7f6466cb659b75130a1d7793d9edc82">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:13865056%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ren</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qiu</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ma</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1561"/>
        <dcterms:isReferencedBy rdf:resource="#item_2291"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>Named entity recognition</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>support vector machine</dc:subject>
        <dc:subject>Machine learning</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Algorithms</dc:subject>
        <dc:subject>Learning algorithms</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>algorithm</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>Humans</dc:subject>
        <dc:subject>priority journal</dc:subject>
        <dc:subject>radiology</dc:subject>
        <dc:subject>female</dc:subject>
        <dc:subject>Female</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Article</dc:subject>
        <dc:subject>Diagnosis</dc:subject>
        <dc:subject>breast cancer</dc:subject>
        <dc:subject>Fine tuning</dc:subject>
        <dc:subject>Support vector machines</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Deep Learning</dc:subject>
        <dc:subject>Long short-term memory</dc:subject>
        <dc:subject>State-of-the-art methods</dc:subject>
        <dc:subject>Breast cancer</dc:subject>
        <dc:subject>Breast Cancer</dc:subject>
        <dc:subject>Breast Neoplasms</dc:subject>
        <dc:subject>breast tumor</dc:subject>
        <dc:subject>cancer center</dc:subject>
        <dc:subject>cancer surgery</dc:subject>
        <dc:subject>China</dc:subject>
        <dc:subject>Clinical information extraction</dc:subject>
        <dc:subject>Comprehensive information</dc:subject>
        <dc:subject>Conditional random field</dc:subject>
        <dc:subject>Diseases</dc:subject>
        <dc:subject>Fine-tuning BERT</dc:subject>
        <dc:subject>hospital discharge</dc:subject>
        <dc:subject>information model</dc:subject>
        <dc:subject>Information model</dc:subject>
        <dc:subject>Information Modeling</dc:subject>
        <dc:subject>Information theory</dc:subject>
        <dc:subject>major clinical study</dc:subject>
        <dc:subject>medical documentation</dc:subject>
        <dc:subject>pathology</dc:subject>
        <dc:subject>Patient treatment</dc:subject>
        <dc:subject>Random processes</dc:subject>
        <dc:subject>Support Vector Machine</dc:subject>
        <dc:title>Extracting comprehensive clinical information for breast cancer using deep learning methods</dc:title>
        <dcterms:abstract>Objective: Breast cancer is the most common malignant tumor among women. The diagnosis and treatment information of breast cancer patients is abundant in multiple types of clinical fields, including clinicopathological data, genotype and phenotype information, treatment information, and prognosis information. However, current studies are mainly focused on extracting information from one specific type of clinical field. This study defines a comprehensive information model to represent the whole-course clinical information of patients. Furthermore, deep learning approaches are used to extract the concepts and their attributes from clinical breast cancer documents by fine-tuning pretrained Bidirectional Encoder Representations from Transformers (BERT) language models. Materials and methods: The clinical corpus that was used in this study was from one 3A cancer hospital in China, consisting of the encounter notes, operation records, pathology notes, radiology notes, progress notes and discharge summaries of 100 breast cancer patients. Our system consists of two components: a named entity recognition (NER) component and a relation recognition component. For each component, we implemented deep learning-based approaches by fine-tuning BERT, which outperformed other state-of-the-art methods on multiple natural language processing (NLP) tasks. A clinical language model is first pretrained using BERT on a large-scale unlabeled corpus of Chinese clinical text. For NER, the context embeddings that were pretrained using BERT were used as the input features of the Bi-LSTM-CRF (Bidirectional long-short-memory-conditional random fields) model and were fine-tuned using the annotated breast cancer notes. Furthermore, we proposed an approach to fine-tune BERT for relation extraction. It was considered to be a classification problem in which the two entities that were mentioned in the input sentence were replaced with their semantic types. Results: Our best-performing system achieved F1 scores of 93.53% for the NER and 96.73% for the relation extraction. Additional evaluations showed that the deep learning-based approaches that fine-tuned BERT did outperform the traditional Bi-LSTM-CRF and CRF machine learning algorithms in NER and the attention-Bi-LSTM and SVM (support vector machines) algorithms in relation recognition. Conclusion: In this study, we developed a deep learning approach that fine-tuned BERT to extract the breast cancer concepts and their attributes. It demonstrated its superior performance compared to traditional machine learning algorithms, thus supporting its uses in broader NER and relation extraction tasks in the medical domain. © 2019 Elsevier B.V.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073146579&amp;doi=10.1016%2fj.ijmedinf.2019.103985&amp;partnerID=40&amp;md5=b7f6466cb659b75130a1d7793d9edc82</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier Ireland Ltd</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:13865056%20(ISSN)">
        <prism:volume>132</prism:volume>
        <dc:title>International Journal of Medical Informatics</dc:title>
        <dc:identifier>DOI 10.1016/j.ijmedinf.2019.103985</dc:identifier>
        <dcterms:alternative>Int. J. Med. Informatics</dcterms:alternative>
        <dc:identifier>ISSN 13865056 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1561">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 123; Correspondence Address: J. Ma; National Cancer Center/Cancer Hospital, Peking Union Medical College &amp; Chinese Academy of Medical Sciences, Beijing, China; email: majianhui@csco.org.cn; CODEN: IJMIF&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2291">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077809664&amp;doi=10.1109%2fACCESS.2019.2946594&amp;partnerID=40&amp;md5=64adef5d0222ffeadb825a2e0e85e4c1">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>7</prism:volume>
                <dc:title>IEEE Access</dc:title>
                <dc:identifier>DOI 10.1109/ACCESS.2019.2946594</dc:identifier>
                <dcterms:alternative>IEEE Access</dcterms:alternative>
                <dc:identifier>ISSN 21693536 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gao</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Feng</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Song</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1560"/>
        <dcterms:isReferencedBy rdf:resource="#item_2381"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Deep neural networks</dc:subject>
        <dc:subject>Neural networks</dc:subject>
        <dc:subject>State-of-the-art performance</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Embeddings</dc:subject>
        <dc:subject>Sentiment analysis</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>Accuracy Improvement</dc:subject>
        <dc:subject>Complex networks</dc:subject>
        <dc:subject>Complex neural networks</dc:subject>
        <dc:subject>Context independent</dc:subject>
        <dc:subject>Digital storage</dc:subject>
        <dc:subject>Feature engineerings</dc:subject>
        <dc:subject>Sentiment classification</dc:subject>
        <dc:title>Target-dependent sentiment classification with BERT</dc:title>
        <dcterms:abstract>Research on machine assisted text analysis follows the rapid development of digital media, and sentiment analysis is among the prevalent applications. Traditional sentiment analysis methods require complex feature engineering, and embedding representations have dominated leaderboards for a long time. However, the context-independent nature limits their representative power in rich context, hurting performance in Natural Language Processing (NLP) tasks. Bidirectional Encoder Representations from Transformers (BERT), among other pre-trained language models, beats existing best results in eleven NLP tasks (including sentence-level sentiment classification) by a large margin, which makes it the new baseline of text representation. As a more challenging task, fewer applications of BERT have been observed for sentiment classification at the aspect level. We implement three target-dependent variations of the BERTbase model, with positioned output at the target terms and an optional sentence with the target built in. Experiments on three data collections show that our TD-BERT model achieves new state-of-the-art performance, in comparison to traditional feature engineering methods, embedding-based models and earlier applications of BERT. With the successful application of BERT in many NLP tasks, our experiments try to verify if its context-aware representation can achieve similar performance improvement in aspect-based sentiment analysis. Surprisingly, coupling it with complex neural networks that used to work well with embedding representations does not show much value, sometimes with performance below the vanilla BERT-FC implementation. On the other hand, incorporation of target information shows stable accuracy improvement, and the most effective way of utilizing that information is displayed through the experiment. © 2019 IEEE.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077809664&amp;doi=10.1109%2fACCESS.2019.2946594&amp;partnerID=40&amp;md5=64adef5d0222ffeadb825a2e0e85e4c1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Institute of Electrical and Electronics Engineers Inc.</dc:description>
        <bib:pages>154290-154299</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1560">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 328; Correspondence Address: A. Feng; Department of Computer Science, Chengdu University of Information Technology, Chengdu, 610225, China; email: abraham.feng@gmail.com&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2381">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083663490&amp;partnerID=40&amp;md5=04b8908c5bdcbdaf988d79880ac86228">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-195073790-1 (ISBN)</dc:identifier>
                <dc:title>EMNLP-IJCNLP - Conf. Empir. Methods Nat. Lang. Process. Int. Jt. Conf. Nat. Lang. Process., Proc. Conf.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computational Linguistics</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tan</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bansal</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1562"/>
        <dcterms:isReferencedBy rdf:resource="#item_2317"/>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>State of the art</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Modeling languages</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Visual languages</dc:subject>
        <dc:subject>Signal encoding</dc:subject>
        <dc:subject>Cross modality</dc:subject>
        <dc:subject>Language semantics</dc:subject>
        <dc:subject>Model components</dc:subject>
        <dc:subject>Visual concept</dc:subject>
        <dc:subject>Visual reasoning</dc:subject>
        <dc:title>LXMert: Learning cross-modality encoder representations from transformers</dc:title>
        <dcterms:abstract>Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pretraining strategies significantly contribute to our strong results.1. © 2019 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083663490&amp;partnerID=40&amp;md5=04b8908c5bdcbdaf988d79880ac86228</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: EMNLP-IJCNLP - Conf. Empir. Methods Nat. Lang. Process. Int. Jt. Conf. Nat. Lang. Process., Proc. Conf.</dc:description>
        <bib:pages>5100-5111</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1562">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 1164; Conference name: 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019; Conference date: 3 November 2019 through 7 November 2019; Conference code: 159367&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2317">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:0736587X%20(ISSN);%20978-194808740-7%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 0736587X (ISSN); 978-194808740-7 (ISBN)</dc:identifier>
                <dc:title>Proc. Annu. Meet. Assoc. Comput Linguist.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Junczys-Dowmunt</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Heafield</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hoang</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grundkiewicz</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Aue</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1563"/>
        <dcterms:isReferencedBy rdf:resource="#item_2318"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Computer aided language translation</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Personnel training</dc:subject>
        <dc:subject>Autotuning</dc:subject>
        <dc:subject>Cost effective</dc:subject>
        <dc:subject>Cost effectiveness</dc:subject>
        <dc:subject>High performance modeling</dc:subject>
        <dc:subject>High quality</dc:subject>
        <dc:subject>Lower precision</dc:subject>
        <dc:subject>Matrix products</dc:subject>
        <dc:subject>Neural machine translation</dc:subject>
        <dc:subject>Precision matrix</dc:subject>
        <dc:subject>Student training</dc:subject>
        <dc:subject>Teachers'</dc:subject>
        <dc:title>Marian: Cost-effective High-Quality Neural Machine Translation in C++</dc:title>
        <dcterms:abstract>This paper describes the submissions of the “Marian” team to the WNMT 2018 shared task. We investigate combinations of teacher-student training, low-precision matrix products, auto-tuning and other methods to optimize the Transformer model on GPU and CPU. By further integrating these methods with the new averaging attention networks, a recently introduced faster Transformer variant, we create a number of high-quality, high-performance models on the GPU and CPU, dominating the Pareto frontier for this shared task. © 2018 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121997142&amp;partnerID=40&amp;md5=44ed6589faf202243d7c3ab3fef6d3ca</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Annu. Meet. Assoc. Comput Linguist.</dc:description>
        <bib:pages>129-135</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the Annual Meeting of the Association for Computational Linguistics</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1563">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 33; Conference name: ACL 2018 2nd Workshop on Neural Machine Translation and Generation, NMT 2018; Conference code: 173800&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2318">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-194808784-1%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-194808784-1 (ISBN)</dc:identifier>
                <dc:title>Proc. Conf. Empir. Methods Nat. Lang. Process., EMNLP</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computational Linguistics</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luan</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhai</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Riloff E.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Chiang D.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Hockenmaier J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Tsujii J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1565"/>
        <dcterms:isReferencedBy rdf:resource="#item_2301"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Translation (languages)</dc:subject>
        <dc:subject>State-of-the-art performance</dc:subject>
        <dc:subject>Linguistics</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Signal encoding</dc:subject>
        <dc:subject>Translation models</dc:subject>
        <dc:subject>French-english</dc:subject>
        <dc:subject>Parallel corpora</dc:subject>
        <dc:subject>Sentence level</dc:subject>
        <dc:subject>Two-step training</dc:subject>
        <dc:title>Improving the transformer translation model with document-level context</dc:title>
        <dcterms:abstract>Although the Transformer translation model (Vaswani et al., 2017) has achieved state-of-the-art performance in a variety of translation tasks, how to use document-level context to deal with discourse phenomena problematic for Transformer still remains a challenge. In this work, we extend the Transformer model with a new context encoder to represent document-level context, which is then incorporated into the original encoder and decoder. As large-scale document-level parallel corpora are usually not available, we introduce a two-step training method to take full advantage of abundant sentence-level parallel corpora and limited document-level parallel corpora. Experiments on the NIST Chinese-English datasets and the IWSLT French-English datasets show that our approach improves over Transformer significantly. 1 © 2018 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081752395&amp;partnerID=40&amp;md5=2bf76f524198439e655d194d060d3154</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Conf. Empir. Methods Nat. Lang. Process., EMNLP</dc:description>
        <bib:pages>533-542</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1565">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 173; Conference name: 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018; Conference date: 31 October 2018 through 4 November 2018; Conference code: 158085&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2301">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081718759&amp;partnerID=40&amp;md5=ea3bf3752444cd102be25ee169659f72">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-194808784-1 (ISBN)</dc:identifier>
                <dc:title>Proc. Conf. Empir. Methods Nat. Lang. Process., EMNLP</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computational Linguistics</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lei</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>S.I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dai</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Artzi</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Riloff E.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Chiang D.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Hockenmaier J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Tsujii J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1568"/>
        <dcterms:isReferencedBy rdf:resource="#item_2375"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Long short-term memory</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Convolutional model</dc:subject>
        <dc:subject>Neural architectures</dc:subject>
        <dc:subject>Parallelizing</dc:subject>
        <dc:subject>Speed up</dc:subject>
        <dc:subject>Sulfur determination</dc:subject>
        <dc:title>Simple recurrent units for highly parallelizable recurrence</dc:title>
        <dcterms:abstract>Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5-9x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model (Vaswani et al., 2017) on translation by incorporating SRU into the architecture.1 © 2018 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081718759&amp;partnerID=40&amp;md5=ea3bf3752444cd102be25ee169659f72</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Conf. Empir. Methods Nat. Lang. Process., EMNLP</dc:description>
        <bib:pages>4470-4481</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1568">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 157; Conference name: 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018; Conference date: 31 October 2018 through 4 November 2018; Conference code: 158085&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2375">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-194808771-1%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-194808771-1 (ISBN)</dc:identifier>
                <dc:title>EMNLP - EMNLP Workshop BlackboxNLP: Anal. Interpreting Neural Networks NLP, Proc. Workshop</dc:title>
                <dc:identifier>DOI 10.18653/v1/w18-5431</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Raganato</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tiedemann</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1569"/>
        <dcterms:isReferencedBy rdf:resource="#item_2263"/>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>State of the art</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Computer aided language translation</dc:subject>
        <dc:subject>Machine translations</dc:subject>
        <dc:subject>Performance</dc:subject>
        <dc:subject>Property</dc:subject>
        <dc:subject>Network architecture</dc:subject>
        <dc:subject>Syntactics</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Signal encoding</dc:subject>
        <dc:subject>Learn+</dc:subject>
        <dc:subject>Attention mechanisms</dc:subject>
        <dc:subject>Neural machine translation</dc:subject>
        <dc:subject>Dependency relation</dc:subject>
        <dc:subject>Sequence models</dc:subject>
        <dc:subject>Translation quality</dc:subject>
        <dc:title>An Analysis of Encoder Representations in Transformer-Based Machine Translation</dc:title>
        <dcterms:abstract>The attention mechanism is a successful technique in modern NLP, especially in tasks like machine translation. The recently proposed network architecture of the Transformer is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics. © 2018 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094006423&amp;doi=10.18653%2fv1%2fw18-5431&amp;partnerID=40&amp;md5=e997a453e33e026183c6ec9ad91eaeff</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: EMNLP - EMNLP Workshop BlackboxNLP: Anal. Interpreting Neural Networks NLP, Proc. Workshop</dc:description>
        <bib:pages>287-297</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>EMNLP 2018 - 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Proceedings of the 1st Workshop</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1569">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 191; Conference name: 1st Workshop on BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,&amp;nbsp;&amp;nbsp;co-located with the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018; Conference code: 173721&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2263">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122024415&amp;partnerID=40&amp;md5=789d5736c80e0f9fa234e482c49cb792">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 0736587X (ISSN); 978-194808740-7 (ISBN)</dc:identifier>
                <dc:title>Proc. Annu. Meet. Assoc. Comput Linguist.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Imamura</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sumita</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1567"/>
        <dcterms:isReferencedBy rdf:resource="#item_2334"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Computer aided language translation</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Machine translation systems</dc:subject>
        <dc:subject>Neural machine translation</dc:subject>
        <dc:subject>Sequence models</dc:subject>
        <dc:subject>Translation quality</dc:subject>
        <dc:subject>Self-training</dc:subject>
        <dc:subject>Self-training approaches</dc:subject>
        <dc:subject>Translation speed</dc:subject>
        <dc:title>NICT Self-Training Approach to Neural Machine Translation at NMT-2018</dc:title>
        <dcterms:abstract>This paper describes the NICT neural machine translation system submitted at the NMT-2018 shared task. A characteristic of our approach is the introduction of self-training. Since our self-training does not change the model structure, it does not influence the efficiency of translation, such as the translation speed. The experimental results showed that the translation quality improved not only in the sequence-to-sequence (seq-to-seq) models but also in the transformer models. © 2018 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122024415&amp;partnerID=40&amp;md5=789d5736c80e0f9fa234e482c49cb792</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Annu. Meet. Assoc. Comput Linguist.</dc:description>
        <bib:pages>110-115</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the Annual Meeting of the Association for Computational Linguistics</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1567">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 10; Conference name: ACL 2018 2nd Workshop on Neural Machine Translation and Generation, NMT 2018; Conference code: 173800&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2334">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-153862156-1%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-153862156-1 (ISBN)</dc:identifier>
                <dc:title>Int. Conf. Power Renew. Energy, ICPRE</dc:title>
                <dc:identifier>DOI 10.1109/ICPRE.2017.8390544</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dong</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liang</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1564"/>
        <dcterms:isReferencedBy rdf:resource="#item_2336"/>
        <dc:subject>Problem solving</dc:subject>
        <dc:subject>Acoustic generators</dc:subject>
        <dc:subject>Adaptive filtering</dc:subject>
        <dc:subject>Adaptive filters</dc:subject>
        <dc:subject>Electric load flow</dc:subject>
        <dc:subject>Iterative methods</dc:subject>
        <dc:subject>Linear search</dc:subject>
        <dc:subject>Linearization</dc:subject>
        <dc:subject>Nonlinear programming</dc:subject>
        <dc:subject>optimal power flow</dc:subject>
        <dc:subject>Optimal power flows</dc:subject>
        <dc:subject>power system</dc:subject>
        <dc:subject>Power System</dc:subject>
        <dc:subject>transformer tap</dc:subject>
        <dc:subject>Transformer taps</dc:subject>
        <dc:subject>trust region</dc:subject>
        <dc:subject>Trust region</dc:subject>
        <dc:subject>wolfe linear search</dc:subject>
        <dc:title>Optimal power flow algorithm based on self-adaptive filter-trust region method</dc:title>
        <dcterms:abstract>The Optimal Power Flow (OPF) problem for power systems could be expressed by a Non-Linear Programming (NLP) model, which is difficult to be solved. This study presents a practical algorithm to solve OPF through a successive linear programming (SLP), in which conventional power flow and linearized Sub-Problem (LSP) are solved by alternating iteration. A Self-Adaptive Filter-Trust Region (SAIFT) method is proposed to solve the LSP, in which the acceptance of the current iteration point is decided by a filter, and the step size is controlled by the trust region. The Wolfe linear search method is used to provide the direction of optimization when the consequence is dominated by filter. Furthermore, the linearization strategy of transformer ratio and the dynamic adjustment strategy of step size are proposed, which simplifies the formulation and improves the convergence of the algorithm. Case studies based on Ward-Hale 6-bus system demonstrate the efficacy of the new method in solving the OPF problems. © 2017 IEEE.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050349802&amp;doi=10.1109%2fICPRE.2017.8390544&amp;partnerID=40&amp;md5=9674fae4f13e0f446bd26825086b42a6</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Int. Conf. Power Renew. Energy, ICPRE</dc:description>
        <bib:pages>286-290</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2017 2nd International Conference on Power and Renewable Energy, ICPRE 2017</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1564">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 0; Correspondence Address: P. Zhao; School of Electrical Engineering, Shandong University, Jinan, 250061, China; email: zhaopenghui1993@163.com; Conference name: 2nd International Conference on Power and Renewable Energy, ICPRE 2017; Conference date: 20 September 2017 through 23 September 2017; Conference code: 137373&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2336">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-194808781-0%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1</prism:volume>
                <dc:identifier>ISBN 978-194808781-0 (ISBN)</dc:identifier>
                <dc:title>WMT - Conf. Mach. Transl., Proc. Conf.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sennrich</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nivre</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1566"/>
        <dcterms:isReferencedBy rdf:resource="#item_2262"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Computer aided language translation</dc:subject>
        <dc:subject>Decoding</dc:subject>
        <dc:subject>Signal encoding</dc:subject>
        <dc:subject>Word Sense Disambiguation</dc:subject>
        <dc:subject>Learn+</dc:subject>
        <dc:subject>Contextual information</dc:subject>
        <dc:subject>Encoder-decoder</dc:subject>
        <dc:subject>Attention mechanisms</dc:subject>
        <dc:subject>Neural machine translation</dc:subject>
        <dc:subject>Distribution patterns</dc:subject>
        <dc:subject>Hidden state</dc:subject>
        <dc:subject>Machine translation models</dc:subject>
        <dc:subject>Statistical machine translation</dc:subject>
        <dc:subject>Word alignment</dc:subject>
        <dc:title>An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation</dc:title>
        <dcterms:abstract>Recent work has shown that the encoder-decoder attention mechanisms in neural machine translation (NMT) are different from the word alignment in statistical machine translation. In this paper, we focus on analyzing encoder-decoder attention mechanisms, in the case of word sense disambiguation (WSD) in NMT models. We hypothesize that attention mechanisms pay more attention to context tokens when translating ambiguous words. We explore the attention distribution patterns when translating ambiguous nouns. Counter-intuitively, we find that attention mechanisms are likely to distribute more attention to the ambiguous noun itself rather than context tokens, in comparison to other nouns. We conclude that attention is not the main mechanism used by NMT models to incorporate contextual information for WSD. The experimental results suggest that NMT models learn to encode contextual information necessary for WSD in the encoder hidden states. For the attention mechanism in Transformer models, we reveal that the first few layers gradually learn to “align” source and target tokens and the last few layers learn to extract features from the related but unaligned context tokens. © 2018 Association for Computational Linguistics.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122316168&amp;partnerID=40&amp;md5=9fdeae812ebd02fa6fdbeebcc1e0f884</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: WMT - Conf. Mach. Transl., Proc. Conf.</dc:description>
        <bib:pages>26-35</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>WMT 2018 - 3rd Conference on Machine Translation, Proceedings of the Conference</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1566">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 49; Conference name: 3rd Conference on Machine Translation, WMT 2018 at the Conference on Empirical Methods in Natural Language Processing, EMNLP 2018; Conference date: 31 October 2018 through 1 November 2018; Conference code: 173792&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2262">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035244312&amp;doi=10.3390%2fen10030406&amp;partnerID=40&amp;md5=49d67eeb26ed1e574548abb148a2e90e">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:19961073%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wei</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dong</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1571"/>
        <dcterms:isReferencedBy rdf:resource="#item_2363"/>
        <dc:subject>Natural language processing (NLP)</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Brain</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Unstructured data</dc:subject>
        <dc:subject>Natural language text</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Data mining</dc:subject>
        <dc:subject>Long short-term memory</dc:subject>
        <dc:subject>Condition-based maintenance</dc:subject>
        <dc:subject>Electric power transmission networks</dc:subject>
        <dc:subject>Fault classification</dc:subject>
        <dc:subject>Inspection</dc:subject>
        <dc:subject>Long short-term memory (LSTM)</dc:subject>
        <dc:subject>Malfunction inspection report</dc:subject>
        <dc:subject>Operation and maintenance</dc:subject>
        <dc:subject>Power transformers</dc:subject>
        <dc:subject>Recurrent neural network (RNN)</dc:subject>
        <dc:subject>Unstructured texts</dc:subject>
        <dc:title>Research on unstructured text data mining and fault classification based on RNN-LSTM with malfunction inspection report</dc:title>
        <dcterms:abstract>This paper documents the condition-based maintenance (CBM) of power transformers, the analysis of which relies on two basic data groups: structured (e.g., numeric and categorical) and unstructured (e.g., natural language text narratives) which accounts for 80% of data required. However, unstructured data comprised of malfunction inspection reports, as recorded by operation and maintenance of the power grid, constitutes an abundant untapped source of power insights. This paper proposes a method for malfunction inspection report processing by deep learning, which combines the text data mining-oriented recurrent neural networks (RNN) with long short-term memory (LSTM). In this paper, the effectiveness of the RNN-LSTM network for modeling inspection data is established with a straightforward training strategy in which we replicate targets at each sequence step. Then, the corresponding fault labels are given in datasets, in order to calculate the accuracy of fault classification by comparison with the original data labels and output samples. Experimental results can reflect how key parameters may be selected in the configuration of the key variables to achieve optimal results. The accuracy of the fault recognition demonstrates that the method we proposed can provide a more effective way for grid inspection personnel to deal with unstructured data. © 2017 by the author.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035244312&amp;doi=10.3390%2fen10030406&amp;partnerID=40&amp;md5=49d67eeb26ed1e574548abb148a2e90e</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: MDPI AG</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:19961073%20(ISSN)">
        <prism:volume>10</prism:volume>
        <dc:title>Energies</dc:title>
        <dc:identifier>DOI 10.3390/en10030406</dc:identifier>
        <prism:number>3</prism:number>
        <dcterms:alternative>Energies</dcterms:alternative>
        <dc:identifier>ISSN 19961073 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1571">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 72; Correspondence Address: B. Wang; School of Electrical Engineering, Wuhan University, Wuhan, 430072, China; email: whwdwb@whu.edu.cn&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2363">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-150905963-8%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-150905963-8 (ISBN)</dc:identifier>
                <dc:title>Iran. Conf. Electr. Eng., ICEE</dc:title>
                <dc:identifier>DOI 10.1109/IranianCEE.2017.7985230</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hamidi</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nazarpour</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Golshannavaz</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1570"/>
        <dcterms:isReferencedBy rdf:resource="#item_2337"/>
        <dc:subject>Asymmetric operation</dc:subject>
        <dc:subject>Distribution management system (DMS)</dc:subject>
        <dc:subject>Distribution management systems</dc:subject>
        <dc:subject>Electric power distribution</dc:subject>
        <dc:subject>Electric vehicles</dc:subject>
        <dc:subject>Energy resources</dc:subject>
        <dc:subject>High penetration of photovoltaics (PVs)</dc:subject>
        <dc:subject>Operation characteristic</dc:subject>
        <dc:subject>Photovoltaics</dc:subject>
        <dc:subject>Plug-in electric vehicles</dc:subject>
        <dc:subject>Plug-in electric vehicles (PEVs)</dc:subject>
        <dc:subject>Renewable energy resources</dc:subject>
        <dc:subject>Smart charging</dc:subject>
        <dc:subject>Three-phase smart charging/discharging algorithm</dc:subject>
        <dc:subject>Unbalanced distribution network</dc:subject>
        <dc:subject>Unbalanced distribution networks</dc:subject>
        <dc:subject>Unbalanced operations</dc:subject>
        <dc:subject>Voltage unbalance factors</dc:subject>
        <dc:title>Optimal scheduling of unbalanced distribution networks to improve the contribution of renewables and network balancing performance</dc:title>
        <dcterms:abstract>High penetration of photovoltaic (PV) units in distribution networks along with unbalanced nature of residential loads deteriorates the voltage quality metrics and ends in unbalanced operations. The ever increasing number of plug-in electric vehicles (PEVs) and their smart charging/discharging processes provides a potential opportunity for lessening the voltage unbalance factor (VUF), operated independently at each of the single-phase's components. On the other hands, in different types of operation modes, collaboration of voltage conditioning devices such as under-load tap-changing (ULTC) transformer with PEVs and PV units necessitates adoption of efficient distribution management systems (DMSs). Contributing to this context, this study develops a DMS concentrating on various types of control methods say as symmetric and asymmetric operation of devices. The main goal is to achieve the best operation characteristics in unbalanced conditions and boosting the hosting capacity of renewable energy resources. The proposed approach is modeled as a non-linear problem (NLP) and tested on an IEEE 33-bus test network. Results are discussed in depth. © 2017 IEEE.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032806301&amp;doi=10.1109%2fIranianCEE.2017.7985230&amp;partnerID=40&amp;md5=8db3a2e866f6965ae1031b7a749534a9</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Iran. Conf. Electr. Eng., ICEE</dc:description>
        <bib:pages>1236-1241</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2017 25th Iranian Conference on Electrical Engineering, ICEE 2017</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1570">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 2; Conference name: 25th Iranian Conference on Electrical Engineering, ICEE 2017; Conference date: 2 May 2017 through 4 May 2017; Conference code: 129297&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2337">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198605397&amp;doi=10.1016%2fj.engappai.2024.108923&amp;partnerID=40&amp;md5=b132bc6cb195187c9248d199965cd04e">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:09521976%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mamun</foaf:surname>
                        <foaf:givenName>K.A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nabid</foaf:surname>
                        <foaf:givenName>R.A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pranto</foaf:surname>
                        <foaf:givenName>S.I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lamim</foaf:surname>
                        <foaf:givenName>S.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rahman</foaf:surname>
                        <foaf:givenName>M.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mahammed</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huda</foaf:surname>
                        <foaf:givenName>M.N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sarker</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khan</foaf:surname>
                        <foaf:givenName>R.R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1590"/>
        <dcterms:isReferencedBy rdf:resource="#item_2376"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Face recognition</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Speech recognition</dc:subject>
        <dc:subject>Character recognition</dc:subject>
        <dc:subject>Human computer interaction</dc:subject>
        <dc:subject>Speech synthesis</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Artificial intelligence based system</dc:subject>
        <dc:subject>Artificial intelligence based systems</dc:subject>
        <dc:subject>Automatic speech recognition</dc:subject>
        <dc:subject>Human robot interaction</dc:subject>
        <dc:subject>Human-computer interaction</dc:subject>
        <dc:subject>Man machine systems</dc:subject>
        <dc:subject>Real-life implementations</dc:subject>
        <dc:subject>Reception systems</dc:subject>
        <dc:subject>Receptionist system</dc:subject>
        <dc:subject>Service robots</dc:subject>
        <dc:title>Smart reception: An artificial intelligence driven bangla language based receptionist system employing speech, speaker, and face recognition for automating reception services</dc:title>
        <dcterms:abstract>In recent times, service robots (SR) have become widely accepted in a variety of fields as an alternative to traditional reception methods. Artificial Intelligence (AI) driven systems are seen as efficient labor alternatives, resulting in several SR models already being developed, primarily designed for English-speaking environments. Despite a large Bangla-speaking population, the exploration and implementation of Bangla language support within AI reception systems remain unexplored due to limited resources, posing significant challenges for deployment. This study presents a novel AI-enabled receptionist framework tailored to Bangla-speaking environments, addressing the limitation of Bangla language resources for developing automated reception systems. Leveraging advanced AI technologies including Face Recognition, Speaker Recognition, Automatic Speech Recognition (ASR), Text-to-Speech Synthesis, and Question Answering System, our integrated system demonstrates promise to automate Bangla reception systems. Our suite of models yields positive performance, with a face recognition accuracy of 99.38%, a speaker recognition system with 5.83 Equal Error Rate (ERR), ASR model achieving a Word Error Rate (WER) of 9.005%, TTS model scoring a Mean Opinion Score (MOS) of 4.10, and a question-answering system with a validation loss of 0.03%. Real-world evaluation among 1664 users in a university setting achieved over 75% user satisfaction, with 88% expressing interest in real-life implementation, showcasing usability across different domains. Limitations such as limited training data, scalability, and environmental sensitivity persist, underscoring the need for further development. Nevertheless, our framework demonstrates potential for real-life implementation, fostering human-robot interaction in Bangla-speaking contexts and paving the way for future innovations in AI-driven Bangla receptionist systems. © 2024 Elsevier Ltd</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198605397&amp;doi=10.1016%2fj.engappai.2024.108923&amp;partnerID=40&amp;md5=b132bc6cb195187c9248d199965cd04e</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier Ltd</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:09521976%20(ISSN)">
        <prism:volume>136</prism:volume>
        <dc:title>Engineering Applications of Artificial Intelligence</dc:title>
        <dc:identifier>DOI 10.1016/j.engappai.2024.108923</dc:identifier>
        <dcterms:alternative>Eng Appl Artif Intell</dcterms:alternative>
        <dc:identifier>ISSN 09521976 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1590">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 4; Correspondence Address: K.A. Mamun; AIMS Lab, IRIIC, United International University, Dhaka, Bangladesh; email: mamun@cse.uiu.ac.bd; CODEN: EAAIE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2376">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188586513&amp;doi=10.1145%2f3657631&amp;partnerID=40&amp;md5=2968a96946d197508eadf753fe4c63c4">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>56</prism:volume>
                <dc:title>ACM Computing Surveys</dc:title>
                <dc:identifier>DOI 10.1145/3657631</dc:identifier>
                <prism:number>9</prism:number>
                <dcterms:alternative>ACM Comput Surv</dcterms:alternative>
                <dc:identifier>ISSN 03600300 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Biancofiore</foaf:surname>
                        <foaf:givenName>G.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deldjoo</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Noia</foaf:surname>
                        <foaf:givenName>T.D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Di Sciascio</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Narducci</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1589"/>
        <dcterms:isReferencedBy rdf:resource="#item_2227"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Large language model</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Human computer interaction</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Speech processing</dc:subject>
        <dc:subject>large language model</dc:subject>
        <dc:subject>Literature reviews</dc:subject>
        <dc:subject>HTTP</dc:subject>
        <dc:subject>human-computer interaction</dc:subject>
        <dc:subject>Information seeking</dc:subject>
        <dc:subject>Interactive system</dc:subject>
        <dc:subject>interactive systems</dc:subject>
        <dc:subject>Question answering</dc:subject>
        <dc:title>Interactive Question Answering Systems: Literature Review</dc:title>
        <dcterms:abstract>Question-answering systems are recognized as popular and frequently effective means of information seeking on the web. In such systems, information seekers can receive a concise response to their queries by presenting their questions in natural language. Interactive question answering is a recently proposed and increasingly popular solution that resides at the intersection of question answering and dialogue systems. On the one hand, the user can ask questions in normal language and locate the actual response to her inquiry; on the other hand, the system can prolong the question-answering session into a dialogue if there are multiple probable replies, very few, or ambiguities in the initial request. By permitting the user to ask more questions, interactive question answering enables users to interact with the system and receive more precise results dynamically.This survey offers a detailed overview of the interactive question-answering methods that are prevalent in current literature. It begins by explaining the foundational principles of question-answering systems, hence defining new notations and taxonomies to combine all identified works inside a unified framework. The reviewed published work on interactive question-answering systems is then presented and examined in terms of its proposed methodology, evaluation approaches, and dataset/application domain. We also describe trends surrounding specific tasks and issues raised by the community, so shedding light on the future interests of scholars. Our work is further supported by a GitHub page synthesizing all the major topics covered in this literature study. https://sisinflab.github.io/interactive-question-answering-systems-survey/  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188586513&amp;doi=10.1145%2f3657631&amp;partnerID=40&amp;md5=2968a96946d197508eadf753fe4c63c4</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Association for Computing Machinery</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_1589">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 5; Correspondence Address: G.M. Biancofiore; Polytechnic University of Bari, Bari, Puglia, Italy; email: giovannimaria.Biancofiore@poliba.it; CODEN: ACSUE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2227">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185550710&amp;doi=10.1016%2fj.resconrec.2024.107497&amp;partnerID=40&amp;md5=dfafd31e61cbb849ff97cf0d5062e5bd">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:09213449%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mei</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1591"/>
        <dcterms:isReferencedBy rdf:resource="#item_2268"/>
        <dc:subject>Knowledge graphs</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>diagnosis</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Article</dc:subject>
        <dc:subject>information processing</dc:subject>
        <dc:subject>prediction</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>decision making</dc:subject>
        <dc:subject>Data handling</dc:subject>
        <dc:subject>Large language model</dc:subject>
        <dc:subject>Information management</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>large language model</dc:subject>
        <dc:subject>training</dc:subject>
        <dc:subject>data analysis</dc:subject>
        <dc:subject>Digital storage</dc:subject>
        <dc:subject>data processing</dc:subject>
        <dc:subject>knowledge</dc:subject>
        <dc:subject>accuracy assessment</dc:subject>
        <dc:subject>agricultural management</dc:subject>
        <dc:subject>agricultural production</dc:subject>
        <dc:subject>agricultural research</dc:subject>
        <dc:subject>agricultural technology</dc:subject>
        <dc:subject>agriculture</dc:subject>
        <dc:subject>Agriculture</dc:subject>
        <dc:subject>Agriculture management</dc:subject>
        <dc:subject>Agriculture productions</dc:subject>
        <dc:subject>alternative agriculture</dc:subject>
        <dc:subject>demand analysis</dc:subject>
        <dc:subject>information and communication technology</dc:subject>
        <dc:subject>Intelligent agriculture</dc:subject>
        <dc:subject>Knowledge graph</dc:subject>
        <dc:subject>Knowledge storage</dc:subject>
        <dc:subject>Large language models</dc:subject>
        <dc:subject>methodology</dc:subject>
        <dc:subject>pest control</dc:subject>
        <dc:subject>Production management</dc:subject>
        <dc:subject>Question answering system</dc:subject>
        <dc:subject>sustainable agriculture</dc:subject>
        <dc:subject>Sustainable management</dc:subject>
        <dc:subject>technology adoption</dc:subject>
        <dc:title>Application of question answering systems for intelligent agriculture production and sustainable management: A review</dc:title>
        <dcterms:abstract>The increasing application of artificial intelligence in agriculture production and management has generated a large amount of data, leading to a demand for processing this data. This review focuses on the knowledge storage approaches in agricultural question answering systems, namely corpora, knowledge graphs, and large language models. These systems are built on massive amounts of data and aim to process and retrieve information effectively in the context of sustainable agriculture. Corpora refer to large collections of diverse documents that serve as foundational resources for training and fine-tuning question answering systems. Knowledge graphs capture structured and interconnected knowledge by representing entities, relationships, and attributes, enabling efficient organization and querying of information. Large language models, such as GPT-4, enhance the capacity of question answering systems to provide accurate and relevant responses. By exploring these three prominent knowledge storage approaches, this review analyses the methodology and impact of agricultural question answering systems, highlighting their applications in the production process. The findings provide important implications for future research in agriculture, and potential directions for further exploration. © 2024</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185550710&amp;doi=10.1016%2fj.resconrec.2024.107497&amp;partnerID=40&amp;md5=dfafd31e61cbb849ff97cf0d5062e5bd</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier B.V.</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:09213449%20(ISSN)">
        <prism:volume>204</prism:volume>
        <dc:title>Resources, Conservation and Recycling</dc:title>
        <dc:identifier>DOI 10.1016/j.resconrec.2024.107497</dc:identifier>
        <dcterms:alternative>Resour. Conserv. Recycl.</dcterms:alternative>
        <dc:identifier>ISSN 09213449 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1591">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 8; Correspondence Address: H. Yu; National Innovation Center for Digital Fishery, Beijing, 100083, China; email: yuhh1990@bjfu.edu.cn; CODEN: RCREE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2268">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:03029743%20(ISSN);%20978-303143995-7%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>14228 LNCS</prism:volume>
                <dc:identifier>ISBN 03029743 (ISSN); 978-303143995-7 (ISBN)</dc:identifier>
                <dc:title>Lect. Notes Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-031-43996-4_38</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Springer Science and Business Media Deutschland GmbH</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bai</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Islam</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ren</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Greenspan H.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Greenspan H.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Madabhushi A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Mousavi P.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Salcudean S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Duncan J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Syeda-Mahmood T.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Taylor R.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1592"/>
        <dcterms:isReferencedBy rdf:resource="#item_2274"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Simple++</dc:subject>
        <dc:subject>Performance</dc:subject>
        <dc:subject>Embeddings</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Visual languages</dc:subject>
        <dc:subject>Learn+</dc:subject>
        <dc:subject>Feature extraction</dc:subject>
        <dc:subject>Academic work</dc:subject>
        <dc:subject>Clinical work</dc:subject>
        <dc:subject>Localised</dc:subject>
        <dc:subject>Medical students</dc:subject>
        <dc:subject>Robotic surgery</dc:subject>
        <dc:subject>Robotics surgery</dc:subject>
        <dc:title>CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery</dc:title>
        <dcterms:abstract>Medical students and junior surgeons often rely on senior surgeons and specialists to answer their questions when learning surgery. However, experts are often busy with clinical and academic work, and have little time to give guidance. Meanwhile, existing deep learning (DL)-based surgical Visual Question Answering (VQA) systems can only provide simple answers without the location of the answers. In addition, vision-language (ViL) embedding is still a less explored research in these kinds of tasks. Therefore, a surgical Visual Question Localized-Answering (VQLA) system would be helpful for medical students and junior surgeons to learn and understand from recorded surgical videos. We propose an end-to-end Transformer with the Co-Attention gaTed Vision-Language (CAT-ViL) embedding for VQLA in surgical scenarios, which does not require feature extraction through detection models. The CAT-ViL embedding module is designed to fuse multimodal features from visual and textual sources. The fused embedding will feed a standard Data-Efficient Image Transformer (DeiT) module, before the parallel classifier and detector for joint prediction. We conduct the experimental validation on public surgical videos from MICCAI EndoVis Challenge 2017 and 2018. The experimental results highlight the superior performance and robustness of our proposed model compared to the state-of-the-art approaches. Ablation studies further prove the outstanding performance of all the proposed components. The proposed method provides a promising solution for surgical scene understanding, and opens up a primary step in the Artificial Intelligence (AI)-based VQLA system for surgical training. Our code is available at github.com/longbai1006/CAT-ViL. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174709517&amp;doi=10.1007%2f978-3-031-43996-4_38&amp;partnerID=40&amp;md5=91ee0359c460712a11b8d945bb6832b3</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Lect. Notes Comput. Sci.</dc:description>
        <bib:pages>397-407</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1592">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 7; Correspondence Address: H. Ren; Department of Electronic Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong; email: hlren@ee.cuhk.edu.hk; Conference name: 26th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2023; Conference date: 8 October 2023 through 12 October 2023; Conference code: 302279&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2274">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177494424&amp;doi=10.1088%2f1674-1056%2fad04cb&amp;partnerID=40&amp;md5=930a136c8de667c0140d4e04f64ab442">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:16741056%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Z.-Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>F.-K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wan</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yuan</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Z.-G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Meng</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Y.-G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1593"/>
        <dcterms:isReferencedBy rdf:resource="#item_2321"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>materials science</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Forecasting</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>generative artificial intelligence</dc:subject>
        <dc:subject>Generative artificial intelligence</dc:subject>
        <dc:subject>Application services</dc:subject>
        <dc:subject>Chemical process</dc:subject>
        <dc:subject>Matchat</dc:subject>
        <dc:subject>MatChat</dc:subject>
        <dc:subject>Material science</dc:subject>
        <dc:subject>Science research</dc:subject>
        <dc:subject>Service platforms</dc:subject>
        <dc:subject>Synthesis pathways</dc:subject>
        <dc:subject>Text generations</dc:subject>
        <dc:title>MatChat: A large language model and application service platform for materials science</dc:title>
        <dcterms:abstract>The prediction of chemical synthesis pathways plays a pivotal role in materials science research. Challenges, such as the complexity of synthesis pathways and the lack of comprehensive datasets, currently hinder our ability to predict these chemical processes accurately. However, recent advancements in generative artificial intelligence (GAI), including automated text generation and question-answering systems, coupled with fine-tuning techniques, have facilitated the deployment of large-scale AI models tailored to specific domains. In this study, we harness the power of the LLaMA2-7B model and enhance it through a learning process that incorporates 13878 pieces of structured material knowledge data. This specialized AI model, named MatChat, focuses on predicting inorganic material synthesis pathways. MatChat exhibits remarkable proficiency in generating and reasoning with knowledge in materials science. Although MatChat requires further refinement to meet the diverse material design needs, this research undeniably highlights its impressive reasoning capabilities and innovative potential in materials science. MatChat is now accessible online and open for use, with both the model and its application framework available as open source. This study establishes a robust foundation for collaborative innovation in the integration of generative AI in materials science. © 2023 Chinese Physical Society and IOP Publishing Ltd.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177494424&amp;doi=10.1088%2f1674-1056%2fad04cb&amp;partnerID=40&amp;md5=930a136c8de667c0140d4e04f64ab442</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Institute of Physics</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:16741056%20(ISSN)">
        <prism:volume>32</prism:volume>
        <dc:title>Chinese Physics B</dc:title>
        <dc:identifier>DOI 10.1088/1674-1056/ad04cb</dc:identifier>
        <prism:number>11</prism:number>
        <dcterms:alternative>Chin. Phys.</dcterms:alternative>
        <dc:identifier>ISSN 16741056 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1593">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 6&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2321">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:979-840070103-0%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 979-840070103-0 (ISBN)</dc:identifier>
                <dc:title>Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min.</dc:title>
                <dc:identifier>DOI 10.1145/3580305.3599931</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computing Machinery</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lai</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zeng</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Du</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dong</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1595"/>
        <dcterms:isReferencedBy rdf:resource="#item_2233"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Pre-trained language model</dc:subject>
        <dc:subject>Information retrieval</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Human evaluation</dc:subject>
        <dc:subject>Cost effectiveness</dc:subject>
        <dc:subject>HTTP</dc:subject>
        <dc:subject>efficient retrieval enhancement system</dc:subject>
        <dc:subject>Efficient retrieval enhancement system</dc:subject>
        <dc:subject>human preference alignment</dc:subject>
        <dc:subject>Human preference alignment</dc:subject>
        <dc:subject>pre-trained language model</dc:subject>
        <dc:subject>Real world deployment</dc:subject>
        <dc:subject>Search and retrieval</dc:subject>
        <dc:subject>Web retrieval</dc:subject>
        <dc:subject>Web searches</dc:subject>
        <dc:title>WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences</dc:title>
        <dcterms:abstract>We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM). Its goal is to augment a pre-trained large language model (LLM) with web search and retrieval capabilities while being efficient for real-world deployments. To achieve this, we develop WebGLM with strategies for the LLM-augmented retriever, bootstrapped generator, and human preference-aware scorer. Specifically, we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages. In addition, we propose systematic criteria for evaluating web-enhanced QA systems. We conduct multi-dimensional human evaluation and quantitative ablation studies, which suggest the outperformance of the proposed WebGLM designs over existing systems. WebGLM with the 10-billion-parameter GLM (10B) is shown to perform better than the similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human evaluation. The code, demo, and data are at https://github.com/THUDM/WebGLM.  © 2023 ACM.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171328086&amp;doi=10.1145%2f3580305.3599931&amp;partnerID=40&amp;md5=ecbe2b5518603767b74eccfb2c4ec4ea</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min.</dc:description>
        <bib:pages>4549-4560</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1595">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 21; Correspondence Address: Y. Dong; Tsinghua University, Beijing, China; email: yuxiaod@tsinghua.edu.cn; J. Tang; Tsinghua University, Beijing, China; email: jietang@tsinghua.edu.cn; Conference name: 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023; Conference date: 6 August 2023 through 10 August 2023; Conference code: 191432&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2233">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159759354&amp;doi=10.1093%2fjamia%2focad050&amp;partnerID=40&amp;md5=761fc87a07c33adb556ba2e0e1558e64">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:10675027%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Soni</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Datta</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roberts</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1594"/>
        <dcterms:isReferencedBy rdf:resource="#item_2351"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>access to information</dc:subject>
        <dc:subject>Article</dc:subject>
        <dc:subject>electronic health record</dc:subject>
        <dc:subject>Electronic Health Records</dc:subject>
        <dc:subject>medical information</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>nonhuman</dc:subject>
        <dc:subject>clinical evaluation</dc:subject>
        <dc:subject>intensive care unit</dc:subject>
        <dc:subject>Access to Information</dc:subject>
        <dc:subject>analytical error</dc:subject>
        <dc:subject>classifier</dc:subject>
        <dc:subject>data accuracy</dc:subject>
        <dc:subject>data classification</dc:subject>
        <dc:subject>data extraction</dc:subject>
        <dc:subject>data integration</dc:subject>
        <dc:subject>data visualization</dc:subject>
        <dc:subject>descriptive research</dc:subject>
        <dc:subject>diabetes mellitus</dc:subject>
        <dc:subject>electronic health records</dc:subject>
        <dc:subject>FHIR</dc:subject>
        <dc:subject>gold</dc:subject>
        <dc:subject>Gold</dc:subject>
        <dc:subject>introspection</dc:subject>
        <dc:subject>medical terminology</dc:subject>
        <dc:subject>non insulin dependent diabetes mellitus</dc:subject>
        <dc:subject>prediction error</dc:subject>
        <dc:subject>question answering</dc:subject>
        <dc:subject>semantics</dc:subject>
        <dc:subject>structured questionnaire</dc:subject>
        <dc:title>quEHRy: a question answering system to query electronic health records</dc:title>
        <dcterms:abstract>Objective: We propose a system, quEHRy, to retrieve precise, interpretable answers to natural language questions from structured data in electronic health records (EHRs). Materials and Methods: We develop/synthesize the main components of quEHRy: concept normalization (MetaMap), time frame classification (new), semantic parsing (existing), visualization with question understanding (new), and query module for FHIR mapping/processing (new). We evaluate quEHRy on 2 clinical question answering (QA) datasets. We evaluate each component separately as well as holistically to gain deeper insights. We also conduct a thorough error analysis for a crucial subcomponent, medical concept normalization. Results: Using gold concepts, the precision of quEHRy is 98.33% and 90.91% for the 2 datasets, while the overall accuracy was 97.41% and 87.75%. Precision was 94.03% and 87.79% even after employing an automated medical concept extraction system (MetaMap). Most incorrectly predicted medical concepts were broader in nature than gold-annotated concepts (representative of the ones present in EHRs), eg, Diabetes versus Diabetes Mellitus, Non-Insulin-Dependent. Discussion: The primary performance barrier to deployment of the system is due to errors in medical concept extraction (a component not studied in this article), which affects the downstream generation of correct logical structures. This indicates the need to build QA-specific clinical concept normalizers that understand EHR context to extract the &quot;relevant&quot;medical concepts from questions. Conclusion: We present an end-to-end QA system that allows information access from EHRs using natural language and returns an exact, verifiable answer. Our proposed system is high-precision and interpretable, checking off the requirements for clinical use.  © 2023 The Author(s). Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159759354&amp;doi=10.1093%2fjamia%2focad050&amp;partnerID=40&amp;md5=761fc87a07c33adb556ba2e0e1558e64</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Oxford University Press</dc:description>
        <bib:pages>1091-1102</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:10675027%20(ISSN)">
        <prism:volume>30</prism:volume>
        <dc:title>Journal of the American Medical Informatics Association</dc:title>
        <dc:identifier>DOI 10.1093/jamia/ocad050</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>J. Am. Med. Informatics Assoc.</dcterms:alternative>
        <dc:identifier>ISSN 10675027 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1594">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 4; Correspondence Address: K. Roberts; School of Biomedical Informatics, University of Texas Health Science Center at Houston, Houston, 7000 Fannin Street, Suite 600, 77030, United States; email: kirk.roberts@uth.tmc.edu; CODEN: JAMAF&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2351">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164667350&amp;doi=10.1016%2fj.artmed.2023.102611&amp;partnerID=40&amp;md5=971f690ffb8a71e90042cda7ba4857bf">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:09333657%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tao</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shi</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Haffari</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ge</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1597"/>
        <dcterms:isReferencedBy rdf:resource="#item_2228"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>Artificial Intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>review</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>computer vision</dc:subject>
        <dc:subject>human experiment</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Computer vision</dc:subject>
        <dc:subject>Visual languages</dc:subject>
        <dc:subject>Medical imaging</dc:subject>
        <dc:subject>data source</dc:subject>
        <dc:subject>Data-source</dc:subject>
        <dc:subject>Image interpretation</dc:subject>
        <dc:subject>Medical image interpretation</dc:subject>
        <dc:subject>Source data</dc:subject>
        <dc:subject>Visual question answering</dc:subject>
        <dc:title>Medical visual question answering: A survey</dc:title>
        <dcterms:abstract>Medical Visual Question Answering (VQA) is a combination of medical artificial intelligence and popular VQA challenges. Given a medical image and a clinically relevant question in natural language, the medical VQA system is expected to predict a plausible and convincing answer. Although the general-domain VQA has been extensively studied, the medical VQA still needs specific investigation and exploration due to its task features. In the first part of this survey, we collect and discuss the publicly available medical VQA datasets up-to-date about the data source, data quantity, and task feature. In the second part, we review the approaches used in medical VQA tasks. We summarize and discuss their techniques, innovations, and potential improvements. In the last part, we analyze some medical-specific challenges for the field and discuss future research directions. Our goal is to provide comprehensive and helpful information for researchers interested in the medical visual question answering field and encourage them to conduct further research in this field. © 2023 Elsevier B.V.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164667350&amp;doi=10.1016%2fj.artmed.2023.102611&amp;partnerID=40&amp;md5=971f690ffb8a71e90042cda7ba4857bf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier B.V.</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:09333657%20(ISSN)">
        <prism:volume>143</prism:volume>
        <dc:title>Artificial Intelligence in Medicine</dc:title>
        <dc:identifier>DOI 10.1016/j.artmed.2023.102611</dc:identifier>
        <dcterms:alternative>Artif. Intell. Med.</dcterms:alternative>
        <dc:identifier>ISSN 09333657 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1597">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 41; Correspondence Address: Z. Ge; Faculty of Information Technology, Monash University, Clayton, 3800, Australia; email: zongyuan.ge@monash.edu; CODEN: AIMEE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2228">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146170825&amp;doi=10.1002%2fwidm.1487&amp;partnerID=40&amp;md5=ad7a136881a906ae6cfcd1d877d6f840">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:19424787%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Budler</foaf:surname>
                        <foaf:givenName>L.C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gosak</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stiglic</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1598"/>
        <dcterms:isReferencedBy rdf:resource="#item_2366"/>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>Machine learning</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>conversational agents</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Machine-learning</dc:subject>
        <dc:subject>Chatbots</dc:subject>
        <dc:subject>Data mining</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>health care</dc:subject>
        <dc:subject>Health care</dc:subject>
        <dc:subject>Analysis and synthesis</dc:subject>
        <dc:subject>Conversational agents</dc:subject>
        <dc:subject>Evidence-based</dc:subject>
        <dc:subject>Fundamental concepts</dc:subject>
        <dc:subject>Literature search</dc:subject>
        <dc:subject>Thematic analysis</dc:subject>
        <dc:title>Review of artificial intelligence-based question-answering systems in healthcare</dc:title>
        <dcterms:abstract>Use of conversational agents, like chatbots, avatars, and robots is increasing worldwide. Yet, their effectiveness in health care is largely unknown. The aim of this advanced review was to assess the use and effectiveness of conversational agents in various fields of health care. A literature search, analysis, and synthesis were conducted in February 2022 in PubMed and CINAHL. The included evidence was analyzed narratively by employing the principles of thematic analysis. We reviewed articles on artificial intelligence-based question-answering systems in health care. Most of the identified articles report its effectiveness; less is known about its use. We outlined study findings and explored directions of future research, to provide evidence-based knowledge about artificial intelligence-based question-answering systems. This article is categorized under: Fundamental Concepts of Data and Knowledge &gt; Human Centricity and User Interaction Application Areas &gt; Health Care Technologies &gt; Artificial Intelligence. © 2023 The Authors. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals LLC.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146170825&amp;doi=10.1002%2fwidm.1487&amp;partnerID=40&amp;md5=ad7a136881a906ae6cfcd1d877d6f840</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: John Wiley and Sons Inc</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:19424787%20(ISSN)">
        <prism:volume>13</prism:volume>
        <dc:title>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</dc:title>
        <dc:identifier>DOI 10.1002/widm.1487</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>Wiley Interdiscip. Rev. Data Min. Knowl. Discov.</dcterms:alternative>
        <dc:identifier>ISSN 19424787 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1598">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 33; Correspondence Address: G. Stiglic; Faculty of Health Sciences, University of Maribor, Maribor, Zitna ulica 15, 2000, Slovenia; email: gregor.stiglic@um.si&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2366">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164418739&amp;doi=10.13998%2fj.cnki.issn1002-1248.23-0118&amp;partnerID=40&amp;md5=5feb8e8e930ad21cb423e0e646ce6b87">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:10021248%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zeng</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xia</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cai</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1599"/>
        <dcterms:isReferencedBy rdf:resource="#item_2304"/>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>AIGC</dc:subject>
        <dc:subject>cultural heritage</dc:subject>
        <dc:subject>digital ethics</dc:subject>
        <dc:subject>intelligent information processing of ancient books</dc:subject>
        <dc:subject>knowledge organization</dc:subject>
        <dc:subject>library</dc:subject>
        <dc:title>Information Resource Management Researchers' Thinking about the Opportunities and Challenges of AIGC</dc:title>
        <dcterms:abstract>With the explosive popularity of ChatGPT and the development of AI generated content (AIGC), the new generation of artificial intelligence (AI) technology has triggered people's imagination and discussion about the production mode of digital content and the industry transformation. At the same time, it also prompted more scholars' thinking beyond embracing the technological changes in the field information resource management. Based on this, we invited 6 experts to carry out related discussions from the perspective of the disciplinary construction and development of information resource management. (1) AIGC and the field of library and information service: The essence of the rapid development of AI technology lies in the improvement of people's ability in knowledge acquisition brought by the continuous enrichment of data resources and the improvement of quality. The professionals in the field of library and information service should fully understand their value and their role, give full play to the advantages of data resources in libraries, effectively utilize their expertise in knowledge organization and management, actively promote research and development of AI technology, and contribute their wisdom and solutions. (2) The disruptive change brought about by AIGC: The emergence of ChatGPT has brought subversive challenges to the library and information service field. The professionals in this field should actively promote the transformation of the AI era, seize the opportunity, and explore new growth points around knowledge production, resource discovery, knowledge relevance and scientific research ethics. In addition, they should give play to the advantages of our country's political system and intensive development policies, promote the transformation of information services to knowledge content generation service, and raise the knowledge productivity of the whole society. (3) AIGC and cultural heritage resources: ChatGPT has ushered in an era of model-driven content production and carry out innovation in the mode of intelligent service of cultural heritage resources supported by AIGC. In addition, this paper also analyzes the copyright ownership, real-time, interpretability, verifiability, trustworthiness, algorithm bias and cognitive misdirection of large-scale AIGCs. In the future, universal value rationality such as public order and good customs should be endowed with the AIGC, and the paradigm change in the field of cultural heritage should be actively welcomed and promoted. (4) AIGC and researches on intelligent information processing of ancient books: While ChatGPT plays a positive role in promoting intelligent information processing of ancient books and promoting cultural transmission and civilization inheritance, it also brings challenges in cultural value and national security, content security and information governance, information literacy and humanistic education. It is necessary to make efforts in the aspects like providing higher quality and larger scale of precise processing of ancient books in the future, so as to build a scientific, comprehensive and systematic system of information security and digital ethics. (5) AIGC and intelligent tool revolution: This paper analyzes the three elements of big data, giant model and huge computing power in the development of AIGC technology, as well as the four development directions of large and complete data volume, fine granularity of data analysis, multi-source and multi-mode semantic integration of semantic association, human-computer integration of information services and intelligent interaction. It is pointed out that AIGC will bring about the paradigm change of content production in three aspects: production innovation, automatic generation and ecological cultivation. At the same time, it will also bring challenges in technology monopoly and hegemony, network information security technology integration and other aspects. (6) AIGC and library: Based on the analysis of the core technology innovation trend in the ChatGPT model expansion, data enhancement and model fusion, this paper demonstrates the necessity and crisis of using ChatGPT in libraries. In addition, from five aspects of intelligent management system, retrieval system, recommendation system, question answering system and search engine, this paper puts forward some suggestions on the use of GPT in library service integration and innovation promotion. © 2023 Agricultural Information Institute, Chinese Academy of Agricultural Sciences. All rights reserved.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>Chinese</z:language>
        <z:shortTitle>回应AIGC的信息资源管理学人思考</z:shortTitle>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164418739&amp;doi=10.13998%2fj.cnki.issn1002-1248.23-0118&amp;partnerID=40&amp;md5=5feb8e8e930ad21cb423e0e646ce6b87</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Agricultural Information Institute, Chinese Academy of Agricultural Sciences</dc:description>
        <bib:pages>5-28</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:10021248%20(ISSN)">
        <prism:volume>35</prism:volume>
        <dc:title>Journal of Library and Information Science in Agriculture</dc:title>
        <dc:identifier>DOI 10.13998/j.cnki.issn1002-1248.23-0118</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>J. Libr. Inf. Sci. Agric.</dcterms:alternative>
        <dc:identifier>ISSN 10021248 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1599">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 9; Correspondence Address: Z. Zhang; National Science Library, Chinese Academy of Sciences, Beijing, 100190, China; email: zhangzhx@mail.las.ac.cn&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2304">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148382067&amp;doi=10.1007%2fs11277-023-10199-5&amp;partnerID=40&amp;md5=ab8b6ca00b611ebff45326a8feaf894f">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:09296212%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>El-Ansari</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Beni-Hssane</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1596"/>
        <dcterms:isReferencedBy rdf:resource="#item_2372"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Search engines</dc:subject>
        <dc:subject>Chatbot</dc:subject>
        <dc:subject>Performance</dc:subject>
        <dc:subject>Chatbots</dc:subject>
        <dc:subject>Ontology</dc:subject>
        <dc:subject>Sentiment analysis</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Personalizations</dc:subject>
        <dc:subject>Ontology's</dc:subject>
        <dc:subject>Analysis techniques</dc:subject>
        <dc:subject>Customized information</dc:subject>
        <dc:subject>E-Commerce applications</dc:subject>
        <dc:subject>Electronic commerce</dc:subject>
        <dc:subject>Emotional state</dc:subject>
        <dc:subject>Personalization</dc:subject>
        <dc:title>Sentiment Analysis for Personalized Chatbots in E-Commerce Applications</dc:title>
        <dcterms:abstract>Chatbots and question-answering systems aim to provide precise answers to user inquiries, as opposed to simply providing a list of related documents as is typical of traditional search engines. To improve the chatbot’s performance, personalization techniques are employed to deliver customized information to each user based on their interests. Additionally, sentiment analysis techniques are utilized to better understand the user’s queries and emotional state. The goal is to better understand the user and his needs to deliver the required information. In this work, we propose a personalized chatbot enhanced with sentiment analysis features to provide useful and tailored information for each user. The paper details the system’s key components and the techniques employed for handling user queries. To evaluate the effectiveness of our system, we have implemented a proof-of-concept version as customer service chatbot in e-commerce applications. The experimental results obtained based on users’ feedback indicate a significant improvement in user satisfaction with the customized answers and the overall experience. Overall, this system demonstrates the potential of utilizing sentiment analysis in conjunction with personalization techniques to improve the performance and effectiveness of chatbots and question-answering systems. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148382067&amp;doi=10.1007%2fs11277-023-10199-5&amp;partnerID=40&amp;md5=ab8b6ca00b611ebff45326a8feaf894f</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer</dc:description>
        <bib:pages>1623-1644</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:09296212%20(ISSN)">
        <prism:volume>129</prism:volume>
        <dc:title>Wireless Personal Communications</dc:title>
        <dc:identifier>DOI 10.1007/s11277-023-10199-5</dc:identifier>
        <prism:number>3</prism:number>
        <dcterms:alternative>Wireless Pers Commun</dcterms:alternative>
        <dc:identifier>ISSN 09296212 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1596">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 20; Correspondence Address: A. El-Ansari; MASI Laboratory, Computer Science Department, Polydisciplinary Faculty, Mohammed First University, Nador, Morocco; email: anas.elansari@gmail.com; CODEN: WPCOF&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2372">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153774131&amp;doi=10.3390%2fe25040639&amp;partnerID=40&amp;md5=5f3e5a6c7738a5f6c2a8437e32058469">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:10994300%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jia</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1600"/>
        <dcterms:isReferencedBy rdf:resource="#item_2382"/>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>ERNIE</dc:subject>
        <dc:subject>feature extraction</dc:subject>
        <dc:subject>insurance question answering community</dc:subject>
        <dc:subject>text matching</dc:subject>
        <dc:title>Text Matching in Insurance Question-Answering Community Based on an Integrated BiLSTM-TextCNN Model Fusing Multi-Feature</dc:title>
        <dcterms:abstract>Along with the explosion of ChatGPT, the artificial intelligence question-answering system has been pushed to a climax. Intelligent question-answering enables computers to simulate people’s behavior habits of understanding a corpus through machine learning, so as to answer questions in professional fields. How to obtain more accurate answers to personalized questions in professional fields is the core content of intelligent question-answering research. As one of the key technologies of intelligent question-answering, the accuracy of text matching is related to the development of the intelligent question-answering community. Aiming to solve the problem of polysemy of text, the Enhanced Representation through Knowledge Integration (ERNIE) model is used to obtain the word vector representation of text, which makes up for the lack of prior knowledge in the traditional word vector representation model. Additionally, there are also problems of homophones and polyphones in Chinese, so this paper introduces the phonetic character sequence of the text to distinguish them. In addition, aiming at the problem that there are many proper nouns in the insurance field that are difficult to identify, after conventional part-of-speech tagging, proper nouns are distinguished by especially defining their parts of speech. After the above three types of text-based semantic feature extensions, this paper also uses the Bi-directional Long Short-Term Memory (BiLSTM) and TextCNN models to extract the global features and local features of the text, respectively. It can obtain the feature representation of the text more comprehensively. Thus, the text matching model integrating BiLSTM and TextCNN fusing Multi-Feature (namely MFBT) is proposed for the insurance question-answering community. The MFBT model aims to solve the problems that affect the answer selection in the insurance question-answering community, such as proper nouns, nonstandard sentences and sparse features. Taking the question-and-answer data of the insurance library as the sample, the MFBT text-matching model is compared and evaluated with other models. The experimental results show that the MFBT text-matching model has higher evaluation index values, including accuracy, recall and F1, than other models. The model trained by historical search data can better help users in the insurance question-and-answer community obtain the answers they need and improve their satisfaction. © 2023 by the authors.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153774131&amp;doi=10.3390%2fe25040639&amp;partnerID=40&amp;md5=5f3e5a6c7738a5f6c2a8437e32058469</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: MDPI</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:10994300%20(ISSN)">
        <prism:volume>25</prism:volume>
        <dc:title>Entropy</dc:title>
        <dc:identifier>DOI 10.3390/e25040639</dc:identifier>
        <prism:number>4</prism:number>
        <dcterms:alternative>Entropy</dcterms:alternative>
        <dc:identifier>ISSN 10994300 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1600">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 4; Correspondence Address: Z. Li; School of Maritime Economics and Management, Dalian Maritime University, Dalian, 116026, China; email: leezhaohui@dlmu.edu.cn&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2382">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164381228&amp;doi=10.1109%2fACCESS.2023.3291592&amp;partnerID=40&amp;md5=ab749216c52dc9c0fefae829a07d7b0c">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>11</prism:volume>
                <dc:title>IEEE Access</dc:title>
                <dc:identifier>DOI 10.1109/ACCESS.2023.3291592</dc:identifier>
                <dcterms:alternative>IEEE Access</dcterms:alternative>
                <dc:identifier>ISSN 21693536 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tohma</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Okur</foaf:surname>
                        <foaf:givenName>H.I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kutlu</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sertbas</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1601"/>
        <dcterms:isReferencedBy rdf:resource="#item_2373"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Learning algorithms</dc:subject>
        <dc:subject>Search engines</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Intelligent robots</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Support vector machines</dc:subject>
        <dc:subject>Social networking (online)</dc:subject>
        <dc:subject>Data mining</dc:subject>
        <dc:subject>Sentiment analysis</dc:subject>
        <dc:subject>Question answering (information retrieval)</dc:subject>
        <dc:subject>Human robot interaction</dc:subject>
        <dc:subject>Man machine systems</dc:subject>
        <dc:subject>question answering</dc:subject>
        <dc:subject>Humans-robot interactions</dc:subject>
        <dc:subject>Online systems</dc:subject>
        <dc:subject>sentiment analysis</dc:subject>
        <dc:subject>Support vectors machine</dc:subject>
        <dc:subject>Vectors</dc:subject>
        <dc:title>Sentiment Analysis in Turkish Question Answering Systems: An Application of Human-Robot Interaction</dc:title>
        <dcterms:abstract>The use of the sentiment analysis technique, which aims to extract emotions and thoughts from texts, has become a remarkable research topic today, where the importance of human-robot interaction is gradually increasing. In this study, a new hybrid sentiment analysis model is proposed using machine learning algorithms to increase emotional performance for Turkish question and answer systems. In this context, as a first, we apply text preprocessing steps to the Turkish question-answer-emotion dataset. Subsequently, we convert the preprocessed question and answer texts into text vector form using Pretrained Turkish BERT Model and two different word representation methods, TF-IDF and word2vec. Additionally, we incorporate pre-determined polarity vectors containing the positive and negative scores of words into the question-answer text vector. As a result of this study, we propose a new hybrid sentiment analysis model. We separate vectorized and expanded question-answer text vectors into training and testing data and train and test them with machine learning algorithms. By employing this previously unused method in Turkish question-answering systems, we achieve an accuracy value of up to 91.05% in sentiment analysis. Consequently, this study contributes to making human-robot interactions in Turkish more realistic and sensitive. © 2013 IEEE.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164381228&amp;doi=10.1109%2fACCESS.2023.3291592&amp;partnerID=40&amp;md5=ab749216c52dc9c0fefae829a07d7b0c</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Institute of Electrical and Electronics Engineers Inc.</dc:description>
        <bib:pages>66522-66534</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1601">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 6; Correspondence Address: K. Tohma; Iskenderun Technical University, Department of Computer Engineering, Iskenderun, 31200, Turkey; email: kadir.tohma@iste.edu.tr; H.I. Okur; Iskenderun Technical University, Department of Computer Engineering, Iskenderun, 31200, Turkey; email: hibrahim.okur@iste.edu.tr&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2373">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123911965&amp;doi=10.1145%2f3468889&amp;partnerID=40&amp;md5=798fbcc6fbe7f19ada6178603ac81fc7">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:10468188%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fan</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cheng</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1602"/>
        <dcterms:isReferencedBy rdf:resource="#item_2241"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Neural-networks</dc:subject>
        <dc:subject>Surveys</dc:subject>
        <dc:subject>Knowledge based systems</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Data augmentation</dc:subject>
        <dc:subject>Natural language generation</dc:subject>
        <dc:subject>survey</dc:subject>
        <dc:subject>Input format</dc:subject>
        <dc:subject>natural language generation</dc:subject>
        <dc:subject>Natural languages texts</dc:subject>
        <dc:subject>Network-based</dc:subject>
        <dc:subject>Question generation</dc:subject>
        <dc:subject>Rule-based method</dc:subject>
        <dc:subject>Text structure</dc:subject>
        <dc:title>A Review on Question Generation from Natural Language Text</dc:title>
        <dcterms:abstract>Question generation is an important yet challenging problem in Artificial Intelligence (AI), which aims to generate natural and relevant questions from various input formats, e.g., natural language text, structure database, knowledge base, and image. In this article, we focus on question generation from natural language text, which has received tremendous interest in recent years due to the widespread applications such as data augmentation for question answering systems. During the past decades, many different question generation models have been proposed, from traditional rule-based methods to advanced neural network-based methods. Since there have been a large variety of research works proposed, we believe it is the right time to summarize the current status, learn from existing methodologies, and gain some insights for future development. In contrast to existing reviews, in this survey, we try to provide a more comprehensive taxonomy of question generation tasks from three different perspectives, i.e., the types of the input context text, the target answer, and the generated question. We take a deep look into existing models from different dimensions to analyze their underlying ideas, major design principles, and training strategies We compare these models through benchmark tasks to obtain an empirical understanding of the existing techniques. Moreover, we discuss what is missing in the current literature and what are the promising and desired future directions. © 2021 Association for Computing Machinery.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123911965&amp;doi=10.1145%2f3468889&amp;partnerID=40&amp;md5=798fbcc6fbe7f19ada6178603ac81fc7</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Association for Computing Machinery</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:10468188%20(ISSN)">
        <prism:volume>40</prism:volume>
        <dc:title>ACM Transactions on Information Systems</dc:title>
        <dc:identifier>DOI 10.1145/3468889</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>ACM Trans. Inf. Syst.</dcterms:alternative>
        <dc:identifier>ISSN 10468188 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1602">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 46; Correspondence Address: R. Zhang; CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; email: zhangruqing@ict.ac.cn; CODEN: ATISE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2241">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-166548425-1%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-166548425-1 (ISBN)</dc:identifier>
                <dc:title>Proc. Int. Conf. Electron. Renew. Syst., ICEARS</dc:title>
                <dc:identifier>DOI 10.1109/ICEARS53579.2022.9752326</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nagarhalli</foaf:surname>
                        <foaf:givenName>T.P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mhatre</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Patil</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Patil</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1603"/>
        <dcterms:isReferencedBy rdf:resource="#item_2388"/>
        <dc:subject>Artificial Intelligence</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Learning algorithms</dc:subject>
        <dc:subject>Search engines</dc:subject>
        <dc:subject>Machine Learning</dc:subject>
        <dc:subject>Machine translations</dc:subject>
        <dc:subject>Machine-learning</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Deep Learning</dc:subject>
        <dc:subject>Sentiment analysis</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Learning techniques</dc:subject>
        <dc:subject>Dialog System</dc:subject>
        <dc:subject>Dialogue systems</dc:subject>
        <dc:subject>Information Retrieval</dc:subject>
        <dc:subject>Machine Translation</dc:subject>
        <dc:subject>Natural language processing applications</dc:subject>
        <dc:subject>On-machines</dc:subject>
        <dc:subject>Question Answering System</dc:subject>
        <dc:subject>Research and development</dc:subject>
        <dc:subject>Sentiment Analysis</dc:subject>
        <dc:title>The Review of Natural Language Processing Applications with Emphasis on Machine Learning Implementations</dc:title>
        <dcterms:abstract>There is a renewed interest in the recent times for research and development in the field and subfield of Artificial Intelligence, and this is especially true for Machine Learning and Natural Language Processing. With its ability to make predictions by relying on the data rather than any explicit instructions Machine Learning and Deep Learning Techniques have been employed and adapted in many fields of study and businesses with great success of the different areas which have been revolutionised by the implementation of Machine Learning and Deep Learning Techniques, Natural Language Processing is one such field of study which has benefited the most by this adaptation of learning techniques. Not only in processing of natural language, the learning techniques have played a very positive role in the different applications of Natural Language Processing as well. The paper proposes to highlight the important role played by the learning technique in improving the accuracy of the different applications of Natural Language Processing. © 2022 IEEE.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128943216&amp;doi=10.1109%2fICEARS53579.2022.9752326&amp;partnerID=40&amp;md5=06b52d937f9bc5c38883b0a02fad0fbb</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Int. Conf. Electron. Renew. Syst., ICEARS</dc:description>
        <bib:pages>1353-1358</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the International Conference on Electronics and Renewable Systems, ICEARS 2022</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1603">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 10; Conference name: 2022 International Conference on Electronics and Renewable Systems, ICEARS 2022; Conference date: 16 March 2022 through 18 March 2022; Conference code: 178760&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2388">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131130897&amp;doi=10.1186%2fs12859-022-04751-6&amp;partnerID=40&amp;md5=cbee6ebe9ea6cb0d4c2d0aa62406a7dd">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:14712105%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Raza</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schwartz</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rosella</foaf:surname>
                        <foaf:givenName>L.C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1605"/>
        <dcterms:isReferencedBy rdf:resource="#item_2280"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>COVID-19</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Algorithms</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>algorithm</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>Humans</dc:subject>
        <dc:subject>Benchmarking</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>language</dc:subject>
        <dc:subject>Language</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>benchmarking</dc:subject>
        <dc:subject>Clinical research</dc:subject>
        <dc:subject>Question answering system</dc:subject>
        <dc:subject>CORD-19</dc:subject>
        <dc:subject>LitCOVID</dc:subject>
        <dc:subject>Long-COVID</dc:subject>
        <dc:subject>Medical experts</dc:subject>
        <dc:subject>Pipeline</dc:subject>
        <dc:subject>Post-COVID-19</dc:subject>
        <dc:subject>Reference standard</dc:subject>
        <dc:subject>Transformer model</dc:subject>
        <dc:title>CoQUAD: a COVID-19 question answering dataset system, facilitating research, benchmarking, and practice</dc:title>
        <dcterms:abstract>Background: Due to the growing amount of COVID-19 research literature, medical experts, clinical scientists, and researchers frequently struggle to stay up to date on the most recent findings. There is a pressing need to assist researchers and practitioners in mining and responding to COVID-19-related questions on time. Methods: This paper introduces CoQUAD, a question-answering system that can extract answers related to COVID-19 questions in an efficient manner. There are two datasets provided in this work: a reference-standard dataset built using the CORD-19 and LitCOVID initiatives, and a gold-standard dataset prepared by the experts from a public health domain. The CoQUAD has a Retriever component trained on the BM25 algorithm that searches the reference-standard dataset for relevant documents based on a question related to COVID-19. CoQUAD also has a Reader component that consists of a Transformer-based model, namely MPNet, which is used to read the paragraphs and find the answers related to a question from the retrieved documents. In comparison to previous works, the proposed CoQUAD system can answer questions related to early, mid, and post-COVID-19 topics. Results: Extensive experiments on CoQUAD Retriever and Reader modules show that CoQUAD can provide effective and relevant answers to any COVID-19-related questions posed in natural language, with a higher level of accuracy. When compared to state-of-the-art baselines, CoQUAD outperforms the previous models, achieving an exact match ratio score of 77.50% and an F1 score of 77.10%. Conclusion: CoQUAD is a question-answering system that mines COVID-19 literature using natural language processing techniques to help the research community find the most recent findings and answer any related questions. © 2022, The Author(s).</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131130897&amp;doi=10.1186%2fs12859-022-04751-6&amp;partnerID=40&amp;md5=cbee6ebe9ea6cb0d4c2d0aa62406a7dd</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: BioMed Central Ltd</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:14712105%20(ISSN)">
        <prism:volume>23</prism:volume>
        <dc:title>BMC Bioinformatics</dc:title>
        <dc:identifier>DOI 10.1186/s12859-022-04751-6</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>BMC Bioinform.</dcterms:alternative>
        <dc:identifier>ISSN 14712105 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1605">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 23; Correspondence Address: S. Raza; Public Health Ontario (PHO), Toronto, Canada; email: shaina.raza@oahpp.ca; CODEN: BBMIC&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2280">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:03029743%20(ISSN);%20978-303099738-0%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>13186 LNCS</prism:volume>
                <dc:identifier>ISBN 03029743 (ISSN); 978-303099738-0 (ISBN)</dc:identifier>
                <dc:title>Lect. Notes Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-99739-7_3</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Springer Science and Business Media Deutschland GmbH</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Askari</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Verberne</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pasi</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Hagen M.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Verberne S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Macdonald C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Seifert C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Balog K.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Nørvåg K.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Setty V.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1604"/>
        <dcterms:isReferencedBy rdf:resource="#item_2675"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Community question answering</dc:subject>
        <dc:subject>Computers</dc:subject>
        <dc:subject>Data collection</dc:subject>
        <dc:subject>Expert finding</dc:subject>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>Legal community</dc:subject>
        <dc:subject>Legal domains</dc:subject>
        <dc:subject>Legal expert finding</dc:subject>
        <dc:subject>Legal experts</dc:subject>
        <dc:subject>Legal IR</dc:subject>
        <dc:subject>Legal questions</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:title>Expert Finding in Legal Community Question Answering</dc:title>
        <dcterms:abstract>Expert finding has been well-studied in community question answering (QA) systems in various domains. However, none of these studies addresses expert finding in the legal domain, where the goal is for citizens to find lawyers based on their expertise. In the legal domain, there is a large knowledge gap between the experts and the searchers, and the content on the legal QA websites consist of a combination formal and informal communication. In this paper, we propose methods for generating query-dependent textual profiles for lawyers covering several aspects including sentiment, comments, and recency. We combine query-dependent profiles with existing expert finding methods. Our experiments are conducted on a novel dataset gathered from an online legal QA service. We discovered that taking into account different lawyer profile aspects improves the best baseline model. We make our dataset publicly available for future work. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128736818&amp;doi=10.1007%2f978-3-030-99739-7_3&amp;partnerID=40&amp;md5=d54220995645b585b35ec80d443483d3</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Lect. Notes Comput. Sci.</dc:description>
        <bib:pages>22-30</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1604">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 15; Correspondence Address: A. Askari; Leiden Institute of Advanced Computer Science, Leiden University, Leiden, Netherlands; email: a.askari@liacs.leidenuniv.nl; Conference name: 44th European Conference on Information Retrieval, ECIR 2022; Conference date: 10 April 2022 through 14 April 2022; Conference code: 276459&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2675">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser de acesso pago&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137758445&amp;doi=10.1007%2fs10115-022-01744-y&amp;partnerID=40&amp;md5=57b558f5cd815f5908bb19a6b5b1b273">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:02191377%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zaib</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>W.E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sheng</foaf:surname>
                        <foaf:givenName>Q.Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mahmood</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1606"/>
        <dcterms:isReferencedBy rdf:resource="#item_2224"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Surveys</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Conversational AI</dc:subject>
        <dc:subject>Unstructured data</dc:subject>
        <dc:subject>Knowledge based systems</dc:subject>
        <dc:subject>Large dataset</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Reading comprehension</dc:subject>
        <dc:subject>Question answering</dc:subject>
        <dc:subject>Conversational agents</dc:subject>
        <dc:subject>Conversational artificial intelligence</dc:subject>
        <dc:subject>Conversational machine reading comprehension</dc:subject>
        <dc:subject>Knowledge base</dc:subject>
        <dc:subject>Multi-turn</dc:subject>
        <dc:subject>Structured data</dc:subject>
        <dc:title>Conversational question answering: a survey</dc:title>
        <dcterms:abstract>Question answering (QA) systems provide a way of querying the information available in various formats including, but not limited to, unstructured and structured data in natural languages. It constitutes a considerable part of conversational artificial intelligence (AI) which has led to the introduction of a special research topic on conversational question answering (CQA), wherein a system is required to understand the given context and then engages in multi-turn QA to satisfy a user’s information needs. While the focus of most of the existing research work is subjected to single-turn QA, the field of multi-turn QA has recently grasped attention and prominence owing to the availability of large-scale, multi-turn QA datasets and the development of pre-trained language models. With a good amount of models and research papers adding to the literature every year recently, there is a dire need of arranging and presenting the related work in a unified manner to streamline future research. This survey is an effort to present a comprehensive review of the state-of-the-art research trends of CQA primarily based on reviewed papers over the recent years. Our findings show that there has been a trend shift from single-turn to multi-turn QA which empowers the field of Conversational AI from different perspectives. This survey is intended to provide an epitome for the research community with the hope of laying a strong foundation for the field of CQA. © 2022, The Author(s).</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137758445&amp;doi=10.1007%2fs10115-022-01744-y&amp;partnerID=40&amp;md5=57b558f5cd815f5908bb19a6b5b1b273</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer Science and Business Media Deutschland GmbH</dc:description>
        <bib:pages>3151-3195</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:02191377%20(ISSN)">
        <prism:volume>64</prism:volume>
        <dc:title>Knowledge and Information Systems</dc:title>
        <dc:identifier>DOI 10.1007/s10115-022-01744-y</dc:identifier>
        <prism:number>12</prism:number>
        <dcterms:alternative>Knowl. Inf. Systems. Syst.</dcterms:alternative>
        <dc:identifier>ISSN 02191377 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1606">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 54; Correspondence Address: M. Zaib; School of Computing, Faculty of Science and Engineering, Macquarie University, Sydney, 2109, Australia; email: munazza-zaib@hdr.mq.edu.au; Q.Z. Sheng; School of Computing, Faculty of Science and Engineering, Macquarie University, Sydney, 2109, Australia; email: michael.sheng@mq.edu.au&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2224">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124522335&amp;doi=10.1007%2fs41666-022-00114-1&amp;partnerID=40&amp;md5=f9fbc5269b957f488746c88baa8d5ece">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2509498X%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>C.C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1607"/>
        <dcterms:isReferencedBy rdf:resource="#item_2290"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Learning algorithms</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Forecasting</dc:subject>
        <dc:subject>Human intelligence</dc:subject>
        <dc:subject>Autonomous driving</dc:subject>
        <dc:subject>Diagnosis</dc:subject>
        <dc:subject>Explainable artificial intelligence</dc:subject>
        <dc:subject>Predictive models</dc:subject>
        <dc:subject>Human computer interaction</dc:subject>
        <dc:subject>Social media</dc:subject>
        <dc:subject>Natural language questions</dc:subject>
        <dc:subject>Health care</dc:subject>
        <dc:subject>Artificial intelligence learning</dc:subject>
        <dc:subject>Electronic commerce</dc:subject>
        <dc:subject>Decision support systems</dc:subject>
        <dc:subject>Health care informatics</dc:subject>
        <dc:subject>Healthcare informatics</dc:subject>
        <dc:subject>Human–computer interactions</dc:subject>
        <dc:subject>Machine learning algorithms</dc:subject>
        <dc:subject>Medical knowledge</dc:subject>
        <dc:subject>Predictive modeling</dc:subject>
        <dc:title>Explainable Artificial Intelligence for Predictive Modeling in Healthcare</dc:title>
        <dcterms:abstract>The principle behind artificial intelligence is mimicking human intelligence in the way that it can perform tasks, recognize patterns, or predict outcomes through learning from the acquired data of various sources. Artificial intelligence and machine learning algorithms have been widely used in autonomous driving, recommender systems in electronic commerce and social media, fintech, natural language understanding, and question answering systems. Artificial intelligence is also gradually changing the landscape of healthcare research (Yu et al. in Biomed Eng 2:719–731, 25). The rule-based approach that relied on the curation of medical knowledge and the construction of robust decision rules had drawn significant attention in diagnosing diseases and clinical decision support since half a century ago. In recent years, machine learning algorithms such as deep learning that can account for complex interactions between features is shown to be promising in predictive modeling in healthcare (Deo in Circulation 132:1920–1930, 26). Although many of these artificial intelligence and machine learning algorithms can achieve remarkably high performance, it is often difficult to be completely adopted in practical clinical environments due to the lack of explainability in some of these algorithms. Explainable artificial intelligence (XAI) is emerging to assist in the communication of internal decisions, behavior, and actions to health care professionals. Through explaining the prediction outcomes, XAI gains the trust of the clinicians as they may learn how to apply the predictive modeling in practical situations instead of blindly following the predictions. There are still many scenarios to explore how to make XAI effective in clinical settings due to the complexity of medical knowledge. © 2022, The Author(s), under exclusive licence to Springer Nature Switzerland AG.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124522335&amp;doi=10.1007%2fs41666-022-00114-1&amp;partnerID=40&amp;md5=f9fbc5269b957f488746c88baa8d5ece</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer Science and Business Media Deutschland GmbH</dc:description>
        <bib:pages>228-239</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2509498X%20(ISSN)">
        <prism:volume>6</prism:volume>
        <dc:title>Journal of Healthcare Informatics Research</dc:title>
        <dc:identifier>DOI 10.1007/s41666-022-00114-1</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>J. Healthc. Informatics Res.</dcterms:alternative>
        <dc:identifier>ISSN 2509498X (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1607">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 90; Correspondence Address: C.C. Yang; College of Computing and Informatics, Drexel University, Philadelphia, United States; email: chris.yang@drexel.edu&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2290">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-166545911-2%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-166545911-2 (ISBN)</dc:identifier>
                <dc:title>Int. Conf. Comput. Vis., Image Deep Learn. Int. Conf. Comput. Eng. Appl., CVIDL ICCEA</dc:title>
                <dc:identifier>DOI 10.1109/CVIDLICCEA56201.2022.9824408</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yin</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1608"/>
        <dcterms:isReferencedBy rdf:resource="#item_2360"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Attention mechanisms</dc:subject>
        <dc:subject>Statistical tests</dc:subject>
        <dc:subject>Question answering system</dc:subject>
        <dc:subject>attention mechanism</dc:subject>
        <dc:subject>BERT model</dc:subject>
        <dc:subject>Development directions</dc:subject>
        <dc:subject>Focus of researches</dc:subject>
        <dc:subject>Human language</dc:subject>
        <dc:subject>Machine intelligence</dc:subject>
        <dc:subject>Semantic levels</dc:subject>
        <dc:title>Research on Question Answering System Based on BERT Model</dc:title>
        <dcterms:abstract>The core of natural language processing is the science of how computers understand and respond to the influence of human language. This is also the main research direction in the field of machine intelligence development. Question answering system is one of the development directions. How to make the answer of question answering system more intelligent is the focus of research in recent years. Based on this, this thesis proposes a question answering system based on the BERT model that combines the co-attention mechanism and the self-attention mechanism. In order to better show the representation of questions and articles at their respective semantic levels, this article will choose BERT language to encode them. Attention and fusion are then performed horizontally and vertically at different levels of granularity between questions and paragraphs. Experiments show that the model has an average EM of 46.56 and an average F1 of 58.90 on all datasets. The average F1 for the development dataset is 54.85, and the average F1 for the test dataset is 62.95. The content answered by the question answering system based on the BERT model is basically reasonable, and the intelligence of the question answering system is effectively improved. © 2022 IEEE.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135417995&amp;doi=10.1109%2fCVIDLICCEA56201.2022.9824408&amp;partnerID=40&amp;md5=e5efb759bd3fc05a3bc3765d9fc1b26d</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Int. Conf. Comput. Vis., Image Deep Learn. Int. Conf. Comput. Eng. Appl., CVIDL ICCEA</dc:description>
        <bib:pages>68-71</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 3rd International Conference on Computer Vision, Image and Deep Learning and International Conference on Computer Engineering and Applications, CVIDL and ICCEA 2022</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1608">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 11; Correspondence Address: J. Yin; Jiangsu University of Science and Technology, School of Computer, ZhenJiang, China; email: 1206732070@qq.com; Conference name: 3rd International Conference on Computer Vision, Image and Deep Learning and International Conference on Computer Engineering and Applications, CVIDL and ICCEA 2022; Conference date: 20 May 2022 through 22 May 2022; Conference code: 181070&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2360">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123914457&amp;doi=10.1017%2fS0269888921000138&amp;partnerID=40&amp;md5=dc9fad4df0c27ca5d2b1911c2a6bf6b1">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:02698889%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Antoniou</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bassiliades</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1609"/>
        <dcterms:isReferencedBy rdf:resource="#item_2221"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Paper analysis</dc:subject>
        <dc:subject>Surveys</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Common features</dc:subject>
        <dc:subject>Framework/Architecture</dc:subject>
        <dc:subject>Prototype system</dc:subject>
        <dc:subject>Query languages</dc:subject>
        <dc:subject>System use</dc:subject>
        <dc:title>A survey on semantic question answering systems</dc:title>
        <dcterms:abstract>Recently, many question answering systems that derive answers from linked data repositories have been developed. The purpose of this survey is to identify the common features and approaches of the semantic question answering (SQA) systems, although many different and prototype systems have been designed. The SQA systems use a formal query language like SPARQL and knowledge of a specific vocabulary. This paper analyses different frameworks, architectures, or systems that perform SQA and classifies SQA systems based on different criteria.  ©</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123914457&amp;doi=10.1017%2fS0269888921000138&amp;partnerID=40&amp;md5=dc9fad4df0c27ca5d2b1911c2a6bf6b1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Cambridge University Press</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:02698889%20(ISSN)">
        <prism:volume>37</prism:volume>
        <dc:title>Knowledge Engineering Review</dc:title>
        <dc:identifier>DOI 10.1017/S0269888921000138</dc:identifier>
        <prism:number>3</prism:number>
        <dcterms:alternative>Knowl Eng Rev</dcterms:alternative>
        <dc:identifier>ISSN 02698889 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1609">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 16; CODEN: KEREE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2221">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125731549&amp;doi=10.1109%2fACCESS.2022.3155521&amp;partnerID=40&amp;md5=ec9792a9942aedcce2d3cf928699e276">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>10</prism:volume>
                <dc:title>IEEE Access</dc:title>
                <dc:identifier>DOI 10.1109/ACCESS.2022.3155521</dc:identifier>
                <dcterms:alternative>IEEE Access</dcterms:alternative>
                <dc:identifier>ISSN 21693536 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fuad</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Al-Yahya</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1610"/>
        <dcterms:isReferencedBy rdf:resource="#item_2246"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Task analysis</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>chatbots</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>AI systems</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Chatbots</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Speech processing</dc:subject>
        <dc:subject>Job analysis</dc:subject>
        <dc:subject>Dialogue systems</dc:subject>
        <dc:subject>Arabic language</dc:subject>
        <dc:subject>Arabic languages</dc:subject>
        <dc:subject>Conversational AI system</dc:subject>
        <dc:subject>Conversational AI systems</dc:subject>
        <dc:subject>License</dc:subject>
        <dc:subject>question answering systems</dc:subject>
        <dc:subject>Task-oriented</dc:subject>
        <dc:subject>Task-oriented dialog system</dc:subject>
        <dc:subject>task-oriented dialogue systems</dc:subject>
        <dc:title>Recent Developments in Arabic Conversational AI: A Literature Review</dc:title>
        <dcterms:abstract>Conversational AI is one of the most active research areas in AI, and it has gained more attention from academia as well as industry. Given recent advancements in several conversational AI systems in addition to the availability of several datasets, the aim of this study is to explore the landscape of Arabic -based conversational AI systems. In this work, we provide a thorough review of recent Arabic conversational AI systems. We group them into three categories based on their functionality: (1) question-answering (QA) systems, (2) task-oriented dialogue systems (DS), and (3) chatbots. Furthermore, we describe the common datasets used in building and evaluating conversational AI systems in Arabic. Few surveys have targeted the conversational AI field for the Arabic language, and we aim to cover this gap with this study. Our contribution focuses on reviewing and analyzing the literature in the field and highlighting future research directions towards human-like conversational AI systems in Arabic.  © 2013 IEEE.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125731549&amp;doi=10.1109%2fACCESS.2022.3155521&amp;partnerID=40&amp;md5=ec9792a9942aedcce2d3cf928699e276</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Institute of Electrical and Electronics Engineers Inc.</dc:description>
        <bib:pages>23842-23859</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1610">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 13; Correspondence Address: A. Fuad; Department of Information Technology, College of Computer and Information Sciences, King Saud University, Riyadh, 4545, Saudi Arabia; email: aabdulghni@ksu.edu.sa&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2246">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128788346&amp;doi=10.1007%2f978-3-030-99739-7_41&amp;partnerID=40&amp;md5=2f9e48c99acae3554260a344c24d84ad">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>13186 LNCS</prism:volume>
                <dc:identifier>ISBN 03029743 (ISSN); 978-303099738-0 (ISBN)</dc:identifier>
                <dc:title>Lect. Notes Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-99739-7_41</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Springer Science and Business Media Deutschland GmbH</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vachev</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hardalov</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Karadzhov</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Georgiev</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koychev</foaf:surname>
                        <foaf:givenName>I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nakov</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Hagen M.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Verberne S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Macdonald C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Seifert C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Balog K.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Nørvåg K.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Setty V.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1611"/>
        <dcterms:isReferencedBy rdf:resource="#item_2310"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Chatbots</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Education</dc:subject>
        <dc:subject>Computer aided instruction</dc:subject>
        <dc:subject>Online systems</dc:subject>
        <dc:subject>E-learning</dc:subject>
        <dc:subject>Educational process</dc:subject>
        <dc:subject>Industrial settings</dc:subject>
        <dc:subject>Knowledge-sharing</dc:subject>
        <dc:subject>Massive open online course</dc:subject>
        <dc:subject>MOOCs</dc:subject>
        <dc:subject>Multiple-choice questions</dc:subject>
        <dc:subject>Onboarding</dc:subject>
        <dc:subject>Self-assessment</dc:subject>
        <dc:title>Leaf: Multiple-Choice Question Generation</dc:title>
        <dcterms:abstract>Testing with quiz questions has proven to be an effective way to assess and improve the educational process. However, manually creating quizzes is tedious and time-consuming. To address this challenge, we present Leaf, a system for generating multiple-choice questions from factual text. In addition to being very well suited for the classroom, Leaf could also be used in an industrial setting, e.g., to facilitate onboarding and knowledge sharing, or as a component of chatbots, question answering systems, or Massive Open Online Courses (MOOCs). The code and the demo are available on GitHub (https://github.com/KristiyanVachev/Leaf-Question-Generation ). © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128788346&amp;doi=10.1007%2f978-3-030-99739-7_41&amp;partnerID=40&amp;md5=2f9e48c99acae3554260a344c24d84ad</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Lect. Notes Comput. Sci.</dc:description>
        <bib:pages>321-328</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1611">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 16; Correspondence Address: K. Vachev; Faculty of Mathematics and Informatics, Sofia University “St. Kliment Ohridski”, Sofia, Bulgaria; email: kdvachev@uni-sofia.bg; Conference name: 44th European Conference on Information Retrieval, ECIR 2022; Conference date: 10 April 2022 through 14 April 2022; Conference code: 276459&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2310">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:19341768%20(ISSN);%20978-988156380-4%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2021-July</prism:volume>
                <dc:identifier>ISBN 19341768 (ISSN); 978-988156380-4 (ISBN)</dc:identifier>
                <dc:title>Chinese Control Conf., CCC</dc:title>
                <dc:identifier>DOI 10.23919/CCC52363.2021.9550437</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>IEEE Computer Society</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ding</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yi</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                   <foaf:Person><foaf:surname>Peng C.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Sun J.</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1614"/>
        <dcterms:isReferencedBy rdf:resource="#item_2361"/>
        <dc:subject>Knowledge graphs</dc:subject>
        <dc:subject>COVID-19</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Query processing</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Knowledge graph</dc:subject>
        <dc:subject>Question Answering System</dc:subject>
        <dc:subject>Knowledge Graph</dc:subject>
        <dc:subject>Neo4j</dc:subject>
        <dc:subject>Rule-based classifier</dc:subject>
        <dc:subject>User's intentions</dc:subject>
        <dc:title>Research on Question Answering System for COVID-19 Based on Knowledge Graph</dc:title>
        <dcterms:abstract>In the con of the COVID-19 epidemic, it is necessary to establish a question answering (QA) system to help people to get information about the epidemic. This paper creats a framework about the question answering system based on knowledge bases (KBQA) for COVID-19. First, we design the schema of the COVID-19 knowledge graph (KG) and extract knowledge from s. Second, a rule-based classifier is created to find the user's intention when they input a question. Then, the matched template is used to transfer the question into Cypher query, and return answer which is retrieved from KG to users. We collected data from Baidu Encyclopedia and extracted triplet knowledge for constructing COVID-19 KG to prove the feasibility of our framework. The experiment on 86 questions shows that the KBQA for COVID-19 can achieve 81.6% accuracy and get a good experience through the feedback of testers. © 2021 Technical Committee on Control Theory, Chinese Association of Automation.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117321254&amp;doi=10.23919%2fCCC52363.2021.9550437&amp;partnerID=40&amp;md5=324754b93b0ee826d94c192044ec36af</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Chinese Control Conf., CCC</dc:description>
        <bib:pages>4659-4664</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>Chinese Control Conference, CCC</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1614">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 9; Conference name: 40th Chinese Control Conference, CCC 2021; Conference date: 26 July 2021 through 28 July 2021; Conference code: 172297&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2361">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:19449925%20(ISSN);%20978-166540507-2%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2021-July</prism:volume>
                <dc:identifier>ISBN 19449925 (ISSN); 978-166540507-2 (ISBN)</dc:identifier>
                <dc:title>IEEE Power Energy Soc. Gen. Meet.</dc:title>
                <dc:identifier>DOI 10.1109/PESGM46819.2021.9638018</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>IEEE Computer Society</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wei</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1615"/>
        <dcterms:isReferencedBy rdf:resource="#item_2266"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Knowledge graphs</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Search engines</dc:subject>
        <dc:subject>Data visualization</dc:subject>
        <dc:subject>Natural language questions</dc:subject>
        <dc:subject>Query processing</dc:subject>
        <dc:subject>Ontology's</dc:subject>
        <dc:subject>Domain Knowledge</dc:subject>
        <dc:subject>Knowledge graph</dc:subject>
        <dc:subject>Data reasoning</dc:subject>
        <dc:subject>Data searches</dc:subject>
        <dc:subject>intelligent question answering system</dc:subject>
        <dc:subject>Intelligent question answering systems</dc:subject>
        <dc:subject>intelligent reasoning</dc:subject>
        <dc:subject>Intelligent reasoning</dc:subject>
        <dc:subject>knowledge graph</dc:subject>
        <dc:subject>ontology schema</dc:subject>
        <dc:subject>Ontology schema</dc:subject>
        <dc:subject>Power/knowledge</dc:subject>
        <dc:subject>Search intentions</dc:subject>
        <dc:title>An Intelligent Question Answering System based on Power Knowledge Graph</dc:title>
        <dcterms:abstract>The intelligent question answering (IQA) system can accurately capture users' search intention by understanding the natural language questions, searching relevant content efficiently from a massive knowledge-base, and returning the answer directly to the user. Since the IQA system can save inestimable time and workforce in data search and reasoning, it has received more and more attention in data science and artificial intelligence. This article introduced a domain knowledge graph using the graph database and graph computing technologies from massive heterogeneous data in electric power. It then proposed an IQA system based on the electrical power knowledge graph to extract the intent and constraints of natural interrogation based on the natural language processing (NLP) method, to construct graph data query statements via knowledge reasoning, and to complete the accurate knowledge search and analysis to provide users with an intuitive visualization. This method thoroughly combined knowledge graph and graph computing characteristics, realized high-speed multi-hop knowledge correlation reasoning analysis in tremendous knowledge. The proposed work can also provide a basis for the context-aware intelligent question and answer. © 2021 IEEE.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124156427&amp;doi=10.1109%2fPESGM46819.2021.9638018&amp;partnerID=40&amp;md5=6bc63f693b3b49654cc83b841ec8106f</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: IEEE Power Energy Soc. Gen. Meet.</dc:description>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>IEEE Power and Energy Society General Meeting</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1615">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 20; Conference name: 2021 IEEE Power and Energy Society General Meeting, PESGM 2021; Conference date: 26 July 2021 through 29 July 2021; Conference code: 175685&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2266">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103999557&amp;doi=10.1007%2fs10489-021-02348-9&amp;partnerID=40&amp;md5=5d77f9f4aa91ead63813a5872b6fabf3">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0924669X%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Aithal</foaf:surname>
                        <foaf:givenName>S.G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rao</foaf:surname>
                        <foaf:givenName>A.B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1616"/>
        <dcterms:isReferencedBy rdf:resource="#item_2271"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Question answering</dc:subject>
        <dc:subject>Question generation</dc:subject>
        <dc:subject>Common sense</dc:subject>
        <dc:subject>Question comprehension</dc:subject>
        <dc:subject>Question similarity</dc:subject>
        <dc:subject>Question-answer pairs</dc:subject>
        <dc:subject>Similarity scores</dc:subject>
        <dc:subject>Universal sentence encoder</dc:subject>
        <dc:title>Automatic question-answer pairs generation and question similarity mechanism in question answering system</dc:title>
        <dcterms:abstract>With the swift growth of the information over the past few years, taking full benefit is increasingly essential. Question Answering System is one of the promising methods to access this much information. The Question Answering System lacks humans’ common sense and reasoning power and cannot identify unanswerable questions and irrelevant questions. These questions are answered by making unreliable and incorrect guesses. In this paper, we address this limitation by proposing a Question Similarity mechanism. Before a question is posed to a Question-Answering system, it is compared with possible generated questions of the given paragraph, and then a Question Similarity Score is generated. The Question Similarity mechanism effectively identifies the unanswerable and irrelevant questions. The proposed Question Similarity mechanism incorporates a human way of reasoning to identify unanswerable and irrelevant questions. This mechanism can avoid the unanswerable and irrelevant questions altogether from being posed to the Question Answering system. It helps the Question Answering Systems to focus only on the answerable questions to improve their performance. Along with this, we introduce an application of the Question Answering System that generates the question-answer pairs given a passage and is useful in several fields. © 2021, The Author(s).</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103999557&amp;doi=10.1007%2fs10489-021-02348-9&amp;partnerID=40&amp;md5=5d77f9f4aa91ead63813a5872b6fabf3</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer</dc:description>
        <bib:pages>8484-8497</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0924669X%20(ISSN)">
        <prism:volume>51</prism:volume>
        <dc:title>Applied Intelligence</dc:title>
        <dc:identifier>DOI 10.1007/s10489-021-02348-9</dc:identifier>
        <prism:number>11</prism:number>
        <dcterms:alternative>Appl Intell</dcterms:alternative>
        <dc:identifier>ISSN 0924669X (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1616">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 28; Correspondence Address: S. Singh; Department of Information and Communication Technology, Manipal Institute of Technology, MAHE, Manipal, 576104, India; email: sanjay.singh@manipal.edu; CODEN: APITE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2271">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104884252&amp;doi=10.1007%2fs40747-020-00218-4&amp;partnerID=40&amp;md5=64c5fef9f48e2bd51b2249c33eda0909">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:21994536%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Y.-D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Satapathy</foaf:surname>
                        <foaf:givenName>S.C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guttery</foaf:surname>
                        <foaf:givenName>D.S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Górriz</foaf:surname>
                        <foaf:givenName>J.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>S.-H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1612"/>
        <dcterms:isReferencedBy rdf:resource="#item_2299"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Convolutional neural network</dc:subject>
        <dc:subject>Data augmentation</dc:subject>
        <dc:subject>Visual question answering</dc:subject>
        <dc:subject>Breast thermography</dc:subject>
        <dc:subject>Color jittering</dc:subject>
        <dc:subject>Ductal carcinoma in situ</dc:subject>
        <dc:subject>Exponential linear unit</dc:subject>
        <dc:subject>Rank-based weighted pooling</dc:subject>
        <dc:subject>Thermal images</dc:subject>
        <dc:title>Improving ductal carcinoma in situ classification by convolutional neural network with exponential linear unit and rank-based weighted pooling</dc:title>
        <dcterms:abstract>Ductal carcinoma in situ (DCIS) is a pre-cancerous lesion in the ducts of the breast, and early diagnosis is crucial for optimal therapeutic intervention. Thermography imaging is a non-invasive imaging tool that can be utilized for detection of DCIS and although it has high accuracy (~ 88%), it is sensitivity can still be improved. Hence, we aimed to develop an automated artificial intelligence-based system for improved detection of DCIS in thermographs. This study proposed a novel artificial intelligence based system based on convolutional neural network (CNN) termed CNN-BDER on a multisource dataset containing 240 DCIS images and 240 healthy breast images. Based on CNN, batch normalization, dropout, exponential linear unit and rank-based weighted pooling were integrated, along with L-way data augmentation. Ten runs of tenfold cross validation were chosen to report the unbiased performances. Our proposed method achieved a sensitivity of 94.08 ± 1.22%, a specificity of 93.58 ± 1.49 and an accuracy of 93.83 ± 0.96. The proposed method gives superior performance than eight state-of-the-art approaches and manual diagnosis. The trained model could serve as a visual question answering system and improve diagnostic accuracy. © 2020, The Author(s).</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104884252&amp;doi=10.1007%2fs40747-020-00218-4&amp;partnerID=40&amp;md5=64c5fef9f48e2bd51b2249c33eda0909</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer International Publishing</dc:description>
        <bib:pages>1295-1310</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:21994536%20(ISSN)">
        <prism:volume>7</prism:volume>
        <dc:title>Complex and Intelligent Systems</dc:title>
        <dc:identifier>DOI 10.1007/s40747-020-00218-4</dc:identifier>
        <prism:number>3</prism:number>
        <dcterms:alternative>Complex Intell. Syst.</dcterms:alternative>
        <dc:identifier>ISSN 21994536 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1612">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 32; Correspondence Address: S.-H. Wang; School of Architecture Building and Civil Engineering, Loughborough University, Loughborough, LE11 3TU, United Kingdom; email: shuihuawang@ieee.org; D.S. Guttery; Leicester Cancer Research Center, University of Leicester, Leicester, LE1 7RH, United Kingdom; email: dsg6@le.ac.uk; J.M. Górriz; Department of Signal Theory, Networking and Communications, University of Granada, Granada, Spain; email: gorriz@ugr.es&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2299">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-195408540-4%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-195408540-4 (ISBN)</dc:identifier>
                <dc:title>Proc. Workshop Biomed. Lang. Process., BioNLP</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yadav</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sarrouti</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gupta</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Demner-Fushman D.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Cohen K.B.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Ananiadou S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Tsujii J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1617"/>
        <dcterms:isReferencedBy rdf:resource="#item_2335"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Transfer learning</dc:subject>
        <dc:subject>Natural language questions</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Health</dc:subject>
        <dc:subject>Consumer healths</dc:subject>
        <dc:subject>Demographic information</dc:subject>
        <dc:subject>Health informations</dc:subject>
        <dc:subject>Learning-based approach</dc:subject>
        <dc:subject>Medical history</dc:subject>
        <dc:subject>National library of medicines</dc:subject>
        <dc:subject>Pressung</dc:subject>
        <dc:title>NLM at MEDIQA 2021: Transfer Learning-based Approaches for Consumer Question and Multi-Answer Summarization</dc:title>
        <dcterms:abstract>The quest for seeking health information has swamped the web with consumers’ health-related questions, which makes the need for efficient and reliable question answering systems more pressing. The consumers’ questions, however, are very descriptive and contain several peripheral information (like patient’s medical history, demographic information, etc.), that are often not required for answering the question. Furthermore, it contributes to the challenges of understanding natural language questions for automatic answer retrieval. Also, it is crucial to provide the consumers with the exact and relevant answers, rather than the entire pool of answer documents to their question. One of the cardinal tasks in achieving robust consumer health question answering systems is the question summarization and multi-document answer summarization. This paper describes the participation of the U.S. National Library of Medicine (NLM) in Consumer Question and Multi-Answer Summarization tasks of the MEDIQA 2021 challenge at NAACL-BioNLP workshop. In this work, we exploited the capabilities of pre-trained transformer models and introduced a transfer learning approach for the abstractive Question Summarization and extractive Multi-Answer Summarization tasks by first pre-training our model on a task-specific summarization dataset followed by fine-tuning it for both the tasks via incorporating medical entities. We achieved the second, sixth and the fourth position for the Question Summarization task in terms ROUGE-1, ROUGE-2 and ROUGE-L scores respectively. © 2021 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111217209&amp;partnerID=40&amp;md5=ad0177f443b437465100c154f78d2b70</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Workshop Biomed. Lang. Process., BioNLP</dc:description>
        <bib:pages>291-301</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the 20th Workshop on Biomedical Language Processing, BioNLP 2021</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1617">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 15; Conference name: 20th Workshop on Biomedical Language Processing, BioNLP 2021; Conference code: 174277&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2335">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122866794&amp;doi=10.1155%2f2021%2f5089236&amp;partnerID=40&amp;md5=156237d4adb9df296e3115c6c79a3bf8">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:10589244%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1613"/>
        <dcterms:isReferencedBy rdf:resource="#item_2362"/>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Convolutional neural network</dc:subject>
        <dc:subject>Neural networks</dc:subject>
        <dc:subject>Convolution</dc:subject>
        <dc:subject>Service platforms</dc:subject>
        <dc:subject>Cloud-computing</dc:subject>
        <dc:subject>Data clouds</dc:subject>
        <dc:subject>Emergency response</dc:subject>
        <dc:subject>Emergency services</dc:subject>
        <dc:subject>Functional demands</dc:subject>
        <dc:subject>Health management</dc:subject>
        <dc:subject>Intelligent community</dc:subject>
        <dc:subject>Logistics network</dc:subject>
        <dc:subject>Requirement analysis</dc:subject>
        <dc:title>Research on the Construction of Intelligent Community Emergency Service Platform Based on Convolutional Neural Network</dc:title>
        <dcterms:abstract>Aiming at the shortcomings of the existing community emergency service platform, such as single function, poor scalability, and strong subjectivity, an intelligent community emergency service platform based on convolutional neural network was constructed. Firstly, the requirements analysis of the emergency service platform was carried out, and the functional demand of the emergency service platform was analyzed from the aspects of community environment, safety, infrastructure, health management, emergency response, and so on. Secondly, through logistics network, big data, cloud computing, artificial intelligence, and all kinds of applications, the intelligent community emergency service platform was designed. Finally, a semantic matching emergency question answering system based on convolutional neural network was developed to provide key technical support for the emergency preparation stage of intelligent community. The results show that the intelligent community emergency service platform plays an important role in preventing community emergency events and taking active and effective measures to ensure the health and safety of community residents.  © 2021 Yu Chen and Zhong Tang.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122866794&amp;doi=10.1155%2f2021%2f5089236&amp;partnerID=40&amp;md5=156237d4adb9df296e3115c6c79a3bf8</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Hindawi Limited</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:10589244%20(ISSN)">
        <prism:volume>2021</prism:volume>
        <dc:title>Scientific Programming</dc:title>
        <dc:identifier>DOI 10.1155/2021/5089236</dc:identifier>
        <dcterms:alternative>Sci. Program</dcterms:alternative>
        <dc:identifier>ISSN 10589244 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1613">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 10; Correspondence Address: Z. Tang; School of Humanities and Social Science, Guangxi Medical University, Nanning, Guangxi, 530021, China; email: tangzhong@stu.gxmu.edu.cn; CODEN: SCIPE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2362">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103703478&amp;doi=10.14569%2fIJACSA.2021.0120359&amp;partnerID=40&amp;md5=c5ea238f32d7792407c3662e592b319c">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2158107X%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Elfadil</foaf:surname>
                        <foaf:givenName>S.S.A.N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jarajreh</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Algarni</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1619"/>
        <dcterms:isReferencedBy rdf:resource="#item_2356"/>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Knowledge management</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Systematic literature review</dc:subject>
        <dc:subject>Systematic Review</dc:subject>
        <dc:subject>'current</dc:subject>
        <dc:subject>Syntactics</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Body of knowledge</dc:subject>
        <dc:subject>Human use</dc:subject>
        <dc:subject>Knowledge system</dc:subject>
        <dc:subject>knowledge systems</dc:subject>
        <dc:subject>syntax</dc:subject>
        <dc:subject>systematic literature review</dc:subject>
        <dc:subject>Systems research</dc:subject>
        <dc:title>Question Answering Systems: A Systematic Literature Review</dc:title>
        <dcterms:abstract>Question answering systems (QAS) are developed to answer questions presented in natural language by extracting the answer. The development of QAS is aimed at making the Web more suited to human use by eliminating the need to sift through a lot of search results manually to determine the correct answer to a question. Accordingly, the aim of this study was to provide an overview of the current state of QAS research. It also aimed at highlighting the key limitations and gaps in the existing body of knowledge relating to QAS. Furthermore, it intended to identify the most effective methods utilized in the design of QAS. The systematic review of literature research method was selected as the most appropriate methodology for studying the research topic. This method differs from the conventional literature review as it is more comprehensive and objective. Based on the findings, QAS is a highly active area of research, with scholars taking diverse approaches in the development of their systems. Some of the limitations observed in these studies encompass the focused nature of current QAS, weaknesses associated with models that are used as building blocks for QAS, the need for standard datasets and question formats hence limiting the applicability of the QAS in practical settings, and the failure of researchers to examine their QAS solutions comprehensively. The most effective methods for designing QAS include focusing on syntax and context, utilizing word encoding and knowledge systems, leveraging deep learning, and using elements such as machine learning and artificial intelligence. Going forward, modular designs ought to be encouraged to foster collaboration in the creation of QAS. © 2021</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103703478&amp;doi=10.14569%2fIJACSA.2021.0120359&amp;partnerID=40&amp;md5=c5ea238f32d7792407c3662e592b319c</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Science and Information Organization</dc:description>
        <bib:pages>495-502</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2158107X%20(ISSN)">
        <prism:volume>12</prism:volume>
        <dc:title>International Journal of Advanced Computer Science and Applications</dc:title>
        <dc:identifier>DOI 10.14569/IJACSA.2021.0120359</dc:identifier>
        <prism:number>3</prism:number>
        <dcterms:alternative>Intl. J. Adv.  Comput. Sci. Appl.</dcterms:alternative>
        <dc:identifier>ISSN 2158107X (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1619">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 7&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2356">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:03029743%20(ISSN);%20978-303072112-1%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>12656 LNCS</prism:volume>
                <dc:identifier>ISBN 03029743 (ISSN); 978-303072112-1 (ISBN)</dc:identifier>
                <dc:title>Lect. Notes Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-72113-8_20</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Springer Science and Business Media Deutschland GmbH</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lauriola</foaf:surname>
                        <foaf:givenName>I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Moschitti</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Hiemstra D.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Moens M.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Mothe J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Perego R.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Potthast M.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Sebastiani F.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1618"/>
        <dcterms:isReferencedBy rdf:resource="#item_2671"/>
        <dc:subject>Answer Sentence Selection</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Computer science</dc:subject>
        <dc:subject>Computers</dc:subject>
        <dc:subject>Content-based information</dc:subject>
        <dc:subject>Contextual information</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Global context</dc:subject>
        <dc:subject>Information retrieval</dc:subject>
        <dc:subject>Pre-trained Transformer</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Selection function</dc:subject>
        <dc:subject>Sentence selection</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Transformer models</dc:subject>
        <dc:title>Answer Sentence Selection Using Local and Global Context in Transformer Models</dc:title>
        <dcterms:abstract>An essential task for the design of Question Answering systems is the selection of the sentence containing (or constituting) the answer from documents relevant to the asked question. Previous neural models have experimented with using additional text together with the target sentence to learn a selection function but these methods were not powerful enough to effectively encode contextual information. In this paper, we analyze the role of contextual information for the sentence selection task in Transformer based architectures, leveraging two types of context, local and global. The former describes the paragraph containing the sentence, aiming at solving implicit references, whereas the latter describes the entire document containing the candidate sentence, providing content-based information. The results on three different benchmarks show that the combination of the local and global context in a Transformer model significantly improves the accuracy in Answer Sentence Selection. © 2021, Springer Nature Switzerland AG.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107362873&amp;doi=10.1007%2f978-3-030-72113-8_20&amp;partnerID=40&amp;md5=7bf2ac0434fc9b19261363b81984e69f</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Lect. Notes Comput. Sci.</dc:description>
        <bib:pages>298-312</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1618">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 13; Correspondence Address: I. Lauriola; Amazon Alexa AI, Manhattan Beach, United States; email: lauivano@amazon.com; Conference name: 43rd European Conference on Information Retrieval Research, ECIR 2021; Conference date: 28 March 2021 through 1 April 2021; Conference code: 257049&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2671">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser de acesso pago&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116194939&amp;doi=10.1016%2fj.websem.2021.100661&amp;partnerID=40&amp;md5=6eda9f8927afc10e0450b3606b1407db">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:15708268%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Arnaout</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Razniewski</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Weikum</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pan</foaf:surname>
                        <foaf:givenName>J.Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1620"/>
        <dcterms:isReferencedBy rdf:resource="#item_2332"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Property</dc:subject>
        <dc:subject>Statistical methods</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Information extraction</dc:subject>
        <dc:subject>Inference methods</dc:subject>
        <dc:subject>Knowledge bases</dc:subject>
        <dc:subject>Negative knowledge</dc:subject>
        <dc:subject>Peer groups</dc:subject>
        <dc:subject>Ranking</dc:subject>
        <dc:subject>Related entities</dc:subject>
        <dc:subject>Statistical inference</dc:subject>
        <dc:title>Negative statements considered useful</dc:title>
        <dcterms:abstract>Knowledge bases (KBs) about notable entities and their properties are an important asset in applications such as search, question answering and dialog. All popular KBs capture virtually only positive statements, and abstain from taking any stance on statements not stored in the KB. This paper makes the case for explicitly stating salient statements that do not hold. Negative statements are useful to overcome limitations of question answering systems that are mainly geared for positive questions; they can also contribute to informative summaries of entities. Due to the abundance of such invalid statements, any effort to compile them needs to address ranking by saliency. We present a statistical inference method for compiling and ranking negative statements, based on expectations from positive statements of related entities in peer groups. Experimental results, with a variety of datasets, show that the method can effectively discover notable negative statements, and extrinsic studies underline their usefulness for entity summarization. Datasets and code are released as resources for further research. © 2021 Elsevier B.V.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116194939&amp;doi=10.1016%2fj.websem.2021.100661&amp;partnerID=40&amp;md5=6eda9f8927afc10e0450b3606b1407db</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier B.V.</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:15708268%20(ISSN)">
        <prism:volume>71</prism:volume>
        <dc:title>Journal of Web Semantics</dc:title>
        <dc:identifier>DOI 10.1016/j.websem.2021.100661</dc:identifier>
        <dcterms:alternative>J. Web Semant.</dcterms:alternative>
        <dc:identifier>ISSN 15708268 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1620">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 8; Correspondence Address: H. Arnaout; Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, 66123, Germany; email: harnaout@mpi-inf.mpg.de&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2332">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:10450823%20(ISSN);%20978-099924119-6%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 10450823 (ISSN); 978-099924119-6 (ISBN)</dc:identifier>
                <dc:title>IJCAI Int. Joint Conf. Artif. Intell.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>International Joint Conferences on Artificial Intelligence</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>You</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zou</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Zhou Z.-H.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1621"/>
        <dcterms:isReferencedBy rdf:resource="#item_2328"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Knowledge management</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Speech recognition</dc:subject>
        <dc:subject>Character recognition</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Distillation</dc:subject>
        <dc:subject>Personnel training</dc:subject>
        <dc:subject>Multi-modal</dc:subject>
        <dc:subject>Teachers'</dc:subject>
        <dc:subject>Automatic speech recognition</dc:subject>
        <dc:subject>Acoustic levels</dc:subject>
        <dc:subject>Audio acoustics</dc:subject>
        <dc:subject>Distillation method</dc:subject>
        <dc:subject>Performance degradation</dc:subject>
        <dc:subject>Recognition error</dc:subject>
        <dc:subject>Speech-recognition modules</dc:subject>
        <dc:title>MRD-Net: Multi-Modal Residual Knowledge Distillation for Spoken Question Answering</dc:title>
        <dcterms:abstract>Spoken question answering (SQA) has recently drawn considerable attention in the speech community. It requires systems to find correct answers from the given spoken passages simultaneously. The common SQA systems consist of the automatic speech recognition (ASR) module and text-based question answering module. However, previous methods suffer from severe performance degradation due to ASR errors. To alleviate this problem, this work proposes a novel multi-modal residual knowledge distillation method (MRD-Net), which further distills knowledge at the acoustic level from the audio-assistant (Audio-A). Specifically, we utilize the teacher (T) trained on manual transcriptions to guide the training of the student (S) on ASR transcriptions. We also show that introducing an Audio-A helps this procedure by learning residual errors between T and S. Moreover, we propose a simple yet effective attention mechanism to adaptively leverage audio-text features as the new deep attention knowledge to boost the network performance. Extensive experiments demonstrate that the proposed MRD-Net achieves superior results compared with state-of-the-art methods on three spoken question answering benchmark datasets. © 2021 International Joint Conferences on Artificial Intelligence. All rights reserved.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115731218&amp;partnerID=40&amp;md5=6a4178b20d716b2b9026d38e7af77ff4</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: IJCAI Int. Joint Conf. Artif. Intell.</dc:description>
        <bib:pages>3985-3991</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>IJCAI International Joint Conference on Artificial Intelligence</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1621">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 23; Correspondence Address: Y. Zou; ADSPLAB, School of ECE, Peking University, Shenzhen, China; email: zouyx@pku.edu.cn; Conference name: 30th International Joint Conference on Artificial Intelligence, IJCAI 2021; Conference date: 19 August 2021 through 27 August 2021; Conference code: 177242&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2328">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-145036822-3%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-145036822-3 (ISBN)</dc:identifier>
                <dc:title>WSDM - Proc. Int. Conf. Web Search Data Min.</dc:title>
                <dc:identifier>DOI 10.1145/3336191.3371792</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computing Machinery, Inc</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shou</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gong</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiang</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1622"/>
        <dcterms:isReferencedBy rdf:resource="#item_2326"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Information retrieval</dc:subject>
        <dc:subject>Data mining</dc:subject>
        <dc:subject>Websites</dc:subject>
        <dc:subject>Pre-training</dc:subject>
        <dc:subject>Distillation</dc:subject>
        <dc:subject>Personnel training</dc:subject>
        <dc:subject>Students</dc:subject>
        <dc:subject>E-learning</dc:subject>
        <dc:subject>Business scenario</dc:subject>
        <dc:subject>Distillation pre-training</dc:subject>
        <dc:subject>General knowledge</dc:subject>
        <dc:subject>Knowledge distillation</dc:subject>
        <dc:subject>Model compression</dc:subject>
        <dc:subject>Multi-teacher</dc:subject>
        <dc:subject>Practical problems</dc:subject>
        <dc:subject>Two-stage</dc:subject>
        <dc:subject>Web question answering systems</dc:subject>
        <dc:title>Model compression with two-stage multi-teacher knowledge distillation for web question answering system</dc:title>
        <dcterms:abstract>Deep pre-training and fine-tuning models (such as BERT and OpenAI GPT) have demonstrated excellent results in question answering areas. However, due to the sheer amount of model parameters, the inference speed of these models is very slow. How to apply these complex models to real business scenarios becomes a challenging but practical problem. Previous model compression methods usually suffer from information loss during the model compression procedure, leading to inferior models compared with the original one. To tackle this challenge, we propose a Two-stage Multi-teacher Knowledge Distillation (TMKD for short) method for web Question Answering system. We first develop a general Q&amp;A distillation task for student model pre-training, and further fine-tune this pre-trained student model with multi-teacher knowledge distillation on downstream tasks (like Web Q&amp;A task, MNLI, SNLI, RTE tasks from GLUE), which effectively reduces the overfitting bias in individual teacher models, and transfers more general knowledge to the student model. The experiment results show that our method can significantly outperform the baseline methods and even achieve comparable results with the original teacher models, along with substantial speedup of model inference. © 2020 Association for Computing Machinery.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079536665&amp;doi=10.1145%2f3336191.3371792&amp;partnerID=40&amp;md5=4e937b1debf007246aa15d04e949d366</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: WSDM - Proc. Int. Conf. Web Search Data Min.</dc:description>
        <bib:pages>690-698</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>WSDM 2020 - Proceedings of the 13th International Conference on Web Search and Data Mining</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1622">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 73; Conference name: 13th ACM International Conference on Web Search and Data Mining, WSDM 2020; Conference date: 3 February 2020 through 7 February 2020; Conference code: 157225&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2326">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-172815374-2%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-172815374-2 (ISBN)</dc:identifier>
                <dc:title>Proc. Int. Conf. Inven. Res. Comput. Appl., ICIRCA</dc:title>
                <dc:identifier>DOI 10.1109/ICIRCA48905.2020.9182972</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vinodkumar Sadhuram</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Soni</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1624"/>
        <dcterms:isReferencedBy rdf:resource="#item_2331"/>
        <dc:subject>Text mining</dc:subject>
        <dc:subject>Artificial Intelligence</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Data handling</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>TF-IDF</dc:subject>
        <dc:subject>Factoid Question Answering System</dc:subject>
        <dc:subject>Factoid questions</dc:subject>
        <dc:subject>Hot topics</dc:subject>
        <dc:subject>Lexical Chain</dc:subject>
        <dc:subject>New approaches</dc:subject>
        <dc:subject>Passage retrieval</dc:subject>
        <dc:subject>Reasoning system</dc:subject>
        <dc:subject>S QUAD Dataset</dc:subject>
        <dc:title>Natural Language Processing based New Approach to Design Factoid Question Answering System</dc:title>
        <dcterms:abstract>The field of text mining which deals with the providing of answers to the questions of the users is also one of the hot topics for researchers. The difficulty seen in the proper answering of the questions needs to be resolved. The large variety of questions fails in the QA system. In this paper, Natural Language Processing (NLP) has been used which deals with the processing of the data that comes in any form like text, video, image, or audio. This NLP comes under the field of artificial intelligence (AI), which is used in the field of question answering (QA) system. Here proposed work for designing a system that works for factoid QA which will answer the questions that are asked by the users. Lexical Chain and Keyword analysis are used in our system for the answering of questions from a given set of articles. The reasoning system is used for the validity of the answering. The experiment here is done with the SQUAD dataset. In our experiment, the accuracy obtained for the passage retrieval using TFIDF is 69.69%. The overall average of the correct prediction of the answer is 69.93%. © 2020 IEEE.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092041787&amp;doi=10.1109%2fICIRCA48905.2020.9182972&amp;partnerID=40&amp;md5=766d93170d700de64c6bbe29060c2d2f</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Int. Conf. Inven. Res. Comput. Appl., ICIRCA</dc:description>
        <bib:pages>276-281</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the 2nd International Conference on Inventive Research in Computing Applications, ICIRCA 2020</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1624">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 13; Conference name: 2nd International Conference on Inventive Research in Computing Applications, ICIRCA 2020; Conference date: 15 July 2020 through 17 July 2020; Conference code: 162766&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2331">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073937450&amp;doi=10.1007%2fs00500-019-04367-8&amp;partnerID=40&amp;md5=c8aa7968b3ec58e13974e8fed347a699">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:14327643%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>X.-M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Feng</foaf:surname>
                        <foaf:givenName>W.-Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chu</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1625"/>
        <dcterms:isReferencedBy rdf:resource="#item_2265"/>
        <dc:subject>NLP</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Brain</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Large dataset</dc:subject>
        <dc:subject>Long short-term memory</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Attention mechanisms</dc:subject>
        <dc:subject>Complex networks</dc:subject>
        <dc:subject>Evaluation metrics</dc:subject>
        <dc:subject>Multi-granularity</dc:subject>
        <dc:subject>Novel processing</dc:subject>
        <dc:subject>Question-answering system</dc:subject>
        <dc:subject>Word segmentation</dc:subject>
        <dc:title>An attention mechanism and multi-granularity-based Bi-LSTM model for Chinese Q&amp;A system</dc:title>
        <dcterms:abstract>Natural language processing (NLP) is one of the key techniques in intelligent question-answering (Q&amp;A) systems. Although recurrent neural networks and long short-term memory (LSTM) networks exhibit obvious advantages on well-known English Q&amp;A datasets, they still suffer from several defects including indeterminateness, polysemy and the lack of changing morphology in Chinese, which results in complex NLP on large and diverse Chinese Q&amp;A datasets. In this paper, we first analyze limitations of applying LSTM and bidirectional LSTM (Bi-LSTM) models to noisy Chinese Q&amp;A datasets. Then, we focus on integrating attention mechanisms and multi-granularity word segmentation into Bi-LSTM and propose an attention mechanism and multi-granularity-based Bi-LSTM model (AM–Bi-LSTM) which combines the improved attention mechanism with a novel processing of multi-granularity word segmentation to handle the complex NLP in Chinese Q&amp;A datasets. Furthermore, similarity of questions and answers is formulated to implement the quantitative computation which helps to achieve better performance in Chinese Q&amp;A systems. Finally, we verify the proposed model on a noisy Chinese Q&amp;A dataset. The experimental results demonstrate that the novel AM–Bi-LSTM model achieves significant improvement on evaluation metrics of accuracy, mean average precision and so on. Moreover, the experimental results indicate that the novel AM–Bi-LSTM model outperforms baseline methods and other LSTM-based models. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073937450&amp;doi=10.1007%2fs00500-019-04367-8&amp;partnerID=40&amp;md5=c8aa7968b3ec58e13974e8fed347a699</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer</dc:description>
        <bib:pages>5831-5845</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:14327643%20(ISSN)">
        <prism:volume>24</prism:volume>
        <dc:title>Soft Computing</dc:title>
        <dc:identifier>DOI 10.1007/s00500-019-04367-8</dc:identifier>
        <prism:number>8</prism:number>
        <dcterms:alternative>Soft Comput.</dcterms:alternative>
        <dc:identifier>ISSN 14327643 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1625">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 50; Correspondence Address: X.-M. Yu; School of Information Science and Engineering, Shandong Normal University, Jinan, 250014, China; email: yxm0708@126.com&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2265">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080939527&amp;doi=10.1109%2fJSTARS.2019.2948921&amp;partnerID=40&amp;md5=3a00b253333f310805af108e25fedfde">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:19391404%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Martin</foaf:surname>
                        <foaf:givenName>A.V.I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Selva</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1623"/>
        <dcterms:isReferencedBy rdf:resource="#item_2243"/>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>satellite imagery</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Surveys</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>cognition</dc:subject>
        <dc:subject>Cognitive assistance</dc:subject>
        <dc:subject>Daphne</dc:subject>
        <dc:subject>design</dc:subject>
        <dc:subject>Design-space exploration tool</dc:subject>
        <dc:subject>Distributed spacecraft missions</dc:subject>
        <dc:subject>Distributed spacecrafts</dc:subject>
        <dc:subject>earth observation</dc:subject>
        <dc:subject>Earth observations</dc:subject>
        <dc:subject>EOS</dc:subject>
        <dc:subject>Interplanetary flight</dc:subject>
        <dc:subject>Jet Propulsion Laboratory</dc:subject>
        <dc:subject>mixed initiative</dc:subject>
        <dc:subject>Mixed initiative</dc:subject>
        <dc:subject>NASA</dc:subject>
        <dc:subject>Observatories</dc:subject>
        <dc:subject>performance assessment</dc:subject>
        <dc:subject>satellite data</dc:subject>
        <dc:subject>satellite mission</dc:subject>
        <dc:subject>spacecraft</dc:subject>
        <dc:subject>Spacecraft</dc:subject>
        <dc:subject>Systems analysis</dc:subject>
        <dc:subject>User experience</dc:subject>
        <dc:subject>virtual assistant</dc:subject>
        <dc:subject>Virtual assistants</dc:subject>
        <dc:title>Daphne: A Virtual Assistant for Designing Earth Observation Distributed Spacecraft Missions</dc:title>
        <dcterms:abstract>This article describes Daphne, a virtual assistant for designing Earth observation distributed spacecraft missions. It is, to the best of our knowledge, the first virtual assistant for such application. The article provides a thorough description of Daphne, including its question answering system and the main features we have implemented to help system engineers design distributed spacecraft missions. In addition, the article describes a study performed at NASA's Jet Propulsion Laboratory (JPL) to assess the usefulness of Daphne in this use case. The study was conducted with N = 9 subjects from JPL, who were asked to work on a mission design task with two versions of Daphne, one that was fully featured implementing the cognitive assistance functions, and one that only had the features one would find in a traditional design space exploration tool. After the task, they filled out a standard user experience survey, completed a test to assess how much they learned about the task, and were asked a number of questions in a semi-structured exit interview. Results of the study suggest that Daphne can help improve performance during system design tasks compared to traditional tools, while keeping the system usable. However, the study also raises some concerns with respect to a potential reduction in human learning due to the use of the cognitive assistant. The article ends with a list of suggestions for future development of virtual assistants for space mission design. © 2008-2012 IEEE.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080939527&amp;doi=10.1109%2fJSTARS.2019.2948921&amp;partnerID=40&amp;md5=3a00b253333f310805af108e25fedfde</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Institute of Electrical and Electronics Engineers</dc:description>
        <bib:pages>30-48</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:19391404%20(ISSN)">
        <prism:volume>13</prism:volume>
        <dc:title>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</dc:title>
        <dc:identifier>DOI 10.1109/JSTARS.2019.2948921</dc:identifier>
        <dcterms:alternative>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</dcterms:alternative>
        <dc:identifier>ISSN 19391404 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1623">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 22; Correspondence Address: A.V.I. Martin; Department of Aerospace Engineering, Texas A and M University, College Station, 77843, United States; email: aviros@tamu.edu&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2243">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097339688&amp;partnerID=40&amp;md5=74c36d64435aa990a7dfc956181eff20">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2021-January</prism:volume>
                <dc:identifier>ISBN 10450823 (ISSN); 978-099924116-5 (ISBN)</dc:identifier>
                <dc:title>IJCAI Int. Joint Conf. Artif. Intell.</dc:title>
                <dc:identifier>DOI 10.24963/ijcai.2020/762</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>International Joint Conferences on Artificial Intelligence</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiang</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qu</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Bessiere C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1626"/>
        <link:link rdf:resource="#item_2670"/>
        <dc:subject>Best matching</dc:subject>
        <dc:subject>Domain expertise</dc:subject>
        <dc:subject>Domain knowledge</dc:subject>
        <dc:subject>Knowledge representation</dc:subject>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>Legal contexts</dc:subject>
        <dc:subject>Legal knowledge</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Natural language queries</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:title>AILA: A question answering system in the legal domain</dc:title>
        <dcterms:abstract>Question answering (QA) in the legal domain has gained increasing popularity for people to seek legal advice. However, existing QA systems struggle to comprehend the legal context and provide jurisdictionally relevant answers due to the lack of domain expertise. In this paper, we develop an Artificial Intelligence Law Assistant (AILA) for question answering in the domain of Chinese laws. AILA system automatically comprehends users' natural language queries with the help of the legal knowledge graph (KG) and provides the best-matching answers for given queries. In addition, AILA provides visual cues to interpret the input queries and candidate answers based on the legal KG. Experimental results on a large-scale legal QA corpus show the effectiveness of AILA. To the best of our knowledge, AILA is the first Chinese legal QA system which integrates the domain knowledge from legal KG to comprehend the questions and answers for ranking QA pairs. AILA is available at http://bmilab.ticp.io:48478/. © 2020 Inst. Sci. inf., Univ. Defence in Belgrade. All rights reserved.</dcterms:abstract>
        <dc:date>2020/07/09</dc:date>
        <z:language>English</z:language>
        <z:shortTitle>AILA</z:shortTitle>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097339688&amp;partnerID=40&amp;md5=74c36d64435aa990a7dfc956181eff20</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: IJCAI Int. Joint Conf. Artif. Intell.</dc:description>
        <bib:pages>5258-5260</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Twenty-Ninth International Joint Conference on Artificial Intelligence</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1626">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 13; Correspondence Address: Q. Qu; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China; email: qiang@siat.ac.cn; M. Yang; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China; email: min.yang@siat.ac.cn; Conference name: 29th International Joint Conference on Artificial Intelligence, IJCAI 2020; Conference code: 165342&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2670">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2670/Huang et al. - 2020 - AILA A Question Answering System in the Legal Domain.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.ijcai.org/proceedings/2020/0762.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:34:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:03029743%20(ISSN);%20978-303045441-8%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>12036 LNCS</prism:volume>
                <dc:identifier>ISBN 03029743 (ISSN); 978-303045441-8 (ISBN)</dc:identifier>
                <dc:title>Lect. Notes Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-45442-5_73</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>Springer</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mansouri</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Agarwal</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oard</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zanibbi</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Jose J.M.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Yilmaz E.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Magalhães J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Martins F.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Castells P.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Ferro N.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Silva M.J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1627"/>
        <dcterms:isReferencedBy rdf:resource="#item_2293"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Computer science</dc:subject>
        <dc:subject>Community question answering</dc:subject>
        <dc:subject>Computers</dc:subject>
        <dc:subject>Formula retrieval</dc:subject>
        <dc:subject>Math-aware search</dc:subject>
        <dc:subject>Mathematical Information Retrieval</dc:subject>
        <dc:subject>Standard tests</dc:subject>
        <dc:subject>Subtasks</dc:subject>
        <dc:subject>Test Collection</dc:subject>
        <dc:title>Finding old answers to new math questions: The ARQmath lab at CLEF 2020</dc:title>
        <dcterms:abstract>The ARQMath Lab at CLEF 2020 considers the problem of finding answers to new mathematical questions among posted answers on a community question answering site (Math Stack Exchange). Queries are question postings held out from the test collection, each containing both text and at least one formula. We expect this to be a challenging task, as both math and text may be needed to find relevant answer posts. While several models have been proposed for text question answering, math question answering is in an earlier stage of development. To advance math-aware search and mathematical question answering systems, we will create a standard test collection for researchers to use for benchmarking. ARQMath will also include a formula retrieval sub-task: individual formulas from question posts are used to locate formulas in earlier answer posts, with relevance determined by narrative fields created based on the original question. We will use these narrative fields to explore diverse information needs for formula search (e.g., alternative notation, applications in specific fields or definition). © Springer Nature Switzerland AG 2020.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084177647&amp;doi=10.1007%2f978-3-030-45442-5_73&amp;partnerID=40&amp;md5=2c70b59677df5c84d61d7784d52b9416</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Lect. Notes Comput. Sci.</dc:description>
        <bib:pages>564-571</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1627">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 16; Correspondence Address: B. Mansouri; Rochester Institute of Technology, Rochester, United States; email: bm3302@rit.edu; Conference name: 42nd European Conference on IR Research, ECIR 2020; Conference date: 14 April 2020 through 17 April 2020; Conference code: 239129&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2293">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:03029743%20(ISSN);%20978-303049460-5%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>12123 LNCS</prism:volume>
                <dc:identifier>ISBN 03029743 (ISSN); 978-303049460-5 (ISBN)</dc:identifier>
                <dc:title>Lect. Notes Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-49461-2_25</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>Springer</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Diefenbach</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Giménez-García</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Both</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maret</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Harth A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Kirrane S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Ngonga Ngomo A.-C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Paulheim H.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Rula A.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Gentile A.L.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Haase P.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Cochez M.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1628"/>
        <dcterms:isReferencedBy rdf:resource="#item_2349"/>
        <dc:subject>Knowledge graphs</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>End users</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Design</dc:subject>
        <dc:subject>Natural language questions</dc:subject>
        <dc:subject>Semantic Web</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Build systems</dc:subject>
        <dc:subject>Expert users</dc:subject>
        <dc:subject>Knowledge Graphs</dc:subject>
        <dc:subject>On demands</dc:subject>
        <dc:subject>On-demand</dc:subject>
        <dc:subject>Portability</dc:subject>
        <dc:subject>QAnswer</dc:subject>
        <dc:subject>RDF</dc:subject>
        <dc:title>QAnswer KG: Designing a Portable Question Answering System over RDF Data</dc:title>
        <dcterms:abstract>While RDF was designed to make data easily readable by machines, it does not make data easily usable by end-users. Question Answering (QA) over Knowledge Graphs (KGs) is seen as the technology which is able to bridge this gap. It aims to build systems which are capable of extracting the answer to a user’s natural language question from an RDF dataset. In recent years, many approaches were proposed which tackle the problem of QA over KGs. Despite such efforts, it is hard and cumbersome to create a Question Answering system on top of a new RDF dataset. The main open challenge remains portability, i.e., the possibility to apply a QA algorithm easily on new and previously untested RDF datasets. In this publication, we address the problem of portability by presenting an architecture for a portable QA system. We present a novel approach called QAnswer KG, which allows the construction of on-demand QA systems over new RDF datasets. Hence, our approach addresses non-expert users in QA domain. In this paper, we provide the details of QA system generation process. We show that it is possible to build a QA system over any RDF dataset while requiring minimal investments in terms of training. We run experiments using 3 different datasets. To the best of our knowledge, we are the first to design a process for non-expert users. We enable such users to efficiently create an on-demand, scalable, multilingual, QA system on top of any RDF dataset. © Springer Nature Switzerland AG 2020.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086138570&amp;doi=10.1007%2f978-3-030-49461-2_25&amp;partnerID=40&amp;md5=4c440c186b245a943ac560c91abea310</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Lect. Notes Comput. Sci.</dc:description>
        <bib:pages>429-445</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1628">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 19; Correspondence Address: D. Diefenbach; The QA Company SAS, Saint-Etienne, France; email: dennis.diefenbach@qanswer.eu; Conference name: 17th Extended Semantic Web Conference, ESWC 2020; Conference date: 31 May 2020 through 4 June 2020; Conference code: 240429&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2349">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:17426588%20(ISSN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1501</prism:volume>
                <dc:identifier>ISBN 17426588 (ISSN)</dc:identifier>
                <dc:title>J. Phys. Conf. Ser.</dc:title>
                <dc:identifier>DOI 10.1088/1742-6596/1501/1/012022</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Institute of Physics Publishing</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Utomo</foaf:surname>
                        <foaf:givenName>F.S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Suryana</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Azmi</foaf:surname>
                        <foaf:givenName>M.S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1629"/>
        <dcterms:isReferencedBy rdf:resource="#item_2355"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Document Retrieval</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Morphology analysis</dc:subject>
        <dc:subject>Physics</dc:subject>
        <dc:subject>Potential researches</dc:subject>
        <dc:subject>Question classification</dc:subject>
        <dc:subject>Question processing</dc:subject>
        <dc:title>Question Answering Systems on Holy Quran: A Review of Existing Frameworks, Approaches, Algorithms and Research Issues</dc:title>
        <dcterms:abstract>Question Answering System has the ability to present an answer based on a question submitted by the user in natural languages. This system consists of question processing, document retrieval, and answer extraction component. Challenge to optimize Question Answering's system is to increase the performance of all components in the framework. The performance of all component which has not been optimized has caused to the lack of accurate answer from the systems. Based on this issue, the purpose of this research is to investigate the research gaps in the current state of existing Question Answering Systems on Holy Quran. The result of this study reveals potential research issues, namely morphology analysis, question classification, search techniques, and ontology resources. © Published under licence by IOP Publishing Ltd.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086499110&amp;doi=10.1088%2f1742-6596%2f1501%2f1%2f012022&amp;partnerID=40&amp;md5=597da38bac853f7a3340ee6e2e76e010</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Issue: 1
Journal Abbreviation: J. Phys. Conf. Ser.</dc:description>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>Journal of Physics: Conference Series</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1629">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 10; Correspondence Address: F.S. Utomo; Department of Information System, Faculty of Computer Science, Universitas Amikom Purwokerto, Purwokerto, Indonesia; email: fandy_setyo_utomo@amikompurwokerto.ac.id; Conference name: 2019 International Conference on Science and Technology, ICoST 2019; Conference date: 2 November 2019 through 3 November 2019; Conference code: 160582&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2355">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083273058&amp;doi=10.2478%2fcait-2020-0008&amp;partnerID=40&amp;md5=7ec3ba51575723c7e47c7ed66b6b17a8">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:13119702%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bach</foaf:surname>
                        <foaf:givenName>N.X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Thanh</foaf:surname>
                        <foaf:givenName>P.D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oanh</foaf:surname>
                        <foaf:givenName>T.T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1630"/>
        <dcterms:isReferencedBy rdf:resource="#item_2352"/>
        <dc:subject>convolutional neural networks</dc:subject>
        <dc:subject>question answering</dc:subject>
        <dc:subject>bidirectional long-short term memory</dc:subject>
        <dc:subject>conditional random fields</dc:subject>
        <dc:subject>Question analysis</dc:subject>
        <dc:title>Question Analysis towards a Vietnamese Question Answering System in the Education Domain</dc:title>
        <dcterms:abstract>Building a computer system, which can automatically answer questions in the human language, speech or text, is a long-standing goal of the Artificial Intelligence (AI) field. Question analysis, the task of extracting important information from the input question, is the first and crucial step towards a question answering system. In this paper, we focus on the task of Vietnamese question analysis in the education domain. Our goal is to extract important information expressed by named entities in an input question, such as university names, campus names, major names, and teacher names. We present several extraction models that utilize the advantages of both traditional statistical methods with handcrafted features and more recent advanced deep neural networks with automatically learned features. Our best model achieves 88.11% in the F1 score on a corpus consisting of 3,600 Vietnamese questions collected from the fan page of the International School, Vietnam National University, Hanoi. © 2020 Ngo Xuan Bach et al., published by Sciendo.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083273058&amp;doi=10.2478%2fcait-2020-0008&amp;partnerID=40&amp;md5=7ec3ba51575723c7e47c7ed66b6b17a8</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Sciendo</dc:description>
        <bib:pages>112-128</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:13119702%20(ISSN)">
        <prism:volume>20</prism:volume>
        <dc:title>Cybernetics and Information Technologies</dc:title>
        <dc:identifier>DOI 10.2478/cait-2020-0008</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Cybern. Inf. Technol.</dcterms:alternative>
        <dc:identifier>ISSN 13119702 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1630">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 10; Correspondence Address: T.T. Oanh; Vnu International School, Vietnam National University, Hanoi, Viet Nam; email: oanhtt@isvnu.vn&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2352">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073096103&amp;doi=10.1016%2fj.csl.2019.101023&amp;partnerID=40&amp;md5=2408364d096cf5dc9fdacd88ab4b59aa">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:08852308%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Abdi</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hasan</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Arshi</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shamsuddin</foaf:surname>
                        <foaf:givenName>S.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Idris</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1631"/>
        <dcterms:isReferencedBy rdf:resource="#item_2259"/>
        <dc:subject>Text mining</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Information retrieval</dc:subject>
        <dc:subject>Data mining</dc:subject>
        <dc:subject>Query processing</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Linguistic knowledge</dc:subject>
        <dc:subject>Information extraction</dc:subject>
        <dc:subject>Question answering</dc:subject>
        <dc:subject>Collection of documents</dc:subject>
        <dc:subject>Hadith</dc:subject>
        <dc:subject>Query expansion</dc:subject>
        <dc:subject>Syntactic similarities</dc:subject>
        <dc:title>A question answering system in hadith using linguistic knowledge</dc:title>
        <dcterms:abstract>Question answering system aims at retrieving precise information from a large collection of documents. This work presents a question answering method to apply on Hadith in order to provide an informative answer corresponding to the user's query. Hadith englobes stories and qualification of the prophet Muhammad (PBSL). It also includes the sayings of his companions and their disciples. The problem with current methods is that they fail to capture the meaning when comparing a sentence and a user's query; hence there is often a conflict between the extracted sentences and user's requirements. However, our proposed method has successfully tackled this problem through: (1) avoiding extract a passage whose similarity with the query is high but whose meaning is different. (2) Computing the semantic and syntactic similarity of the sentence-to-sentence and sentence-to-query. (3) Expanding the words in both the query and sentences to tackle the fundamental problem of term mismatch between sentences and the user's query. Furthermore, in order to reduce redundant Hadith texts, the proposed method uses the greedy algorithm to impose diversity penalty on the sentences. The experimental results display that the proposed method is able to improve performance compared with the existing methods on Hadith datasets. © 2019 Elsevier Ltd</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073096103&amp;doi=10.1016%2fj.csl.2019.101023&amp;partnerID=40&amp;md5=2408364d096cf5dc9fdacd88ab4b59aa</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Academic Press</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:08852308%20(ISSN)">
        <prism:volume>60</prism:volume>
        <dc:title>Computer Speech and Language</dc:title>
        <dc:identifier>DOI 10.1016/j.csl.2019.101023</dc:identifier>
        <dcterms:alternative>Comput Speech Lang</dcterms:alternative>
        <dc:identifier>ISSN 08852308 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1631">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 22; Correspondence Address: A. Abdi; UTM Big Data Centre (BDC), Universiti Teknologi Malaysia, Johor, Malaysia; email: seyedasadollah-pd@utm.my; CODEN: CSPLE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2259">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:18770509%20(ISSN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>164</prism:volume>
                <dc:identifier>ISBN 18770509 (ISSN)</dc:identifier>
                <dc:title>Procedia Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1016/j.procs.2019.12.154</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Elsevier B.V.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Elnozahy</foaf:surname>
                        <foaf:givenName>W.A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>El Khayat</foaf:surname>
                        <foaf:givenName>G.A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cheniti-Belcadhi</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Said</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Cruz-Cunha M.M.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Varajao J.E.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Martinho R.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Rijo R.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Peres E.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Domingos D.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1632"/>
        <dcterms:isReferencedBy rdf:resource="#item_2353"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Project management</dc:subject>
        <dc:subject>Ontology</dc:subject>
        <dc:subject>Information systems</dc:subject>
        <dc:subject>Information use</dc:subject>
        <dc:subject>Information management</dc:subject>
        <dc:subject>Students</dc:subject>
        <dc:subject>Decision support systems</dc:subject>
        <dc:subject>Decision Support</dc:subject>
        <dc:subject>Decision supports</dc:subject>
        <dc:subject>Education computing</dc:subject>
        <dc:subject>Knowledge extraction</dc:subject>
        <dc:subject>Knowledge Extraction</dc:subject>
        <dc:subject>Linked data</dc:subject>
        <dc:subject>Open Education Data</dc:subject>
        <dc:subject>Open educations</dc:subject>
        <dc:subject>Student recruitment</dc:subject>
        <dc:subject>University Admission</dc:subject>
        <dc:subject>University admissions</dc:subject>
        <dc:title>Question Answering System to Support University Students' Orientation, Recruitment and Retention</dc:title>
        <dcterms:abstract>Educational institutions are creating more and more programs and developing new techniques to identify and select the right students. Many universities started to develop their own ontologies to get an understanding of their students by analyzing their data. In this paper, an information extraction framework is proposed as a part of the research project &quot;LET'SeGA&quot; to support educational institutions in selecting students for different programs. This would also help target specific student segments in marketing decisions for academic programs. In the proposed framework, an ontological model is created for universities' data which is used to support students' recruitment and retention ensuring students' success after admission. Future work will focus on testing the framework over a time period to verify that it supports the recruitment of appropriate student profiles. © 2019 The Authors. Published by Elsevier B.V.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079834504&amp;doi=10.1016%2fj.procs.2019.12.154&amp;partnerID=40&amp;md5=2dcacc5f62a71d49244e96b5f4feaf13</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Procedia Comput. Sci.</dc:description>
        <bib:pages>56-63</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>Procedia Computer Science</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1632">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 16; Correspondence Address: W.A. Elnozahy; Information Systems and Computers Department, Faculty of Commerce, Alexandria University, Alexandria, Egypt; email: walaa.elnozahy@gmail.com; Conference name: 2019 International Conference on ENTERprise Information Systems, International Conference on Project MANagement and International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019; Conference date: 16 October 2019 through 18 October 2019; Conference code: 157351&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2353">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-145036674-8%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-145036674-8 (ISBN)</dc:identifier>
                <dc:title>Web Conf. - Proc. World Wide Web Conf., WWW</dc:title>
                <dc:identifier>DOI 10.1145/3308558.3313661</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computing Machinery, Inc</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kratzwald</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Feuerriegel</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1634"/>
        <dcterms:isReferencedBy rdf:resource="#item_2311"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Websites</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Question answering</dc:subject>
        <dc:subject>E-learning</dc:subject>
        <dc:subject>Computational experiment</dc:subject>
        <dc:subject>Continuous improvements</dc:subject>
        <dc:subject>Distant Supervision</dc:subject>
        <dc:subject>Dynamics</dc:subject>
        <dc:subject>Feedback control</dc:subject>
        <dc:subject>On-line learning</dc:subject>
        <dc:subject>Online learning</dc:subject>
        <dc:subject>State-of-the-art system</dc:subject>
        <dc:subject>User feedback</dc:subject>
        <dc:title>Learning from on-line user feedback in neural question answering on the web</dc:title>
        <dcterms:abstract>Question answering promises a means of efficiently searching web-based content repositories such as Wikipedia. However, the systems of this type most prevalent today merely conduct their learning once in an offline training phase while, afterwards, all parameters remain static. Thus, the possibility of improvement over time is precluded. As a consequence of this shortcoming, question answering is not currently taking advantage of the wealth of feedback mechanisms that have become prominent on the web (e. g., buttons for liking, voting, or sharing). This is the first work that introduces a question-answering system for (web-based) content repositories with an on-line mechanism for user feedback. Our efforts have resulted in QApedia - a framework for on-line improvement based on shallow user feedback. In detail, we develop a simple feedback mechanism that allows users to express whether a question was answered satisfactorily or whether a different answer is needed. Even for this simple mechanism, the implementation represents a daunting undertaking due to the complex, multi-staged operations that underlie state-of-the-art systems for neural questions answering. Another challenge with regard to web-based use is that feedback is limited (and possibly even noisy), as the true labels remain unknown. We thus address these challenges through a novel combination of neural question answering and a dynamic process based on distant supervision, asynchronous updates, and an automatic validation of feedback credibility in order to mine high-quality training samples from the web for the purpose of achieving continuous improvement over time. Our QApedia framework is the first question-answering system that continuously refines its capabilities by improving its now dynamic content repository and thus the underlying answer extraction. QApedia not only achieves state-of-the-art results over several benchmarking datasets, but we further show that it successfully manages to learn from shallow user feedback, even when the feedback is noisy or adversarial. Altogether, our extensive experimental evaluation, with more than 2,500 hours of computational experiments, demonstrates that a feedback mechanism as simple as a binary vote (which is widespread on the web) can considerably improve performance when combined with an efficient framework for continuous learning. © 2019 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066882486&amp;doi=10.1145%2f3308558.3313661&amp;partnerID=40&amp;md5=51dc6917a9193418b536710a25fea8e9</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Web Conf. - Proc. World Wide Web Conf., WWW</dc:description>
        <bib:pages>906-916</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>The Web Conference 2019 - Proceedings of the World Wide Web Conference, WWW 2019</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1634">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 18; Conference name: 2019 World Wide Web Conference, WWW 2019; Conference date: 13 May 2019 through 17 May 2019; Conference code: 147966&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2311">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:15206149%20(ISSN);%20978-147998131-1%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2019-May</prism:volume>
                <dc:identifier>ISBN 15206149 (ISSN); 978-147998131-1 (ISBN)</dc:identifier>
                <dc:title>ICASSP IEEE Int Conf Acoust Speech Signal Process Proc</dc:title>
                <dc:identifier>DOI 10.1109/ICASSP.2019.8683538</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Agarwal</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sachdeva</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yadav</foaf:surname>
                        <foaf:givenName>R.K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Udandarao</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mittal</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gupta</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mathur</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1633"/>
        <dcterms:isReferencedBy rdf:resource="#item_2288"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Knowledge based systems</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Conversational agents</dc:subject>
        <dc:subject>Audio signal processing</dc:subject>
        <dc:subject>Concept Network</dc:subject>
        <dc:subject>Concept networks</dc:subject>
        <dc:subject>Distributed computer systems</dc:subject>
        <dc:subject>Educational Question Answering System</dc:subject>
        <dc:subject>Indexing algorithms</dc:subject>
        <dc:subject>On the flies</dc:subject>
        <dc:subject>On-The-Fly Learning</dc:subject>
        <dc:subject>Open domain question answering</dc:subject>
        <dc:subject>Pedagogical Semantic Correlations</dc:subject>
        <dc:subject>Speech communication</dc:subject>
        <dc:title>EDUQA: Educational Domain Question Answering System Using Conceptual Network Mapping</dc:title>
        <dcterms:abstract>Most of the existing question answering models can be largely compiled into two categories: i) open domain question answering models that answer generic questions and use large-scale knowledge base along with the targeted web-corpus retrieval and ii) closed domain question answering models that address focused questioning area and use complex deep learning models. Both the above models derive answers through textual comprehension methods. Due to their inability to capture the pedagogical meaning of textual content, these models are not appropriately suited to the educational field for pedagogy. In this paper, we propose an on-the-fly conceptual network model that incorporates educational semantics. The proposed model preserves correlations between conceptual entities by applying intelligent indexing algorithms on the concept network so as to improve answer generation. This model can be utilized for building interactive conversational agents for aiding classroom learning. © 2019 IEEE.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069004861&amp;doi=10.1109%2fICASSP.2019.8683538&amp;partnerID=40&amp;md5=01d0790678ba1b22daf7c5314e40b3c6</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: ICASSP IEEE Int Conf Acoust Speech Signal Process Proc</dc:description>
        <bib:pages>8137-8141</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1633">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 17; Conference name: 44th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2019; Conference date: 12 May 2019 through 17 May 2019; Conference code: 149034; CODEN: IPROD&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2288">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066883874&amp;doi=10.1145%2f3308558.3314124&amp;partnerID=40&amp;md5=a649092c9247b659b9530fb4c14d0f89">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-145036674-8 (ISBN)</dc:identifier>
                <dc:title>Web Conf. - Proc. World Wide Web Conf., WWW</dc:title>
                <dc:identifier>DOI 10.1145/3308558.3314124</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computing Machinery, Inc</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Diefenbach</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lully</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Migliatti</foaf:surname>
                        <foaf:givenName>P.H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qawasmeh</foaf:surname>
                        <foaf:givenName>O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maret</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1638"/>
        <dcterms:isReferencedBy rdf:resource="#item_2350"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Search engines</dc:subject>
        <dc:subject>End users</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>World Wide Web</dc:subject>
        <dc:subject>Knowledge based systems</dc:subject>
        <dc:subject>Semantic Web</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Knowledge base</dc:subject>
        <dc:subject>Multi-Knowledge-Base</dc:subject>
        <dc:subject>Multilingual</dc:subject>
        <dc:subject>Question Answering prototypes</dc:subject>
        <dc:title>QAnswer: A question answering prototype bridging the gap between a considerable part of the LOD cloud and end-users</dc:title>
        <dcterms:abstract>We present QAnswer, a Question Answering system which queries at the same time 3 core datasets of the Semantic Web, that are relevant for end-users. These datasets are Wikidata with Lexemes, LinkedGeodata and Musicbrainz. Additionally, it is possible to query these datasets in English, German, French, Italian, Spanish, Pourtuguese, Arabic and Chinese. Moreover, QAnswer includes a fallback option to the search engine Qwant when the answer to a question cannot be found in the datasets mentioned above. These features make QAnswer as the first prototype of a Question Answering System over a considerable part of the LOD cloud. © 2019 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066883874&amp;doi=10.1145%2f3308558.3314124&amp;partnerID=40&amp;md5=a649092c9247b659b9530fb4c14d0f89</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Web Conf. - Proc. World Wide Web Conf., WWW</dc:description>
        <bib:pages>3507-3510</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>The Web Conference 2019 - Proceedings of the World Wide Web Conference, WWW 2019</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1638">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 23; Conference name: 2019 World Wide Web Conference, WWW 2019; Conference date: 13 May 2019 through 17 May 2019; Conference code: 147966&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2350">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062591784&amp;doi=10.1145%2f3309706&amp;partnerID=40&amp;md5=dc4c008b9e1661602d5bfc9eed8bd2f8">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2158656X%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kratzwald</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Feuerriegel</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1639"/>
        <dcterms:isReferencedBy rdf:resource="#item_2347"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Search engines</dc:subject>
        <dc:subject>Knowledge management</dc:subject>
        <dc:subject>Transfer learning</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Information retrieval</dc:subject>
        <dc:subject>Information retrieval systems</dc:subject>
        <dc:subject>Information systems</dc:subject>
        <dc:subject>Information use</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Question answering</dc:subject>
        <dc:subject>Question-answer pairs</dc:subject>
        <dc:subject>Domain customization</dc:subject>
        <dc:subject>Domain-specific application</dc:subject>
        <dc:subject>Information overloads</dc:subject>
        <dc:subject>Machine comprehension</dc:subject>
        <dc:subject>Potential difference</dc:subject>
        <dc:title>Putting question-answering systems into practice: Transfer learning for efficient domain customization</dc:title>
        <dcterms:abstract>Traditional information retrieval (such as that offered by web search engines) impedes users with information overload from extensive result pages and the need to manually locate the desired information therein. Conversely, question-answering systems change how humans interact with information systems: Users can now ask specific questions and obtain a tailored answer-both conveniently in natural language. Despite obvious benefits, their use is often limited to an academic context, largely because of expensive domain customizations, which means that the performance in domain-specific applications often fails to meet expectations. This article proposes cost-efficient remedies: (i) we leverage metadata through a filtering mechanism, which increases the precision of document retrieval, and (ii) we develop a novel fuse-and-oversample approach for transfer learning to improve the performance of answer extraction. Here, knowledge is inductively transferred from related, yet different, tasks to the domain-specific application, while accounting for potential differences in the sample sizes across both tasks. The resulting performance is demonstrated with actual use cases from a finance company and the film industry, where fewer than 400 question-answer pairs had to be annotated to yield significant performance gains. As a direct implication to management, this presents a promising path to better leveraging of knowledge stored in information systems. ©2019 Association for Computing Machinery.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062591784&amp;doi=10.1145%2f3309706&amp;partnerID=40&amp;md5=dc4c008b9e1661602d5bfc9eed8bd2f8</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Association for Computing Machinery</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2158656X%20(ISSN)">
        <prism:volume>9</prism:volume>
        <dc:title>ACM Transactions on Management Information Systems</dc:title>
        <dc:identifier>DOI 10.1145/3309706</dc:identifier>
        <prism:number>4</prism:number>
        <dcterms:alternative>ACM Trans. Manage. Inf. Syst.</dcterms:alternative>
        <dc:identifier>ISSN 2158656X (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1639">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 17&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2347">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073109629&amp;doi=10.1016%2fj.procs.2019.08.179&amp;partnerID=40&amp;md5=ce4334a42c6373cd5152d7392d818446">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>157</prism:volume>
                <dc:identifier>ISBN 18770509 (ISSN)</dc:identifier>
                <dc:title>Procedia Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1016/j.procs.2019.08.179</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Elsevier B.V.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chandra</foaf:surname>
                        <foaf:givenName>Y.W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Suyanto</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Budiharto W.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1636"/>
        <dcterms:isReferencedBy rdf:resource="#item_2303"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Chatbot</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Attention mechanism</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Attention mechanisms</dc:subject>
        <dc:subject>Question-answering system</dc:subject>
        <dc:subject>University admissions</dc:subject>
        <dc:subject>Admission chatbot</dc:subject>
        <dc:subject>Instant messaging</dc:subject>
        <dc:subject>Sequence modeling</dc:subject>
        <dc:subject>Sequence-to-sequence</dc:subject>
        <dc:subject>Telecommunication services</dc:subject>
        <dc:title>Indonesian chatbot of university admission using a question answering system based on sequence-to-sequence model</dc:title>
        <dcterms:abstract>Question and Answering (QA) system is a problem in natural language processing that can be used as the system of dialogs and chatbots. It can be used as a customer service that can provide a response to the customer quickly. A QA system receives an input in the form of sentences and produces the predictive sentences that are responses to the input. Therefore, a model that can learn such conversations is needed. This research focuses on developing a chatbot based on a sequence-to-sequence model. It is trained using a data set of conversation from a university admission. Evaluation on a small dataset obtained from the Telkom University admission on Whatsapp instant messaging application shows that the model produces a quite high BLEU score of 41.04. An attention mechanism technique using the reversed sentences improves the model to gives a higher BLEU up to 44.68. © 2019 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/) Peer-review under responsibility of the scientific committee of the 4th International Conference on Computer Science and Computational Intelligence 2019.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073109629&amp;doi=10.1016%2fj.procs.2019.08.179&amp;partnerID=40&amp;md5=ce4334a42c6373cd5152d7392d818446</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Procedia Comput. Sci.</dc:description>
        <bib:pages>367-374</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>Procedia Computer Science</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1636">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 41; Correspondence Address: S. Suyanto; School of Computing, Telkom University, Bandung, West Java, Jl. Telekomunikasi No. 01 Terusan Buah Batu, 40257, Indonesia; email: suyanto@telkomuniversity.ac.id; Conference name: 4th International Conference on Computer Science and Computational Intelligence, ICCSCI 2019; Conference date: 12 September 2019 through 13 September 2019; Conference code: 152141&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2303">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-172811985-4%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2019-July</prism:volume>
                <dc:identifier>ISBN 978-172811985-4 (ISBN)</dc:identifier>
                <dc:title>Proc Int Jt Conf Neural Networks</dc:title>
                <dc:identifier>DOI 10.1109/IJCNN.2019.8852327</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bu</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1635"/>
        <dcterms:isReferencedBy rdf:resource="#item_2330"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Nearest neighbor search</dc:subject>
        <dc:subject>Knowledge based systems</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Encoding (symbols)</dc:subject>
        <dc:subject>Signal encoding</dc:subject>
        <dc:subject>encoding model</dc:subject>
        <dc:subject>Encoding models</dc:subject>
        <dc:subject>multi-task learning</dc:subject>
        <dc:subject>Multitask learning</dc:subject>
        <dc:subject>Question Answering systems</dc:subject>
        <dc:subject>Semantic retrieval</dc:subject>
        <dc:subject>semantic retrieval framework</dc:subject>
        <dc:subject>sentence matching</dc:subject>
        <dc:title>Multi-task Sentence Encoding Model for Semantic Retrieval in Question Answering Systems</dc:title>
        <dcterms:abstract>Question Answering (QA) systems are used to provide proper responses to users' questions automatically. Sentence matching is an essential task in the QA systems and is usually reformulated as a Paraphrase Identification (PI) problem. Given a question, the aim of the task is to find the most similar question from a QA knowledge base. In this paper, we propose a Multi-task Sentence Encoding Model (MSEM) for the PI problem, wherein a connected graph is employed to depict the relation between sentences, and a multi-task learning model is applied to address both the sentence matching and sentence intent classification problem. In addition, we implement a general semantic retrieval framework that combines our proposed model and the Approximate Nearest Neighbor (ANN) technology, which enables us to find the most similar question from all available candidates very quickly during online serving. The experiments show the superiority of our proposed method as compared with the existing sentence matching models. © 2019 IEEE.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073251828&amp;doi=10.1109%2fIJCNN.2019.8852327&amp;partnerID=40&amp;md5=a1820166e3aeeaade758839e5eeb98dc</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc Int Jt Conf Neural Networks</dc:description>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the International Joint Conference on Neural Networks</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1635">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 19; Conference name: 2019 International Joint Conference on Neural Networks, IJCNN 2019; Conference date: 14 July 2019 through 19 July 2019; Conference code: 152291; CODEN: 85OFA&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2330">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069962310&amp;doi=10.1109%2fTASLP.2019.2926125&amp;partnerID=40&amp;md5=847142e342248c27afe38d0df2da1aa7">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>27</prism:volume>
                <dc:title>IEEE/ACM Transactions on Audio Speech and Language Processing</dc:title>
                <dc:identifier>DOI 10.1109/TASLP.2019.2926125</dc:identifier>
                <prism:number>10</prism:number>
                <dcterms:alternative>IEEE ACM Trans. Audio Speech Lang. Process.</dcterms:alternative>
                <dc:identifier>ISSN 23299290 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lan</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiang</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1637"/>
        <dcterms:isReferencedBy rdf:resource="#item_2309"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Benchmarking</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>State-of-the-art performance</dc:subject>
        <dc:subject>Benchmark datasets</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Knowledge based systems</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Aggregation model</dc:subject>
        <dc:subject>knowledge base question answering</dc:subject>
        <dc:subject>Knowledge basis</dc:subject>
        <dc:subject>Sequence matching</dc:subject>
        <dc:title>Knowledge Base Question Answering with a Matching-Aggregation Model and Question-Specific Contextual Relations</dc:title>
        <dcterms:abstract>Making use of knowledge bases to answer questions (KBQA) is a key direction in question answering systems. Researchers have developed a diverse range of methods to address this problem, but there are still some limitations with the existing methods. Specifically, the existing neural network-based methods for KBQA have not taken advantage of the recent 'matching-Aggregation' framework for the sequence matching, and when representing a candidate answer entity, they may not choose the most useful context of the candidate for matching. In this paper, we explore the use of a 'matching-Aggregation' framework to match candidate answers with questions. We further make use of question-specific contextual relations to enhance the representations of candidate answer entities. Our complete method is able to achieve state-of-The-Art performance on two benchmark datasets: WebQuestions and SimpleQuestions. © 2014 IEEE.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069962310&amp;doi=10.1109%2fTASLP.2019.2926125&amp;partnerID=40&amp;md5=847142e342248c27afe38d0df2da1aa7</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Institute of Electrical and Electronics Engineers Inc.</dc:description>
        <bib:pages>1629-1638</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1637">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 44; Correspondence Address: Y. Lan; School of Information Systems, Singapore Management University, Singapore, Singapore; email: yslan.2015@phdis.smu.edu.sg&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2309">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:18650929%20(ISSN);%20978-981139186-6%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1037</prism:volume>
                <dc:identifier>ISBN 18650929 (ISSN); 978-981139186-6 (ISBN)</dc:identifier>
                <dc:title>Commun. Comput. Info. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-981-13-9187-3_53</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Springer Verlag</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yogish</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Manjunath</foaf:surname>
                        <foaf:givenName>T.N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hegadi</foaf:surname>
                        <foaf:givenName>R.S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Santosh K.C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Hegadi R.S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1640"/>
        <dcterms:isReferencedBy rdf:resource="#item_2367"/>
        <dc:subject>Named entity recognition</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Artificial Intelligence (AI)</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Search engines</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Character recognition</dc:subject>
        <dc:subject>Social networking (online)</dc:subject>
        <dc:subject>Information retrieval</dc:subject>
        <dc:subject>Data mining</dc:subject>
        <dc:subject>Image processing</dc:subject>
        <dc:subject>Sentiment analysis</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Digital storage</dc:subject>
        <dc:subject>Computational technique</dc:subject>
        <dc:subject>Information Retrieval (IR)</dc:subject>
        <dc:subject>Information retrieval efficiency</dc:subject>
        <dc:subject>Natural Language Processing (NLP)</dc:subject>
        <dc:subject>Natural Language Tool Kit (NLTK)</dc:subject>
        <dc:subject>Program translators</dc:subject>
        <dc:subject>Relationship extraction</dc:subject>
        <dc:subject>Social media monitoring</dc:subject>
        <dc:title>Review on Natural Language Processing Trends and Techniques Using NLTK</dc:title>
        <dcterms:abstract>In modern age of information explosion, every day millions of gigabytes of data are generated in the form of documents, web pages, e-mail, social media text, blogs etc., so importance of effective and efficient Natural Language Processing techniques become crucial for an information retrieval system, text summarization, sentiment analysis, information extraction, named entity recognition, relationship extraction, social media monitoring, text mining, language translation program, and question answering system. Natural Language Processing is a computational technique applies different levels of linguistic analysis for representing natural language into a useful representation for further processing. NLP is recognized as a challenging task in computer science and artificial intelligence because understanding human natural language is not only depends on the words but how those words are linked together to form precise meaning is also considered. Regardless of language being one of the easiest concepts for human to learn, but for training computers to understand natural language is a difficult task due to the ambiguity of language syntax and semantics. Natural Language processing techniques involves processing documents or text which reduces storage space and also reduces the size of index and understanding the given information which satisfies user’s need. NLP techniques improve the performance of the information retrieval efficiency and effective documentation processes. Common dialect handling procedures incorporates tokenization, stop word expulsion, stemming, lemmatization, parts of discourse labeling, lumping and named substance recognizer which enhances execution of NLP applications. The Natural Language Toolkit is the best possible solution for learning the ropes of NLP domain. NLTK, a collection of application packages which encourage researchers and learners in natural language processing, computational linguistics and artificial intelligence. © 2019, Springer Nature Singapore Pte Ltd.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070235639&amp;doi=10.1007%2f978-981-13-9187-3_53&amp;partnerID=40&amp;md5=e33c9aeb56917af03e45bc9b3d5d3a5f</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Commun. Comput. Info. Sci.</dc:description>
        <bib:pages>589-606</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Communications in Computer and Information Science</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1640">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 39; Correspondence Address: T.N. Manjunath; ISE, BMSIT&amp;M, Bangalore, India; email: manju.tn@bmsit.in; Conference name: 2nd International Conference on Recent Trends in Image Processing and Pattern Recognition, RTIP2R 2018; Conference date: 21 December 2018 through 22 December 2018; Conference code: 228899&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2367">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-145036754-7%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-145036754-7 (ISBN)</dc:identifier>
                <dc:title>Proc. Int. Conf. Artif. Intell. Law, ICAIL</dc:title>
                <dc:identifier>DOI 10.1145/3322640.3326742</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computing Machinery, Inc</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>M.-Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rabelo</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Goebel</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1641"/>
        <link:link rdf:resource="#item_2647"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Search engines</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Legal questions</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Information retrieval</dc:subject>
        <dc:subject>Query processing</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Question answering</dc:subject>
        <dc:subject>Evaluation results</dc:subject>
        <dc:subject>Experimental evaluation</dc:subject>
        <dc:subject>Heuristic selections</dc:subject>
        <dc:subject>Information retrieval approach</dc:subject>
        <dc:subject>Legal AI</dc:subject>
        <dc:subject>Textual entailment</dc:subject>
        <dc:title>Statute law information retrieval and entailment</dc:title>
        <dcterms:abstract>Our Yes/No statute law question answering system combines components for both statute law information retrieval and confirmation of textual entailment between statues and legal questions. We describe a statute law question answering system that exploits TF-IDF and a language model for information retrieval, and inter-paragraph entailment. We have evaluated our system using the data from the competition on legal information extraction/entailment (COLIEE-2019). The competition consists of four tasks: Tasks 1 and 2 are for the case law information extraction/entailment, and Tasks 3 and 4 are for the statute law information extraction/entailment. Here we explain our methods and evaluation results for Tasks 3 and 4. Task 3 requires the identification of civil law articles relevant to Japan legal bar exam query. For this task, we used TF-IDF and language model-based information retrieval approaches. Task 4 requires a decision on yes/no answer for previously unseen queries given relevant civil law articles. Our approach compares the approximate meanings of queries with relevant articles. Because many statute law and queries consist of more than one paragraph, we need an inter-paragraph entailment method. Our inter-paragraph entailment process exploits an analysis of statute law structure, and negation patterns to predict entailments. Using our heuristic selection of attributes, we perform two experiments which provide the basis for making a decision on the yes/no questions. One experiment uses an SVM model, and the other uses a general heuristic rule. Our experimental evaluation demonstrates the value of our method, and the results show that our method was ranked No. 1 in both of the Tasks 3 and 4 in COLIEE 2019. © 2019 Association for Computing Machinery.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071253052&amp;doi=10.1145%2f3322640.3326742&amp;partnerID=40&amp;md5=27bf8c42ea383935e2db02f24131c0cd</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Int. Conf. Artif. Intell. Law, ICAIL</dc:description>
        <bib:pages>283-289</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the 17th International Conference on Artificial Intelligence and Law, ICAIL 2019</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1641">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 23; Conference name: 17th International Conference on Artificial Intelligence and Law, ICAIL 2019; Conference date: 17 June 2019 through 21 June 2019; Conference code: 149776&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2647">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2647/Kim et al. - 2019 - Statute law information retrieval and entailment.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/pdf/10.1145/3322640.3326742</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:09:17</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044822979&amp;partnerID=40&amp;md5=30b93a903e34e28d996193c23bc0591f">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:19928645%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jusoh</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1642"/>
        <link:link rdf:resource="#item_2667"/>
        <dc:subject>Ambiguity in NLP</dc:subject>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>NLP applications</dc:subject>
        <dc:title>A study on nlp applications and ambiguity problems</dc:title>
        <dcterms:abstract>Natural language processing (NLP) has been considered as one of the important area in Artificial Intelligence. However, the progress made in natural language processing is quite slow, compared to other areas. The aim of this study is to conduct a systematic literature review for identifying the most prominent applications, techniques and challenging issues in NLP applications. To conduct this review, I had screened 587 retrieved papers from major databases such as SCOPUS and IEEE Explore, and also from Google search engine. In searching relevant papers search keywords such as &quot;natural language processing, NLP applications, and complexity of NLP applications&quot; had been used. However, to focus to the scope of the study 503 papers were excluded. Only the most prominent NLP applications namely information extraction, question answering system and automated text summarization were chosen to be reviewed. It is obvious that the challenging issue in NLP is the complexity of the natural language itself, which is the ambiguity problems that occur in various level of the language. This paper also aims at addressing ambiguity problems which occur at lexical and structural levels and significance techniques or approaches for solving the problems. Finally, the paper briefly discuss the future of NLP. © 2005 – ongoing JATIT &amp; LLS.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044822979&amp;partnerID=40&amp;md5=30b93a903e34e28d996193c23bc0591f</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Little Lion Scientific</dc:description>
        <bib:pages>1486-1499</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:19928645%20(ISSN)">
        <prism:volume>96</prism:volume>
        <dc:title>Journal of Theoretical and Applied Information Technology</dc:title>
        <prism:number>6</prism:number>
        <dcterms:alternative>J. Theor. Appl. Inf. Technol.</dcterms:alternative>
        <dc:identifier>ISSN 19928645 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1642">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 30; Correspondence Address: S. Jusoh; King Hussein School of Computing Sciences, Princess Sumaya University for Technology, Amman, Jordan; email: s.ibrahim@psut.edu.jo&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2667">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2667/Jusoh - 2005 - A STUDY ON NLP APPLICATIONS AND AMBIGUITY PROBLEMS.pdf"/>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.jatit.org/volumes/Vol96No6/4Vol96No6.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:33:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062959942&amp;doi=10.7544%2fissn1000-1239.2018.20180623&amp;partnerID=40&amp;md5=3709f0bfed83f7f3b1ae371eece64fa5">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:10001239%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hou</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wei</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lan</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cai</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1643"/>
        <dcterms:isReferencedBy rdf:resource="#item_2364"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Knowledge graphs</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Big data</dc:subject>
        <dc:subject>Medical problems</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Data mining</dc:subject>
        <dc:subject>Engineering education</dc:subject>
        <dc:subject>Knowledge engineering</dc:subject>
        <dc:subject>Knowledge graph</dc:subject>
        <dc:subject>Decision support systems</dc:subject>
        <dc:subject>Clinical decision support</dc:subject>
        <dc:subject>Construction technologies</dc:subject>
        <dc:subject>Knowledge expression</dc:subject>
        <dc:subject>Knowledge fusion</dc:subject>
        <dc:subject>Medical question answering</dc:subject>
        <dc:subject>Medical wisdom</dc:subject>
        <dc:title>Research Review of Knowledge Graph and Its Application in Medical Domain</dc:title>
        <dcterms:abstract>With the advent of the medical big data era, knowledge interconnection has received extensive attention. How to extract useful medical knowledge from massive data is the key for medical big data analysis. Knowledge graph technology provides a means to extract structured knowledge from massive texts and images.The combination of knowledge graph, big data technology and deep learning technology is becoming the core driving force for the development of artificial intelligence. The knowledge graph technology has a broad application prospect in the medical domain. The application of knowledge graph technology in the medical domain will play an important role in solving the contradiction between the supply of high-quality medical resources and the continuous increase of demand for medical services.At present, the research on medical knowledge graph is still in the exploratory stage. The existing knowledge graph technology generally has several problems such as low efficiency, multiple restrictions and poor expansion in the medical domain. This paper firstly analyzes the medical knowledge graph architecture and construction technology for the strong professionalism and complex structure of big data in the medical domain. Secondly, the key technologies and research progress of the three modules of knowledge extraction, knowledge expression, knowledge fusion and knowledge reasoning in medical knowledge map are summarized. In addition, the application status of medical knowledge maps in clinical decision support, medical intelligence semantic retrieval, medical question answering system and other medical services are introduced. Finally, the existing problems and challenges of current research are discussed and analyzed, and its development is prospected. © 2018, Science Press. All right reserved.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>Chinese</z:language>
        <z:shortTitle>知识图谱研究综述及其在医疗领域的应用</z:shortTitle>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062959942&amp;doi=10.7544%2fissn1000-1239.2018.20180623&amp;partnerID=40&amp;md5=3709f0bfed83f7f3b1ae371eece64fa5</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Science Press</dc:description>
        <bib:pages>2587-2599</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:10001239%20(ISSN)">
        <prism:volume>55</prism:volume>
        <dc:title>Jisuanji Yanjiu yu Fazhan/Computer Research and Development</dc:title>
        <dc:identifier>DOI 10.7544/issn1000-1239.2018.20180623</dc:identifier>
        <prism:number>12</prism:number>
        <dcterms:alternative>Jisuanji Yanjiu yu Fazhan</dcterms:alternative>
        <dc:identifier>ISSN 10001239 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1643">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 58; Correspondence Address: R. Wei; Network and Information Department, The First Affiliated Hospital of Xi'an Jiaotong University, Xi'an, 710061, China; email: weirong@xjtu.edu.cn; CODEN: JYYFE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2364">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047547966&amp;doi=10.1016%2fj.ipm.2018.05.001&amp;partnerID=40&amp;md5=a582ae56711263aa7eff71669f3abe80">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:03064573%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mohasseb</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bader-El-Den</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cocea</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1644"/>
        <dcterms:isReferencedBy rdf:resource="#item_2357"/>
        <dc:subject>Natural language processing (NLP)</dc:subject>
        <dc:subject>Text mining</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Machine learning</dc:subject>
        <dc:subject>Learning algorithms</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Text classification</dc:subject>
        <dc:subject>Data mining</dc:subject>
        <dc:subject>Syntactics</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Question classification</dc:subject>
        <dc:subject>Classification methods</dc:subject>
        <dc:subject>Classification process</dc:subject>
        <dc:subject>Grammar based approach</dc:subject>
        <dc:subject>Retrieval applications</dc:subject>
        <dc:title>Question categorization and classification using grammar based approach</dc:title>
        <dcterms:abstract>Question-answering has become one of the most popular information retrieval applications. Despite that most question-answering systems try to improve the user experience and the technology used in finding relevant results, many difficulties are still faced because of the continuous increase in the amount of web content. Questions Classification (QC) plays an important role in question-answering systems, with one of the major tasks in the enhancement of the classification process being the identification of questions types. A broad range of QC approaches has been proposed with the aim of helping to find a solution for the classification problems; most of these are approaches based on bag-of-words or dictionaries. In this research, we present an analysis of the different type of questions based on their grammatical structure. We identify different patterns and use machine learning algorithms to classify them. A framework is proposed for question classification using a grammar-based approach (GQCC) which exploits the structure of the questions. Our findings indicate that using syntactic categories related to different domain-specific types of Common Nouns, Numeral Numbers and Proper Nouns enable the machine learning algorithms to better differentiate between different question types. The paper presents a wide range of experiments the results show that the GQCC using J48 classifier has outperformed other classification methods with 90.1% accuracy. © 2018 Elsevier Ltd</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047547966&amp;doi=10.1016%2fj.ipm.2018.05.001&amp;partnerID=40&amp;md5=a582ae56711263aa7eff71669f3abe80</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier Ltd</dc:description>
        <bib:pages>1228-1243</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:03064573%20(ISSN)">
        <prism:volume>54</prism:volume>
        <dc:title>Information Processing and Management</dc:title>
        <dc:identifier>DOI 10.1016/j.ipm.2018.05.001</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>Inf. Process. Manage.</dcterms:alternative>
        <dc:identifier>ISSN 03064573 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1644">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 64; Correspondence Address: A. Mohasseb; School of Computing, University of Portsmouth, United Kingdom; email: alaa.mohasseb@port.ac.uk; CODEN: IPMAD&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2357">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026525620&amp;doi=10.1016%2fj.cogsys.2017.07.002&amp;partnerID=40&amp;md5=9d75b9df4b05758867cca261ae1f79cd">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:13890417%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Razzaghnoori</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sajedi</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jazani</foaf:surname>
                        <foaf:givenName>I.K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1645"/>
        <dcterms:isReferencedBy rdf:resource="#item_2358"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>support vector machine</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>learning</dc:subject>
        <dc:subject>Search engines</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>algorithm</dc:subject>
        <dc:subject>priority journal</dc:subject>
        <dc:subject>Article</dc:subject>
        <dc:subject>controlled study</dc:subject>
        <dc:subject>information processing</dc:subject>
        <dc:subject>artificial neural network</dc:subject>
        <dc:subject>Recurrent neural networks</dc:subject>
        <dc:subject>linguistics</dc:subject>
        <dc:subject>Word2vec</dc:subject>
        <dc:subject>Recurrent neural network (RNN)</dc:subject>
        <dc:subject>search engine</dc:subject>
        <dc:subject>classifier</dc:subject>
        <dc:subject>Vectors</dc:subject>
        <dc:subject>Question classification</dc:subject>
        <dc:subject>Clustering algorithms</dc:subject>
        <dc:subject>Feedforward neural networks</dc:subject>
        <dc:subject>Image retrieval</dc:subject>
        <dc:subject>lingua franca</dc:subject>
        <dc:subject>LSTM</dc:subject>
        <dc:subject>Persian (language)</dc:subject>
        <dc:subject>question classification system</dc:subject>
        <dc:subject>Recurrent Neural Networks (RNN)</dc:subject>
        <dc:subject>Tf-idf</dc:subject>
        <dc:title>Question classification in Persian using word vectors and frequencies</dc:title>
        <dcterms:abstract>The necessity of the existence of Question Answering (QA) systems becomes evident by considering the fact that the enormous amount of unstructured data created by humans nowadays, results in ineffectiveness of search engines to provide the exact solution for a given question. However, an outstanding question answering system requires an outstanding Question Classification (QC) system. Question classifier is a system that assigns a label to each question. There exist different ways of solving this problem such as rule-based, machine learning, and hybrid approaches. This paper provides a better solution for QC using machine-learning approaches. Three methods of feature extraction are proposed in this paper. The First method uses clustering algorithms to partition vocabulary into clusters and acquires feature vector corresponding to each question using clustering information. The second one suggests a method of extracting features from questions to dispose of using recurrent neural networks and to use feedforward neural networks, which have the advantage of learning faster and less need for data, instead. Each question is converted to a feature vector, which is obtained by the Word2vec method and weighted by tf-idf coefficients. The results of question classification using Support Vector Machine and Neural Network classifiers indicate the effectiveness of this type of feature vector and based on that, high performance of the proposed QC system. Finally, the third approach keeps the innovation behind first approach, but it also keeps the fact that we are dealing with a sequence based type of data into consideration. Eventually, it would be concluded that even with a limited amount of data it is reasonable to take Recurrent Neural Networks into consideration. © 2017 Elsevier B.V.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026525620&amp;doi=10.1016%2fj.cogsys.2017.07.002&amp;partnerID=40&amp;md5=9d75b9df4b05758867cca261ae1f79cd</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier B.V.</dc:description>
        <bib:pages>16-27</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:13890417%20(ISSN)">
        <prism:volume>47</prism:volume>
        <dc:title>Cognitive Systems Research</dc:title>
        <dc:identifier>DOI 10.1016/j.cogsys.2017.07.002</dc:identifier>
        <dcterms:alternative>Cogn. Sys. Res.</dcterms:alternative>
        <dc:identifier>ISSN 13890417 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1645">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 35; Correspondence Address: H. Sajedi; Dept. of Mathematics, Statistics and Computer Science, College of Science, University of Tehran, Iran; email: hhsajedi@ut.ac.ir; CODEN: CSROA&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2358">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-145036012-8%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-145036012-8 (ISBN)</dc:identifier>
                <dc:title>AIES - Proc. AAAI/ACM Conf. AI, Ethics, Soc.</dc:title>
                <dc:identifier>DOI 10.1145/3278721.3278760</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computing Machinery, Inc</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Eicher</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Polepeddi</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Goel</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1646"/>
        <dcterms:isReferencedBy rdf:resource="#item_2306"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Philosophical aspects</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Online systems</dc:subject>
        <dc:subject>E-learning</dc:subject>
        <dc:subject>education access</dc:subject>
        <dc:subject>ethics of artificial intelligence</dc:subject>
        <dc:subject>metacognition</dc:subject>
        <dc:subject>Metacognition</dc:subject>
        <dc:subject>On-line education</dc:subject>
        <dc:subject>online education</dc:subject>
        <dc:subject>questionanswering systems</dc:subject>
        <dc:subject>theory of mind</dc:subject>
        <dc:subject>Theory of minds</dc:subject>
        <dc:subject>Turing test</dc:subject>
        <dc:subject>Turing tests</dc:subject>
        <dc:subject>virtual teaching assistant</dc:subject>
        <dc:subject>Virtual teaching assistants</dc:subject>
        <dc:title>Jill Watson Doesn't Care if You're Pregnant: Grounding AI Ethics in Empirical Studies</dc:title>
        <dcterms:abstract>Jill Watson is our name for a virtual teaching assistant for a Georgia Tech course on artificial intelligence: Jill answers routine, frequently asked questions on the class discussion forum. In this paper, we outline some of the ethical issues that arose in the development and deployment of the virtual teaching assistant. We posit that experiments such as Jill Watson are critical for deeply understanding AI ethics. © 2018 ACM.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049379870&amp;doi=10.1145%2f3278721.3278760&amp;partnerID=40&amp;md5=62c1cdc5fb639d4e1ee2c5b9a99b4b83</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: AIES - Proc. AAAI/ACM Conf. AI, Ethics, Soc.</dc:description>
        <bib:pages>88-94</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>AIES 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1646">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 38; Conference name: 1st AAAI/ACM Conference on AI, Ethics, and Society, AIES 2018; Conference date: 2 February 2018 through 3 February 2018; Conference code: 144126&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2306">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049120836&amp;doi=10.1016%2fj.procs.2018.05.090&amp;partnerID=40&amp;md5=1138498159b4bc2211b80395e8c874d1">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>132</prism:volume>
                <dc:identifier>ISBN 18770509 (ISSN)</dc:identifier>
                <dc:title>Procedia Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1016/j.procs.2018.05.090</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Elsevier B.V.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sharma</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gupta</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Singh S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Asari V.K.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Patel R.B.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Sidike P.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1647"/>
        <dcterms:isReferencedBy rdf:resource="#item_2286"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>State of the art</dc:subject>
        <dc:subject>Neural networks</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Word vectors</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>neural networks</dc:subject>
        <dc:subject>question answering</dc:subject>
        <dc:subject>coattention</dc:subject>
        <dc:subject>Facebook</dc:subject>
        <dc:subject>Learning approach</dc:subject>
        <dc:subject>memory nets</dc:subject>
        <dc:subject>word vectors</dc:subject>
        <dc:title>Deep Learning Approaches for Question Answering System</dc:title>
        <dcterms:abstract>Question Answering (QA) System is very useful as most of the deep learning related problems can be modeled as a question answering problem. Consequently, the field is one of the most researched fields in computer science today. The last few years have seen considerable developments and improvement in the state of the art, much of which can be credited to upcoming of Deep Learning. In this paper, a discussion about various approaches starting from the basic NLP and algorithms based approach has been done and the paper eventually builds towards the recently proposed methods of Deep Learning. Implementation details and various tweaks in the algorithms that produced better results have also been discussed. The evaluation of the proposed models was done on twenty tasks of babI dataset of Facebook. © 2018 The Authors. Published by Elsevier Ltd.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049120836&amp;doi=10.1016%2fj.procs.2018.05.090&amp;partnerID=40&amp;md5=1138498159b4bc2211b80395e8c874d1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Procedia Comput. Sci.</dc:description>
        <bib:pages>785-794</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>Procedia Computer Science</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1647">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 53; Correspondence Address: Y. Sharma; Department of Computer Science Information System Birla, Institute of Technology Science, Pilani Campus, Pilani, 333031, India; email: yash@pilani.bits-pilani.ac.in; Conference name: 2018 International Conference on Computational Intelligence and Data Science, ICCIDS 2018; Conference date: 7 April 2018 through 8 April 2018; Conference code: 137053&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2286">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:03029743%20(ISSN);%20978-303003839-7%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>11298 LNAI</prism:volume>
                <dc:identifier>ISBN 03029743 (ISSN); 978-303003839-7 (ISBN)</dc:identifier>
                <dc:title>Lect. Notes Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-03840-3_29</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Springer Verlag</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Croce</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zelenanska</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Basili</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Ghidini C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Traverso P.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Magnini B.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Passerini A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1648"/>
        <dcterms:isReferencedBy rdf:resource="#item_2333"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>State of the art</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Non-English languages</dc:subject>
        <dc:subject>Question-answer pairs</dc:subject>
        <dc:subject>Factoid questions</dc:subject>
        <dc:subject>Neural learning</dc:subject>
        <dc:subject>Training requirement</dc:subject>
        <dc:title>Neural Learning for Question Answering in Italian</dc:title>
        <dcterms:abstract>The recent breakthroughs in the field of deep learning have lead to state-of-the-art results in several NLP tasks such as Question Answering (QA). Nevertheless, the training requirements in cross-linguistic settings are not satisfied: the datasets suitable for training of question answering systems for non English languages are often not available, which represents a significant barrier for most neural methods. This paper explores the possibility of acquiring a large scale although lower quality dataset for an open-domain factoid questions answering system in Italian. It consists of more than 60 thousands question-answer pairs and was used to train a system able to answer factoid questions against the Italian Wikipedia. The paper describes the dataset and the experiments, inspired by an equivalent counterpart for English. These show that results achievable for Italian are worse, even though they are already applicable to concrete QA tasks. © 2018, Springer Nature Switzerland AG.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057424769&amp;doi=10.1007%2f978-3-030-03840-3_29&amp;partnerID=40&amp;md5=c267f7676d78c44e8e834d66b8e613b6</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Lect. Notes Comput. Sci.</dc:description>
        <bib:pages>389-402</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1648">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 33; Correspondence Address: D. Croce; Department of Enterprise Engineering, University of Roma Tor Vergata, Rome, Italy; email: croce@info.uniroma2.it; Conference name: 17th Conference of the Italian Association for Artificial Intelligence, AI*IA 2018; Conference date: 20 November 2018 through 23 November 2018; Conference code: 221299&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2333">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:03029743%20(ISSN);%20978-303000670-9%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>11136 LNCS</prism:volume>
                <dc:identifier>ISBN 03029743 (ISSN); 978-303000670-9 (ISBN)</dc:identifier>
                <dc:title>Lect. Notes Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-00671-6_7</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Springer Verlag</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dubey</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Banerjee</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chaudhuri</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lehmann</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Suárez-Figueroa M.C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Presutti V.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Kaffee L.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Simperl E.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Sabou M.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Vrandecic D.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Celino I.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Bontcheva K.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1649"/>
        <dcterms:isReferencedBy rdf:resource="#item_2287"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Semantic Web</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Question answering</dc:subject>
        <dc:subject>Comparative analysis</dc:subject>
        <dc:subject>Entity linking</dc:subject>
        <dc:subject>GTSP</dc:subject>
        <dc:subject>Relation linking</dc:subject>
        <dc:subject>State-of-the-art approach</dc:subject>
        <dc:subject>Traveling salesman problem</dc:subject>
        <dc:subject>Travelling salesman problem</dc:subject>
        <dc:title>EARL: Joint entity and relation linking for question answering over knowledge graphs</dc:title>
        <dcterms:abstract>Many question answering systems over knowledge graphs rely on entity and relation linking components in order to connect the natural language input to the underlying knowledge graph. Traditionally, entity linking and relation linking have been performed either as dependent sequential tasks or as independent parallel tasks. In this paper, we propose a framework called EARL, which performs entity linking and relation linking as a joint task. EARL implements two different solution strategies for which we provide a comparative analysis in this paper: The first strategy is a formalisation of the joint entity and relation linking tasks as an instance of the Generalised Travelling Salesman Problem (GTSP). In order to be computationally feasible, we employ approximate GTSP solvers. The second strategy uses machine learning in order to exploit the connection density between nodes in the knowledge graph. It relies on three base features and re-ranking steps in order to predict entities and relations. We compare the strategies and evaluate them on a dataset with 5000 questions. Both strategies significantly outperform the current state-of-the-art approaches for entity and relation linking. © Springer Nature Switzerland AG 2018.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054809353&amp;doi=10.1007%2f978-3-030-00671-6_7&amp;partnerID=40&amp;md5=78f07e014fa5214f2472801f43a31468</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Lect. Notes Comput. Sci.</dc:description>
        <bib:pages>108-126</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1649">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 96; Correspondence Address: M. Dubey; Smart Data Analytics Group (SDA), University of Bonn, Bonn, Germany; email: dubey@cs.uni-bonn.de; Conference name: 17th International Semantic Web Conference, ISWC 2018; Conference date: 8 October 2018 through 12 October 2018; Conference code: 219319&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2287">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053131809&amp;doi=10.1016%2fj.procs.2018.08.221&amp;partnerID=40&amp;md5=2ccf5954618e5c6a8d32354d0cd1b9f9">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>135</prism:volume>
                <dc:identifier>ISBN 18770509 (ISSN)</dc:identifier>
                <dc:title>Procedia Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1016/j.procs.2018.08.221</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Elsevier B.V.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Arifin</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sastria</foaf:surname>
                        <foaf:givenName>T.G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barlian</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Meiliana null</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Arifin Y.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Budiharto W.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Wulandhari L.A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Sutoyo R.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Faisal null</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Gunawan A.A.S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Williem null</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Suryani D.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1651"/>
        <dcterms:isReferencedBy rdf:resource="#item_2235"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Intelligent robots</dc:subject>
        <dc:subject>Engineering education</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Anthropomorphic robots</dc:subject>
        <dc:subject>arithmetic word problem</dc:subject>
        <dc:subject>Augmented reality</dc:subject>
        <dc:subject>Augmented reality applications</dc:subject>
        <dc:subject>Augmented reality technology</dc:subject>
        <dc:subject>intelligent humanoid robot</dc:subject>
        <dc:subject>Intelligent humanoid robots</dc:subject>
        <dc:subject>Measurement standards</dc:subject>
        <dc:subject>question answering system</dc:subject>
        <dc:subject>Standard measurements</dc:subject>
        <dc:subject>User experiences (ux)</dc:subject>
        <dc:subject>Word problem</dc:subject>
        <dc:title>User Experience Metric for Augmented Reality Application: A Review</dc:title>
        <dcterms:abstract>Augmented Reality Technology is developing rapidly and has been widely used in the field of education. Due to the ease of developing AR by non-professionals, User Experience (UX) is often not considered in the application. Currently, there are no standard measurements of UX for AR applications especially in Education. The authors reviewed previous research to obtain UX references and the existing measurement standards. From the results of the review, the metrics were analyzed based on the characteristics of AR, especially in the field of education. The metrics will be recommended for the UX measurement of an AR application. The available standard metrics can be used to determine the UX quality of an AR application and contribute to the improvement of UX in AR applications, especially in the field of Education. © 2018 The Authors. Published by Elsevier Ltd.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053131809&amp;doi=10.1016%2fj.procs.2018.08.221&amp;partnerID=40&amp;md5=2ccf5954618e5c6a8d32354d0cd1b9f9</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Procedia Comput. Sci.</dc:description>
        <bib:pages>648-656</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>Procedia Computer Science</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1651">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 43; Conference name: 3rd International Conference on Computer Science and Computational Intelligence, ICCSCI 2018; Conference date: 7 September 2018 through 8 September 2018; Conference code: 138963&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2235">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:10450823%20(ISSN);%20978-099924112-7%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2018-July</prism:volume>
                <dc:identifier>ISBN 10450823 (ISSN); 978-099924112-7 (ISBN)</dc:identifier>
                <dc:title>IJCAI Int. Joint Conf. Artif. Intell.</dc:title>
                <dc:identifier>DOI 10.24963/ijcai.2018/587</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>International Joint Conferences on Artificial Intelligence</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                   <foaf:Person><foaf:surname>Lang J.</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1650"/>
        <dcterms:isReferencedBy rdf:resource="#item_2283"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>State of the art</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Generative model</dc:subject>
        <dc:subject>Question-answer pairs</dc:subject>
        <dc:subject>Complex questions</dc:subject>
        <dc:subject>Curricula</dc:subject>
        <dc:subject>Low qualities</dc:subject>
        <dc:subject>Practical measures</dc:subject>
        <dc:title>Curriculum learning for natural answer generation</dc:title>
        <dcterms:abstract>By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpora. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and unevenquality corpora could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-art, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively. © 2018 International Joint Conferences on Artificial Intelligence. All right reserved.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055722674&amp;doi=10.24963%2fijcai.2018%2f587&amp;partnerID=40&amp;md5=1564995325fdc44a7dd0b3a27b13fff3</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: IJCAI Int. Joint Conf. Artif. Intell.</dc:description>
        <bib:pages>4223-4229</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>IJCAI International Joint Conference on Artificial Intelligence</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1650">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 63; Correspondence Address: C. Liu; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; email: cao.liu@nlpr.ia.ac.cn; Conference name: 27th International Joint Conference on Artificial Intelligence, IJCAI 2018; Conference date: 13 July 2018 through 19 July 2018; Conference code: 140653&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2283">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021799133&amp;doi=10.1016%2fj.knosys.2017.06.030&amp;partnerID=40&amp;md5=860d0eaa7f8c40c9c09b5ed65f5d2cfb">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:09507051%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hao</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Weng</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qu</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1652"/>
        <dcterms:isReferencedBy rdf:resource="#item_2314"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>State of the art</dc:subject>
        <dc:subject>Software engineering</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Knowledge based systems</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Classification</dc:subject>
        <dc:subject>Dependency relation</dc:subject>
        <dc:subject>Answer type identification</dc:subject>
        <dc:subject>Baseline methods</dc:subject>
        <dc:subject>Question target</dc:subject>
        <dc:subject>Semantic analysis</dc:subject>
        <dc:subject>Semantic relations</dc:subject>
        <dc:subject>Type classifications</dc:subject>
        <dc:subject>Wordnet</dc:subject>
        <dc:subject>WordNet</dc:subject>
        <dc:title>Leveraging question target word features through semantic relation expansion for answer type classification</dc:title>
        <dcterms:abstract>Answer type classification is a vital step of question answering systems to detect the most suitable target answer type. Highly accurate identification and classification of an answer type can help identify users’ question targets and filter out irrelevant candidate answers to improve system performances. This paper proposes a novel hybrid approach, named as ATICM, for automated answer type identification and classification by utilizing both syntactic and semantic analysis. We firstly propose to integrate four strategies to identify question target features by using dependency relations and rules. Afterwards, we leverage semantic relations to expand the extracted features. Our experiment datasets are publicly available UIUC and TREC10 annotated question datasets. The result shows the ATICM approach achieves an accuracy of 93.9% on the UIUC dataset and 92.8% on the TREC10 dataset. The performance outperforms the state-of-the-art baseline methods, demonstrating its effectiveness in answer type classification. © 2017 Elsevier B.V.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021799133&amp;doi=10.1016%2fj.knosys.2017.06.030&amp;partnerID=40&amp;md5=860d0eaa7f8c40c9c09b5ed65f5d2cfb</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier B.V.</dc:description>
        <bib:pages>43-52</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:09507051%20(ISSN)">
        <prism:volume>133</prism:volume>
        <dc:title>Knowledge-Based Systems</dc:title>
        <dc:identifier>DOI 10.1016/j.knosys.2017.06.030</dc:identifier>
        <dcterms:alternative>Knowl Based Syst</dcterms:alternative>
        <dc:identifier>ISSN 09507051 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1652">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 27; Correspondence Address: H. Weng; School of Information Science and Technology, Guangdong University of Foreign Studies, China; email: ww128@qq.com; CODEN: KNSYE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2314">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015285840&amp;partnerID=40&amp;md5=0d2117c10fd37754d441df686d1dec0d">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>AAAI Conf. Artif. Intell., AAAI</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>AAAI press</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kruengkrai</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Torisawa</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hashimoto</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kloetzer</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oh</foaf:surname>
                        <foaf:givenName>J.-H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tanaka</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1653"/>
        <dcterms:isReferencedBy rdf:resource="#item_2300"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Convolutional neural network</dc:subject>
        <dc:subject>Neural networks</dc:subject>
        <dc:subject>Network architecture</dc:subject>
        <dc:subject>Websites</dc:subject>
        <dc:subject>State-of-the-art methods</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Convolution</dc:subject>
        <dc:subject>Back-ground knowledge</dc:subject>
        <dc:subject>Lung Cancer</dc:subject>
        <dc:subject>Web texts</dc:subject>
        <dc:title>Improving event causality recognition with multiple background knowledge sources using multi-column convolutional neural networks</dc:title>
        <dcterms:abstract>We propose a method for recognizing such event causalities as &quot;smoke cigarettes&quot;? &quot;die of lung cancer&quot; using background knowledge taken from web texts as well as original sentences from which candidates for the causalities were extracted. We retrieve texts related to our event causality candidates from four billion web pages by three distinct methods, including a why-question answering system, and feed them to our multi-column convolutional neural networks. This allows us to identify the useful background knowledge scattered in web texts and effectively exploit the identified knowledge to recognize event causalities. We empirically show that the combination of our neural network architecture and background knowledge significantly improves average precision, while the previous state-of-the-art method gains just a small benefit from such background knowledge. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015285840&amp;partnerID=40&amp;md5=0d2117c10fd37754d441df686d1dec0d</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: AAAI Conf. Artif. Intell., AAAI</dc:description>
        <bib:pages>3466-3473</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>31st AAAI Conference on Artificial Intelligence, AAAI 2017</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1653">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 62; Conference name: 31st AAAI Conference on Artificial Intelligence, AAAI 2017; Conference date: 4 February 2017 through 10 February 2017; Conference code: 130407&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2300">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-194562653-1%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-194562653-1 (ISBN)</dc:identifier>
                <dc:title>*SEM - Jt. Conf. Lex. Comput. Semant., Proc.</dc:title>
                <dc:identifier>DOI 10.18653/v1/s17-1029</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sachan</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xing</foaf:surname>
                        <foaf:givenName>E.P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1654"/>
        <dcterms:isReferencedBy rdf:resource="#item_2312"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Demonstrations</dc:subject>
        <dc:subject>Geometry</dc:subject>
        <dc:subject>Geometry problems</dc:subject>
        <dc:subject>Learning by demonstration</dc:subject>
        <dc:subject>Textbooks</dc:subject>
        <dc:title>Learning to solve geometry problems from natural language demonstrations in textbooks</dc:title>
        <dcterms:abstract>Humans as well as animals are good at imitation. Inspired by this, the learning by demonstration view of machine learning learns to perform a task from detailed example demonstrations. In this paper, we introduce the task of question answering using natural language demonstrations where the question answering system is provided with detailed demonstrative solutions to questions in natural language. As a case study, we explore the task of learning to solve geometry problems using demonstrative solutions available in textbooks. We collect a new dataset of demonstrative geometry solutions from textbooks and explore approaches that learn to interpret these demonstrations as well as to use these interpretations to solve geometry problems. Our approaches show improvements over the best previously published system for solving geometry problems. © 2017 Association for Computational Linguistics.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036620336&amp;doi=10.18653%2fv1%2fs17-1029&amp;partnerID=40&amp;md5=fd09e7899fd7c3427b194e7c9dd71ef8</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: *SEM - Jt. Conf. Lex. Comput. Semant., Proc.</dc:description>
        <bib:pages>251-261</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>*SEM 2017 - 6th Joint Conference on Lexical and Computational Semantics, Proceedings</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1654">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 30; Conference name: 6th Joint Conference on Lexical and Computational Semantics, *SEM 2017; Conference date: 3 August 2017 through 4 August 2017; Conference code: 131701&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2312">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-194562675-3%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1</prism:volume>
                <dc:identifier>ISBN 978-194562675-3 (ISBN)</dc:identifier>
                <dc:title>ACL - Annu. Meet. Assoc. Comput. Linguist., Proc. Conf. (Long Papers)</dc:title>
                <dc:identifier>DOI 10.18653/v1/P17-1019</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1657"/>
        <dcterms:isReferencedBy rdf:resource="#item_2294"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Linguistics</dc:subject>
        <dc:subject>Knowledge based systems</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Sequence learning</dc:subject>
        <dc:subject>Encoder-decoder</dc:subject>
        <dc:subject>Empirical studies</dc:subject>
        <dc:subject>Knowledge base</dc:subject>
        <dc:subject>Natural response</dc:subject>
        <dc:subject>Real-world datasets</dc:subject>
        <dc:title>Generating natural answers by incorporating copying and retrieving mechanisms in sequence-to-sequence learning</dc:title>
        <dcterms:abstract>Generating answer with natural language sentence is very important in real-world question answering systems, which needs to obtain a right answer as well as a coherent natural response. In this paper, we propose an end-to-end question answering system called COREQA in sequence-to-sequence learning, which incorporates copying and retrieving mechanisms to generate natural answers within an encoder-decoder framework. Specifically, in COREQA, the semantic units (words, phrases and entities) in a natural answer are dynamically predicted from the vocabulary, copied from the given question and/or retrieved from the corresponding knowledge base jointly. Our empirical study on both synthetic and real-world datasets demonstrates the efficiency of COREQA, which is able to generate correct, coherent and natural answers for knowledge inquired questions. © 2017 Association for Computational Linguistics.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040946732&amp;doi=10.18653%2fv1%2fP17-1019&amp;partnerID=40&amp;md5=a322d03968175ebb9ab98478bfd2c3c4</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: ACL - Annu. Meet. Assoc. Comput. Linguist., Proc. Conf. (Long Papers)</dc:description>
        <bib:pages>199-208</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1657">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 104; Conference name: 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017; Conference date: 30 July 2017 through 4 August 2017; Conference code: 132950&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2294">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019964925&amp;doi=10.1016%2fj.specom.2017.05.001&amp;partnerID=40&amp;md5=76110d9c502fc41ff5a1f1daa1ed513e">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:01676393%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jaya Kumar</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schmidt</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Köhler</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1655"/>
        <dcterms:isReferencedBy rdf:resource="#item_2231"/>
        <dc:subject>Knowledge graphs</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Speech recognition</dc:subject>
        <dc:subject>Speech synthesis</dc:subject>
        <dc:subject>Graphic methods</dc:subject>
        <dc:subject>Automatic speech recognition</dc:subject>
        <dc:subject>Linked data</dc:subject>
        <dc:subject>Linked datum</dc:subject>
        <dc:subject>Speech</dc:subject>
        <dc:subject>Spoken interface</dc:subject>
        <dc:subject>Spoken language understanding</dc:subject>
        <dc:subject>Spoken question answering</dc:subject>
        <dc:title>A knowledge graph based speech interface for question answering systems</dc:title>
        <dcterms:abstract>Speech interfaces to conversational systems have been a focus in academia and industry for over a decade due to its applicability as a natural interface. Speech recognition and speech synthesis constitute the important input and output modules respectively for such spoken interface systems. In this paper, the speech recognition interface for question answering applications is reviewed, and existing limitations are discussed. The existing spoken question answering (QA) systems use an automatic speech recogniser by adapting acoustic and language models for the speech interface and off-the-shelf language processing systems for question interpretation. In the process, the impact of recognition errors and language processing inaccuracies is neglected. It is illustrated in the paper how a semantically rich knowledge graph can be used to solve automatic speech recognition and language processing specific problems. A simple concatenation of a speech recogniser and a natural language processing system is a shallow method for a speech interface. An effort beyond merely concatenating these two units is required to develop a successful spoken question answering system. It is illustrated in this paper how a knowledge graph based structured data can be used to build a unified system combining speech recognition and language understanding. This facilitates the use of a semantically rich data model for speech interface. © 2017 Elsevier B.V.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019964925&amp;doi=10.1016%2fj.specom.2017.05.001&amp;partnerID=40&amp;md5=76110d9c502fc41ff5a1f1daa1ed513e</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier B.V.</dc:description>
        <bib:pages>1-12</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:01676393%20(ISSN)">
        <prism:volume>92</prism:volume>
        <dc:title>Speech Communication</dc:title>
        <dc:identifier>DOI 10.1016/j.specom.2017.05.001</dc:identifier>
        <dcterms:alternative>Speech Commun</dcterms:alternative>
        <dc:identifier>ISSN 01676393 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1655">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 24; Correspondence Address: A. Jaya Kumar; Netmedia, Fraunhofer IAIS, Germany and Enterprise Information Systems, University of Bonn, Germany; email: ashwini.jaya.kumar@iais.fraunhofer.de; CODEN: SCOMD&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2231">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027923512&amp;doi=10.1016%2fj.ipm.2016.06.006&amp;partnerID=40&amp;md5=0bc9131b66bff800ad04a151931fd850">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>53</prism:volume>
                <dc:title>Information Processing and Management</dc:title>
                <dc:identifier>DOI 10.1016/j.ipm.2016.06.006</dc:identifier>
                <prism:number>1</prism:number>
                <dcterms:alternative>Inf. Process. Manage.</dcterms:alternative>
                <dc:identifier>ISSN 03064573 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hazrina</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sharef</foaf:surname>
                        <foaf:givenName>N.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ibrahim</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Murad</foaf:surname>
                        <foaf:givenName>M.A.A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Noah</foaf:surname>
                        <foaf:givenName>S.A.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1656"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Linguistics</dc:subject>
        <dc:subject>Natural language questions</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Question processing</dc:subject>
        <dc:subject>Ambiguity types</dc:subject>
        <dc:subject>Conceptual frameworks</dc:subject>
        <dc:subject>Linked open data (LOD)</dc:subject>
        <dc:subject>Natural language question</dc:subject>
        <dc:subject>Semantic question answering</dc:subject>
        <dc:subject>SQA disambiguation solution</dc:subject>
        <dc:subject>Structure complexity</dc:subject>
        <dc:title>Review on the advancements of disambiguation in semantic question answering system</dc:title>
        <dcterms:abstract>Ambiguity is a potential problem in any semantic question answering (SQA) system due to the nature of idiosyncrasy in composing natural language (NL) question and semantic resources. Thus, disambiguation of SQA systems is a field of ongoing research. Ambiguity occurs in SQA because a word or a sentence can have more than one meaning or multiple words in the same language can share the same meaning. Therefore, an SQA system needs disambiguation solutions to select the correct meaning when the linguistic triples matched with multiple KB concepts, and enumerate similar words especially when linguistic triples do not match with any KB concept. The latest development in this field is a solution for SQA systems that is able to process a complex NL question while accessing open-domain data from linked open data (LOD). The contributions in this paper include (1) formulating an SQA conceptual framework based on an in-depth study of existing SQA processes; (2) identifying the ambiguity types, specifically in English based on an interdisciplinary literature review; (3) highlighting the ambiguity types that had been resolved by the previous SQA studies; and (4) analysing the results of the existing SQA disambiguation solutions, the complexity of NL question processing, and the complexity of data retrieval from KB(s) or LOD. The results of this review demonstrated that out of thirteen types of ambiguity identified in the literature, only six types had been successfully resolved by the previous studies. Efforts to improve the disambiguation are in progress for the remaining unresolved ambiguity types to improve the accuracy of the formulated answers by the SQA system. The remaining ambiguity types are potentially resolved in the identified SQA process based on ambiguity scenarios elaborated in this paper. The results of this review also demonstrated that most existing research on SQA systems have treated the processing of the NL question complexity separate from the processing of the KB structure complexity. © 2016 Elsevier Ltd</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027923512&amp;doi=10.1016%2fj.ipm.2016.06.006&amp;partnerID=40&amp;md5=0bc9131b66bff800ad04a151931fd850</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier Ltd</dc:description>
        <bib:pages>52-69</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1656">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 32; Correspondence Address: S. Hazrina; Department of Computer Science, Faculty of Computer Science and Information Technology, University Putra Malaysia, Selangor, Malaysia; email: hazrina@gmail.com; CODEN: IPMAD&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038030204&amp;doi=10.1017%2fS1351324917000304&amp;partnerID=40&amp;md5=72938518ba7c5b521e78bff2539569ab">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:13513249%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Azmi</foaf:surname>
                        <foaf:givenName>A.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alshenaifi</foaf:surname>
                        <foaf:givenName>N.A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1660"/>
        <dcterms:isReferencedBy rdf:resource="#item_2313"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Performance measure</dc:subject>
        <dc:subject>Question-answer pairs</dc:subject>
        <dc:subject>Arabic texts</dc:subject>
        <dc:subject>Named entities</dc:subject>
        <dc:subject>Open sources</dc:subject>
        <dc:subject>Rhetorical structure theory</dc:subject>
        <dc:subject>Stop word</dc:subject>
        <dc:title>Lemaza: An Arabic why-question answering system</dc:title>
        <dcterms:abstract>Question answering systems retrieve information from documents in response to queries. Most of the questions are who- and what-type questions that deal with named entities. A less common and more challenging question to deal with is the why -question. In this paper, we introduce Lemaza (Arabic for why), a system for automatically answering why -questions for Arabic texts. The system is composed of four main components that make use of the Rhetorical Structure Theory. To evaluate Lemaza, we prepared a set of why -question-answer pairs whose answer can be found in a corpus that we compiled out of Open Source Arabic Corpora. Lemaza performed best when the stop-words were not removed. The performance measure was 72.7%, 79.2% and 78.7% for recall, precision and c@1, respectively. Copyright © Cambridge University Press 2017.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038030204&amp;doi=10.1017%2fS1351324917000304&amp;partnerID=40&amp;md5=72938518ba7c5b521e78bff2539569ab</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Cambridge University Press</dc:description>
        <bib:pages>877-903</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:13513249%20(ISSN)">
        <prism:volume>23</prism:volume>
        <dc:title>Natural Language Engineering</dc:title>
        <dc:identifier>DOI 10.1017/S1351324917000304</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>Nat Lang Eng</dcterms:alternative>
        <dc:identifier>ISSN 13513249 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1660">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 26; CODEN: NLENF&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2313">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-145034891-1%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-145034891-1 (ISBN)</dc:identifier>
                <dc:title>Proc Int Conf Artif Intell Law</dc:title>
                <dc:identifier>DOI 10.1145/3086512.3086550</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computing Machinery</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>M.-Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Goebel</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1659"/>
        <link:link rdf:resource="#item_2649"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Search engines</dc:subject>
        <dc:subject>Computer circuits</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Legal texts</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Legal text mining</dc:subject>
        <dc:subject>Information retrieval</dc:subject>
        <dc:subject>Data mining</dc:subject>
        <dc:subject>Information use</dc:subject>
        <dc:subject>Question answering</dc:subject>
        <dc:subject>Information retrieval approach</dc:subject>
        <dc:subject>Textual entailment</dc:subject>
        <dc:subject>Legal information retrieval</dc:subject>
        <dc:subject>Logic representation</dc:subject>
        <dc:subject>Logic-based representation</dc:subject>
        <dc:subject>Unsupervised learning</dc:subject>
        <dc:subject>Unsupervised learning method</dc:subject>
        <dc:title>Two-step cascaded textual entailment for legal bar exam question answering</dc:title>
        <dcterms:abstract>Our legal question answering system combines legal information retrieval and textual entailment, and exploits semantic information using a logic-based representation. We have evaluated our system using the data from the competition on legal information extraction/entailment (COLIEE)-2017. The competition focuses on the legal information processing required to answer yes/no questions from Japanese legal bar exams, and it consists of two phases: ad hoc legal information retrieval (Phase 1), and textual entailment (Phase 2). Phase 1 requires the identification of Japan civil law articles relevant to a legal bar exam query. For this phase, we have used an information retrieval approach using TF-IDF combined with a simple language model. Phase 2 requires a yes/no decision for previously unseen queries, which we approach by comparing the approximate meanings of queries with relevant statutes. Our meaning extraction process uses a selection of features based on a kind of paraphrase, coupled with a condition/conclusion/exception analysis of articles and queries. We also extract and exploit negation patterns from the articles. We construct a logic-based representation as a semantic analysis result, and then classify questions into easy and difficult types by analyzing the logic representation. If a question is in our easy category, we simply obtain the entailment answer from the logic representation; otherwise we use an unsupervised learning method to obtain the entailment answer. Experimental evaluation shows that our result ranked highest in the Phase 2 amongst all COLIEE-2017 competitors. © 2017 Copyright is held by the owner/author(s).</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045921804&amp;doi=10.1145%2f3086512.3086550&amp;partnerID=40&amp;md5=061b9baf5683403ebdb1a5d58a3ae9c7</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc Int Conf Artif Intell Law</dc:description>
        <bib:pages>283-290</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the International Conference on Artificial Intelligence and Law</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1659">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 26; Conference name: 16th International Conference on Artificial Intelligence and Law, ICAIL 2017; Conference date: 12 June 2017 through 16 June 2017; Conference code: 133052; CODEN: 85OAA&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2649">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2649/Kim e Goebel - 2017 - Two-step cascaded textual entailment for legal bar exam question answering.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/pdf/10.1145/3086512.3086550</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:09:24</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029680484&amp;doi=10.1016%2fj.knosys.2017.09.015&amp;partnerID=40&amp;md5=c957615188d01666b7e84997989f079d">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0950-7051"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rodrigo</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Peñas</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1658"/>
        <link:link rdf:resource="#item_2665"/>
        <link:link rdf:resource="#item_2664"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Complex questions</dc:subject>
        <dc:subject>Evaluation campaigns</dc:subject>
        <dc:subject>Knowledge based systems</dc:subject>
        <dc:subject>Knowledge basis</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Software engineering</dc:subject>
        <dc:subject>Textual inference</dc:subject>
        <dc:subject>Traditional approaches</dc:subject>
        <dc:subject>Validation</dc:subject>
        <dc:title>A study about the future evaluation of Question-Answering systems</dc:title>
        <dcterms:abstract>Evaluation campaigns of Question Answering (QA) systems have contributed to the development of such technologies. These campaigns have promoted some changes oriented to overcome results. However, at this period we see how systems have reached an upper bound, as well as systems are still far away from answering complex questions. In this paper, we overview the main QA evaluations over free text, paying special attention to the changes encouraged at such campaigns. We observe that systems still return a high proportion of incorrect answers and that the changes are almost not included in traditional approaches. Moreover, we analyze QA collections in order to obtain better insights about the main challenges for current QA systems. We detect that QA systems find very difficult to deal with different rewordings in questions and documents, as well as to infer information that is not explicitly mentioned in texts. Based on those observations, we recommend a set of directions for future evaluations, suggesting the application of textual inference and knowledge bases as a way for improving results. © 2017</dcterms:abstract>
        <dc:date>2017-12-01</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029680484&amp;doi=10.1016%2fj.knosys.2017.09.015&amp;partnerID=40&amp;md5=c957615188d01666b7e84997989f079d</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier B.V.</dc:description>
        <bib:pages>83-93</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0950-7051">
        <prism:volume>137</prism:volume>
        <dc:title>Knowledge-Based Systems</dc:title>
        <dc:identifier>DOI 10.1016/j.knosys.2017.09.015</dc:identifier>
        <dcterms:alternative>Knowl Based Syst</dcterms:alternative>
        <dc:identifier>ISSN 0950-7051</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1658">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 30; Correspondence Address: A. Rodrigo; NLP &amp; IR Group at UNED, Juan del Rosal 16, Madrid, Spain; email: alvarory@lsi.uned.es; CODEN: KNSYE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2665">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2665/Rodrigo e Peñas - 2017 - A study about the future evaluation of Question-Answering systems.pdf"/>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://pdf.sciencedirectassets.com/271505/1-s2.0-S0950705117X0021X/1-s2.0-S0950705117304161/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAQaCXVzLWVhc3QtMSJHMEUCIQCzSFE06soa7rhbiUxhsJStdl%2BOKWpweI8Xj9IkLNhDngIgCZXOrTW8JE2Ccpq5Ra%2FnirCaPWWiAHL%2F0RigwEgRN54qvAUI3f%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDKlWjA0UZp7xwwdtgyqQBa7r%2F04dGyt02svEX3zgb13btSbHZRoGEXTSHnseXOWlGqnJhHqqLXJnjQc3tDw3tphuX4fZkMC1Xiz6bF8DtkVg47Hl9kqKLFiEwRpBcBRf7BHRBE3QobLtfD6vElt6BzkOo3L32oSc1pHza%2BcczFpH4rKMLeoxkqmW4yZH2c5S3tZj8VEkjO66hQ%2BxUFX6L5lz07Wgooy0gLmBf2PmnfwPCUdIiOHQ9dzz%2Bte6WRvskIRYcG9FcTukrHYBbpwElkpXiNAurxqfBWiC6UmN3o5M96KwfPR4zdOIjRvwEARzuGWL4vOYvIkZxpHY8llo2bH0pFPlPKVW4yWC2Gth%2BawP9zlE%2B%2B%2F0WJL6x4Tnbak7%2FgUSTkp8%2BvbPiicTaI9mo9LO1JKFpmLeElyzfNPpBsue3YjDB557lWtAYeZ%2BaF1QRnU8rSuBAH77kHAjY8GoZADLjHoVBoMr3jKrFayIgUGC0Wyw2zou36MvCfUIlW3aAmROnrtfqeBz81VeX%2BMeGnRGNuRd7v1%2BdRrivtmIkU3fHSMP2R3ksd2Ct3lsVNZPYBdiCBqsuloO5ca99Ls1JiQzQ9LwGmm9p6NptHxpTM9APUbhEhX09ZoKrrqCrVft%2FkJViWgO2dwuL1yEKxi4XuijyE2k%2BiZ%2F9%2FPHdSOcLq4Gvz2yfGrLH0hw%2Frxt4KcadRKkAaHIyYZdHP3laLm5RrasRAWHkTF0uUdOT3UhY0OIakWKvpa%2FL4FnKmaD5NSHmf67kaWpQJLxGr8sF8o5tNa6tAf9MTXeuXr9jniJnDpoJe4ljtf7qk0PF6YEQ8T7Q2jXDcTmy1bFZDtQ8ZE1gZcuYPZK%2BNY4RV9OfEt0NExYj5AcGEDnMjoWP8XP8Nd7MKHX27sGOrEBJiesu6x0qvA10KU3Wj1EV0AX70GVGN3%2FcSA4AsVV341xtiAxlFPLZf3mIwB1QWmoq5gFunb1%2Bl3BCNlw60GaDZQ5jNQqIDsOWXr%2FAGTO%2Fd2pOoK7Cqmwsy4pW1zohrAm%2ByIC6JEdl%2BZLiZCnaD1z3yczYMRyjKJhXRvB2whJk4viFNHNl3mz4j%2BhsjJQf8uTydjXbLo0PGMHrv6bYOmq6DzlLraCVPfb2k0qSQ3GnVa6&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20250102T203157Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTY7MCBW2EE%2F20250102%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=d7f8e898f6554811561d5a927046e2e5324c5662e5198ecf29a35bf953e4b02a&amp;hash=e1781a044e6cac6f86fd60d9326d4fd9d5aa6aa57fd3cb074bad1f03ee178a77&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S0950705117304161&amp;tid=spdf-94e6c1fc-c28b-4b50-a505-5b0299982dec&amp;sid=af8ef9f4253fd043dd586b081af4b12954d3gxrqb&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=16145e02040657515351&amp;rr=8fbd84027eb2e3b8&amp;cc=pt</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:32:23</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_2664">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2664/S0950705117304161.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0950705117304161?pes=vor&amp;utm_source=scopus&amp;getft_integrator=scopus</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:32:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-145034655-9%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2017-May</prism:volume>
                <dc:identifier>ISBN 978-145034655-9 (ISBN)</dc:identifier>
                <dc:title>Conf Hum Fact Comput Syst Proc</dc:title>
                <dc:identifier>DOI 10.1145/3025453.3025781</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computing Machinery</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gurari</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grauman</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1661"/>
        <dcterms:isReferencedBy rdf:resource="#item_2282"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Machine learning</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Human engineering</dc:subject>
        <dc:subject>Forecasting</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Novel applications</dc:subject>
        <dc:subject>Visual question answering</dc:subject>
        <dc:subject>Blind people</dc:subject>
        <dc:subject>Crowdsourcing</dc:subject>
        <dc:subject>Existing systems</dc:subject>
        <dc:subject>Human response</dc:subject>
        <dc:title>CrowdVerge: Predicting if people will agree on the answer to a visual question</dc:title>
        <dcterms:abstract>Visual question answering systems empower users to ask any question about any image and receive a valid answer. However, existing systems do not yet account for the fact that a visual question can lead to a single answer or multiple different answers. While a crowd often agrees, disagreements do arise for many reasons including that visual questions are ambiguous, subjective, or difficult. We propose a model, CrowdVerge, for automatically predicting from a visual question whether a crowd would agree on one answer. We then propose how to exploit these predictions in a novel application to efficiently collect all valid answers to visual questions. Specifically, we solicit fewer human responses when answer agreement is expected and more human responses otherwise. Experiments on 121, 811 visual questions asked by sighted and blind people show that, compared to existing crowdsourcing systems, our system captures the same answer diversity with typically 14-23% less crowd involvement. © 2017 ACM.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044846499&amp;doi=10.1145%2f3025453.3025781&amp;partnerID=40&amp;md5=6474e0a90cafd0dad68dd01d40e1cd3f</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Conf Hum Fact Comput Syst Proc</dc:description>
        <bib:pages>3511-3522</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Conference on Human Factors in Computing Systems - Proceedings</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1661">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 37; Conference name: 2017 ACM SIGCHI Conference on Human Factors in Computing Systems, CHI 2017; Conference date: 6 May 2017 through 11 May 2017; Conference code: 127654&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2282">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168887254&amp;doi=10.1007%2fs00146-023-01756-4&amp;partnerID=40&amp;md5=b8862d84a40c54ca88126cffec89259e">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:09515666%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Munn</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Magee</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Arora</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1662"/>
        <dcterms:isReferencedBy rdf:resource="#item_2239"/>
        <dc:subject>AI</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>AI systems</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>AI Technologies</dc:subject>
        <dc:subject>Large language model</dc:subject>
        <dc:subject>Data harvesting</dc:subject>
        <dc:subject>GPT-3</dc:subject>
        <dc:subject>Instructgpt</dc:subject>
        <dc:subject>InstructGPT</dc:subject>
        <dc:subject>Truthfulness</dc:subject>
        <dc:subject>Veracity</dc:subject>
        <dc:title>Truth machines: synthesizing veracity in AI language models</dc:title>
        <dcterms:abstract>As AI technologies are rolled out into healthcare, academia, human resources, law, and a multitude of other domains, they become de-facto arbiters of truth. But truth is highly contested, with many different definitions and approaches. This article discusses the struggle for truth in AI systems and the general responses to date. It then investigates the production of truth in InstructGPT, a large language model, highlighting how data harvesting, model architectures, and social feedback mechanisms weave together disparate understandings of veracity. It conceptualizes this performance as an operationalization of truth, where distinct, often-conflicting claims are smoothly synthesized and confidently presented into truth-statements. We argue that these same logics and inconsistencies play out in Instruct’s successor, ChatGPT, reiterating truth as a non-trivial problem. We suggest that enriching sociality and thickening “reality” are two promising vectors for enhancing the truth-evaluating capacities of future language models. We conclude, however, by stepping back to consider AI truth-telling as a social practice: what kind of “truth” do we as listeners desire? © The Author(s) 2023.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168887254&amp;doi=10.1007%2fs00146-023-01756-4&amp;partnerID=40&amp;md5=b8862d84a40c54ca88126cffec89259e</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer Science and Business Media Deutschland GmbH</dc:description>
        <bib:pages>2759-2773</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:09515666%20(ISSN)">
        <prism:volume>39</prism:volume>
        <dc:title>AI and Society</dc:title>
        <dc:identifier>DOI 10.1007/s00146-023-01756-4</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>AI Soc.</dcterms:alternative>
        <dc:identifier>ISSN 09515666 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1662">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 10; Correspondence Address: L. Munn; Digital Cultures and Societies, University of Queensland, Saint Lucia, Australia; email: l.munn@uq.edu.au&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2239">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186143430&amp;doi=10.1098%2frsta.2023.0254&amp;partnerID=40&amp;md5=0b39f0303457f7345f006969b8bfd5de">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1364503X%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Katz</foaf:surname>
                        <foaf:givenName>D.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bommarito</foaf:surname>
                        <foaf:givenName>M.J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gao</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Arredondo</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1663"/>
        <link:link rdf:resource="#item_2639"/>
        <dc:subject>GPT-4</dc:subject>
        <dc:subject>ChatGPT</dc:subject>
        <dc:subject>large language models</dc:subject>
        <dc:subject>diagnosis</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>article</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Large language model</dc:subject>
        <dc:subject>Zero-shot learning</dc:subject>
        <dc:subject>large language model</dc:subject>
        <dc:subject>Legal services</dc:subject>
        <dc:subject>alanine aminotransferase</dc:subject>
        <dc:subject>Bar exam</dc:subject>
        <dc:subject>Bar Exam</dc:subject>
        <dc:subject>legal complexity</dc:subject>
        <dc:subject>Legal complexity</dc:subject>
        <dc:subject>legal language</dc:subject>
        <dc:subject>Legal language</dc:subject>
        <dc:subject>legal services</dc:subject>
        <dc:subject>Multi-state</dc:subject>
        <dc:subject>multiple choice test</dc:subject>
        <dc:subject>Performance tests</dc:subject>
        <dc:subject>Uniform bar</dc:subject>
        <dc:title>GPT-4 passes the bar exam</dc:title>
        <dcterms:abstract>In this paper, we experimentally evaluate the zero-shot performance of GPT-4 against prior generations of GPT on the entire uniform bar examination (UBE), including not only the multiple-choice multistate bar examination (MBE), but also the open-ended multistate essay exam (MEE) and multistate performance test (MPT) components. On the MBE, GPT-4 significantly outperforms both human test-takers and prior models, demonstrating a 26% increase over ChatGPT and beating humans in five of seven subject areas. On the MEE and MPT, which have not previously been evaluated by scholars, GPT-4 scores an average of 4.2/6.0 when compared with much lower scores for ChatGPT. Graded across the UBE components, in the manner in which a human test-taker would be, GPT-4 scores approximately 297 points, significantly in excess of the passing threshold for all UBE jurisdictions. These findings document not just the rapid and remarkable advance of large language model performance generally, but also the potential for such models to support the delivery of legal services in society. This article is part of the theme issue 'A complexity science approach to law and governance'.  © 2024 The Authors.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186143430&amp;doi=10.1098%2frsta.2023.0254&amp;partnerID=40&amp;md5=0b39f0303457f7345f006969b8bfd5de</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Royal Society Publishing</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1364503X%20(ISSN)">
        <prism:volume>382</prism:volume>
        <dc:title>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</dc:title>
        <dc:identifier>DOI 10.1098/rsta.2023.0254</dc:identifier>
        <prism:number>2270</prism:number>
        <dcterms:alternative>Philos. Trans. R. Soc. A Math. Phys. Eng. Sci.</dcterms:alternative>
        <dc:identifier>ISSN 1364503X (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1663">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 41; Correspondence Address: D.M. Katz; Illinois Tech, Chicago Kent College of Law, Chicago, United States; email: dkatz3@kentlaw.iit.edu&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2639">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2639/Katz et al. - 2024 - GPT-4 passes the bar exam.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2023.0254</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:08:32</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186140789&amp;doi=10.1098%2frsta.2023.0159&amp;partnerID=40&amp;md5=e43de89be2d93775cb39d4a6e45b3dbc">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>382</prism:volume>
                <dc:title>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</dc:title>
                <dc:identifier>DOI 10.1098/rsta.2023.0159</dc:identifier>
                <prism:number>2270</prism:number>
                <dcterms:alternative>Philos. Trans. R. Soc. A Math. Phys. Eng. Sci.</dcterms:alternative>
                <dc:identifier>ISSN 1364503X (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nay</foaf:surname>
                        <foaf:givenName>J.J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Karamardian</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lawsky</foaf:surname>
                        <foaf:givenName>S.B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tao</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bhat</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jain</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>A.T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Choi</foaf:surname>
                        <foaf:givenName>J.H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kasai</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1665"/>
        <link:link rdf:resource="#item_2641"/>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>Artificial Intelligence</dc:subject>
        <dc:subject>Machine learning</dc:subject>
        <dc:subject>Lawyers</dc:subject>
        <dc:subject>large language models</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>Humans</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Machine-learning</dc:subject>
        <dc:subject>Codes (symbols)</dc:subject>
        <dc:subject>Large language model</dc:subject>
        <dc:subject>Performance</dc:subject>
        <dc:subject>Case-studies</dc:subject>
        <dc:subject>language</dc:subject>
        <dc:subject>Language</dc:subject>
        <dc:subject>Legal services</dc:subject>
        <dc:subject>computational law</dc:subject>
        <dc:subject>Computational law</dc:subject>
        <dc:subject>Law inform code</dc:subject>
        <dc:subject>law informs code</dc:subject>
        <dc:subject>law-informed AI</dc:subject>
        <dc:subject>Law-informed AI</dc:subject>
        <dc:subject>lawyer</dc:subject>
        <dc:subject>Tax attorneys</dc:subject>
        <dc:subject>Taxation</dc:subject>
        <dc:title>Large language models as tax attorneys: a case study in legal capabilities emergence</dc:title>
        <dcterms:abstract>Better understanding of Large Language Models' (LLMs) legal analysis abilities can contribute to improving the efficiency of legal services, governing artificial intelligence and leveraging LLMs to identify inconsistencies in law. This paper explores LLM capabilities in applying tax law. We choose this area of law because it has a structure that allows us to set up automated validation pipelines across thousands of examples, requires logical reasoning and maths skills, and enables us to test LLM capabilities in a manner relevant to real-world economic lives of citizens and companies. Our experiments demonstrate emerging legal understanding capabilities, with improved performance in each subsequent OpenAI model release. We experiment with retrieving and using the relevant legal authority to assess the impact of providing additional legal context to LLMs. Few-shot prompting, presenting examples of question-answer pairs, is also found to significantly enhance the performance of the most advanced model, GPT-4. The findings indicate that LLMs, particularly when combined with prompting enhancements and the correct legal texts, can perform at high levels of accuracy but not yet at expert tax lawyer levels. As LLMs continue to advance, their ability to reason about law autonomously could have significant implications for the legal profession and AI governance. This article is part of the theme issue 'A complexity science approach to law and governance'.  © 2024 The Authors.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186140789&amp;doi=10.1098%2frsta.2023.0159&amp;partnerID=40&amp;md5=e43de89be2d93775cb39d4a6e45b3dbc</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Royal Society Publishing</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_1665">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 9; Correspondence Address: J.J. Nay; CodeX, Center for Legal Informatics, Stanford University, Stanford, United States; email: john.j.nay@gmail.com&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2641">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2641/Nay et al. - 2024 - Large language models as tax attorneys a case study in legal capabilities emergence.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2023.0159</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:08:40</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171432053&amp;doi=10.1007%2fs11934-023-01184-3&amp;partnerID=40&amp;md5=800d9fc4add0957dab0c0fe5bafdd32e">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:15272737%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Talyshinskii</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Naik</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hameed</foaf:surname>
                        <foaf:givenName>B.M.Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Juliebø-Jones</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Somani</foaf:surname>
                        <foaf:givenName>B.K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1666"/>
        <dcterms:isReferencedBy rdf:resource="#item_2341"/>
        <dc:subject>privacy</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>Artificial Intelligence</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>GPT</dc:subject>
        <dc:subject>Healthcare</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>Humans</dc:subject>
        <dc:subject>workflow</dc:subject>
        <dc:subject>adult</dc:subject>
        <dc:subject>security</dc:subject>
        <dc:subject>physician</dc:subject>
        <dc:subject>review</dc:subject>
        <dc:subject>Chatbot</dc:subject>
        <dc:subject>patient coding</dc:subject>
        <dc:subject>health care personnel</dc:subject>
        <dc:subject>patient care</dc:subject>
        <dc:subject>Physicians</dc:subject>
        <dc:subject>decision support system</dc:subject>
        <dc:subject>artificial intelligence chatbot</dc:subject>
        <dc:subject>Patient Care</dc:subject>
        <dc:subject>urology</dc:subject>
        <dc:subject>Urology</dc:subject>
        <dc:title>Potential of AI-Driven Chatbots in Urology: Revolutionizing Patient Care Through Artificial Intelligence</dc:title>
        <dcterms:abstract>Purpose of Review: Artificial intelligence (AI) chatbots have emerged as a potential tool to transform urology by improving patient care and physician efficiency. With an emphasis on their potential advantages and drawbacks, this literature review offers a thorough assessment of the state of AI-driven chatbots in urology today. Recent Findings: The capacity of AI-driven chatbots in urology to give patients individualized and timely medical advice is one of its key advantages. Chatbots can help patients prioritize their symptoms and give advice on the best course of treatment. By automating administrative duties and offering clinical decision support, chatbots can also help healthcare providers. Before chatbots are widely used in urology, there are a few issues that need to be resolved. The precision of chatbot diagnoses and recommendations might be impacted by technical constraints like system errors and flaws. Additionally, issues regarding the security and privacy of patient data must be resolved, and chatbots must adhere to all applicable laws. Important issues that must be addressed include accuracy and dependability because any mistakes or inaccuracies could seriously harm patients. The final obstacle is resistance from patients and healthcare professionals who are hesitant to use new technology or who value in-person encounters. Summary: AI-driven chatbots have the potential to significantly improve urology care and efficiency. However, it is essential to thoroughly test and ensure the accuracy of chatbots, address privacy and security concerns, and design user-friendly chatbots that can integrate into existing workflows. By exploring various scenarios and examining the current literature, this review provides an analysis of the prospects and limitations of implementing chatbots in urology. © 2023, The Author(s).</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171432053&amp;doi=10.1007%2fs11934-023-01184-3&amp;partnerID=40&amp;md5=800d9fc4add0957dab0c0fe5bafdd32e</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer</dc:description>
        <bib:pages>9-18</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:15272737%20(ISSN)">
        <prism:volume>25</prism:volume>
        <dc:title>Current Urology Reports</dc:title>
        <dc:identifier>DOI 10.1007/s11934-023-01184-3</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Curr. Urol. Rep.</dcterms:alternative>
        <dc:identifier>ISSN 15272737 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1666">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 13; Correspondence Address: P. Juliebø-Jones; Department of Urology, Haukeland University Hospital, Bergen, Norway; email: jonesurology@gmail.com&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2341">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178203067&amp;doi=10.1002%2fadma.202306733&amp;partnerID=40&amp;md5=496aa6ff8f35a8d27c127750a652da32">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0935-9648"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tao</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1667"/>
        <dcterms:isReferencedBy rdf:resource="#item_2322"/>
        <dcterms:isReferencedBy rdf:resource="#item_1999"/>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>AI for Science</dc:subject>
        <dc:subject>CRYSTAL-STRUCTURE PREDICTION</dc:subject>
        <dc:subject>ELECTROCATALYSTS</dc:subject>
        <dc:subject>ELECTROLYTES</dc:subject>
        <dc:subject>GUIDED DISCOVERY</dc:subject>
        <dc:subject>INORGANIC CRYSTALS</dc:subject>
        <dc:subject>MACHINE LEARNING FRAMEWORK</dc:subject>
        <dc:subject>materials informatics</dc:subject>
        <dc:subject>materials science</dc:subject>
        <dc:subject>NEURAL-NETWORK</dc:subject>
        <dc:subject>OPTIMIZATION</dc:subject>
        <dc:subject>SINGLE</dc:subject>
        <dc:subject>STRUCTURE DATABASE</dc:subject>
        <dc:subject>Research fields</dc:subject>
        <dc:subject>Machine design</dc:subject>
        <dc:subject>Computing power</dc:subject>
        <dc:subject>Material science</dc:subject>
        <dc:subject>Algorithm computing</dc:subject>
        <dc:subject>Artificial intelligence for science</dc:subject>
        <dc:subject>Cognitive law</dc:subject>
        <dc:subject>Continuous innovation</dc:subject>
        <dc:subject>Material Informatics</dc:subject>
        <dc:subject>Physical chemistry</dc:subject>
        <dc:subject>Robot programming</dc:subject>
        <dc:subject>Scientific method</dc:subject>
        <dc:subject>Scientific researches</dc:subject>
        <dc:title>MatGPT: A Vane of Materials Informatics from Past, Present, to Future</dc:title>
        <dcterms:abstract>Combining materials science, artificial intelligence (AI), physical chemistry, and other disciplines, materials informatics is continuously accelerating the vigorous development of new materials. The emergence of “GPT (Generative Pre-trained Transformer) AI” shows that the scientific research field has entered the era of intelligent civilization with “data” as the basic factor and “algorithm + computing power” as the core productivity. The continuous innovation of AI will impact the cognitive laws and scientific methods, and reconstruct the knowledge and wisdom system. This leads to think more about materials informatics. Here, a comprehensive discussion of AI models and materials infrastructures is provided, and the advances in the discovery and design of new materials are reviewed. With the rise of new research paradigms triggered by “AI for Science”, the vane of materials informatics: “MatGPT”, is proposed and the technical path planning from the aspects of data, descriptors, generative models, pretraining models, directed design models, collaborative training, experimental robots, as well as the efforts and preparations needed to develop a new generation of materials informatics, is carried out. Finally, the challenges and constraints faced by materials informatics are discussed, in order to achieve a more digital, intelligent, and automated construction of materials informatics with the joint efforts of more interdisciplinary scientists. © 2023 Wiley-VCH GmbH.</dcterms:abstract>
        <dc:date>2024 FEB</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:coverage>WOS:001112112900001</dc:coverage>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178203067&amp;doi=10.1002%2fadma.202306733&amp;partnerID=40&amp;md5=496aa6ff8f35a8d27c127750a652da32</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: John Wiley and Sons Inc</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0935-9648">
        <prism:volume>36</prism:volume>
        <dc:title>Advanced Materials</dc:title>
        <dc:identifier>DOI 10.1002/adma.202306733</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>Adv Mater</dcterms:alternative>
        <dc:identifier>ISSN 0935-9648</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1667">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 15; Correspondence Address: J. Li; National Key Laboratory of Science and Technology on Micro/Nano Fabrication, Shanghai Jiao Tong University, Shanghai, 200240, China; email: lijinjin@sjtu.edu.cn; CODEN: ADVME&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2322">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_1999">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;13&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;13&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;236&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188067362&amp;doi=10.1016%2fj.heliyon.2024.e28106&amp;partnerID=40&amp;md5=cb8a1cb6d725198e618ba25389438478">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:24058440%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Moneus</foaf:surname>
                        <foaf:givenName>A.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sahari</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1668"/>
        <link:link rdf:resource="#item_2652"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Machine translation</dc:subject>
        <dc:subject>Human translation</dc:subject>
        <dc:subject>Legal translation</dc:subject>
        <dc:subject>Translation software</dc:subject>
        <dc:title>Artificial intelligence and human translation: A contrastive study based on legal texts</dc:title>
        <dcterms:abstract>Artificial intelligence has advanced significantly in recent years, affecting multiple aspects of life. In particular, this has had an impact on the machine translation of texts, reducing or removing human interaction. Artificial intelligence (AI)-based translation software models have thus become widely available, and these now include Google Translate, Bing, Microsoft Translator, DeepL, Reverso, Systran Translate, and Amazon Translate. Several computer-aided translation (CAT) tools such as Memoq, Trados, Smartcat, Lokalise, Smartling, Crowdin, TextUnited, and Memsource are also available. More recently, artificial intelligence has been applied in the development of applications such as ChatGPT, ChatSonic, GPT-3 Playground, Chat GPT 4 and YouChat, which simulate conversational responses to researchers' inquiries, mimicking human interactions more directly. This study thus aimed to examine any remaining contrasts between human and AI translation in the legal field to investigate the potential hypothesis that there is now no difference between human and AI translation. The paper thus also examined concerns about whether the need for human translators will decline in the face of AI development, as well as beginning to assess whether it will ever be possible for those in the legal field to depend only on machine translation. To achieve this, a collection of legal texts from various contracts was chosen, and these pieces were both allocated to legal translators and subjected to AI translation systems. Using a contrastive methodology, the study thus examined the differences between AI and human translation, examining the strengths and weaknesses of both approaches and discussing the situations in which each approach might be most effective. © 2024 The Authors</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188067362&amp;doi=10.1016%2fj.heliyon.2024.e28106&amp;partnerID=40&amp;md5=cb8a1cb6d725198e618ba25389438478</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier Ltd</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:24058440%20(ISSN)">
        <prism:volume>10</prism:volume>
        <dc:title>Heliyon</dc:title>
        <dc:identifier>DOI 10.1016/j.heliyon.2024.e28106</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>Heliyon</dcterms:alternative>
        <dc:identifier>ISSN 24058440 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1668">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 8; Correspondence Address: A.M. Moneus; Sana'a University, Yemen; email: moneus55@gmail.com&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2652">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2652/Moneus e Sahari - 2024 - Artificial intelligence and human translation A contrastive study based on legal texts.pdf"/>
        <dc:title>Texto Completo</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.cell.com/article/S2405844024041379/pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:09:50</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:979-835037086-7%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 979-835037086-7 (ISBN)</dc:identifier>
                <dc:title>Proc. - Int. Conf. Smart Technol., Commun. Robot., STCR</dc:title>
                <dc:identifier>DOI 10.1109/STCR59085.2023.10397023</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Senthil Pandi</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Farook</foaf:surname>
                        <foaf:givenName>A.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kingston</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Harikumar R.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Babu C.G.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Poongodi C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Deepa D.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1669"/>
        <link:link rdf:resource="#item_2646"/>
        <dc:subject>CNN</dc:subject>
        <dc:subject>Network architecture</dc:subject>
        <dc:subject>Long short-term memory</dc:subject>
        <dc:subject>Legal judgements</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>LSTM</dc:subject>
        <dc:subject>BI-LSTM</dc:subject>
        <dc:subject>Bismuth compounds</dc:subject>
        <dc:subject>Crime scenes</dc:subject>
        <dc:subject>Critical analysis</dc:subject>
        <dc:subject>Illiterate peoples</dc:subject>
        <dc:subject>Real power</dc:subject>
        <dc:subject>RNN</dc:subject>
        <dc:subject>Scenario-based</dc:subject>
        <dc:title>Scenario based Deep Learning Prediction for Legal Judgment using LSTM and CNN</dc:title>
        <dcterms:abstract>The problem facing society is the dark side of law ignorance. This paper is planned to build the solution to solve the problem of people's ignorance in the law department. This paper provides deep analyses of problem scenarios or crime scenes and provides technical suggestions for people irrespective of branch. It guides the people to take the necessary steps to make them aware of the case section and the evidence to be needed. This helps to glow the light of justice for illiterate people. It gives the work of architecture of law with chains of the future. This function gives the superpower of critical analysis and support assistance of legal terms. The lawyer's more critical work and legal terms are not aware by the people, this gives ultimate protection to survive and grip the handle of justice. This shows the real power of justice and the work of architecture of law. This is the best system people wish for the assistance of law and the judiciary. This shows the percentage of success in terms of law and evidence required. This gives real-time assistance based on the understanding of the scenario of the case and the type of case for which it is required. The NLP is used for the preprocessing steps to get the feature of the documents and also helps in linear vectorizing. This works on the methodology of vectorization using BERT with the dataset of cases. The vectorized input is trained into the LSTM and CNN for model generation. The generated model is accumulated for the best results for each scenario and propagates to find the network and form the connections.  © 2023 IEEE.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185001934&amp;doi=10.1109%2fSTCR59085.2023.10397023&amp;partnerID=40&amp;md5=e7bdb909d6c10e19a3efc997c85634d2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. - Int. Conf. Smart Technol., Commun. Robot., STCR</dc:description>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings - 3rd International Conference on Smart Technologies, Communication and Robotics 2023, STCR 2023</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1669">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 32; Correspondence Address: S. Senthil Pandi; Rajalakshmi Engineering College, Department of Ces, Chennai, India; email: mailtosenthil.ks@gmail.com; Conference name: 3rd International Conference on Smart Technologies, Communication and Robotics, STCR 2023; Conference date: 9 December 2023 through 10 December 2023; Conference code: 196842&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2646">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2646/Senthil Pandi et al. - 2023 - Scenario based Deep Learning Prediction for Legal Judgment using LSTM and CNN.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=10397023&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:09:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170633110&amp;partnerID=40&amp;md5=cb3bdc7cb66f22fc9e9f7b951d347fd9">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>202</prism:volume>
                <dc:identifier>ISBN 26403498 (ISSN)</dc:identifier>
                <dc:title>Proc. Mach. Learn. Res.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>ML Research Press</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dettmers</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zettlemoyer</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Krause A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Brunskill E.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Cho K.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Engelhardt B.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Sabato S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Scarlett J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1670"/>
        <dcterms:isReferencedBy rdf:resource="#item_2386"/>
        <dc:subject>Economic and social effects</dc:subject>
        <dc:subject>Model size</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Performance</dc:subject>
        <dc:subject>Zero-shot learning</dc:subject>
        <dc:subject>Bit precision</dc:subject>
        <dc:subject>Original model</dc:subject>
        <dc:subject>Quantisation</dc:subject>
        <dc:subject>Scalings</dc:subject>
        <dc:subject>Shot accuracy</dc:subject>
        <dc:subject>Small memory footprint</dc:subject>
        <dc:subject>Trade off</dc:subject>
        <dc:title>The case for 4-bit precision: k-bit Inference Scaling Laws</dc:title>
        <dcterms:abstract>Quantization methods reduce the number of bits required to represent each parameter in a model, trading accuracy for smaller memory footprints and inference latencies. However, the final model size depends on both the number of parameters of the original model and the rate of compression. For example, a 30B 8-bit model and a 60B 4-bit model have the same number of bits but may have very different zero-shot accuracies. In this work, we study this trade-off by developing inference scaling laws of zero-shot performance in Large Language Models (LLMs) to determine the bit-precision and model size that maximizes zero-shot performance. We run more than 35,000 experiments with 16-bit inputs and k-bit parameters to examine which zero-shot quantization methods improve scaling for 3 to 8-bit precision at scales of 19M to 176B parameters across the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size - splitting the parameters into small independently quantized blocks - and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that 4-bit precision is almost universally optimal for total model bits and zero-shot accuracy. © 2023 Proceedings of Machine Learning Research. All rights reserved.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170633110&amp;partnerID=40&amp;md5=cb3bdc7cb66f22fc9e9f7b951d347fd9</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Mach. Learn. Res.</dc:description>
        <bib:pages>7750-7774</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>Proceedings of Machine Learning Research</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1670">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 33; Correspondence Address: T. Dettmers; University of Washington, United States; email: dettmers@cs.washington.edu; Conference name: 40th International Conference on Machine Learning, ICML 2023; Conference date: 23 July 2023 through 29 July 2023; Conference code: 191855&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2386">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164593928&amp;partnerID=40&amp;md5=fef886c8c7e3dd05e778fa4ef24dcc51">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>203</prism:volume>
                <dc:identifier>ISBN 26403498 (ISSN)</dc:identifier>
                <dc:title>Proc. Mach. Learn. Res.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>ML Research Press</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jang</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ye</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Seo</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Albalak A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Zhou C.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Raffel C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Ramachandran D.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Ruder S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Ma X.</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1672"/>
        <dcterms:isReferencedBy rdf:resource="#item_2273"/>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Performance</dc:subject>
        <dc:subject>Zero-shot learning</dc:subject>
        <dc:subject>Down-stream</dc:subject>
        <dc:subject>Case-studies</dc:subject>
        <dc:subject>Inverse problems</dc:subject>
        <dc:subject>Human performance</dc:subject>
        <dc:subject>New approaches</dc:subject>
        <dc:subject>Modeling type</dc:subject>
        <dc:subject>Performance gaps</dc:subject>
        <dc:title>Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts</dc:title>
        <dcterms:abstract>Previous work has shown that there exists a scaling law between the size of Language Models (LMs) and their zero-shot performance on different downstream NLP tasks. In this work, we show that this phenomenon does not hold when evaluating large LMs on tasks with negated prompts, but instead shows an inverse scaling law. We evaluate 9 different tasks with negated prompts on (1) pretrained LMs (OPT &amp; GPT-3) of varying sizes (125M - 175B), (2) LMs further pretrained to generalize to novel prompts (InstructGPT), (3) LMs provided with few-shot examples, and (4) LMs fine-tuned specifically on negated prompts; all LM types perform worse on negated prompts as they scale and show a huge performance gap between the human performance when comparing the average score on both original and negated prompts. By highlighting a critical limitation of existing LMs and methods, we urge the community to develop new approaches of developing LMs that actually follow the given instructions. We provide the code and the datasets to explore negated prompts at this link. © 2023 Proceedings of Machine Learning Research. All rights reserved.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164593928&amp;partnerID=40&amp;md5=fef886c8c7e3dd05e778fa4ef24dcc51</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Mach. Learn. Res.</dc:description>
        <bib:pages>52-62</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>Proceedings of Machine Learning Research</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1672">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 25; Correspondence Address: J. Jang; KAIST, South Korea; email: joeljang@kaist.ac.kr; S. Ye; KAIST, South Korea; email: seonghyeon.ye@kaist.ac.kr; Conference name: 1st Transfer Learning for Natural Language Processing Workshop, NLP 2022; Conference code: 189765&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2273">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:10816011%20(ISSN);%20978-166549336-9%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2023-May</prism:volume>
                <dc:identifier>ISBN 10816011 (ISSN); 978-166549336-9 (ISBN)</dc:identifier>
                <dc:title>Proc. IEEE Symp. Secur. Privacy</dc:title>
                <dc:identifier>DOI 10.1109/SP46215.2023.10179300</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lukas</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Salem</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sim</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tople</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wutschitz</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zanella-Béguelin</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1673"/>
        <dcterms:isReferencedBy rdf:resource="#item_2267"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Economic and social effects</dc:subject>
        <dc:subject>Risk perception</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Differential privacies</dc:subject>
        <dc:subject>Extraction</dc:subject>
        <dc:subject>Data mining</dc:subject>
        <dc:subject>Sentence level</dc:subject>
        <dc:subject>Data extraction</dc:subject>
        <dc:subject>Data reconstruction</dc:subject>
        <dc:subject>Data-Extraction</dc:subject>
        <dc:subject>Data-Reconstruction</dc:subject>
        <dc:subject>Differential-Privacy</dc:subject>
        <dc:subject>Inference attacks</dc:subject>
        <dc:subject>Information leakage</dc:subject>
        <dc:subject>Language-Models</dc:subject>
        <dc:subject>Personally identifiable information</dc:subject>
        <dc:subject>Personally-Identifiable-Information</dc:subject>
        <dc:subject>Reconstruction attacks</dc:subject>
        <dc:subject>Scrubbing</dc:subject>
        <dc:title>Analyzing Leakage of Personally Identifiable Information in Language Models</dc:title>
        <dcterms:abstract>Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence-or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10× more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing-pii-leakage.  © 2023 IEEE.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159803874&amp;doi=10.1109%2fSP46215.2023.10179300&amp;partnerID=40&amp;md5=e87948c24215af8d66a2b9cbe47f1a4a</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. IEEE Symp. Secur. Privacy</dc:description>
        <bib:pages>346-363</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings - IEEE Symposium on Security and Privacy</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1673">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 27; Correspondence Address: N. Lukas; University of Waterloo, Canada; email: nlukas@uwaterloo.ca; Conference name: 44th IEEE Symposium on Security and Privacy, SP 2023; Conference date: 22 May 2023 through 25 May 2023; Conference code: 190916&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2267">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:1090705X%20(ISSN);%20978-166547000-1%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2022-August</prism:volume>
                <dc:identifier>ISBN 1090705X (ISSN); 978-166547000-1 (ISBN)</dc:identifier>
                <dc:title>Proc. Int. Conf. Requir. Eng.</dc:title>
                <dc:identifier>DOI 10.1109/RE54965.2022.00011</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>IEEE Computer Society</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Abualhaija</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Arora</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sleimi</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Briand</foaf:surname>
                        <foaf:givenName>L.C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Knauss E.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Mussbacher G.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Arora C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Bano M.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Schneider J.-G.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1677"/>
        <link:link rdf:resource="#item_2633"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Requirements engineering</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Requirement engineering</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Regulatory compliance</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>Natural Language Processing (NLP)</dc:subject>
        <dc:subject>Language Models (LMs)</dc:subject>
        <dc:subject>Multidocuments</dc:subject>
        <dc:subject>Regulatory Compliance</dc:subject>
        <dc:subject>Requirements Engineering</dc:subject>
        <dc:subject>Software-systems</dc:subject>
        <dc:title>Automated Question Answering for Improved Understanding of Compliance Requirements: A Multi-Document Study</dc:title>
        <dcterms:abstract>Software systems are increasingly subject to regulatory compliance. Extracting compliance requirements from regulations is challenging. Ideally, locating compliance-related information in a regulation requires a joint effort from requirements engineers and legal experts, whose availability is limited. However, regulations are typically long documents spanning hundreds of pages, containing legal jargon, applying complicated natural language structures, and including cross-references, thus making their analysis effort-intensive. In this paper, we propose an automated question-answering (QA) approach that assists requirements engineers in finding the legal text passages relevant to compliance requirements. Our approach utilizes large-scale language models fine-tuned for QA, including BERT and three variants. We evaluate our approach on 107 question-answer pairs, manually curated by subject-matter experts, for four different European regulatory documents. Among these documents is the general data protection regulation (GDPR) - a major source for privacy-related requirements. Our empirical results show that, in $\approx 94$% of the cases, our approach finds the text passage containing the answer to a given question among the top five passages that our approach marks as most relevant. Further, our approach successfully demarcates, in the selected passage, the right answer with an average accuracy of $\approx$91%.  © 2022 IEEE.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138923987&amp;doi=10.1109%2fRE54965.2022.00011&amp;partnerID=40&amp;md5=7cf2c7cf23621ca83b2f0cb20317838e</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Int. Conf. Requir. Eng.</dc:description>
        <bib:pages>39-50</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the IEEE International Conference on Requirements Engineering</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1677">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 23; Conference name: 30th IEEE International Requirements Engineering Conference, RE 2022; Conference date: 15 August 2022 through 19 August 2022; Conference code: 183667&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2633">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2633/Abualhaija et al. - 2022 - Automated Question Answering for Improved Understanding of Compliance Requirements A Multi-Document.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=9920030&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:07:53</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117364329&amp;doi=10.1007%2fs11227-021-04097-5&amp;partnerID=40&amp;md5=69ec1e9813b5cc2a2339c11d8c4f1c44">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:09208542%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wan</foaf:surname>
                        <foaf:givenName>C.-X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1675"/>
        <dcterms:isReferencedBy rdf:resource="#item_2292"/>
        <dc:subject>Convolutional neural networks</dc:subject>
        <dc:subject>Convolutional neural network</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Character recognition</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Text classification</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Convolution</dc:subject>
        <dc:subject>Signal encoding</dc:subject>
        <dc:subject>Multilayer neural networks</dc:subject>
        <dc:subject>Complex networks</dc:subject>
        <dc:subject>BERT model</dc:subject>
        <dc:subject>Bidirectional encoder representation from transformer model</dc:subject>
        <dc:subject>Economic activities</dc:subject>
        <dc:subject>Economic development</dc:subject>
        <dc:subject>Economics</dc:subject>
        <dc:subject>Finance</dc:subject>
        <dc:subject>Multiple effect</dc:subject>
        <dc:subject>Potential law</dc:subject>
        <dc:subject>Recognition of causality</dc:subject>
        <dc:subject>Sentence recognition</dc:subject>
        <dc:title>Financial causal sentence recognition based on BERT-CNN text classification</dc:title>
        <dcterms:abstract>By studying the causality contained in financial texts, we can further reveal more potential laws of economic activities, such as “factors promoting stable and healthy economic development,” “The central bank's use of the loan window to issue money will increase the probability of inflation,” “The consequence of overcapacity is a decline in product prices,” and so on. Causal sentence recognition usually includes two sub-tasks: one is to design rules or templates to find candidate causal sentences; the other is to design a classifier to sort candidate causal sentences to finally identify the causal sentence. This article first focuses on the characteristics of complex sentence patterns of multiple causes and one effect, multiple effects and one cause, and multiple causes and multiple effects in financial review texts, and provides a relatively complete candidate causal sentence identification rules, which can identify both simple causal sentences and complex causal sentences. A BERT-CNN (Bidirectional Encoder Representations from Transformers-Convolutional Neural Networks) combination model is proposed for the classification of candidate causal sentences. On the one hand, by adding a CNN (Convolutional Neural Networks) structure to the specific task layer of the BERT (Bidirectional Encoder Representations from Transformers) model to capture important local information in the text. On the other hand, in order to make better use of the self-attention mechanism, the local text representation and the output of the BERT are input together in the multi-layer transformer encoder. A complete representation of the text is finally obtained through a single-layer transformer encoder. Experimental results show that our model is significantly better than the most advanced baseline model, with a 5.31 pts improvement in F1 over previous analyzers. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117364329&amp;doi=10.1007%2fs11227-021-04097-5&amp;partnerID=40&amp;md5=69ec1e9813b5cc2a2339c11d8c4f1c44</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer</dc:description>
        <bib:pages>6503-6527</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:09208542%20(ISSN)">
        <prism:volume>78</prism:volume>
        <dc:title>Journal of Supercomputing</dc:title>
        <dc:identifier>DOI 10.1007/s11227-021-04097-5</dc:identifier>
        <prism:number>5</prism:number>
        <dcterms:alternative>J Supercomput</dcterms:alternative>
        <dc:identifier>ISSN 09208542 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1675">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 44; Correspondence Address: C.-X. Wan; School of Information and Technology, Jiangxi University of Finance and Economics, Nanchang, 330013, China; email: wanchangxuan@263.net; CODEN: JOSUE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2292">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135026611&amp;doi=10.1371%2fjournal.pone.0272287&amp;partnerID=40&amp;md5=a317d663629fa3085bd0bef1fca2fe69">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:19326203%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>de Menezes-Neto</foaf:surname>
                        <foaf:givenName>E.J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Clementino</foaf:surname>
                        <foaf:givenName>M.B.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1676"/>
        <link:link rdf:resource="#item_2651"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>Humans</dc:subject>
        <dc:subject>article</dc:subject>
        <dc:subject>prediction</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Deep Learning</dc:subject>
        <dc:subject>law enforcement</dc:subject>
        <dc:subject>human experiment</dc:subject>
        <dc:subject>nonhuman</dc:subject>
        <dc:subject>court</dc:subject>
        <dc:subject>bird</dc:subject>
        <dc:subject>Brazil</dc:subject>
        <dc:subject>correlation coefficient</dc:subject>
        <dc:subject>Law Enforcement</dc:subject>
        <dc:title>Using deep learning to predict outcomes of legal appeals better than human experts: A study with data from Brazilian federal courts</dc:title>
        <dcterms:abstract>Legal scholars have been trying to predict the outcomes of trials for a long time. In recent years, researchers have been harnessing advancements in machine learning to predict the behavior of natural and social processes. At the same time, the Brazilian judiciary faces a challenging number of new cases every year, which generates the need to improve the throughput of the justice system. Based on those premises, we trained three deep learning architectures, ULMFiT, BERT, and Big Bird, on 612,961 Federal Small Claims Courts appeals within the Brazilian 5th Regional Federal Court to predict their outcomes. We compare the predictive performance of the models to the predictions of 22 highly skilled experts. All models outperform human experts, with the best one achieving a Matthews Correlation Coefficient of 0.3688 compared to 0.1253 from the human experts. Our results demonstrate that natural language processing and machine learning techniques provide a promising approach for predicting legal outcomes. We also release the Brazilian Courts Appeal Dataset for the 5th Regional Federal Court (BrCAD-5), containing data from 765,602 appeals to promote further developments in this area.  © 2022 Jacob de Menezes-Neto, Clementino. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135026611&amp;doi=10.1371%2fjournal.pone.0272287&amp;partnerID=40&amp;md5=a317d663629fa3085bd0bef1fca2fe69</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Public Library of Science</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:19326203%20(ISSN)">
        <prism:volume>17</prism:volume>
        <dc:title>PLoS ONE</dc:title>
        <dc:identifier>DOI 10.1371/journal.pone.0272287</dc:identifier>
        <prism:number>7 July</prism:number>
        <dcterms:alternative>PLoS ONE</dcterms:alternative>
        <dc:identifier>ISSN 19326203 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1676">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 16; Correspondence Address: E.J. de Menezes-Neto; Department of Law, Universidade Federal do Rio Grande do Norte, Caicó, RN, Brazil; email: elias.jacob@ufrn.br; CODEN: POLNC&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2651">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2651/de Menezes-Neto e Clementino - 2022 - Using deep learning to predict outcomes of legal appeals better than human experts A study with dat.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0272287&amp;type=printable</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:09:30</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:0736587X%20(ISSN);%20978-195591721-6%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1</prism:volume>
                <dc:identifier>ISBN 0736587X (ISSN); 978-195591721-6 (ISBN)</dc:identifier>
                <dc:title>Proc. Annu. Meet. Assoc. Comput Linguist.</dc:title>
                <dc:identifier>DOI 10.18653/v1/2022.acl-long.229</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hilton</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Evans</foaf:surname>
                        <foaf:givenName>O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Muresan S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Nakov P.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Villavicencio A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1678"/>
        <dcterms:isReferencedBy rdf:resource="#item_2238"/>
        <dc:subject>Model size</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Performance</dc:subject>
        <dc:subject>Human performance</dc:subject>
        <dc:subject>Best model</dc:subject>
        <dc:subject>False beliefs</dc:subject>
        <dc:subject>Large models</dc:subject>
        <dc:subject>Scaling-up</dc:subject>
        <dc:subject>Well model</dc:subject>
        <dc:title>TruthfulQA: Measuring How Models Mimic Human Falsehoods</dc:title>
        <dcterms:abstract>We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than finetuning using training objectives other than imitation of text from the web. © 2022 Association for Computational Linguistics.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139104299&amp;doi=10.18653%2fv1%2f2022.acl-long.229&amp;partnerID=40&amp;md5=e72e7fa18ab7e569520935ef25d8423d</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Annu. Meet. Assoc. Comput Linguist.</dc:description>
        <bib:pages>3214-3252</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the Annual Meeting of the Association for Computational Linguistics</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1678">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 251; Conference name: 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022; Conference date: 22 May 2022 through 27 May 2022; Conference code: 181737&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2238">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134360349&amp;doi=10.1021%2fjacs.2c04278&amp;partnerID=40&amp;md5=80789726e4b31cc3d3b0227a66797e2e">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:00027863%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fang</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xiang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1681"/>
        <dcterms:isReferencedBy rdf:resource="#item_2237"/>
        <dc:subject>Article</dc:subject>
        <dc:subject>controlled study</dc:subject>
        <dc:subject>]+ catalyst</dc:subject>
        <dc:subject>acidity</dc:subject>
        <dc:subject>aromatization</dc:subject>
        <dc:subject>Atmospheric pressure</dc:subject>
        <dc:subject>benzene</dc:subject>
        <dc:subject>catalysis</dc:subject>
        <dc:subject>catalyst</dc:subject>
        <dc:subject>Catalysts</dc:subject>
        <dc:subject>Catalytic performance</dc:subject>
        <dc:subject>chemical loading</dc:subject>
        <dc:subject>chemical procedures</dc:subject>
        <dc:subject>Cost-efficient</dc:subject>
        <dc:subject>dehydroaromatization</dc:subject>
        <dc:subject>Dehydroaromatization</dc:subject>
        <dc:subject>energy dispersive X ray spectroscopy</dc:subject>
        <dc:subject>ethane</dc:subject>
        <dc:subject>Hybrid clusters</dc:subject>
        <dc:subject>Hydrocarbons</dc:subject>
        <dc:subject>HZSM-5 catalyst</dc:subject>
        <dc:subject>impregnation</dc:subject>
        <dc:subject>ion exchange</dc:subject>
        <dc:subject>Ion exchange</dc:subject>
        <dc:subject>Monometallics</dc:subject>
        <dc:subject>platinum</dc:subject>
        <dc:subject>proton</dc:subject>
        <dc:subject>Pt loading</dc:subject>
        <dc:subject>Rate constants</dc:subject>
        <dc:subject>scanning transmission electron microscopy</dc:subject>
        <dc:subject>structure analysis</dc:subject>
        <dc:subject>synthesis</dc:subject>
        <dc:subject>toluene</dc:subject>
        <dc:subject>Ultralow loading</dc:subject>
        <dc:subject>X ray absorption spectroscopy</dc:subject>
        <dc:subject>xylene</dc:subject>
        <dc:subject>zeolite</dc:subject>
        <dc:subject>Zeolite HZSM-5</dc:subject>
        <dc:subject>Zeolites</dc:subject>
        <dc:subject>zinc</dc:subject>
        <dc:title>Ultralow-Loading Pt/Zn Hybrid Cluster in Zeolite HZSM-5 for Efficient Dehydroaromatization</dc:title>
        <dcterms:abstract>Minimizing Pt loading without sacrificing catalytic performance is critical, particularly for designing cost-efficient hydrocarbon transformation catalysts. Here, we show that ultralow-loading (0.001-0.05 wt %) Pt- and Zn-functionalized HZSM-5 catalysts, prepared through simple ion exchange and impregnation, are highly active and stable for light alkane dehydroaromatization (DHA). The specific activity of benzene, toluene, and xylene is up to 8.2 mol/gPt/min (or 1592 min-1) over the 0.001 wt % Pt-Zn2/HZSM-5 catalyst during ethane DHA at 550 °C under atmospheric pressure. Additionally, such bimetallic Ptx-Zny/HZSM-5 catalysts are highly stable in contrast to the monometallic Pt/HZSM-5 catalysts. The rate constant of deactivation (kdeactiv), according to the first-order generalized power law equation model, for the bimetallic catalysts is up to 120 times lower than that of the monometallic counterparts, depending on the Pt loading. This breakthrough is achieved through the formation of the [Pt1-Znn]δ+ hybrid cluster, instead of Pt0 cluster-proton adducts, in the micropores of the ZSM-5 zeolite.  © 2022 American Chemical Society.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134360349&amp;doi=10.1021%2fjacs.2c04278&amp;partnerID=40&amp;md5=80789726e4b31cc3d3b0227a66797e2e</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: American Chemical Society</dc:description>
        <bib:pages>11831-11839</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:00027863%20(ISSN)">
        <prism:volume>144</prism:volume>
        <dc:title>Journal of the American Chemical Society</dc:title>
        <dc:identifier>DOI 10.1021/jacs.2c04278</dc:identifier>
        <prism:number>26</prism:number>
        <dcterms:alternative>J. Am. Chem. Soc.</dcterms:alternative>
        <dc:identifier>ISSN 00027863 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1681">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 33; Correspondence Address: Y. Xiang; Dave C. Swalm School of Chemical Engineering, Mississippi State University, Mississippi State, 39762, United States; email: yzxiang@che.msstate.edu; CODEN: JACSA&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2237">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121749243&amp;doi=10.1016%2fj.petrol.2021.110027&amp;partnerID=40&amp;md5=b3710465c63617ddad4bda86b855d1ce">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:09204105%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Weijermars</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1679"/>
        <dcterms:isReferencedBy rdf:resource="#item_2344"/>
        <dc:subject>Flow of fluids</dc:subject>
        <dc:subject>Decline curve analysis</dc:subject>
        <dc:subject>diffusivity</dc:subject>
        <dc:subject>fluid injection</dc:subject>
        <dc:subject>Fracture</dc:subject>
        <dc:subject>gas well</dc:subject>
        <dc:subject>Gaussian distribution</dc:subject>
        <dc:subject>Gaussian pressure transient</dc:subject>
        <dc:subject>Gaussian pressure transients</dc:subject>
        <dc:subject>Gaussians</dc:subject>
        <dc:subject>Geothermal energy</dc:subject>
        <dc:subject>Horizontal wells</dc:subject>
        <dc:subject>Hydraulic diffusivity</dc:subject>
        <dc:subject>hydraulic fracturing</dc:subject>
        <dc:subject>Hydraulic fracturing</dc:subject>
        <dc:subject>hydrocarbon reservoir</dc:subject>
        <dc:subject>Multi-fractured well</dc:subject>
        <dc:subject>Multi-fractured wells</dc:subject>
        <dc:subject>oil shale</dc:subject>
        <dc:subject>Oil wells</dc:subject>
        <dc:subject>Porous materials</dc:subject>
        <dc:subject>porous medium</dc:subject>
        <dc:subject>pressure field</dc:subject>
        <dc:subject>Pressure transient</dc:subject>
        <dc:subject>Production forecasting</dc:subject>
        <dc:subject>Reservoir modeling</dc:subject>
        <dc:subject>Reservoir models</dc:subject>
        <dc:subject>Shale</dc:subject>
        <dc:subject>Shale well performance</dc:subject>
        <dc:subject>Transient analysis</dc:subject>
        <dc:subject>Velocity</dc:subject>
        <dc:subject>Well performance</dc:subject>
        <dc:subject>Well testing</dc:subject>
        <dc:title>Production rate of multi-fractured wells modeled with Gaussian pressure transients</dc:title>
        <dcterms:abstract>This study presents new pressure transient solutions, illustrated with some examples of the vast practical application potential. Gaussian pressure transients (GPT) are derived here to quantify the temporal and spatial propagation of instantaneous pressure changes in porous media, as initiated from cylindrical sources (vertical wells) and planar sources (hydraulic fractures). After solving the scalar pressure field in the reservoir space, and adequately accounting for the interference of the various pressure fronts by mathematical integration and superposition, the resulting pressure gradients solve for the velocity field in the reservoir space. Unique for GPT solutions is that the well rate, unlike in the traditional well-testing equations, does not appear as an input. Applying Darcy's Law, the fluid flux from the reservoir into the well and hydraulic fractures can be directly computed from the GPT solutions. The closed-form production-forecasting model can be implemented either in matrix-coded flow-visualizations of pressure depletion and flow paths for reservoir sections or in grid-less spreadsheet solutions to instantaneously generate production profiles for wells in any type of fluid injection/extraction project (water production, geothermal energy extraction, hydrocarbon production, and fluid disposal wells). Additionally, the Gaussian method also is suitable for physics-based decline curve analysis. The practical examples included in this study are for Eagle Ford shale oil and Marcellus dry gas wells. The hydraulic diffusivities are constrained by the field data, and range between 2.36 x10−10 and 3.48 x10−10 m2 s−1 for the Eagle Ford Formation; for the Marcellus the range is 3.64x10−9 to 5.67x10−8 m2 s−1. The breakthrough solution method of Gaussian pressure transients is placed in the context of past and present modeling approaches for shale plays developed with multi-fractured wells. © 2021 Elsevier B.V.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121749243&amp;doi=10.1016%2fj.petrol.2021.110027&amp;partnerID=40&amp;md5=b3710465c63617ddad4bda86b855d1ce</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier B.V.</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:09204105%20(ISSN)">
        <prism:volume>210</prism:volume>
        <dc:title>Journal of Petroleum Science and Engineering</dc:title>
        <dc:identifier>DOI 10.1016/j.petrol.2021.110027</dc:identifier>
        <dcterms:alternative>J. Pet. Sci. Eng.</dcterms:alternative>
        <dc:identifier>ISSN 09204105 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1679">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 19&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2344">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:978-195408598-5%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-195408598-5 (ISBN)</dc:identifier>
                <dc:title>Nat. Leg. Lang. Process., NLLP - Proc. Workshop</dc:title>
                <dc:identifier>DOI 10.18653/v1/2021.nllp-1.9</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                        <vcard:locality>Punta Cana, Dominican Republic</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Douka</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Abdine</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vazirgiannis</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hamdani</foaf:surname>
                        <foaf:givenName>R.El.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Amariles</foaf:surname>
                        <foaf:givenName>D.R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Aletras N.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Androutsopoulos I.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Barrett L.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Goanta C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Preotiuc-Pietro D.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1482"/>
        <link:link rdf:resource="#item_2685"/>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Domain specific</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Language model adaptation</dc:subject>
        <dc:subject>Large amounts of data</dc:subject>
        <dc:subject>Legal domains</dc:subject>
        <dc:subject>Legal texts</dc:subject>
        <dc:subject>Specific tasks</dc:subject>
        <dc:title>JuriBERT: A Masked-Language Model Adaptation for French Legal Text</dc:title>
        <dcterms:abstract>Language models have proven to be very useful when adapted to specific domains. Nonetheless, little research has been done on the adaptation of domain-specific BERT models in the French language. In this paper, we focus on creating a language model adapted to French legal text with the goal of helping law professionals. We conclude that some specific tasks do not benefit from generic language models pre-trained on large amounts of data. We explore the use of smaller architectures in domain-specific sub-languages and their benefits for French legal text. We prove that domain-specific pre-trained models can perform better than their equivalent generalised ones in the legal domain. Finally, we release JuriBERT, a new set of BERT models adapted to the French legal domain. © 2021 Association for Computational Linguistics.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:shortTitle>JuriBERT</z:shortTitle>
        <z:archive>Scopus</z:archive>
        <z:libraryCatalog>ACLWeb</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130916067&amp;partnerID=40&amp;md5=cd2cac22ab70279b9b4e01bac37c56a8</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Nat. Leg. Lang. Process., NLLP - Proc. Workshop</dc:description>
        <bib:pages>95-101</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Natural Legal Language Processing, NLLP 2021 - Proceedings of the 2021 Workshop</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1482">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 18; Conference name: 3rd Natural Legal Language Processing, NLLP 2021; Conference code: 182350&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2685">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2685/Douka et al. - 2021 - JuriBERT A Masked-Language Model Adaptation for French Legal Text.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://aclanthology.org/2021.nllp-1.9.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:41:42</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-145038526-8%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-145038526-8 (ISBN)</dc:identifier>
                <dc:title>Proc. Int. Conf. Artif. Intell. Law, ICAIL</dc:title>
                <dc:identifier>DOI 10.1145/3462757.3466105</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computing Machinery, Inc</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yoshioka</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Aoki</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Suzuki</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1483"/>
        <link:link rdf:resource="#item_2634"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Law enforcement</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Natural Language Processing Tools</dc:subject>
        <dc:subject>Syntactics</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>Data augmentation</dc:subject>
        <dc:subject>Textual entailment</dc:subject>
        <dc:subject>data augmentation</dc:subject>
        <dc:subject>ensemble method</dc:subject>
        <dc:subject>Ensemble methods</dc:subject>
        <dc:subject>Performance system</dc:subject>
        <dc:subject>Syntactic structure</dc:subject>
        <dc:subject>Systematic method</dc:subject>
        <dc:subject>textual entailment</dc:subject>
        <dc:subject>Word Semantics</dc:subject>
        <dc:title>BERT-based ensemble methods with data augmentation for legal textual entailment in COLIEE statute law task</dc:title>
        <dcterms:abstract>The Competition on Legal Information Extraction/Entailment (COLIEE) statute law legal textual entailment task (task 4) is a task to make a system judge whether a given question statement is true or not by provided articles. In the last COLIEE 2020, the best performance system used bidirectional encoder representations from transformers (BERT), a deep-learning-based natural language processing tool for handling word semantics by considering their context. However, there are problems related to the small amount of training data and the variability of the questions. In this paper, we propose a BERT-based ensemble method with data augmentation to solve this problem. For the data augmentation, we propose a systematic method to make training data for understanding the syntactic structure of the questions and articles for entailment. In addition, due to the nature of the non-deterministic characteristics of BERT fine-tuning and the variability of the questions, we propose a method to construct multiple BERT fine-tuning models and select an appropriate set of models for ensemble. The accuracy of our proposed method for task 4 was 0.7037, which was the best performance among all submissions.  © 2021 ACM.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112367195&amp;doi=10.1145%2f3462757.3466105&amp;partnerID=40&amp;md5=1960d2ab1cfa612fdf059a5a9e42af64</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Int. Conf. Artif. Intell. Law, ICAIL</dc:description>
        <bib:pages>278-284</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the 18th International Conference on Artificial Intelligence and Law, ICAIL 2021</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1483">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 18; Conference name: 18th International Conference on Artificial Intelligence and Law, ICAIL 2021; Conference date: 21 June 2021 through 25 June 2021; Conference code: 170686&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2634">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2634/Yoshioka et al. - 2021 - BERT-based ensemble methods with data augmentation for legal textual entailment in COLIEE statute la.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/pdf/10.1145/3462757.3466105</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:07:56</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112387185&amp;doi=10.1145%2f3462757.3466104&amp;partnerID=40&amp;md5=45a5e27c532117a94fd8843a78db634a">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-145038526-8 (ISBN)</dc:identifier>
                <dc:title>Proc. Int. Conf. Artif. Intell. Law, ICAIL</dc:title>
                <dc:identifier>DOI 10.1145/3462757.3466104</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computing Machinery, Inc</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wehnert</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sudhi</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dureja</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kutty</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shahania</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>De Luca</foaf:surname>
                        <foaf:givenName>E.W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1484"/>
        <link:link rdf:resource="#item_2642"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Knowledge management</dc:subject>
        <dc:subject>Law enforcement</dc:subject>
        <dc:subject>Fine tuning</dc:subject>
        <dc:subject>Legal norms</dc:subject>
        <dc:subject>Contextual words</dc:subject>
        <dc:subject>Data augmentation</dc:subject>
        <dc:subject>data augmentation</dc:subject>
        <dc:subject>contextual word embeddings</dc:subject>
        <dc:subject>Design decisions</dc:subject>
        <dc:subject>document enrichment</dc:subject>
        <dc:subject>External knowledge</dc:subject>
        <dc:subject>legal information retrieval</dc:subject>
        <dc:subject>Vectorization</dc:subject>
        <dc:title>Legal norm retrieval with variations of the bert model combined with TF-IDF vectorization</dc:title>
        <dcterms:abstract>In this work, we examine variations of the BERT model on the statute law retrieval task of the COLIEE competition. This includes approaches to leverage BERT's contextual word embeddings, fine-tuning the model, combining it with TF-IDF vectorization, adding external knowledge to the statutes and data augmentation. Our ensemble of Sentence-BERT with two different TF-IDF representations and document enrichment exhibits the best performance on this task regarding the F2 score. This is followed by a fine-tuned LEGAL-BERT with TF-IDF and data augmentation and our third approach with the BERTScore. As a result, we show that there are significant differences between the chosen BERT approaches and discuss several design decisions in the context of statute law retrieval.  © 2021 ACM.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112387185&amp;doi=10.1145%2f3462757.3466104&amp;partnerID=40&amp;md5=45a5e27c532117a94fd8843a78db634a</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Int. Conf. Artif. Intell. Law, ICAIL</dc:description>
        <bib:pages>285-294</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the 18th International Conference on Artificial Intelligence and Law, ICAIL 2021</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1484">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 20; Conference name: 18th International Conference on Artificial Intelligence and Law, ICAIL 2021; Conference date: 21 June 2021 through 25 June 2021; Conference code: 170686&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2642">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2642/Wehnert et al. - 2021 - Legal norm retrieval with variations of the bert model combined with TF-IDF vectorization.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/pdf/10.1145/3462757.3466104</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:08:44</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150309440&amp;partnerID=40&amp;md5=b5a9fbb2129ec8d6a35e0d911e90d0fd">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>ICLR - Int. Conf. Learn. Represent.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>International Conference on Learning Representations, ICLR</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hendrycks</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Burns</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Basart</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zou</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mazeika</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Song</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Steinhardt</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1485"/>
        <dcterms:isReferencedBy rdf:resource="#item_2324"/>
        <dc:subject>Performance</dc:subject>
        <dc:subject>Language understanding</dc:subject>
        <dc:subject>Best model</dc:subject>
        <dc:subject>High-accuracy</dc:subject>
        <dc:subject>Percentage points</dc:subject>
        <dc:subject>Problem-solving abilities</dc:subject>
        <dc:subject>Test models</dc:subject>
        <dc:subject>Text modeling</dc:subject>
        <dc:subject>World knowledge</dc:subject>
        <dc:title>MEASURING MASSIVE MULTITASK LANGUAGE UNDERSTANDING</dc:title>
        <dcterms:abstract>We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings. © 2021 ICLR 2021 - 9th International Conference on Learning Representations. All rights reserved.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150309440&amp;partnerID=40&amp;md5=b5a9fbb2129ec8d6a35e0d911e90d0fd</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: ICLR - Int. Conf. Learn. Represent.</dc:description>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>ICLR 2021 - 9th International Conference on Learning Representations</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1485">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 357; Conference name: 9th International Conference on Learning Representations, ICLR 2021; Conference date: 3 May 2021 through 7 May 2021; Conference code: 186703&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2324">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:03029743%20(ISSN);%20978-303079941-0%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>12758 LNAI</prism:volume>
                <dc:identifier>ISBN 03029743 (ISSN); 978-303079941-0 (ISBN)</dc:identifier>
                <dc:title>Lect. Notes Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-79942-7_18</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Springer Science and Business Media Deutschland GmbH</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Westermann</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Savelka</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Benyekhlef</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Okazaki N.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Yada K.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Satoh K.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Mineshima K.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1486"/>
        <dcterms:isReferencedBy rdf:resource="#item_2698"/>
        <dc:subject>Approximate nearest neighbor</dc:subject>
        <dc:subject>Authentication</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>BM25</dc:subject>
        <dc:subject>Complex task</dc:subject>
        <dc:subject>Data set</dc:subject>
        <dc:subject>Domain specific</dc:subject>
        <dc:subject>Key process</dc:subject>
        <dc:subject>Law enforcement</dc:subject>
        <dc:subject>Legal documents</dc:subject>
        <dc:subject>Legal entailment</dc:subject>
        <dc:subject>Legal information retrieval</dc:subject>
        <dc:subject>Legal rules</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Predictive analytics</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Support vector machine</dc:subject>
        <dc:subject>Support vector machines</dc:subject>
        <dc:subject>Transformer modeling</dc:subject>
        <dc:subject>Universal sentence encoder</dc:subject>
        <dc:title>Paragraph Similarity Scoring and Fine-Tuned BERT for Legal Information Retrieval and Entailment</dc:title>
        <dcterms:abstract>The assessment of the relevance of legal documents and the application of legal rules embodied in legal documents are some of the key processes in the field of law. In this paper, we present our approach to the 2020 Competition on Legal Information Extraction/Entailment (COLIEE-2020), which provides researchers with the opportunity to find ways of accomplishing these complex tasks using computers. Here, we describe the methods used to build the models for the four tasks that are part of the competition and the results of their application. For Task 1, concerning the prediction of whether a base case cites a candidate case, we devise a method for evaluating the similarity between cases based on individual paragraph similarity. This method can be used to reduce the number of candidate cases by 85%, while maintaining over 80% of the cited cases. We then train a Support Vector Machines model to make the final prediction. The model is the best solution submitted for Task 1. We use a similar method for Task 2. For Task 3, we use an approach based on BM25 measure in combination with the identification of similar previously asked questions. For Task 4, we use a transformer model fine-tuned on existing entailment data sets as well as on the provided domain-specific statutory law data set. © 2021, Springer Nature Switzerland AG.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112239514&amp;doi=10.1007%2f978-3-030-79942-7_18&amp;partnerID=40&amp;md5=b91a37fc0760f6bec0343048fd9c48f8</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Lect. Notes Comput. Sci.</dc:description>
        <bib:pages>269-285</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1486">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 19; Correspondence Address: H. Westermann; Cyberjustice Laboratory, Faculté de droit, Université de Montréal, Montréal, H3T 1J7, Canada; email: hannes.westermann@umontreal.ca; Conference name: 12th International Symposium on Artificial Intelligence supported by the Japanese Society for Artificial Intelligence, JSAI-isAI 2020, International Workshop on Logic and Engineering of Natural Language Semantics, LENLS 2020, 14th International Workshop on Juris-informatics, JURISIN 2020; Conference date: 15 November 2020 through 17 November 2020; Conference code: 261949&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2698">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser de acesso pago&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099723497&amp;doi=10.1109%2fACCESS.2021.3052566&amp;partnerID=40&amp;md5=ac1d068e2600ec8172edb89071c70d7f">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>9</prism:volume>
                <dc:title>IEEE Access</dc:title>
                <dc:identifier>DOI 10.1109/ACCESS.2021.3052566</dc:identifier>
                <dcterms:alternative>IEEE Access</dcterms:alternative>
                <dc:identifier>ISSN 21693536 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tan</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhuang</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mao</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1487"/>
        <dcterms:isReferencedBy rdf:resource="#item_2264"/>
        <dc:subject>Social aspects</dc:subject>
        <dc:subject>Public sentiments</dc:subject>
        <dc:subject>Empirical analysis</dc:subject>
        <dc:subject>Sentiment classification</dc:subject>
        <dc:subject>BERT-LDA hybrid model</dc:subject>
        <dc:subject>Emotional change</dc:subject>
        <dc:subject>emotional evolution</dc:subject>
        <dc:subject>Large scale Internet</dc:subject>
        <dc:subject>large-scale Internet public opinion</dc:subject>
        <dc:subject>Large-scale network</dc:subject>
        <dc:subject>Response mechanisms</dc:subject>
        <dc:subject>the anti-ELAB movement</dc:subject>
        <dc:subject>Transition stage</dc:subject>
        <dc:title>An Analysis of the Emotional Evolution of Large-Scale Internet Public Opinion Events Based on the BERT-LDA Hybrid Model</dc:title>
        <dcterms:abstract>The purpose of this article is to analyse the emotional evolution of the netizens in reaction to the events of the Anti-ELAB (Anti-Extradition Law Amendment Bill) movement in Hong Kong. We attempt to investigate evolving laws of large-scale Internet public opinion events and provide relevant agencies with a theoretical basis for a public opinion response mechanism. On the basis of improving the Bidirectional Encoder Representations from Transformers (BERT) pre-training task, we add in-depth pre-training tasks, and based on the optimisation results of the LDA topic embedding, we integrate deeply with the LDA model to dynamically present the fine-grained public sentiment of the event. Through the collection of large-scale text data related to the Anti-ELAB Movement from a well-known forum in Hong Kong, a BERT-LDA hybrid model for large-scale network public opinion analysis is constructed in a complex context. Through empirical analysis, we calculate and reveal the emotional change process of netizens and opinion leaders in the three transition stages of the Anti-ELAB Movement with the evolution of the topic word as the core by visualisation. We also analyse the emotional distribution and evolution trend of public opinion under the 'text topic', and deeply analyse the character and role of opinion leaders in Anti-ELAB public opinion events. The improved BERT-LDA model or sentiment classification AUC value exceeds 99.6% in the sentiment classification task for the Anti-ELAB Movement. © 2013 IEEE.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099723497&amp;doi=10.1109%2fACCESS.2021.3052566&amp;partnerID=40&amp;md5=ac1d068e2600ec8172edb89071c70d7f</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Institute of Electrical and Electronics Engineers Inc.</dc:description>
        <bib:pages>15860-15871</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1487">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 29; Correspondence Address: M. Zhuang; Business School, Southern University of Science and Technology, Shenzhen, China; email: 201821100562@smail.xtu.edu.cn&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2264">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114670519&amp;doi=10.1080%2f13572334.2021.1976947&amp;partnerID=40&amp;md5=4a4971a0dce523f0265357b7257fbe1e">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:13572334%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fitsilis</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1488"/>
        <link:link rdf:resource="#item_2632"/>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>GPT-3</dc:subject>
        <dc:subject>Eduskunta</dc:subject>
        <dc:subject>Project December</dc:subject>
        <dc:subject>simulated personality</dc:subject>
        <dc:subject>techno-ethical committee</dc:subject>
        <dc:title>Artificial Intelligence (AI) in parliaments–preliminary analysis of the Eduskunta experiment</dc:title>
        <dcterms:abstract>In April 2021, the Committee for the Future of the Parliament of Finland (Eduskunta) organised an extraordinary hearing, that is, of an artificial intelligence (AI). While some legislatures and research groups had already begun to study the implication of AI in the parliamentary domain, this took the parliamentary world by surprise. It was the first time a parliament has directly interacted with an AI system in an actual parliamentary process. This research note attempts to conduct a preliminary analysis on the experiment and discusses its implications for future actions in the development of parliamentary tools and services using AI-based technologies. Analysis is dedicated to intra-parliamentary workspace, while considering possible effects on the main functions of parliament such as law-making and oversight. The note aims to spark the discussion around the implementation strategy, but also for the regulation of such systems in the parliamentary environment. © 2021 Informa UK Limited, trading as Taylor &amp; Francis Group.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114670519&amp;doi=10.1080%2f13572334.2021.1976947&amp;partnerID=40&amp;md5=4a4971a0dce523f0265357b7257fbe1e</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Routledge</dc:description>
        <bib:pages>621-633</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:13572334%20(ISSN)">
        <prism:volume>27</prism:volume>
        <dc:title>Journal of Legislative Studies</dc:title>
        <dc:identifier>DOI 10.1080/13572334.2021.1976947</dc:identifier>
        <prism:number>4</prism:number>
        <dcterms:alternative>J. Legis. Stud.</dcterms:alternative>
        <dc:identifier>ISSN 13572334 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1488">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 21; Correspondence Address: F. Fitsilis; Scientific Service, Hellenic Parliament, Athens, Greece; email: fitsilisf@parliament.gr&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2632">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2632/Fitsilis - 2021 - Artificial Intelligence (AI) in parliaments–preliminary analysis of the Eduskunta experiment.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.tandfonline.com/doi/pdf/10.1080/13572334.2021.1976947</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:07:47</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098963328&amp;doi=10.1007%2fs10506-020-09280-2&amp;partnerID=40&amp;md5=94d858c5cacedc14739d4f27a113c98e">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:09248463%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mandal</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ghosh</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ghosh</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mandal</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1489"/>
        <link:link rdf:resource="#item_2650"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Legal documents</dc:subject>
        <dc:subject>Management</dc:subject>
        <dc:subject>Information retrieval</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>Word2vec</dc:subject>
        <dc:subject>Legal information retrieval</dc:subject>
        <dc:subject>Citation networks</dc:subject>
        <dc:subject>Court case reports</dc:subject>
        <dc:subject>Court case similarity</dc:subject>
        <dc:subject>Doc2vec</dc:subject>
        <dc:subject>Document similarity</dc:subject>
        <dc:subject>Empirical validation</dc:subject>
        <dc:subject>Law2vec</dc:subject>
        <dc:subject>Measuring similarities</dc:subject>
        <dc:subject>Textual similarities</dc:subject>
        <dc:subject>Topic modeling</dc:subject>
        <dc:subject>Unsupervised approaches</dc:subject>
        <dc:title>Unsupervised approaches for measuring textual similarity between legal court case reports</dc:title>
        <dcterms:abstract>In the domain of legal information retrieval, an important challenge is to compute similarity between two legal documents. Precedents (statements from prior cases) play an important role in The Common Law system, where lawyers need to frequently refer to relevant prior cases. Measuring document similarity is one of the most crucial aspects of any document retrieval system which decides the speed, scalability and accuracy of the system. Text-based and network-based methods for computing similarity among case reports have already been proposed in prior works but not without a few pitfalls. Since legal citation networks are generally highly disconnected, network based metrics are not suited for them. Till date, only a few text-based and predominant embedding based methods have been employed, for instance, TF-IDF based approaches, Word2Vec (Mikolov et al. 2013) and Doc2Vec (Le and Mikolov 2014) based approaches. We investigate the performance of 56 different methodologies for computing textual similarity across court case statements when applied on a dataset of Indian Supreme Court Cases. Among the 56 different methods, thirty are adaptations of existing methods and twenty-six are our proposed methods. The methods studied include models such as BERT (Devlin et al. 2018) and Law2Vec (Ilias 2019). It is observed that the more traditional methods (such as the TF-IDF and LDA) that rely on a bag-of-words representation performs better than the more advanced context-aware methods (like BERT and Law2Vec) for computing document-level similarity. Finally we nominate, via empirical validation, five of our best performing methods as appropriate for measuring similarity between case reports. Among these five, two are adaptations of existing methods and the other three are our proposed methods. © 2021, The Author(s), under exclusive licence to Springer Nature B.V. part of Springer Nature.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098963328&amp;doi=10.1007%2fs10506-020-09280-2&amp;partnerID=40&amp;md5=94d858c5cacedc14739d4f27a113c98e</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer Science and Business Media B.V.</dc:description>
        <bib:pages>417-451</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:09248463%20(ISSN)">
        <prism:volume>29</prism:volume>
        <dc:title>Artificial Intelligence and Law</dc:title>
        <dc:identifier>DOI 10.1007/s10506-020-09280-2</dc:identifier>
        <prism:number>3</prism:number>
        <dcterms:alternative>Artif Intell Law</dcterms:alternative>
        <dc:identifier>ISSN 09248463 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1489">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 36; Correspondence Address: A. Mandal; Department of Computer Science and Technology, Indian Institute of Engineering Science and Technology, Howrah, Shibpur, India; email: amarnamarpan@gmail.com; CODEN: AINLE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2650">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2650/Mandal et al. - 2021 - Unsupervised approaches for measuring textual similarity between legal court case reports.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.1007%2Fs10506-020-09280-2.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:09:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084378439&amp;doi=10.1109%2fACCESS.2020.2988493&amp;partnerID=40&amp;md5=334a11249f23f953592379f6a653cc52">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>8</prism:volume>
                <dc:title>IEEE Access</dc:title>
                <dc:identifier>DOI 10.1109/ACCESS.2020.2988493</dc:identifier>
                <dcterms:alternative>IEEE Access</dcterms:alternative>
                <dc:identifier>ISSN 21693536 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fang</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gu</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Weng</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1490"/>
        <link:link rdf:resource="#item_2637"/>
        <dc:subject>Learning algorithms</dc:subject>
        <dc:subject>Similar case</dc:subject>
        <dc:subject>BERT</dc:subject>
        <dc:subject>Classification algorithm</dc:subject>
        <dc:subject>Controversial issues</dc:subject>
        <dc:subject>few-shot learning</dc:subject>
        <dc:subject>Manual annotation</dc:subject>
        <dc:subject>Power law distribution</dc:subject>
        <dc:subject>power-law</dc:subject>
        <dc:subject>Procedural systems</dc:subject>
        <dc:subject>text classification</dc:subject>
        <dc:title>Few-Shot Learning for Chinese Legal Controversial Issues Classification</dc:title>
        <dcterms:abstract>Chinese courts organize debates surrounding controversial issues along with the gradual formation of the new procedural system. With the progress of China's judicial reform, more than 80 million judgement documents have been made public online. Similar controversial issues identified in and among the massive public judgment documents are of significant value for judges in their trial work. Hence, homogeneous controversial issues classification becomes the basis for similar cases retrieval. However, controversial issues follow the power-law distribution, not all of them are within the labels provided by manual annotation and their categories cannot be exhausted. In order to generalize those unfamiliar categories without necessitating extensive retraining, we propose a controversial issues classification algorithm based on few-shot learning. Two few-shot learning algorithms are proposed for our controversial issues problem, Relation Network and Induction Network, respectively. With only a handful of given instances, both of them have shown excellent results on the two datasets, which proves their effectiveness in adapting to accommodating new categories not seen in training. The proposed method provides trial assistance for judges, promotes the dissemination of experience and improves fairness of adjudication. © 2013 IEEE.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084378439&amp;doi=10.1109%2fACCESS.2020.2988493&amp;partnerID=40&amp;md5=334a11249f23f953592379f6a653cc52</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Institute of Electrical and Electronics Engineers Inc.</dc:description>
        <bib:pages>75022-75034</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1490">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 11; Correspondence Address: Y. Weng; College of Mathematics, Sichuan University, Chengdu, 610065, China; email: wengyang@scu.edu.cn&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2637">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2637/Fang et al. - 2020 - Few-Shot Learning for Chinese Legal Controversial Issues Classification.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=9069958&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:08:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084041788&amp;partnerID=40&amp;md5=9842addd1346caf3cfbcb42bc5c091d7">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-195073748-2 (ISBN)</dc:identifier>
                <dc:title>ACL - Annu. Meet. Assoc. Comput. Linguist., Proc. Conf.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Esfahani</foaf:surname>
                        <foaf:givenName>S.S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cafarella</foaf:surname>
                        <foaf:givenName>M.J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pouyan</foaf:surname>
                        <foaf:givenName>M.B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>DeAngelo</foaf:surname>
                        <foaf:givenName>G.J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Eneva</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fano</foaf:surname>
                        <foaf:givenName>A.E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1491"/>
        <dcterms:isReferencedBy rdf:resource="#item_2279"/>
        <dc:subject>Human trafficking</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Law-enforcement agencies</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Crime</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Modeling languages</dc:subject>
        <dc:subject>Composite features</dc:subject>
        <dc:subject>Manual processing</dc:subject>
        <dc:subject>Online advertisements</dc:subject>
        <dc:subject>Specific languages</dc:subject>
        <dc:subject>Textual language</dc:subject>
        <dc:title>Context-specific language modeling for human trafficking detection from online advertisements</dc:title>
        <dcterms:abstract>Human trafficking is a worldwide crisis. Traffickers exploit their victims by anonymously offering sexual services through online advertisements. These ads often contain clues that law enforcement can use to separate out potential trafficking cases from volunteer sex advertisements. The problem is that the sheer volume of ads is too overwhelming for manual processing. Ideally, a centralized semi-automated tool can be used to assist law enforcement agencies with this task. Here, we present an approach using natural language processing to identify trafficking ads on these websites. We propose a classifier by integrating multiple text feature sets, including the publicly available pre-trained textual language model Bi-directional Encoder Representation from transformers (BERT). In this paper, we demonstrate that a classifier using this composite feature set has significantly better performance compared to any single feature set alone. © 2019 Association for Computational Linguistics</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084041788&amp;partnerID=40&amp;md5=9842addd1346caf3cfbcb42bc5c091d7</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: ACL - Annu. Meet. Assoc. Comput. Linguist., Proc. Conf.</dc:description>
        <bib:pages>1180-1184</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1491">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 12; Conference name: 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019; Conference date: 28 July 2019 through 2 August 2019; Conference code: 159206&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2279">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:979-835038634-9%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 979-835038634-9 (ISBN)</dc:identifier>
                <dc:title>Proc. - Int. Conf. Pervasive Comput. Soc. Netw., ICPCSN</dc:title>
                <dc:identifier>DOI 10.1109/ICPCSN62568.2024.00022</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vayadande</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bhat</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bachhav</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bhoyar</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Charoliya</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chavan</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1754"/>
        <link:link rdf:resource="#item_2631"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Legal documents</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Data handling</dc:subject>
        <dc:subject>Anomaly detection</dc:subject>
        <dc:subject>Embeddings</dc:subject>
        <dc:subject>Gluing</dc:subject>
        <dc:subject>Legal technology</dc:subject>
        <dc:subject>Natural Language Processing (NLP)</dc:subject>
        <dc:subject>Document-processing</dc:subject>
        <dc:subject>Irregularities Detection</dc:subject>
        <dc:subject>Irregularity detection</dc:subject>
        <dc:subject>Legal document processing</dc:subject>
        <dc:subject>Legal Document Processing</dc:subject>
        <dc:subject>Legal Technology</dc:subject>
        <dc:subject>Openai embedding</dc:subject>
        <dc:subject>OpenAI Embeddings</dc:subject>
        <dc:title>AI-Powered Legal Documentation Assistant</dc:title>
        <dcterms:abstract>Legal systems worldwide vary in structure and principles, reflecting the diverse legal traditions of different countries. The legal system, inherently complex and reliant on meticulous documentation, often faces challenges related to time-consuming manual processes and the potential for human errors. The system proposed provides a transformative solution to the above problems. The system emerges as a groundbreaking solution within the intricate landscape of legal systems which responds to these challenges by seamlessly integrating advanced AI techniques. At its core, OpenAI embeddings takes center stage, demonstrating unparalleled proficiency in document generation, comprehension, and abnormality detection, addressing the complexities ingrained in legal documentation. In contrast to traditional approaches, this system maximizes the versatility of ChatGPT 3.5, allowing it to not only issue commands but also proficiently generate a diverse array of legal documents. By incorporating an understanding module equipped with PyPDF, Amazon Textract, and langchain utilities, the system adeptly handles document intricacies. The utilization of OpenAI Embeddings further enhances natural language understanding. Leveraging sentiment analysis and Named Entity Recognition (NER) in its natural language processing (NLP) toolkit, the system employs an intuitive web interface for irregularities detection. The exploration of AI for automated irregularity detection showcases its transformative potential in ensuring document accuracy within the legal domain. This project, therefore, stands as a beacon of innovation, promising to reshape the dynamics of legal document processing by merging advanced AI capabilities with the unique demands of legal systems. © 2024 IEEE.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201677479&amp;doi=10.1109%2fICPCSN62568.2024.00022&amp;partnerID=40&amp;md5=0ff6d8e5a788b7fd55dc6ac70432fca7</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. - Int. Conf. Pervasive Comput. Soc. Netw., ICPCSN</dc:description>
        <bib:pages>84-91</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings - 2024 4th International Conference on Pervasive Computing and Social Networking, ICPCSN 2024</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1754">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 0; Conference name: 4th International Conference on Pervasive Computing and Social Networking, ICPCSN 2024; Conference date: 3 May 2024 through 4 May 2024; Conference code: 201477&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2631">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2631/Vayadande et al. - 2024 - AI-Powered Legal Documentation Assistant.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=10607645&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:07:42</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:0736587X%20(ISSN);%20979-889176094-3%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1</prism:volume>
                <dc:identifier>ISBN 0736587X (ISSN); 979-889176094-3 (ISBN)</dc:identifier>
                <dc:title>Proc. Annu. Meet. Assoc. Comput Linguist.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Santosh</foaf:surname>
                        <foaf:givenName>T.Y.S.S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ichim</foaf:surname>
                        <foaf:givenName>O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Plank</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grabmair</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Ku L.-W.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Martins A.F.T.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Srikumar V.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1755"/>
        <dcterms:isReferencedBy rdf:resource="#item_2389"/>
        <dc:subject>AI systems</dc:subject>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Legal arguments</dc:subject>
        <dc:subject>Human rights</dc:subject>
        <dc:subject>Legal case</dc:subject>
        <dc:subject>Calibration</dc:subject>
        <dc:subject>Calibration method</dc:subject>
        <dc:subject>Classification datasets</dc:subject>
        <dc:subject>Human-systems</dc:subject>
        <dc:subject>Perceived difficulties</dc:subject>
        <dc:subject>Predictive performance</dc:subject>
        <dc:subject>Through the lens</dc:subject>
        <dc:title>Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification</dc:title>
        <dcterms:abstract>In legal decisions, split votes (SV) occur when judges cannot reach a unanimous decision, posing a difficulty for lawyers who must navigate diverse legal arguments and opinions. In high-stakes domains, understanding the alignment of perceived difficulty between humans and AI systems is crucial to build trust. However, existing NLP calibration methods focus on a classifier's awareness of predictive performance, measured against the human majority class, overlooking inherent human label variation (HLV). This paper explores split votes as naturally observable human disagreement and value pluralism. We collect judges' vote distributions from the European Court of Human Rights (ECHR), and present SV-ECHR a case outcome classification (COC) dataset with SV information. We build a taxonomy of disagreement with SV-specific subcategories. We further assess the alignment of perceived difficulty between models and humans, as well as confidence- and human-calibration of COC models. We observe limited alignment with the judge vote distribution. To our knowledge, this is the first systematic exploration of calibration to human judgements in legal NLP. Our study underscores the necessity for further research on measuring and enhancing model calibration considering HLV in legal decision tasks. © 2024 Association for Computational Linguistics.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204473787&amp;partnerID=40&amp;md5=17c7c17ee5a0a7fa4a2f6d68a9c301ec</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Annu. Meet. Assoc. Comput Linguist.</dc:description>
        <bib:pages>199-216</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the Annual Meeting of the Association for Computational Linguistics</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1755">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 0; Conference name: 62nd Annual Meeting of the Association for Computational Linguistics, ACL 2024; Conference date: 11 August 2024 through 16 August 2024; Conference code: 202222&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2389">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163744929&amp;doi=10.1007%2fs13132-023-01450-2&amp;partnerID=40&amp;md5=6f58c03b7d267e8c8c9db2f2b4b10ea3">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:18687865%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alexopoulos</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saxena</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saxena</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1756"/>
        <link:link rdf:resource="#item_2643"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>NLP</dc:subject>
        <dc:subject>India</dc:subject>
        <dc:subject>Legal informatics</dc:subject>
        <dc:subject>Judicial innovation</dc:subject>
        <dc:subject>LAMs</dc:subject>
        <dc:subject>Legal automated (teller) machines</dc:subject>
        <dc:subject>Legal technologies</dc:subject>
        <dc:title>Natural Language Processing (NLP)-Powered Legal A(t)Ms (LAMs) in India: Possibilities and Challenges</dc:title>
        <dcterms:abstract>With the infusion of information and communications technology (ICT) in legal domain, of late, the attempts of reforming the judicial landscape have been forthcoming. In this vein, the paper seeks to present the possibility of harnessing natural language processing (NLP) for instituting legal automated (teller) machines (LAMs) in India as an innovative application of legal informatics in a developing country. Literature on legal informatics with a focus on NLP is scanned to drive home the key argument in the paper. Institutionalization of LAMs in a developing country like India would go a long way in expediting the automated judicial arbitration system apart from providing easier and accessible alternatives to the aggrieved parties. However, it is important that the required political will and sustained leadership is there to provide the required wherewithal for the institutionalization of LA(t)Ms in the country. As an innovation in the field of legal informatics, LAM is the first of its kind both in terms of its ideation and in terms of the academic output till date. It is anticipated that academia would be interested to conceive of improvising LAMs in different contexts post-screening of the ecosystemic determinants therein. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163744929&amp;doi=10.1007%2fs13132-023-01450-2&amp;partnerID=40&amp;md5=6f58c03b7d267e8c8c9db2f2b4b10ea3</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer</dc:description>
        <bib:pages>8513-8533</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:18687865%20(ISSN)">
        <prism:volume>15</prism:volume>
        <dc:title>Journal of the Knowledge Economy</dc:title>
        <dc:identifier>DOI 10.1007/s13132-023-01450-2</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>J. Knowl. Econ.</dcterms:alternative>
        <dc:identifier>ISSN 18687865 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1756">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 0; Correspondence Address: S. Saxena; Department of Humanities and Social Sciences, Graphic Era University, Dehradun, India; email: stutisaxenaogd.vishnu@gmail.com&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2643">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2643/Alexopoulos et al. - 2024 - Natural Language Processing (NLP)-Powered Legal A(t)Ms (LAMs) in India Possibilities and Challenges.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.1007%2Fs13132-023-01450-2.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:08:49</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:16130073%20(ISSN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>3764</prism:volume>
                <dc:identifier>ISBN 16130073 (ISSN)</dc:identifier>
                <dc:title>CEUR Workshop Proc.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>CEUR-WS</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sen</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saha</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bollegala</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                   <foaf:Person><foaf:surname>Sen P.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Saha T.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Bollegala D.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1757"/>
        <dcterms:isReferencedBy rdf:resource="#item_2359"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Legal NLP</dc:subject>
        <dc:subject>AI for Social Good</dc:subject>
        <dc:subject>Artificial intelligence for social good</dc:subject>
        <dc:subject>Breeding grounds</dc:subject>
        <dc:subject>Climate Change</dc:subject>
        <dc:subject>Innovation potential</dc:subject>
        <dc:subject>Legal natural language processing</dc:subject>
        <dc:subject>Societal issues</dc:subject>
        <dc:title>Report on the 2nd Symposium on NLP for Social Good (NSG 2024)</dc:title>
        <dcterms:abstract>Artificial intelligence (AI), specifically, Natural Language Processing (NLP) is being hailed as a new breeding ground for immense innovation potential. Researchers believe that NLP- based technologies could help to solve societal issues such as equality and inclusion, education, health, and hunger, climate action etc., and many more. Tackling these questions requires a concerted, collaborative effort across all sectors of society. The first Symposium on NLP for Social Good (NSG) was a novel effort that aimed to enable NLP researchers and scholars from inter-disciplinary field who want to think about the societal implications of their work for solving humanitarian and environmental challenges. The objective of the symposium was to support fundamental research and engineering efforts and empower the social sector with tools and resources, while collaborating with partners from all sectors to maximise effect in solving problems within public health, nature &amp; society, accessibility, crisis response etc. In its inception, we invited speakers from academia and industry to provide an overview of some areas from NLP applications such as education, healthcare and legal domains in order to provide a platform to stimulate discussion regarding the current state of NLP in these varied fields. © 2022 Copyright for this paper by its authors.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205822161&amp;partnerID=40&amp;md5=0b9943bed2a81e0eab6c26341cfa8935</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: CEUR Workshop Proc.</dc:description>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>CEUR Workshop Proceedings</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1757">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 0; Conference name: 2nd Symposium on NLP for Social Good, NSG 2024; Conference date: 25 April 2024 through 26 April 2024; Conference code: 202850&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2359">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188989228&amp;doi=10.1007%2fs10506-024-09396-9&amp;partnerID=40&amp;md5=3fbd42ec1200ce6e250e24ee43fff5af">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0924-8463"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Martínez</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1760"/>
        <dcterms:isReferencedBy rdf:resource="#item_1828"/>
        <link:link rdf:resource="#item_2657"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>NLP</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>LAW</dc:subject>
        <dc:subject>Machine learning</dc:subject>
        <dc:subject>Learning algorithms</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Artificial intelligence and laws</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Machine-learning</dc:subject>
        <dc:subject>Performance</dc:subject>
        <dc:subject>Artificial intelligence and law</dc:subject>
        <dc:subject>Law and technology</dc:subject>
        <dc:subject>Legal analytics</dc:subject>
        <dc:subject>Legal NLP</dc:subject>
        <dc:subject>Legal profession</dc:subject>
        <dc:subject>Grading</dc:subject>
        <dc:subject>Legal analytic</dc:subject>
        <dc:title>Re-evaluating GPT-4’s bar exam performance</dc:title>
        <dcterms:abstract>Perhaps the most widely touted of GPT-4’s at-launch, zero-shot capabilities has been its reported 90th-percentile performance on the Uniform Bar Exam. This paper begins by investigating the methodological challenges in documenting and verifying the 90th-percentile claim, presenting four sets of findings that indicate that OpenAI’s estimates of GPT-4’s UBE percentile are overinflated. First, although GPT-4’s UBE score nears the 90th percentile when examining approximate conversions from February administrations of the Illinois Bar Exam, these estimates are heavily skewed towards repeat test-takers who failed the July administration and score significantly lower than the general test-taking population. Second, data from a recent July administration of the same exam suggests GPT-4’s overall UBE percentile was below the 69th percentile, and ∼48th percentile on essays. Third, examining official NCBE data and using several conservative statistical assumptions, GPT-4’s performance against first-time test takers is estimated to be ∼62nd percentile, including ∼42nd percentile on essays. Fourth, when examining only those who passed the exam (i.e. licensed or license-pending attorneys), GPT-4’s performance is estimated to drop to ∼48th percentile overall, and ∼15th percentile on essays. In addition to investigating the validity of the percentile claim, the paper also investigates the validity of GPT-4’s reported scaled UBE score of 298. The paper successfully replicates the MBE score, but highlights several methodological issues in the grading of the MPT + MEE components of the exam, which call into question the validity of the reported essay score. Finally, the paper investigates the effect of different hyperparameter combinations on GPT-4’s MBE performance, finding no significant effect of adjusting temperature settings, and a significant effect of few-shot chain-of-thought prompting over basic zero-shot prompting. Taken together, these findings carry timely insights for the desirability and feasibility of outsourcing legally relevant tasks to AI models, as well as for the importance for AI developers to implement rigorous and transparent capabilities evaluations to help secure safe and trustworthy AI. © The Author(s) 2024.</dcterms:abstract>
        <dc:date>2024 MAR 30</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:coverage>WOS:001194581100001</dc:coverage>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188989228&amp;doi=10.1007%2fs10506-024-09396-9&amp;partnerID=40&amp;md5=3fbd42ec1200ce6e250e24ee43fff5af</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer Nature</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0924-8463">
        <dc:title>Artificial Intelligence and Law</dc:title>
        <dc:identifier>DOI 10.1007/s10506-024-09396-9</dc:identifier>
        <dcterms:alternative>Artif Intell Law</dcterms:alternative>
        <dc:identifier>ISSN 0924-8463</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1760">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 4; Correspondence Address: E. Martínez; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology (MIT), Cambridge, 02138, United States; email: ericmart@mit.edu; CODEN: AINLE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_1828">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;4&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;5&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;77&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2657">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2657/Martínez - 2024 - Re-evaluating GPT-4’s bar exam performance.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.1007%2Fs10506-024-09396-9.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:10:08</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:979-833130057-9%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>2</prism:volume>
                <dc:identifier>ISBN 979-833130057-9 (ISBN)</dc:identifier>
                <dc:title>Int. Conf. Adv. Comput., Control, Telecommun. Technol., ACT</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Grenze Scientific Society</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Geetha</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sanjushree</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dharshini</foaf:surname>
                        <foaf:givenName>P.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sneha</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Stephen J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Sharma P.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Chaba Y.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Abraham K.U.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Anooj P.K.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Mohammad N.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Thomas G.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Srikiran S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1758"/>
        <link:link rdf:resource="#item_2683"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Network security</dc:subject>
        <dc:subject>Natural Language Processing (NLP)</dc:subject>
        <dc:subject>Extractive Summarization</dc:subject>
        <dc:subject>Extractive summarizations</dc:subject>
        <dc:subject>Audio summarization</dc:subject>
        <dc:subject>Audio Summarization</dc:subject>
        <dc:subject>Case analysis</dc:subject>
        <dc:subject>Critical steps</dc:subject>
        <dc:subject>Legal research</dc:subject>
        <dc:subject>Rouge score</dc:subject>
        <dc:subject>Speech summarization</dc:subject>
        <dc:title>Judicial Speech Summarization Tool</dc:title>
        <dcterms:abstract>Audio-to-text summary entails turning spoken knowledge into brief written summaries to help in effective information retrieval and understanding.Recapitulating an oral argument in court In audio, spoken legal discourse is condensed into a brief written form yet important details and ideas are maintained for effective understanding and reference.Court oral argument records must be summarized in order to conduct case analysis, legal research, and transparency all properly.Long speeches are condensed so that legal professionals can swiftly grasp and recollect important information.This research investigates the critical steps involved in precisely and concisely transcription of a complicated legal argument from audio.For both academics and legal professionals, accurate transcribing of legal discourse is crucial.This research investigates these methodologies, from state-of-the-art Natural Language Processing (NLP) software to conventional transcription procedures.It emphasizes how crucial this technology is to ensuring courtroom transparency, enhancing case analysis, and speeding up legal research.This research contributes to a revolution in the understanding and accessibility of legal knowledge at a time when legal technology is undergoing rapid change.Our project focuses on producing the transcript and summary of the court oral argument audio. © Grenze Scientific Society, 2024.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209143682&amp;partnerID=40&amp;md5=9c6ecb83ce27b1e1f76de16a7d9fdb9b</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Int. Conf. Adv. Comput., Control, Telecommun. Technol., ACT</dc:description>
        <bib:pages>6266-6272</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>15th International Conference on Advances in Computing, Control, and Telecommunication Technologies, ACT 2024</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1758">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 0; Conference name: 15th International Conference on Advances in Computing, Control, and Telecommunication Technologies, ACT 2024; Conference date: 21 June 2024 through 22 June 2024; Conference code: 203500&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2683">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2683/Geetha et al. - 2024 - Judicial Speech Summarization Tool.pdf"/>
        <dc:title>PDF</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175155572&amp;doi=10.1016%2fj.clsr.2023.105908&amp;partnerID=40&amp;md5=accfb96d0030e348a7c523c754fc48f1">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0267-3649"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Licari</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Comandè</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1759"/>
        <link:link rdf:resource="#item_2680"/>
        <link:link rdf:resource="#item_2681"/>
        <dc:subject>Information retrieval systems</dc:subject>
        <dc:subject>Italian legal BERT</dc:subject>
        <dc:subject>Italian Legal BERT</dc:subject>
        <dc:subject>Italian legal NLP</dc:subject>
        <dc:subject>Italian Legal NLP</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Legal AI</dc:subject>
        <dc:subject>Legal domains</dc:subject>
        <dc:subject>Legal research</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Pre-trained language model</dc:subject>
        <dc:subject>Pre-trained Language Models</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>State of the art</dc:subject>
        <dc:title>ITALIAN-LEGAL-BERT models for improving natural language processing tasks in the Italian legal domain</dc:title>
        <dcterms:abstract>Legal-BERT models are based on the BERT architecture (or its variants) and have been developed specifically for the legal domain. They have reached the state of the art in complex legal tasks such as legal research, document synthesis, contract analysis, argument extraction, and legal prediction. In this paper, we proposed four versions of Legal-BERT models pre-trained on the Italian legal domain. They aim to improve NLP applications in the Italian legal context. We have shown that they outperforms the Italian &quot;generalpurpose&quot; BERT in several domain-specific tasks, such as named entity recognition, sentence classification, semantic similarity with Bi-encoders, and document classification. © 2023 Elsevier Ltd</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175155572&amp;doi=10.1016%2fj.clsr.2023.105908&amp;partnerID=40&amp;md5=accfb96d0030e348a7c523c754fc48f1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Elsevier Ltd</dc:description>
        <bib:pages>105908</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0267-3649">
        <prism:volume>52</prism:volume>
        <dc:title>Computer Law &amp; Security Review</dc:title>
        <dc:identifier>DOI 10.1016/j.clsr.2023.105908</dc:identifier>
        <dcterms:alternative>Computer Law &amp; Security Review</dcterms:alternative>
        <dc:identifier>ISSN 0267-3649</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1759">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 6; Correspondence Address: D. Licari; Sant'Anna School of Advanced Studies, Pisa, Piazza Martiri della Libertà, 33, 56127, Italy; email: d.licari@santannapisa.it; CODEN: CLSRE&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2680">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2680/Licari e Comandè - 2024 - ITALIAN-LEGAL-BERT models for improving natural language processing tasks in the Italian legal domai.pdf"/>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://pdf.sciencedirectassets.com/271884/1-s2.0-S0267364923X00053/1-s2.0-S0267364923001188/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAQaCXVzLWVhc3QtMSJHMEUCIQCzSFE06soa7rhbiUxhsJStdl%2BOKWpweI8Xj9IkLNhDngIgCZXOrTW8JE2Ccpq5Ra%2FnirCaPWWiAHL%2F0RigwEgRN54qvAUI3f%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDKlWjA0UZp7xwwdtgyqQBa7r%2F04dGyt02svEX3zgb13btSbHZRoGEXTSHnseXOWlGqnJhHqqLXJnjQc3tDw3tphuX4fZkMC1Xiz6bF8DtkVg47Hl9kqKLFiEwRpBcBRf7BHRBE3QobLtfD6vElt6BzkOo3L32oSc1pHza%2BcczFpH4rKMLeoxkqmW4yZH2c5S3tZj8VEkjO66hQ%2BxUFX6L5lz07Wgooy0gLmBf2PmnfwPCUdIiOHQ9dzz%2Bte6WRvskIRYcG9FcTukrHYBbpwElkpXiNAurxqfBWiC6UmN3o5M96KwfPR4zdOIjRvwEARzuGWL4vOYvIkZxpHY8llo2bH0pFPlPKVW4yWC2Gth%2BawP9zlE%2B%2B%2F0WJL6x4Tnbak7%2FgUSTkp8%2BvbPiicTaI9mo9LO1JKFpmLeElyzfNPpBsue3YjDB557lWtAYeZ%2BaF1QRnU8rSuBAH77kHAjY8GoZADLjHoVBoMr3jKrFayIgUGC0Wyw2zou36MvCfUIlW3aAmROnrtfqeBz81VeX%2BMeGnRGNuRd7v1%2BdRrivtmIkU3fHSMP2R3ksd2Ct3lsVNZPYBdiCBqsuloO5ca99Ls1JiQzQ9LwGmm9p6NptHxpTM9APUbhEhX09ZoKrrqCrVft%2FkJViWgO2dwuL1yEKxi4XuijyE2k%2BiZ%2F9%2FPHdSOcLq4Gvz2yfGrLH0hw%2Frxt4KcadRKkAaHIyYZdHP3laLm5RrasRAWHkTF0uUdOT3UhY0OIakWKvpa%2FL4FnKmaD5NSHmf67kaWpQJLxGr8sF8o5tNa6tAf9MTXeuXr9jniJnDpoJe4ljtf7qk0PF6YEQ8T7Q2jXDcTmy1bFZDtQ8ZE1gZcuYPZK%2BNY4RV9OfEt0NExYj5AcGEDnMjoWP8XP8Nd7MKHX27sGOrEBJiesu6x0qvA10KU3Wj1EV0AX70GVGN3%2FcSA4AsVV341xtiAxlFPLZf3mIwB1QWmoq5gFunb1%2Bl3BCNlw60GaDZQ5jNQqIDsOWXr%2FAGTO%2Fd2pOoK7Cqmwsy4pW1zohrAm%2ByIC6JEdl%2BZLiZCnaD1z3yczYMRyjKJhXRvB2whJk4viFNHNl3mz4j%2BhsjJQf8uTydjXbLo0PGMHrv6bYOmq6DzlLraCVPfb2k0qSQ3GnVa6&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20250102T203924Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTY7MCBW2EE%2F20250102%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=f601d3a04e981c4cb19c87a8beaaf39524fc7b5a7557f7aed1943abe0e9c6e46&amp;hash=8964221ab10452b7fc0bc604fd43f32f3eae519abf591f77261ff75bfb73b13e&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S0267364923001188&amp;tid=spdf-dc5066c4-6272-46ed-a08f-c11627288fa2&amp;sid=af8ef9f4253fd043dd586b081af4b12954d3gxrqb&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=16145e02040657000602&amp;rr=8fbd8eea9b53785f&amp;cc=pt</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:39:38</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_2681">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2681/S0267364923001188.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0267364923001188?via%3Dihub</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:39:41</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:21844992%20(ISSN);%20978-989758692-7%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1</prism:volume>
                <dc:identifier>ISBN 21844992 (ISSN); 978-989758692-7 (ISBN)</dc:identifier>
                <dc:title>International Conference on Enterprise Information Systems, ICEIS - Proceedings</dc:title>
                <dc:identifier>DOI 10.5220/0012624700003690</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Science and Technology Publications, Lda</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nunes</foaf:surname>
                        <foaf:givenName>R.O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Spritzer</foaf:surname>
                        <foaf:givenName>A.S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Freitas</foaf:surname>
                        <foaf:givenName>C.M.D.S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Balreira</foaf:surname>
                        <foaf:givenName>D.G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Filipe J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Smialek M.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Brodsky A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Hammoudi S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1761"/>
        <link:link rdf:resource="#item_2697"/>
        <link:link rdf:resource="#item_2696"/>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Context learning</dc:subject>
        <dc:subject>Heuristic methods</dc:subject>
        <dc:subject>In contexts</dc:subject>
        <dc:subject>In-context learning</dc:subject>
        <dc:subject>In-Context Learning</dc:subject>
        <dc:subject>Knowledge management</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Large language model</dc:subject>
        <dc:subject>Large Language Models</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Legal domains</dc:subject>
        <dc:subject>Legal tech</dc:subject>
        <dc:subject>Legal Tech</dc:subject>
        <dc:subject>Llama</dc:subject>
        <dc:subject>LLama</dc:subject>
        <dc:subject>Named entity recognition</dc:subject>
        <dc:subject>Named Entity Recognition</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Portuguese languages</dc:subject>
        <dc:title>Out of Sesame Street: A Study of Portuguese Legal Named Entity Recognition Through In-Context Learning</dc:title>
        <dcterms:abstract>This paper explores the application of the In-Context Learning (ICL) paradigm for Named Entity Recognition (NER) within the Portuguese language legal domain. Identifying named entities in legal documents is complex due to the intricate nature of legal language and the specificity of legal terms. This task is important for a range of applications, from legal information retrieval to automated summarization and analysis. However, the manual annotation of these entities is costly due to the specialized knowledge required from legal experts and the large volume of documents. Recent advancements in Large Language Models (LLM) have led to studies exploring the use of ICL to improve the performance of Generative Language Models (GLMs). In this work, we used Sabiá, a Portuguese language LLM, to extract named entities within the legal domain. Our goal was to evaluate the consistency of these extractions and derive insights from the results. Our methodology involved using a legal-domain NER corpus as input and selecting specific samples for a prompting task. We then instructed the GLM to catalog its own NER corpus, which we compared with the original test examples. Our study examined various aspects, including context examples, selection strategies, heuristic methodologies, post-processing techniques, and quantitative and qualitative analyses across specific domain classes. Our results indicate promising directions for future research and applications in specialized domains. Copyright © 2024 by SCITEPRESS – Science and Technology Publications, Lda.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:shortTitle>Out of Sesame Street</z:shortTitle>
        <z:archive>Scopus</z:archive>
        <z:libraryCatalog>www.scitepress.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193953512&amp;doi=10.5220%2f0012624700003690&amp;partnerID=40&amp;md5=bddfabd813511b22e8b8b5cb942517ee</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: International Conference on Enterprise Information Systems, ICEIS - Proceedings</dc:description>
        <bib:pages>477-489</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>International Conference on Enterprise Information Systems, ICEIS - Proceedings</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1761">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 1; Conference name: 26th International Conference on Enterprise Information Systems, ICEIS 2024; Conference date: 28 April 2024 through 30 April 2024; Conference code: 199602&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2697">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2697/Nunes et al. - 2024 - Out of Sesame Street A Study of Portuguese Legal Named Entity Recognition Through In-Context Learni.pdf"/>
        <dc:title>PDF</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_2696">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2696/Link.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scitepress.org/Link.aspx?doi=10.5220/0012624700003690</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:47:08</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:18650929%20(ISSN);%20978-303135923-1%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1718 CCIS</prism:volume>
                <dc:identifier>ISBN 18650929 (ISSN); 978-303135923-1 (ISBN)</dc:identifier>
                <dc:title>Commun. Comput. Info. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-031-35924-8_3</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Springer Science and Business Media Deutschland GmbH</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Darji</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mitrović</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Granitzer</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                   <foaf:Person><foaf:surname>Fred A.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Fred A.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Aveiro D.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Aveiro D.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Dietz J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Bernardino J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Masciari E.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Filipe J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Filipe J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1764"/>
        <dcterms:isReferencedBy rdf:resource="#item_2676"/>
        <dc:subject>Data handling</dc:subject>
        <dc:subject>Embeddings</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Legal language processing</dc:subject>
        <dc:subject>Legal texts</dc:subject>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>NLP</dc:subject>
        <dc:subject>Open Data</dc:subject>
        <dc:subject>Open legal data</dc:subject>
        <dc:subject>Semantic similarity</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Sentence transformer</dc:subject>
        <dc:subject>Sentence transformers</dc:subject>
        <dc:subject>Similarity scores</dc:subject>
        <dc:title>Exploring Semantic Similarity Between German Legal Texts and Referred Laws</dc:title>
        <dcterms:abstract>The calculation of semantic similarity is an important task in Natural Language Processing (NLP). There is a growing interest in this task in the research community, especially following the advent of new, ever-evolving neural architectures. However, this technique has not been explored in-depth in the realm of automatic processing of legal data, the area we often call Legal NLP or Legal Tech. In this paper, we aim to use semantic similarity to identify the relations between legal sentences that refer to a certain law and the law text itself. The semantic similarity score is calculated using cosine similarity between sentence embeddings. In our work, we use sentence transformers to get the sentence embeddings for our legal text. The results we achieve by using two separate sentence transformers, Cross English &amp; German RoBERTa and all-MiniLM-L6-v2, provide a semantic similarity score of approximately 0.45 and 0.4, respectively. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169001968&amp;doi=10.1007%2f978-3-031-35924-8_3&amp;partnerID=40&amp;md5=d428569765e59a82eb241d4ab1641624</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Commun. Comput. Info. Sci.</dc:description>
        <bib:pages>37-50</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Communications in Computer and Information Science</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1764">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 1; Correspondence Address: J. Mitrović; Chair of Data Science, University of Passau, Passau, Innstraße 41, 94032, Germany; email: jelena.mitrovic@uni-passau.de; Conference name: 13th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, IC3K 2021; Conference date: 25 October 2021 through 27 October 2021; Conference code: 297709&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2676">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser de acesso pago&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:09226389%20(ISSN);%20978-164368472-7%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>379</prism:volume>
                <dc:identifier>ISBN 09226389 (ISSN); 978-164368472-7 (ISBN)</dc:identifier>
                <dc:title>Front. Artif. Intell. Appl.</dc:title>
                <dc:identifier>DOI 10.3233/FAIA230971</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>IOS Press BV</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khatri</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sheik</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wadhwa</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Satija</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kumar</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shah</foaf:surname>
                        <foaf:givenName>R.R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kumaraguru</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Sileno G.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Spanakis J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>van Dijck G.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1762"/>
        <link:link rdf:resource="#item_2653"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Legal documents</dc:subject>
        <dc:subject>Legal domains</dc:subject>
        <dc:subject>Performance</dc:subject>
        <dc:subject>Classification</dc:subject>
        <dc:subject>Language semantics</dc:subject>
        <dc:subject>Legal NLP</dc:subject>
        <dc:subject>Assistive</dc:subject>
        <dc:subject>Caselaw</dc:subject>
        <dc:subject>Citation</dc:subject>
        <dc:subject>Citation contexts</dc:subject>
        <dc:subject>Domain specificity</dc:subject>
        <dc:subject>High level languages</dc:subject>
        <dc:title>CiteCaseLAW: Citation Worthiness Detection in Caselaw for Legal Assistive Writing</dc:title>
        <dcterms:abstract>Complex legal language, filled with jargon, nuanced language semantics, and a high level of domain specificity, poses a significant challenge for automation in handling various legal tasks. In the realm of legal document composition, a pivotal component revolves around accurately referencing case laws and other sources to substantiate assertions and arguments. Understanding the legal domain and identifying appropriate citation context or cite-worthy sentences automatically is challenging. Our research is centered on the issue of citation-worthiness identification of a given sentence. This serves as the initial phase in contemporary citation recommendation systems, aimed at alleviating the effort involved in extracting a suitable array of citation contexts. To address this, we first introduce a labeled dataset comprising 178 million sentences, specifically tailored for detecting citation-worthy content within the legal domain. This dataset is curated from the Caselaw Access Project (CAP) (https://case.law/). We proceeded to assess the performance of a range of deep learning models on this novel dataset. Among the models examined, the domain-specific pre-trained model consistently demonstrated superior performance, achieving an 88% F1-score in the task of detecting citation-worthy material. To enhance our insights, we employed inputXGradient explainable AI techniques to dissect the predictions, thereby identifying the tokens that contribute to specific citation classes.  © 2023 The Authors.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181166957&amp;doi=10.3233%2fFAIA230971&amp;partnerID=40&amp;md5=0d6c1efc05c112068730f10cde10e802</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Front. Artif. Intell. Appl.</dc:description>
        <bib:pages>257-262</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Frontiers in Artificial Intelligence and Applications</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1762">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 1; Correspondence Address: M. Khatri; Iiit Delhi, India; email: mannk@iiitd.ac.in; Conference name: 36th International Conference on Legal Knowledge and Information Systems, JURIX 2023; Conference date: 18 December 2023 through 20 December 2023; Conference code: 195587&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2653">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2653/Khatri et al. - 2023 - CiteCaseLAW Citation Worthiness Detection in Caselaw for Legal Assistive Writing.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA230971</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:09:51</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:979-835034355-7%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 979-835034355-7 (ISBN)</dc:identifier>
                <dc:title>IEEE Conf. Signal Process. Commun. Appl., SIU</dc:title>
                <dc:identifier>DOI 10.1109/SIU59756.2023.10223938</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Öztürk</foaf:surname>
                        <foaf:givenName>C.E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Özçelik</foaf:surname>
                        <foaf:givenName>Ş.B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koç</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1763"/>
        <link:link rdf:resource="#item_2630"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Case retrieval</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Legal case</dc:subject>
        <dc:subject>Zero-shot learning</dc:subject>
        <dc:subject>Turkishs</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Modeling languages</dc:subject>
        <dc:subject>Legal NLP</dc:subject>
        <dc:subject>Legal tech</dc:subject>
        <dc:subject>legal NLP</dc:subject>
        <dc:subject>legal tech</dc:subject>
        <dc:subject>prior legal case retrieval</dc:subject>
        <dc:subject>Prior legal case retrieval</dc:subject>
        <dc:subject>Turkish NLP</dc:subject>
        <dc:title>A Transformer-Based Prior Legal Case Retrieval Method</dc:title>
        <dcterms:abstract>In this work, BERTurk-Legal, a transformer-based language model, is introduced to retrieve prior legal cases. BERTurk-Legal is pre-trained on a dataset from the Turkish legal domain. This dataset does not contain any labels related to the prior court case retrieval task. Masked language modeling is used to train BERTurk-Legal in a self-supervised manner. With zero-shot classification, BERTurk-Legal provides state-of-the-art results on the dataset consisting of legal cases of the Court of Cassation of Turkey. The results of the experiments show the necessity of developing language models specific to the Turkish law domain. © 2023 IEEE.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>Turkish</z:language>
        <z:shortTitle>Dönüştürücü Tabanli Emsal Karar Bulma Yöntemi</z:shortTitle>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173471266&amp;doi=10.1109%2fSIU59756.2023.10223938&amp;partnerID=40&amp;md5=44e9d72382e34f280b5fa3e121d8278b</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: IEEE Conf. Signal Process. Commun. Appl., SIU</dc:description>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>31st IEEE Conference on Signal Processing and Communications Applications, SIU 2023</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1763">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 0; Conference name: 31st IEEE Conference on Signal Processing and Communications Applications, SIU 2023; Conference date: 5 July 2023 through 8 July 2023; Conference code: 192084&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2630">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2630/Öztürk et al. - 2023 - A Transformer-Based Prior Legal Case Retrieval Method.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=10223938&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:07:36</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:23673370%20(ISSN);%20978-303137962-8%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>739 LNNS</prism:volume>
                <dc:identifier>ISBN 23673370 (ISSN); 978-303137962-8 (ISBN)</dc:identifier>
                <dc:title>Lect. Notes Networks Syst.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-031-37963-5_75</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Springer Science and Business Media Deutschland GmbH</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ivaschenko</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Golovnin</foaf:surname>
                        <foaf:givenName>O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Syusin</foaf:surname>
                        <foaf:givenName>I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krivosheev</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Aleksandrova</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                   <foaf:Person><foaf:surname>Arai K.</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1765"/>
        <dcterms:isReferencedBy rdf:resource="#item_2692"/>
        <dc:subject>Intelligent systems</dc:subject>
        <dc:subject>Knowledge based systems</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>Legal technology</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Ontology</dc:subject>
        <dc:subject>Ontology-based</dc:subject>
        <dc:subject>Ontology's</dc:subject>
        <dc:subject>Problem domain</dc:subject>
        <dc:subject>Software solution</dc:subject>
        <dc:subject>Technology application</dc:subject>
        <dc:subject>Text generation</dc:subject>
        <dc:subject>Text generations</dc:subject>
        <dc:subject>Text understanding</dc:subject>
        <dc:title>Ontology Based Text Understanding and Text Generation for Legal Technology Applications</dc:title>
        <dcterms:abstract>The paper presents a software solution for text understanding and text generation, which finds new opportunities for being applied in the area of Legal Tech. The solution belongs to the problem domain of natural language processing and covers the gaps in legal documentation comparative analysis and generation. Modern trends in information technology implementation in Legal Tech are overviewed and summarized to generate the recommendations of text understanding and text generation solutions, practical implementation and use. Based on the attributes extracted from legal text documents, a subject area ontological model is formed to describe, organize and present the relationships between the facts extracted from documents, for example, normative legal acts, their individual clauses, subparagraphs, specific named entities, facts and their relationships. Experimental research was carried out to extract entities including facts and attributes from a set of administrative regulations which contains 537 documents. The proposed solution demonstrates the benefits of knowledge based text understanding and text generation in Legal Tech. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172231295&amp;doi=10.1007%2f978-3-031-37963-5_75&amp;partnerID=40&amp;md5=2753ffdbe2ece47e6fd320113c4a4251</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Lect. Notes Networks Syst.</dc:description>
        <bib:pages>1080-1089</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>Lecture Notes in Networks and Systems</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1765">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 0; Correspondence Address: A. Ivaschenko; Samara State Medical University, Samara, Chapaevskaya, 89, Russian Federation; email: anton.ivashenko@gmail.com; Conference name: Proceedings of the Computing Conference 2023; Conference date: 22 June 2023 through 23 June 2023; Conference code: 301079&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2692">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser de acesso pago&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180771286&amp;partnerID=40&amp;md5=24c6a990a25ffedf8908379fb7d07d88">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>3594</prism:volume>
                <dc:identifier>ISBN 16130073 (ISSN)</dc:identifier>
                <dc:title>CEUR Workshop Proc.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>CEUR-WS</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Greco</foaf:surname>
                        <foaf:givenName>C.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tagarelli</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Wehnert S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Wehnert S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Fiorelli M.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Picca D.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>De Luca E.W.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>De Luca E.W.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Stellato A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1767"/>
        <link:link rdf:resource="#item_2699"/>
        <dc:subject>AI systems</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Basic principles</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>European Constitution</dc:subject>
        <dc:subject>European constitutions</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>law</dc:subject>
        <dc:subject>Law</dc:subject>
        <dc:subject>Legal case</dc:subject>
        <dc:subject>Legal language model</dc:subject>
        <dc:subject>legal language models</dc:subject>
        <dc:subject>Power</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Structure functions</dc:subject>
        <dc:subject>topic similarity</dc:subject>
        <dc:subject>Topic similarity</dc:subject>
        <dc:title>Topic Similarities in Rights and Duties across European Constitutions using Transformer-based Language Models</dc:title>
        <dcterms:abstract>The use of language models in the legal NLP field has brought significant advances in the use of AI systems to support legal professionals. However, most of the efforts so far have focused on processing documents such as legal cases, contracts and statutes. There are several types of legal resources that are still overlooked, and these include constitutions. A constitution establishes the basic principles, structures, functions and powers of a country’s governance. Several portions of the constitutions are devoted to rights and duties of the citizens, which are essential to define and protect the status of citizens as individuals and as members of the society. To this regard, in this work we focus on the range of topics covered in the European constitutions that guarantee rights and duties to citizens. We present the first study providing lexical and semantic similarity analysis of the European constitutions, which especially takes advantage of using several Transformer-based models. © 2023 Copyright for this paper by its authors.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180771286&amp;partnerID=40&amp;md5=24c6a990a25ffedf8908379fb7d07d88</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: CEUR Workshop Proc.</dc:description>
        <bib:pages>47-62</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>CEUR Workshop Proceedings</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1767">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 1; Correspondence Address: C.M. Greco; Dept. Computer Engineering, Modeling, Electronics, and Systems Engineering (DIMES), University of Calabria, Rende, CS, 87036, Italy; email: candida.greco@dimes.unical.it; Conference name: 1st Legal Information Retrieval meets Artificial Intelligence Workshop, LIRAI 2023; Conference code: 195434&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2699">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2699/Greco - Topic Similarities in Rights and Duties across European Constitutions using Transformer-based Langua.pdf"/>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://ceur-ws.org/Vol-3594/paper4.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:49:23</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:21843589%20(ISSN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>3</prism:volume>
                <dc:identifier>ISBN 21843589 (ISSN)</dc:identifier>
                <dc:title>Int. Conf. Agent. Artif. Intell.</dc:title>
                <dc:identifier>DOI 10.5220/0011749400003393</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Science and Technology Publications, Lda</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Darji</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mitroviċ</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Granitzer</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Rocha A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Steels L.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>van den Herik J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1768"/>
        <link:link rdf:resource="#item_2638"/>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Language Models</dc:subject>
        <dc:subject>Named Entity Recognition</dc:subject>
        <dc:subject>Legal Entity Recognition</dc:subject>
        <dc:subject>Legal Language Processing</dc:subject>
        <dc:title>German BERT Model for Legal Named Entity Recognition</dc:title>
        <dcterms:abstract>The use of BERT, one of the most popular language models, has led to improvements in many Natural Language Processing (NLP) tasks. One such task is Named Entity Recognition (NER) i.e. automatic identification of named entities such as location, person, organization, etc. from a given text. It is also an important base step for many NLP tasks such as information extraction and argumentation mining. Even though there is much research done on NER using BERT and other popular language models, the same is not explored in detail when it comes to Legal NLP or Legal Tech. Legal NLP applies various NLP techniques such as sentence similarity or NER specifically on legal data. There are only a handful of models for NER tasks using BERT language models, however, none of these are aimed at legal documents in German. In this paper, we fine-tune a popular BERT language model trained on German data (German BERT) on a Legal Entity Recognition (LER) dataset. To make sure our model is not overfitting, we performed a stratified 10-fold cross-validation. The results we achieve by fine-tuning German BERT on the LER dataset outperform the BiLSTM-CRF+ model used by the authors of the same LER dataset. Finally, we make the model openly available via HuggingFace. © 2023 by SCITEPRESS – Science and Technology Publications, Lda.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164414711&amp;doi=10.5220%2f0011749400003393&amp;partnerID=40&amp;md5=1e2da94d2b094351a49382a61a602d63</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Int. Conf. Agent. Artif. Intell.</dc:description>
        <bib:pages>723-728</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>International Conference on Agents and Artificial Intelligence</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1768">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 6; Conference name: 15th International Conference on Agents and Artificial Intelligence, ICAART 2023; Conference date: 22 February 2023 through 24 February 2023; Conference code: 301209&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2638">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2638/Darji et al. - 2023 - German BERT Model for Legal Named Entity Recognition.pdf"/>
        <dc:title>Versão Submetida</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2303.05388</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:08:29</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181168104&amp;doi=10.3233%2fFAIA230979&amp;partnerID=40&amp;md5=d76511ebfeffb8ec58b9f86b436942f8">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>379</prism:volume>
                <dc:identifier>ISBN 09226389 (ISSN); 978-164368472-7 (ISBN)</dc:identifier>
                <dc:title>Front. Artif. Intell. Appl.</dc:title>
                <dc:identifier>DOI 10.3233/FAIA230979</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>IOS Press BV</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ribary</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krause</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Orban</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vaccari</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wood</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Sileno G.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Spanakis J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>van Dijck G.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1769"/>
        <link:link rdf:resource="#item_2656"/>
        <dc:subject>NLP</dc:subject>
        <dc:subject>chatbot</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Quality control</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Chatbots</dc:subject>
        <dc:subject>Knowledge based systems</dc:subject>
        <dc:subject>Domain specific</dc:subject>
        <dc:subject>Prompt engineering</dc:subject>
        <dc:subject>Legal tech</dc:subject>
        <dc:subject>legal tech</dc:subject>
        <dc:subject>England</dc:subject>
        <dc:subject>Expert advice</dc:subject>
        <dc:subject>insolvency law (England)</dc:subject>
        <dc:subject>Insolvency law (england)</dc:subject>
        <dc:subject>Large language model (GPT)</dc:subject>
        <dc:subject>LLMs (GPT)</dc:subject>
        <dc:subject>prompt engineering</dc:subject>
        <dc:subject>Test sets</dc:subject>
        <dc:title>Prompt Engineering and Provision of Context in Domain Specific Use of GPT</dc:title>
        <dcterms:abstract>Large Language Models (LLMs) can appear to generate expert advice on legal matters. However, at closer analysis, some of the advice provided has proven unsound or erroneous. We tested LLMs' performance in the procedural and technical area of insolvency law in which our team has relevant expertise. This paper demonstrates that statistically more accurate results to evaluation questions come from a design which adds a curated knowledge base to produce quality responses when querying LLMs. We evaluated our bot head-to-head on an unseen test set of twelve questions about insolvency law against the unmodified versions of gpt-3.5-turbo and gpt-4 with a mark scheme similar to those used in examinations in law schools. On the 'unseen test set', the Insolvency Bot based on gpt-3.5-turbo outper-formed gpt-3.5-turbo (p = 1.8%), and our gpt-4 based bot outperformed unmodified gpt-4 (p = 0.05%). These promising results can be expanded to cross-jurisdictional queries and be further improved by matching on-point legal information to user queries. Overall, they demonstrate the importance of incorporating trusted knowledge sources into traditional LLMs in answering domain-specific queries.  © 2023 The Authors.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181168104&amp;doi=10.3233%2fFAIA230979&amp;partnerID=40&amp;md5=d76511ebfeffb8ec58b9f86b436942f8</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Front. Artif. Intell. Appl.</dc:description>
        <bib:pages>305-310</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Frontiers in Artificial Intelligence and Applications</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1769">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 2; Correspondence Address: M. Ribary; Royal Holloway, University of London, United Kingdom; email: marton.ribary@rhul.ac.uk; Conference name: 36th International Conference on Legal Knowledge and Information Systems, JURIX 2023; Conference date: 18 December 2023 through 20 December 2023; Conference code: 195587&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2656">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2656/Ribary et al. - 2023 - Prompt Engineering and Provision of Context in Domain Specific Use of GPT.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA230979</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:10:05</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:03029743%20(ISSN);%20978-303136189-0%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>13856 LNAI</prism:volume>
                <dc:identifier>ISBN 03029743 (ISSN); 978-303136189-0 (ISBN)</dc:identifier>
                <dc:title>Lect. Notes Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-031-36190-6_3</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Springer Science and Business Media Deutschland GmbH</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kalamkar</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Venugopalan</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Raghavan</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                   <foaf:Person><foaf:surname>Yada K.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Takama Y.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Mineshima K.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Satoh K.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1766"/>
        <dcterms:isReferencedBy rdf:resource="#item_2223"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Legal texts</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Text-processing</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Legal text processing</dc:subject>
        <dc:subject>Legal Text Processing</dc:subject>
        <dc:subject>Natural language processing benchmark</dc:subject>
        <dc:subject>NLP Benchmarks</dc:subject>
        <dc:subject>Wikipedia</dc:subject>
        <dc:title>Benchmarks for Indian Legal NLP: A Survey</dc:title>
        <dcterms:abstract>Legal text is significantly different from English text (e.g. Wikipedia, News) used for training most natural language processing (NLP) algorithms. As a result, the state of the art algorithms (e.g. GPT-3, BERT derivatives), need additional effort (e.g. fine-tuning and further pre-training) to achieve optimal performance on legal text. Hence there is a need to create separate NLP data sets and benchmarks for legal text which are challenging and focus on tasks specific to legal systems. This will spur innovation in applications of NLP for legal text and will benefit AI community and legal fraternity. This paper focuses on an empirical review of the existing work in the use of NLP in Indian legal text and proposes ideas to create new benchmarks for Indian Legal NLP. © 2023, Springer Nature Switzerland AG.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172422765&amp;doi=10.1007%2f978-3-031-36190-6_3&amp;partnerID=40&amp;md5=8d14338fcafa98a88c754aec0d6b463a</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Lect. Notes Comput. Sci.</dc:description>
        <bib:pages>33-48</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1766">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 0; Correspondence Address: P. Kalamkar; Thoughtworks India Pvt. Ltd., Chennai, India; email: prathamk@thoughtworks.com; Conference name: The 13th International Symposium on Artificial Intelligence supported by the Japanese Society for Artificial Intelligence, JSAI-isAI 2021; Conference date: 13 November 2021 through 15 November 2021; Conference code: 298299&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2223">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176556934&amp;partnerID=40&amp;md5=046de06af74eb6029f2c65c596230138">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>3399</prism:volume>
                <dc:identifier>ISBN 16130073 (ISSN)</dc:identifier>
                <dc:title>CEUR Workshop Proc.</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>CEUR-WS</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Csáki</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Homoki</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Görög</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vadász</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Janssen M.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Csaki C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Johannessen M.R.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Krimmer R.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Lampoltshammer T.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Lindgren I.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Loukis E.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Melin U.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Parycek P.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Pereira G.V.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Bolivar M.P.R.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Schwabe G.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Tambouris E.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Ubacht J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1770"/>
        <link:link rdf:resource="#item_2690"/>
        <dc:subject>AI regulation</dc:subject>
        <dc:subject>AI regulations</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>Legal department</dc:subject>
        <dc:subject>Legal profession</dc:subject>
        <dc:subject>Legal services</dc:subject>
        <dc:subject>Legal tech</dc:subject>
        <dc:subject>Legal Tech</dc:subject>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Service provider</dc:subject>
        <dc:subject>Small language</dc:subject>
        <dc:subject>small languages</dc:subject>
        <dc:title>NLP in the Legal Profession: How about Small Languages?</dc:title>
        <dcterms:abstract>Over the last three decades legal service providers as well as legal departments of various firms have embraced the opportunity to apply the latest digital technology to improve the efficiency and effectiveness of their work. Since language is central to both law-making and during the application of the law, Natural Language Processing solutions have found their way to this profession. One particular research area relates to the issue of small languages. The problem is rooted in the size of the population speaking a given language: in a small market, it is not economically feasible to develop NLP technologies as they require considerable time and effort to develop a sufficient language corpus. This paper reviews the challenges countries and jurisdictions of small languages face in light of increasing NLP applications in legal contexts, while also examining the role of the public sector in relation to addressing such issues. © 2022 Copyright for this paper by its authors.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176556934&amp;partnerID=40&amp;md5=046de06af74eb6029f2c65c596230138</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: CEUR Workshop Proc.</dc:description>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>CEUR Workshop Proceedings</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1770">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 0; Conference name: 2022 International Conference on IFIP WG 8.5 Electronic Government, E-Democracy and Open Government and IFIP WG 8.5 IFIP Electronic Participation, EGOV-CeDEM-ePart 2022; Conference date: 6 September 2022 through 8 September 2022; Conference code: 193901&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2690">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2690/Csáki et al. - NLP in the Legal Profession How about Small Languages.pdf"/>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://ceur-ws.org/Vol-3399/paper19.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:44:29</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-195591790-2%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-195591790-2 (ISBN)</dc:identifier>
                <dc:title>HCI+NLP - Workshop Bridg. Hum.-Comput. Interact. Nat. Lang. Process., Proc. Workshop</dc:title>
                <dc:identifier>DOI 10.18653/v1/2022.hcinlp-1.4</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barale</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Blodgett S.L.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Daume H.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Madaio M.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Nenkova A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>O'Connor B.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Wallach H.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Yang Q.</foaf:surname></foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1771"/>
        <link:link rdf:resource="#item_2655"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Ethical technology</dc:subject>
        <dc:subject>Decision makers</dc:subject>
        <dc:subject>Decision making</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Legal texts</dc:subject>
        <dc:subject>Text data</dc:subject>
        <dc:subject>Iterative methods</dc:subject>
        <dc:subject>Support systems</dc:subject>
        <dc:subject>Design method</dc:subject>
        <dc:subject>Human-centered computing</dc:subject>
        <dc:subject>Status determination</dc:subject>
        <dc:subject>Text analytics</dc:subject>
        <dc:title>Human-Centered Computing in Legal NLP An Application to Refugee Status Determination</dc:title>
        <dcterms:abstract>This paper proposes an approach to the design of an ethical human-AI reasoning support system for decision makers in refugee law. In the context of refugee status determination, practitioners mostly rely on text data. We therefore investigate human-AI cooperation in legal natural language processing. Specifically, we want to determine which design methods can be transposed to legal text analytics. Although little work has been done so far on human-centered design methods applicable to the legal domain, we assume that introducing iterative cooperation and user engagement in the design process is (1) a method to reduce technical limitations of an NLP system and (2) that it will help design more ethical and effective applications by taking users' preferences and feedback into account. The proposed methodology is based on three main design steps: cognitive process formalization in models understandable by both humans and computers, speculative design of prototypes, and semi-directed interviews with a sample of potential users. © 2022 Association for Computational Linguistics.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137581347&amp;doi=10.18653%2fv1%2f2022.hcinlp-1.4&amp;partnerID=40&amp;md5=e5c0df558dfa5ae736d0fbd50dea3fa1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: HCI+NLP - Workshop Bridg. Hum.-Comput. Interact. Nat. Lang. Process., Proc. Workshop</dc:description>
        <bib:pages>28-33</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>HCI+NLP 2022 - 2nd Workshop on Bridging Human-Computer Interaction and Natural Language Processing, Proceedings of the Workshop</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1771">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 2; Correspondence Address: C. Barale; The University of Edinburgh, School of Informatics, Edinburgh, United Kingdom; email: claire.barale@ed.ac.uk; Conference name: 2nd Workshop on Bridging Human-Computer Interaction and Natural Language Processing, HCI+NLP 2022; Conference code: 182130&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2655">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2655/Barale - 2022 - Human-Centered Computing in Legal NLP An Application to Refugee Status Determination.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://aclanthology.org/2022.hcinlp-1.4.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:09:56</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-145038732-3%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-145038732-3 (ISBN)</dc:identifier>
                <dc:title>SIGIR - Proc. Int. ACM SIGIR Conf. Res. Dev. Inf. Retr.</dc:title>
                <dc:identifier>DOI 10.1145/3477495.3536329</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computing Machinery, Inc</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vianna</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Silva De Moura</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1774"/>
        <link:link rdf:resource="#item_2644"/>
        <dc:subject>Learning algorithms</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Case retrieval</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Legal documents</dc:subject>
        <dc:subject>Legal domains</dc:subject>
        <dc:subject>Legal case</dc:subject>
        <dc:subject>Legal judgements</dc:subject>
        <dc:subject>Modeling languages</dc:subject>
        <dc:subject>Inherent complexity</dc:subject>
        <dc:subject>language models</dc:subject>
        <dc:subject>law tech</dc:subject>
        <dc:subject>Law tech</dc:subject>
        <dc:subject>legal cases</dc:subject>
        <dc:subject>Topic Discovery</dc:subject>
        <dc:subject>topic model</dc:subject>
        <dc:subject>Topic Modeling</dc:subject>
        <dc:title>Organizing Portuguese Legal Documents through Topic Discovery</dc:title>
        <dcterms:abstract>A significant challenge in the legal domain is to organize and summarize a constantly growing collection of legal documents, uncovering hidden topics, or themes, that later can support tasks such as legal case retrieval and legal judgment prediction. This massive amount of digital legal documents, combined with the inherent complexity of judiciary systems worldwide, presents a promising scenario for Machine Learning solutions, mainly those taking advantage of all the advancements in the area of Natural Language Processing (NLP). It is in this scenario that Jusbrasil, the largest legal tech company in Brazil, is situated. Using a dataset partially curated by the Jusbrasil legal team, we explore topic modeling solutions using state of the art language models, trained with legal Portuguese documents, to automatically organize and summarize this complex collection of documents. Instead of using an entire legal case, which usually is composed of many pages, we show that it is possible to efficiently organize the collection using the syllabus (in Portuguese, ementa jurisprudencial) from each court decision as they concisely summarize the main points presented by the entire decision. © 2022 ACM.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135014586&amp;doi=10.1145%2f3477495.3536329&amp;partnerID=40&amp;md5=b3f5c481561935d4ba649092d9f5daf9</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: SIGIR - Proc. Int. ACM SIGIR Conf. Res. Dev. Inf. Retr.</dc:description>
        <bib:pages>3388-3392</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>SIGIR 2022 - Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1774">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 2; Conference name: 45th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2022; Conference date: 11 July 2022 through 15 July 2022; Conference code: 180740&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2644">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2644/Vianna e Silva De Moura - 2022 - Organizing Portuguese Legal Documents through Topic Discovery.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/pdf/10.1145/3477495.3536329</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:08:57</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-166545092-8%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-166545092-8 (ISBN)</dc:identifier>
                <dc:title>Signal Process. Commun. Appl. Conf., SIU</dc:title>
                <dc:identifier>DOI 10.1109/SIU55565.2022.9864914</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ozturk</foaf:surname>
                        <foaf:givenName>C.E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ozcelik</foaf:surname>
                        <foaf:givenName>S.B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koc</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1772"/>
        <link:link rdf:resource="#item_2645"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Law</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Machine Learning</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Machine-learning</dc:subject>
        <dc:subject>Legal texts</dc:subject>
        <dc:subject>AI in law</dc:subject>
        <dc:subject>Legal text mining</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Deep Learning</dc:subject>
        <dc:subject>Embeddings</dc:subject>
        <dc:subject>Long short-term memory</dc:subject>
        <dc:subject>Text-mining</dc:subject>
        <dc:subject>AI in Law</dc:subject>
        <dc:title>Predicting Outcomes of the Court of Cassation of Turkey with Recurrent Neural Networks</dc:title>
        <dcterms:abstract>Natural Language Processing (NLP) based approaches have recently become very popular for studies in legal domain. In this work, the outcomes of the cases of the Court of Cassation of Turkey were predicted with the use of Deep Learning models. These models are GRU, LSTM and BiLSTM which are variants of the recurrent neural network. Models saw only fact description parts of the case decision texts during training. Firstly, the models were trained with the word embeddings that were created from the texts from daily language. Then, the models were trained with the word embeddings that were created from downloaded legal cases from Turkish courts. The results of the experiments on the models are given in a comparative and detailed manner. It is observed based on this study and the past studies that the outcomes of the Court of Cassation can be predicted with higher accuracy than most of the courts that were investigated in previous legal NLP studies. The model which is best at predicting decisions is GRU. The GRU model achieves 96.8% accuracy in the decision prediction task. © 2022 IEEE.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>Turkish</z:language>
        <z:shortTitle>Yargitay Kararlarinin Mükerrer Sinir Aǧlari ile Tahmini</z:shortTitle>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138685687&amp;doi=10.1109%2fSIU55565.2022.9864914&amp;partnerID=40&amp;md5=d8447f290c4002566781158036ab03d2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Signal Process. Commun. Appl. Conf., SIU</dc:description>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 30th Signal Processing and Communications Applications Conference, SIU 2022</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1772">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 3; Conference name: 30th Signal Processing and Communications Applications Conference, SIU 2022; Conference date: 15 May 2022 through 18 May 2022; Conference code: 182415&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2645">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2645/Ozturk et al. - 2022 - Predicting Outcomes of the Court of Cassation of Turkey with Recurrent Neural Networks.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=9864914&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:09:09</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138702454&amp;doi=10.1109%2fSIU55565.2022.9864970&amp;partnerID=40&amp;md5=e6e613f32c12bbfbb6a9b979a9320f0b">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-166545092-8 (ISBN)</dc:identifier>
                <dc:title>Signal Process. Commun. Appl. Conf., SIU</dc:title>
                <dc:identifier>DOI 10.1109/SIU55565.2022.9864970</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Institute of Electrical and Electronics Engineers Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Aras</foaf:surname>
                        <foaf:givenName>A.C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ozturk</foaf:surname>
                        <foaf:givenName>C.E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koc</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1775"/>
        <link:link rdf:resource="#item_2636"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Turkishs</dc:subject>
        <dc:subject>Principal component analysis</dc:subject>
        <dc:subject>Network-based</dc:subject>
        <dc:subject>Feedforward neural networks</dc:subject>
        <dc:subject>Legal natural language processing</dc:subject>
        <dc:subject>Legal tech</dc:subject>
        <dc:subject>legal NLP</dc:subject>
        <dc:subject>legal tech</dc:subject>
        <dc:subject>High Courts</dc:subject>
        <dc:title>Feedforward Neural Network Based Case Prediction in Turkish Higher Courts</dc:title>
        <dcterms:abstract>Thanks to natural language processing (NLP) methods, legal texts can be processed by computers and decision prediction applications can be developed in the legal tech field. Increase in the available data sources in the Turkish legal system provides an opportunity to develop NLP applications as well. In order to develop these applications, the necessary corpora and datasets should be created. In this work, legal case texts from the Turkish Higher Courts that are open to public access and free from personal data are used to develop decision prediction methods. Feedforward neural networks (FFNN) are deployed using word embeddings and the features extracted from texts via the Principal Component Analysis (PCA) algorithm. %85.4 Macro F1 score level is achieved. © 2022 IEEE.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>Turkish</z:language>
        <z:shortTitle>Ileri Beslemeli Sinir Aǧlari ile Türk Üst Mahkemelerinde Karar Tahmini</z:shortTitle>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138702454&amp;doi=10.1109%2fSIU55565.2022.9864970&amp;partnerID=40&amp;md5=e6e613f32c12bbfbb6a9b979a9320f0b</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Signal Process. Commun. Appl. Conf., SIU</dc:description>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 30th Signal Processing and Communications Applications Conference, SIU 2022</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1775">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 3; Conference name: 30th Signal Processing and Communications Applications Conference, SIU 2022; Conference date: 15 May 2022 through 18 May 2022; Conference code: 182415&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2636">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2636/Aras et al. - 2022 - Feedforward Neural Network Based Case Prediction in Turkish Higher Courts.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=9864970&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:08:15</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-195942926-5%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-195942926-5 (ISBN)</dc:identifier>
                <dc:title>UM-IoS - Unimodal Multimodal Induction Linguist. Struct., Proc. Workshop</dc:title>
                <dc:identifier>DOI 10.18653/v1/2022.umios-1.8</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pal</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1773"/>
        <link:link rdf:resource="#item_2654"/>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Legal domains</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Benchmark datasets</dc:subject>
        <dc:subject>Classification (of information)</dc:subject>
        <dc:subject>Prediction tasks</dc:subject>
        <dc:subject>Information retrieval systems</dc:subject>
        <dc:subject>Data collection</dc:subject>
        <dc:subject>Classification tasks</dc:subject>
        <dc:subject>Members of parliaments</dc:subject>
        <dc:subject>Process Improving</dc:subject>
        <dc:subject>State legislatures</dc:subject>
        <dc:subject>Statistics and analysis</dc:subject>
        <dc:subject>Time consumption</dc:subject>
        <dc:title>DeepParliament: A Legal domain Benchmark &amp; Dataset for Parliament Bills Prediction</dc:title>
        <dcterms:abstract>This paper introduces DeepParliament, a legal domain Benchmark Dataset that gathers bill documents and metadata and performs various bill status classification tasks. The proposed dataset text covers a broad range of bills from 1986 to the present and contains richer information on parliament bill content. Data collection, detailed statistics and analyses are provided in the paper. Moreover, we experimented with different types of models ranging from RNN to pretrained and reported the results. We are proposing two new benchmarks: Binary and Multi-Class Bill Status classification. Models developed for bill documents and relevant supportive tasks may assist Members of Parliament (MPs), presidents, and other legal practitioners. It will help review or prioritise bills, thus speeding up the billing process, improving the quality of decisions and reducing the time consumption in both houses. Considering that the foundation of the country's democracy is Parliament and state legislatures, we anticipate that our research will be an essential addition to the Legal NLP community. This work will be the first to present a Parliament bill prediction task. In order to improve the accessibility of legal AI resources and promote reproducibility, we have made our code and dataset publicly accessible at github.com/monk1337/DeepParliament. © 2022 Association for Computational Linguistics.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154542917&amp;doi=10.18653%2fv1%2f2022.umios-1.8&amp;partnerID=40&amp;md5=772bf85a123a4ee06f31d3365c9d31b7</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: UM-IoS - Unimodal Multimodal Induction Linguist. Struct., Proc. Workshop</dc:description>
        <bib:pages>73-81</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>UM-IoS 2022 - Unimodal and Multimodal Induction of Linguistic Structures, Proceedings of the Workshop</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1773">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 0; Correspondence Address: A. Pal; Open Legal AI, United Kingdom; email: openlegalai@gmail.com; Conference name: 2022 Workshop on Unimodal and Multimodal Induction of Linguistic Structures, UM-IoS 2022; Conference code: 187864&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2654">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2654/Pal - 2022 - DeepParliament A Legal domain Benchmark &amp; Dataset for Parliament Bills Prediction.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://aclanthology.org/2022.umios-1.8.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:09:55</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:21945357%20(ISSN);%20978-981153382-2%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1141</prism:volume>
                <dc:identifier>ISBN 21945357 (ISSN); 978-981153382-2 (ISBN)</dc:identifier>
                <dc:title>Adv. Intell. Sys. Comput.</dc:title>
                <dc:identifier>DOI 10.1007/978-981-15-3383-9_43</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>Springer</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vardhan</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Surana</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tripathy</foaf:surname>
                        <foaf:givenName>B.K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Hassanien A.E.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Bhatnagar R.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Darwish A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1776"/>
        <dcterms:isReferencedBy rdf:resource="#item_2689"/>
        <dc:subject>Authentication</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Learning systems</dc:subject>
        <dc:subject>Learning techniques</dc:subject>
        <dc:subject>Legal documents</dc:subject>
        <dc:subject>Legal tech</dc:subject>
        <dc:subject>Named entity recognition</dc:subject>
        <dc:subject>Named-entity recognition</dc:subject>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:title>Named-entity recognition for legal documents</dc:title>
        <dcterms:abstract>The law has language at its core, so it is not surprising that software operating on natural language has played a role in certain areas of the legal industry for a long time. The last few years have seen a significant upsurge of interest in this area, including an increasing number of start-ups applying deep learning techniques in the context of specific legal applications. In this paper, we present a simple yet powerful method that is applied to legal documents from different legal bodies to correctly recognize a numerous entity to find relevant information for some specific matter at hand. To the best of our knowledge, no attempt has been made in this direction so far and as such our work opens a new direction of research, which will be very much useful for the society. © Springer Nature Singapore Pte Ltd 2021.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087008022&amp;doi=10.1007%2f978-981-15-3383-9_43&amp;partnerID=40&amp;md5=964a2b24728851042b5d56aa517d0039</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Adv. Intell. Sys. Comput.</dc:description>
        <bib:pages>469-479</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>Advances in Intelligent Systems and Computing</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1776">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 7; Correspondence Address: N. Surana; FindMind Analytics Pvt. Ltd., Vellore, India; email: nitishsurana@findmind.in; Conference name: 5th International Conference on Advanced Machine Learning Technologies and Applications, AMLTA 2020; Conference date: 13 February 2020 through 15 February 2020; Conference code: 240399&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2689">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser de acesso pago&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112347578&amp;doi=10.1145%2f3462757.3466103&amp;partnerID=40&amp;md5=15fc0802eddae8035a4f47f95509946b">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-145038526-8 (ISBN)</dc:identifier>
                <dc:title>Proc. Int. Conf. Artif. Intell. Law, ICAIL</dc:title>
                <dc:identifier>DOI 10.1145/3462757.3466103</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computing Machinery, Inc</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rosa</foaf:surname>
                        <foaf:givenName>G.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rodrigues</foaf:surname>
                        <foaf:givenName>R.C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>De Alencar Lotufo</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nogueira</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1777"/>
        <link:link rdf:resource="#item_2648"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Law enforcement</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Legal domains</dc:subject>
        <dc:subject>Legal case</dc:subject>
        <dc:subject>Large dataset</dc:subject>
        <dc:subject>Percentage points</dc:subject>
        <dc:subject>legal NLP</dc:subject>
        <dc:subject>Data distribution</dc:subject>
        <dc:subject>deberta</dc:subject>
        <dc:subject>legal case entailment</dc:subject>
        <dc:subject>T5</dc:subject>
        <dc:subject>Target domain</dc:subject>
        <dc:subject>zero-shot</dc:subject>
        <dc:title>To tune or not to tune?: Zero-shot models for legal case entailment</dc:title>
        <dcterms:abstract>There has been mounting evidence that pretrained language models fine-tuned on large and diverse supervised datasets can transfer well to a variety of out-of-domain tasks. In this work, we investigate this transfer ability to the legal domain. For that, we participated in the legal case entailment task of COLIEE 2021, in which we use such models with no adaptations to the target domain. Our submissions achieved the highest scores, surpassing the second-best submission by more than six percentage points. Our experiments confirm a counter-intuitive result in the new paradigm of pretrained language models: given limited labeled data, models with little or no adaption to the target task can be more robust to changes in the data distribution than models fine-tuned on it. Code is available at https://github.com/neuralmind-ai/coliee.  © 2021 ACM.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112347578&amp;doi=10.1145%2f3462757.3466103&amp;partnerID=40&amp;md5=15fc0802eddae8035a4f47f95509946b</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Proc. Int. Conf. Artif. Intell. Law, ICAIL</dc:description>
        <bib:pages>295-300</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the 18th International Conference on Artificial Intelligence and Law, ICAIL 2021</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1777">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 6; Conference name: 18th International Conference on Artificial Intelligence and Law, ICAIL 2021; Conference date: 21 June 2021 through 25 June 2021; Conference code: 170686&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2648">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2648/Rosa et al. - 2021 - To tune or not to tune Zero-shot models for legal case entailment.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/pdf/10.1145/3462757.3466103</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:09:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:03029743%20(ISSN);%20978-303060275-8%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>12335 LNAI</prism:volume>
                <dc:identifier>ISBN 03029743 (ISSN); 978-303060275-8 (ISBN)</dc:identifier>
                <dc:title>Lect. Notes Comput. Sci.</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-60276-5_25</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <foaf:name>Springer Science and Business Media Deutschland GmbH</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kuleshov</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zaytseva</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nenausnikov</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Karpov A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Potapova R.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1778"/>
        <dcterms:isReferencedBy rdf:resource="#item_2688"/>
        <dc:subject>Associative processing</dc:subject>
        <dc:subject>Authentication</dc:subject>
        <dc:subject>Experimental verification</dc:subject>
        <dc:subject>Human-readable</dc:subject>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>Legal documents</dc:subject>
        <dc:subject>Legal tech</dc:subject>
        <dc:subject>Machine learning</dc:subject>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Ontological approach</dc:subject>
        <dc:subject>Ontological representation</dc:subject>
        <dc:subject>Ontology</dc:subject>
        <dc:subject>Russian languages</dc:subject>
        <dc:subject>Semantic graph</dc:subject>
        <dc:subject>Structural elements</dc:subject>
        <dc:title>Legal Tech: Documents’ Validation Method Based on the Associative-Ontological Approach</dc:title>
        <dcterms:abstract>Trend of Legal Tech is actively developing, thus, allowing an automation of various legal tasks solved both by professional lawyers and ordinary (common) users. Because of the legal documents properties, natural language processing (NLP) technologies are widely used in Legal Tech development. One of the tasks in Legal Tech, whose solution is necessary for both professionals and non-professionals, is a validation documents’ texts, including certain checking for the presence of mandatory structural elements in them. This article considers an implementation of a method and an algorithm for validating, for instance, the documents called “Consent to the personal data processing” in the Russian Language legal practice based on machine learning and using an associative-ontological representation of the text. Such validation occurs by checking documents through a set of rules, at that, each rule describes the documents’ structural elements. Associative ontological representation of the text makes such rules human-readable, and simplifies their adjustment and fine-tuning to the changing legislation norms. Results of experimental verification of the proposed algorithm on a set of texts of real legal documents show its effectiveness when applied to Legal Tech systems. © 2020, Springer Nature Switzerland AG.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092902177&amp;doi=10.1007%2f978-3-030-60276-5_25&amp;partnerID=40&amp;md5=1c1d9721e1a5cf9a1811c76387015fda</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Lect. Notes Comput. Sci.</dc:description>
        <bib:pages>244-254</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1778">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 0; Correspondence Address: A. Zaytseva; St.-Petersburg Federal Research Center of the Russian Academy of Sciences, St.-Petersburg, Russian Federation; email: cher@iias.spb.su; Conference name: 22nd International Conference on Speech and Computer, SPECOM 2020; Conference date: 7 October 2020 through 9 October 2020; Conference code: 249919&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2688">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser de acesso pago&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:979-109554634-4%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 979-109554634-4 (ISBN)</dc:identifier>
                <dc:title>Proceedings of the Twelfth Language Resources and Evaluation Conference</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Marseille, France</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>European Language Resources Association (ELRA)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Moreno-Schneider</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rehm</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Montiel-Ponsoda</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rodríguez-Doncel</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Revenko</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Karampatakis</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khvalchik</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sageder</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gracia</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maganza</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Calzolari N.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Bechet F.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Blache P.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Choukri K.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Cieri C.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Declerck T.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Goggi S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Isahara H.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Maegaard B.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Mariani J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                   <foaf:Person><foaf:surname>Mazo H.</foaf:surname></foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Moreno A.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Odijk J.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Piperidis S.</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dcterms:isReferencedBy rdf:resource="#item_1779"/>
        <link:link rdf:resource="#item_2694"/>
        <dc:subject>Applications</dc:subject>
        <dc:subject>Content curation</dc:subject>
        <dc:subject>Innovation projects</dc:subject>
        <dc:subject>Knowledge Discovery/Representation</dc:subject>
        <dc:subject>Knowledge representation</dc:subject>
        <dc:subject>Legal documents</dc:subject>
        <dc:subject>Legal domains</dc:subject>
        <dc:subject>Legal knowledge</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Semantic information</dc:subject>
        <dc:subject>Semantics</dc:subject>
        <dc:subject>Systems</dc:subject>
        <dc:subject>Text Analytics</dc:subject>
        <dc:subject>Tools</dc:subject>
        <dc:subject>Work-flows</dc:subject>
        <dc:title>Orchestrating NLP services for the legal domain</dc:title>
        <dcterms:abstract>Legal technology is currently receiving a lot of attention from various angles. In this contribution we describe the main technical components of a system that is currently under development in the European innovation project Lynx, which includes partners from industry and research. The key contribution of this paper is a workflow manager that enables the flexible orchestration of workflows based on a portfolio of Natural Language Processing and Content Curation services as well as a Multilingual Legal Knowledge Graph that contains semantic information and meaningful references to legal documents. We also describe different use cases with which we experiment and develop prototypical solutions. © European Language Resources Association (ELRA), licensed under CC-BY-NC</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <z:libraryCatalog>ACLWeb</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094515387&amp;partnerID=40&amp;md5=33cbdc2f68cab07c3c4f581ca3d2edf9</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: LREC - Int. Conf. Lang. Resour. Eval., Conf. Proc.</dc:description>
        <bib:pages>2332-2340</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>LREC 2020 - 12th International Conference on Language Resources and Evaluation, Conference Proceedings</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1779">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 16; Correspondence Address: J. Moreno-Schneider; DFKI GmbH, Germany; email: julian.morenoschneider@dfki.de; Conference name: 12th International Conference on Language Resources and Evaluation, LREC 2020; Conference date: 11 May 2020 through 16 May 2020; Conference code: 164155&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2694">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2694/Moreno-Schneider et al. - 2020 - Orchestrating NLP Services for the Legal Domain.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://aclanthology.org/2020.lrec-1.284.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:46:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-145036859-9%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-145036859-9 (ISBN)</dc:identifier>
                <dc:title>Int Conf Inf Knowledge Manage</dc:title>
                <dc:identifier>DOI 10.1145/3340531.3412746</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computing Machinery</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Donnelly</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roegiest</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1780"/>
        <link:link rdf:resource="#item_2658"/>
        <dc:subject>Named entity recognition</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Knowledge management</dc:subject>
        <dc:subject>Legal documents</dc:subject>
        <dc:subject>Authentication</dc:subject>
        <dc:subject>nlp</dc:subject>
        <dc:subject>Sentence level</dc:subject>
        <dc:subject>legal technology</dc:subject>
        <dc:subject>Traditional approaches</dc:subject>
        <dc:subject>Granular informations</dc:subject>
        <dc:subject>Learning architectures</dc:subject>
        <dc:subject>Review process</dc:subject>
        <dc:subject>Target information</dc:subject>
        <dc:title>The Utility of Context When Extracting Entities from Legal Documents</dc:title>
        <dcterms:abstract>When reviewing documents for legal tasks such as Mergers and Acquisitions, granular information (such as start dates and exit clauses) need to be identified and extracted. Inspired by previous work in Named Entity Recognition (NER), we investigate how NER techniques can be leveraged to aid lawyers in this review process. Due to the extremely low prevalence of target information in legal documents, we find that the traditional approach of tagging all sentences in a document is inferior, in both effectiveness and data required to train and predict, to using a first-pass layer to identify sentences that are likely to contain the relevant information and then running the more traditional sentence-level sequence tagging. Moreover, we find that such entity-level models can be improved by training on a balanced sample of relevant and non-relevant sentences. We additionally describe the use of our system in production and how its usage by clients means that deep learning architectures tend to be cost inefficient, especially with respect to the necessary time to train models. © 2020 ACM.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095864117&amp;doi=10.1145%2f3340531.3412746&amp;partnerID=40&amp;md5=4ebe450f0be1b3cdb418d23186ea9bcf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: Int Conf Inf Knowledge Manage</dc:description>
        <bib:pages>2397-2404</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>International Conference on Information and Knowledge Management, Proceedings</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1780">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 6; Conference name: 29th ACM International Conference on Information and Knowledge Management, CIKM 2020; Conference date: 19 October 2020 through 23 October 2020; Conference code: 164320&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2658">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2658/Donnelly e Roegiest - 2020 - The Utility of Context When Extracting Entities from Legal Documents.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/pdf/10.1145/3340531.3412746</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:10:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-950737-92-5">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-950737-92-5</dc:identifier>
                <dc:title>EMNLP-IJCNLP - Conf. Empir. Methods Nat. Lang. Process. Int. Jt. Conf. Nat. Lang. Process., Proc. Syst. Demonstr.</dc:title>
                <dc:identifier>DOI 10.18653/v1/D19-3017</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Hong Kong, China</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Association for Computational Linguistics (ACL)</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Duan</foaf:surname>
                        <foaf:givenName>X.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hu</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1781"/>
        <dcterms:isReferencedBy rdf:resource="#item_1881"/>
        <link:link rdf:resource="#item_2678"/>
        <dc:subject>Attention mechanisms</dc:subject>
        <dc:subject>Document analysis</dc:subject>
        <dc:subject>Integrated systems</dc:subject>
        <dc:subject>Laws and legislation</dc:subject>
        <dc:subject>NAtural language processing</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Text categorization</dc:subject>
        <dc:subject>Text processing</dc:subject>
        <dc:subject>Text representation</dc:subject>
        <dc:title>IFlyLegal: A Chinese legal system for consultation, law searching, and document analysis</dc:title>
        <dcterms:abstract>Legal Tech is developed to help people with legal services and solve legal problems via machines. To achieve this, one of the key requirements for machines is to utilize legal knowledge and comprehend legal context. This can be fulfilled by natural language processing (NLP) techniques, for instance, text representation, text categorization, question answering (QA) and natural language inference, etc. To this end, we introduce a freely available Chinese Legal Tech system (IFlyLegal) that benefits from multiple NLP tasks. It is an integrated system that performs legal consulting, multi-way law searching, and legal document analysis by exploiting techniques such as deep contextual representations and various attention mechanisms. To our knowledge, IFlyLegal is the first Chinese legal system that employs up-to-date NLP techniques and caters for needs of different user groups, such as lawyers, judges, procurators, and clients. Since Jan, 2019, we have gathered 2,349 users and 28,238 page views (till June, 23, 2019). © 2019 Association for Computational Linguistics.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <z:language>English</z:language>
        <z:shortTitle>IFlyLegal</z:shortTitle>
        <z:archive>Scopus</z:archive>
        <dc:coverage>WOS:000855231500017</dc:coverage>
        <z:libraryCatalog>ACLWeb</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081114954&amp;partnerID=40&amp;md5=41a410707fb24cf3089d50408c13dda5</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: EMNLP-IJCNLP - Conf. Empir. Methods Nat. Lang. Process. Int. Jt. Conf. Nat. Lang. Process., Proc. Syst. Demonstr.</dc:description>
        <bib:pages>97-102</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Proceedings of System Demonstrations</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1781">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 9; Conference name: 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019; Conference date: 3 November 2019 through 7 November 2019; Conference code: 160431&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_1881">
        <rdf:value>&lt;p&gt;Times Cited in Web of Science Core Collection:&amp;nbsp;&amp;nbsp;24&lt;br/&gt;Total Times Cited:&amp;nbsp;&amp;nbsp;24&lt;br/&gt;Cited Reference Count:&amp;nbsp;&amp;nbsp;12&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_2678">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/2678/Wang et al. - 2019 - IFlyLegal A Chinese Legal System for Consultation, Law Searching, and Document Analysis.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://aclanthology.org/D19-3017.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-02 20:38:53</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182668818&amp;doi=10.1007%2fs00500-023-09536-4&amp;partnerID=40&amp;md5=1d795064a7af66fb101218e30ea19633">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>28</prism:volume>
                <dc:title>Soft Computing</dc:title>
                <dc:identifier>DOI 10.1007/s00500-023-09536-4</dc:identifier>
                <prism:number>11-12</prism:number>
                <dcterms:alternative>Soft Comput.</dcterms:alternative>
                <dc:identifier>ISSN 14327643 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dan</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Feng</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1583"/>
        <dcterms:isReferencedBy rdf:resource="#item_2289"/>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Intelligent robots</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Large language model</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Image enhancement</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Visual languages</dc:subject>
        <dc:subject>LLM</dc:subject>
        <dc:subject>Visual question answering</dc:subject>
        <dc:subject>Innovative technology</dc:subject>
        <dc:subject>Machine-vision</dc:subject>
        <dc:subject>Robot vision</dc:subject>
        <dc:subject>Text images</dc:subject>
        <dc:subject>Text-image matching</dc:subject>
        <dc:subject>VLM</dc:subject>
        <dc:title>Enhancing machine vision: the impact of a novel innovative technology on video question-answering</dc:title>
        <dcterms:abstract>The robot video question-answering system is an artificial intelligence application that integrates computer vision and natural language processing technologies. Recently, it has received widespread attention, especially with the rapid development of large language models (LLMs). The core technical challenge lies in the application of visual question answering (VQA). However, visual question answering currently faces several challenges. Firstly, the acquisition of human annotations is costly, and secondly, existing models require expensive retraining when replacing a particular module. We propose the VLM2LLM model, which significantly improves the performance of multimodal question-answering tasks by integrating visual-language matching and large-scale language models. Specifically, it overcomes the limitations of requiring massive computational resources for training and inference in previous models. Furthermore, it allows for the upgrading of our LLM version according to the latest research advancements and needs. The results demonstrate that the VLM2LLM model achieves the highest accuracy compared to other state-of-the-art models on three datasets: QAv2, A-OKVQA, and OK-VQA. We hope that the VLM2LLM model can drive advancements in the field of robot video question-answering and provide innovative solutions for a wider range of application domains. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182668818&amp;doi=10.1007%2fs00500-023-09536-4&amp;partnerID=40&amp;md5=1d795064a7af66fb101218e30ea19633</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer Science and Business Media Deutschland GmbH</dc:description>
        <bib:pages>6969-6982</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1583">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 1; Correspondence Address: S. Dan; School of Mathematics and Big Data, Chongqing University of Education, Chongqing, China; email: dansj@cque.edu.cn&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2289">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="urn:isbn:979-840070877-0%20(ISBN)">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 979-840070877-0 (ISBN)</dc:identifier>
                <dc:title>ACM Int. Conf. Proc. Ser.</dc:title>
                <dc:identifier>DOI 10.1145/3634713.3634729</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computing Machinery</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>May</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Biermann</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zerweck</foaf:surname>
                        <foaf:givenName>X.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ludwig</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krüger</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Leich</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1582"/>
        <dcterms:isReferencedBy rdf:resource="#item_2232"/>
        <dc:subject>Security</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>security</dc:subject>
        <dc:subject>Software engineering</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Community question answering</dc:subject>
        <dc:subject>community-question-answering system</dc:subject>
        <dc:subject>Community-question-answering system</dc:subject>
        <dc:subject>configuration</dc:subject>
        <dc:subject>Configuration</dc:subject>
        <dc:subject>Configuration options</dc:subject>
        <dc:subject>Stack overflow</dc:subject>
        <dc:subject>Stack Overflow</dc:subject>
        <dc:subject>Statistics</dc:subject>
        <dc:subject>System vulnerability</dc:subject>
        <dc:subject>Variability</dc:subject>
        <dc:subject>vulnerability management</dc:subject>
        <dc:subject>Vulnerability management</dc:subject>
        <dc:title>Vulnerably (Mis)Configured? Exploring 10 Years of Developers' Q&amp;As on Stack Overflow</dc:title>
        <dcterms:abstract>The increasing number of attacks exploiting system vulnerabilities in recent years underpins the growing importance of security; especially for software comprising configuration options that may cause unintended vulnerabilities. So, not surprisingly, developers discuss secure software configurations extensively, for instance, via community-question-answering systems like Stack Overflow. In this exploratory study, we analyzed 651 Stack Overflow posts from 2013 until 2022 to investigate what vulnerabilities in the context of configuring software developers discuss. We employed a manual data analysis and automated topic modeling using Latent Dirichlet Allocation to identify and classify relevant topics and contexts. Our results show that vulnerabilities in the context of configuring receive more and more interest, with most posts discussing issues related to faulty security configurations and dependencies causing vulnerabilities that could be or have actually been exploited. Overall, we contribute insights into configuration and security issues that developers experience in the real world. Such insights help researchers and practitioners understand and resolve these issues, thereby guiding future improvements.  © 2024 ACM.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184284565&amp;doi=10.1145%2f3634713.3634729&amp;partnerID=40&amp;md5=1665d8ec1d3e327b8a67889f4f0c55b5</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Journal Abbreviation: ACM Int. Conf. Proc. Ser.</dc:description>
        <bib:pages>112-122</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>ACM International Conference Proceeding Series</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_1582">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 2; Conference name: 18th International Working Conference on Variability Modelling of Software-Intensive Systems, VaMoS 2024; Conference date: 7 February 2024 through 9 February 2024; Conference code: 196687&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2232">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197849548&amp;doi=10.11591%2fijeecs.v35.i3.pp1751-1764&amp;partnerID=40&amp;md5=9bad82202a24650da58e58ce4958cf1e">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:25024752%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bouhlali</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Elmansouri</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>El Mhouti</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fahim</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Boudaa</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1584"/>
        <dcterms:isReferencedBy rdf:resource="#item_2368"/>
        <dc:subject>Deep learning</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>AIML</dc:subject>
        <dc:subject>Arabic chatbot</dc:subject>
        <dc:subject>Arabic NLP</dc:subject>
        <dc:subject>Pattern matching</dc:subject>
        <dc:title>Reviewing approaches employed in Arabic chatbots</dc:title>
        <dcterms:abstract>The field of chatbots has witnessed a remarkable evolution in recent years, marked by a transition from simplistic rule-based structures to sophisticated systems employing advanced natural language processing (NLP) techniques. While most languages benefit from NLP support, the majority of chatbot research and development has been conducted in English, leaving a notable scarcity of comparable works in Arabic. This scarcity is attributed to the myriad challenges posed by the linguistically intricate nature of Arabic, encompassing orthographic variations and diverse dialects. This study systematically reviews articles that represent implementations of Arabic chatbots, revealing a discernible shift from rule-based frameworks to the predominant adoption of machine learning (ML) and deep learning (DL) methods. The results highlight the dynamic trajectory of chatbot technology, with a notable emphasis on the pivotal role of DL, as evidenced by a significant peak in 2023. Looking forward, the study anticipates a more sophisticated future for chatbot development, driven by ongoing advancements in artificial intelligence (AI) and NLP, offering valuable insights into the current state of Arabic chatbot research and laying the foundation for continued exploration in this evolving and dynamic field. © 2024 Institute of Advanced Engineering and Science. All rights reserved.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197849548&amp;doi=10.11591%2fijeecs.v35.i3.pp1751-1764&amp;partnerID=40&amp;md5=9bad82202a24650da58e58ce4958cf1e</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Institute of Advanced Engineering and Science</dc:description>
        <bib:pages>1751-1764</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:25024752%20(ISSN)">
        <prism:volume>35</prism:volume>
        <dc:title>Indonesian Journal of Electrical Engineering and Computer Science</dc:title>
        <dc:identifier>DOI 10.11591/ijeecs.v35.i3.pp1751-1764</dc:identifier>
        <prism:number>3</prism:number>
        <dcterms:alternative>Indones. J. Electrical Eng. Comput. Sci.</dcterms:alternative>
        <dc:identifier>ISSN 25024752 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1584">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 1; Correspondence Address: A. Bouhlali; Laboratoy of Information Security Intelligent Systems and application (ISISA), Faculty of Sciences, Abdelmalek Essaadi University, Tetouan, 93030, Morocco; email: abdelmounaim.bouhlali@etu.uae.ac.ma&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2368">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189943350&amp;doi=10.14569%2fIJACSA.2024.0150379&amp;partnerID=40&amp;md5=b3c38533a1a335a606e07d42a7489644">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>15</prism:volume>
                <dc:title>International Journal of Advanced Computer Science and Applications</dc:title>
                <dc:identifier>DOI 10.14569/IJACSA.2024.0150379</dc:identifier>
                <prism:number>3</prism:number>
                <dcterms:alternative>Intl. J. Adv.  Comput. Sci. Appl.</dcterms:alternative>
                <dc:identifier>ISSN 2158107X (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Muludi</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fitria</foaf:surname>
                        <foaf:givenName>K.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Triloka</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1585"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>GPT</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Search engines</dc:subject>
        <dc:subject>Computational linguistics</dc:subject>
        <dc:subject>Language model</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Data handling</dc:subject>
        <dc:subject>Large language model</dc:subject>
        <dc:subject>Question Answering</dc:subject>
        <dc:subject>Natural Language Processing</dc:subject>
        <dc:subject>Knowledge representation</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>Statistical tests</dc:subject>
        <dc:subject>Competition</dc:subject>
        <dc:subject>Generation method</dc:subject>
        <dc:subject>Large Language Model</dc:subject>
        <dc:subject>Retrieval augmented generation</dc:subject>
        <dc:subject>Retrieval Augmented Generation</dc:subject>
        <dc:title>Retrieval-Augmented Generation Approach: Document Question Answering using Large Language Model</dc:title>
        <dcterms:abstract>This study introduces the Retrieval Augmented Generation (RAG) method to improve Question-Answering (QA) systems by addressing document processing in Natural Language Processing problems. It represents the latest breakthrough in applying RAG to document question and answer applications, overcoming previous QA system obstacles. RAG combines search techniques in vector store and text generation mechanism developed by Large Language Models, offering a time-efficient alternative to manual reading limitations. The research evaluates RAG's that use Generative Pre-trained Transformer 3.5 or GPT-3.5-turbo from the ChatGPT model and its impact on document data processing, comparing it with other applications. This research also provides datasets to test the capabilities of the QA document system. The proposed dataset and Stanford Question Answering Dataset (SQuAD) are used for performance testing. The study contributes theoretically by advancing methodologies and knowledge representation, supporting benchmarking in research communities. Results highlight RAG's superiority: achieving a precision of 0.74 in Recall-Oriented Understudy for Gisting Evaluation (ROUGE) testing, outperforming others at 0.5; obtaining an F1 score of 0.88 in BERTScore, surpassing other QA apps at 0.81; attaining a precision of 0.28 in Bilingual Evaluation Understudy (BLEU) testing, surpassing others with a precision of 0.09; and scoring 0.33 in Jaccard Similarity, outshining others at 0.04. These findings underscore RAG's efficiency and competitiveness, promising a positive impact on various industrial sectors through advanced Artificial Intelligence (AI) technology. © (2024), (Science and Information Organization). All Rights Reserved.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189943350&amp;doi=10.14569%2fIJACSA.2024.0150379&amp;partnerID=40&amp;md5=b3c38533a1a335a606e07d42a7489644</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Science and Information Organization</dc:description>
        <bib:pages>776-785</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1585">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 2&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189747049&amp;doi=10.1093%2fjamia%2focae015&amp;partnerID=40&amp;md5=e04fc88e6b58a4aa402e8555dc031061">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>31</prism:volume>
                <dc:title>Journal of the American Medical Informatics Association</dc:title>
                <dc:identifier>DOI 10.1093/jamia/ocae015</dc:identifier>
                <prism:number>4</prism:number>
                <dcterms:alternative>J. Am. Med. Informatics Assoc.</dcterms:alternative>
                <dc:identifier>ISSN 10675027 (ISSN)</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kell</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roberts</foaf:surname>
                        <foaf:givenName>A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Umansky</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qian</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ferrari</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Soboczenski</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wallace</foaf:surname>
                        <foaf:givenName>B.C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Patel</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Marshall</foaf:surname>
                        <foaf:givenName>I.J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1587"/>
        <dcterms:isReferencedBy rdf:resource="#item_2354"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>Machine Learning</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>Humans</dc:subject>
        <dc:subject>review</dc:subject>
        <dc:subject>systematic review</dc:subject>
        <dc:subject>health care personnel</dc:subject>
        <dc:subject>clinical decision support system</dc:subject>
        <dc:subject>Medline</dc:subject>
        <dc:subject>PubMed</dc:subject>
        <dc:subject>evidence based medicine</dc:subject>
        <dc:subject>Health Personnel</dc:subject>
        <dc:subject>medical informatics</dc:subject>
        <dc:subject>medical information system</dc:subject>
        <dc:subject>reproducibility</dc:subject>
        <dc:subject>Reproducibility of Results</dc:subject>
        <dc:subject>question answering</dc:subject>
        <dc:subject>reliability</dc:subject>
        <dc:subject>clinical decision support</dc:subject>
        <dc:subject>conference paper</dc:subject>
        <dc:subject>evidence-based medicine</dc:subject>
        <dc:subject>health practitioner</dc:subject>
        <dc:subject>Point-of-Care Systems</dc:subject>
        <dc:subject>research priority</dc:subject>
        <dc:subject>specialization</dc:subject>
        <dc:title>Question answering systems for health professionals at the point of care - a systematic review</dc:title>
        <dcterms:abstract>Objectives: Question answering (QA) systems have the potential to improve the quality of clinical care by providing health professionals with the latest and most relevant evidence. However, QA systems have not been widely adopted. This systematic review aims to characterize current medical QA systems, assess their suitability for healthcare, and identify areas of improvement. Materials and methods: We searched PubMed, IEEE Xplore, ACM Digital Library, ACL Anthology, and forward and backward citations on February 7, 2023. We included peer-reviewed journal and conference papers describing the design and evaluation of biomedical QA systems. Two reviewers screened titles, abstracts, and full-text articles. We conducted a narrative synthesis and risk of bias assessment for each study. We assessed the utility of biomedical QA systems. Results: We included 79 studies and identified themes, including question realism, answer reliability, answer utility, clinical specialism, systems, usability, and evaluation methods. Clinicians' questions used to train and evaluate QA systems were restricted to certain sources, types and complexity levels. No system communicated confidence levels in the answers or sources. Many studies suffered from high risks of bias and applicability concerns. Only 8 studies completely satisfied any criterion for clinical utility, and only 7 reported user evaluations. Most systems were built with limited input from clinicians. Discussion: While machine learning methods have led to increased accuracy, most studies imperfectly reflected real-world healthcare information needs. Key research priorities include developing more realistic healthcare QA datasets and considering the reliability of answer sources, rather than merely focusing on accuracy.  © 2024 The Author(s). Published by Oxford University Press on behalf of the American Medical Informatics Association.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189747049&amp;doi=10.1093%2fjamia%2focae015&amp;partnerID=40&amp;md5=e04fc88e6b58a4aa402e8555dc031061</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Oxford University Press</dc:description>
        <bib:pages>1009-1024</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_1587">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 2; Correspondence Address: G. Kell; Department of Population Health Sciences, King's College London, London, Addison House, Guy's Campus, Greater London, SE1 1UL, United Kingdom; email: gregory.kell@kcl.ac.uk; CODEN: JAMAF&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2354">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192194727&amp;doi=10.1007%2fs44163-024-00126-3&amp;partnerID=40&amp;md5=bfd9ba44246d420e414d74efadf63495">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:27310809%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Emmert-Streib</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1586"/>
        <dc:subject>Natural language processing</dc:subject>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>AI systems</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Conversational AI</dc:subject>
        <dc:subject>Data science</dc:subject>
        <dc:subject>Question answering systems</dc:subject>
        <dc:subject>ITS applications</dc:subject>
        <dc:subject>Question-answering system</dc:subject>
        <dc:subject>Artificial general intelligence</dc:subject>
        <dc:subject>Artificial general intelligences</dc:subject>
        <dc:subject>Current situation</dc:subject>
        <dc:subject>Electric grounding</dc:subject>
        <dc:subject>Human level intelligence</dc:subject>
        <dc:title>Is ChatGPT the way toward artificial general intelligence</dc:title>
        <dcterms:abstract>The success of the conversational AI system ChatGPT has triggered an avalanche of studies that explore its applications in research and education. There are also high hopes that, in addition to such particular usages, it could lead to artificial general intelligence (AGI) that means to human-level intelligence. Such aspirations, however, need to be grounded by actual scientific means to ensure faithful statements and evaluations of the current situation. The purpose of this article is to put ChatGPT into perspective and to outline a way forward that might instead lead to an artificial special intelligence (ASI), a notion we introduce. The underlying idea of ASI is based on an environment that consists only of text. We will show that this avoids the problem of embodiment of an agent and leads to a system with restricted capabilities compared to AGI. Furthermore, we discuss gated actions as a means of large language models to moderate ethical concerns. © The Author(s) 2024.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192194727&amp;doi=10.1007%2fs44163-024-00126-3&amp;partnerID=40&amp;md5=bfd9ba44246d420e414d74efadf63495</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Springer Nature</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:27310809%20(ISSN)">
        <prism:volume>4</prism:volume>
        <dc:title>Discover Artificial Intelligence</dc:title>
        <dc:identifier>DOI 10.1007/s44163-024-00126-3</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Discov. Artif. Intell.</dcterms:alternative>
        <dc:identifier>ISSN 27310809 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1586">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 2; Correspondence Address: F. Emmert-Streib; Predictive Society and Data Analytics Lab Department, Tampere University, Tampere, 33720, Finland; email: frank.emmert-streib@tuni.fi&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183372327&amp;doi=10.3390%2fpr12010058&amp;partnerID=40&amp;md5=2314937759e1e24bdb618fe331e0b75e">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:22279717%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gao</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Q.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1588"/>
        <dcterms:isReferencedBy rdf:resource="#item_2308"/>
        <dc:subject>Artificial intelligence</dc:subject>
        <dc:subject>Natural language processing systems</dc:subject>
        <dc:subject>Search engines</dc:subject>
        <dc:subject>Language processing</dc:subject>
        <dc:subject>Natural languages</dc:subject>
        <dc:subject>Information retrieval</dc:subject>
        <dc:subject>Intelligent question answering systems</dc:subject>
        <dc:subject>Power</dc:subject>
        <dc:subject>Electric power systems</dc:subject>
        <dc:subject>Energy policy</dc:subject>
        <dc:subject>improved BERTserini algorithm</dc:subject>
        <dc:subject>Improved bertserini algorithm</dc:subject>
        <dc:subject>information retrieval</dc:subject>
        <dc:subject>intelligent question-answering system</dc:subject>
        <dc:subject>Key technologies</dc:subject>
        <dc:subject>Poor performance</dc:subject>
        <dc:subject>Question answering technology</dc:subject>
        <dc:subject>rules and regulations</dc:subject>
        <dc:subject>Rules and regulations</dc:subject>
        <dc:subject>Wages</dc:subject>
        <dc:title>Key Technologies of Intelligent Question-Answering System for Power System Rules and Regulations Based on Improved BERTserini Algorithm</dc:title>
        <dcterms:abstract>With the continuous breakthrough of natural language processing, the application of intelligent question-answering technology in electric power systems has attracted wide attention. However, at present, the traditional question-answering system has poor performance and is difficult to apply in engineering practice. This paper proposes an improved BERTserini algorithm for the intelligent answering of electric power regulations based on a BERT model. The proposed algorithm is implemented in two stages. The first stage is the text-segmentation stage, where a multi-document long text preprocessing technique is utilized that accommodates the rules and regulations text, and then Anserini is used to extract paragraphs with high relevance to the given question. The second stage is the answer-generation and source-retrieval stage, where a two-step fine-tuning based on the Chinese BERT model is applied to generate precise answers based on given questions, while the information regarding documents, chapters, and page numbers of these answers are also output simultaneously. The algorithm proposed in this paper eliminates the necessity for the manual organization of professional question–answer pairs, thereby effectively reducing the manual labor cost compared to traditional question-answering systems. Additionally, this algorithm exhibits a higher degree of exact match rate and a faster response time for providing answers. © 2023 by the authors.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183372327&amp;doi=10.3390%2fpr12010058&amp;partnerID=40&amp;md5=2314937759e1e24bdb618fe331e0b75e</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Multidisciplinary Digital Publishing Institute (MDPI)</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:22279717%20(ISSN)">
        <prism:volume>12</prism:volume>
        <dc:title>Processes</dc:title>
        <dc:identifier>DOI 10.3390/pr12010058</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Process.</dcterms:alternative>
        <dc:identifier>ISSN 22279717 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1588">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 2; Correspondence Address: T. Ji; College of Electric Power Engineering, South China University of Technology, Guangzhou, 510640, China; email: tyji@scut.edu.cn&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2308">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162027719&amp;doi=10.3389%2ffeduc.2023.1183162&amp;partnerID=40&amp;md5=8d14b971e2143e5afa9c432ade0dc94a">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2504284X%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1671"/>
        <dcterms:isReferencedBy rdf:resource="#item_2295"/>
        <dc:subject>generative artificial intelligence</dc:subject>
        <dc:subject>countermeasure research</dc:subject>
        <dc:subject>current status</dc:subject>
        <dc:subject>development prospects</dc:subject>
        <dc:subject>educational applications</dc:subject>
        <dc:title>Generative artificial intelligence empowers educational reform: current status, issues, and prospects</dc:title>
        <dcterms:abstract>The emergence of Chat GPT has once again sparked a wave of information revolution in generative artificial intelligence. This article provides a detailed overview of the development and technical support of generative artificial intelligence. It conducts an in-depth analysis of the current application of generative artificial intelligence in the field of education, and identifies problems in four aspects: opacity and unexplainability, data privacy and security, personalization and fairness, and effectiveness and reliability. Corresponding solutions are proposed, such as developing explainable and fair algorithms, upgrading encryption technology, and formulating relevant laws and regulations to protect data, as well as improving the quality and quantity of datasets. The article also looks ahead to the future development trends of generative artificial intelligence in education from four perspectives: personalized education, intelligent teaching, collaborative education, and virtual teaching. The aim of the study is to provide important reference value for research and practice in this field. Copyright © 2023 Yu and Guo.</dcterms:abstract>
        <dc:date>2023</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162027719&amp;doi=10.3389%2ffeduc.2023.1183162&amp;partnerID=40&amp;md5=8d14b971e2143e5afa9c432ade0dc94a</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: Frontiers Media S.A.</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2504284X%20(ISSN)">
        <prism:volume>8</prism:volume>
        <dc:title>Frontiers in Education</dc:title>
        <dc:identifier>DOI 10.3389/feduc.2023.1183162</dc:identifier>
        <dcterms:alternative>Front. Educ.</dcterms:alternative>
        <dc:identifier>ISSN 2504284X (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1671">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 77; Correspondence Address: H. Yu; Faculty of Education, Shaanxi Normal University (SNNU), Xi’an, Shaanxi, China; email: yh13213986381@163.com&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2295">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187472990&amp;doi=10.1001%2fjamanetworkopen.2024.0357&amp;partnerID=40&amp;md5=f69bdc78efc778e78319f1f817bc41fb">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:25743805%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zaretsky</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Min Kim</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Baskharoun</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Austrian</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Aphinyanaphongs</foaf:surname>
                        <foaf:givenName>Y.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gupta</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Blecker</foaf:surname>
                        <foaf:givenName>S.B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Feldman</foaf:surname>
                        <foaf:givenName>J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1664"/>
        <dcterms:isReferencedBy rdf:resource="#item_2296"/>
        <dc:subject>artificial intelligence</dc:subject>
        <dc:subject>Artificial Intelligence</dc:subject>
        <dc:subject>human</dc:subject>
        <dc:subject>Humans</dc:subject>
        <dc:subject>adult</dc:subject>
        <dc:subject>Adult</dc:subject>
        <dc:subject>female</dc:subject>
        <dc:subject>Female</dc:subject>
        <dc:subject>middle aged</dc:subject>
        <dc:subject>Middle Aged</dc:subject>
        <dc:subject>electronic health record</dc:subject>
        <dc:subject>Electronic Health Records</dc:subject>
        <dc:subject>male</dc:subject>
        <dc:subject>Male</dc:subject>
        <dc:subject>language</dc:subject>
        <dc:subject>Language</dc:subject>
        <dc:subject>United States</dc:subject>
        <dc:subject>hospital discharge</dc:subject>
        <dc:subject>aged</dc:subject>
        <dc:subject>Aged</dc:subject>
        <dc:subject>Cross-Sectional Studies</dc:subject>
        <dc:subject>cross-sectional study</dc:subject>
        <dc:subject>hospital patient</dc:subject>
        <dc:subject>Inpatients</dc:subject>
        <dc:subject>Patient Discharge</dc:subject>
        <dc:title>Generative Artificial Intelligence to Transform Inpatient Discharge Summaries to Patient-Friendly Language and Format</dc:title>
        <dcterms:abstract>IMPORTANCE By law, patients have immediate access to discharge notes in their medical records. Technical language and abbreviations make notes difficult to read and understand for a typical patient. Large language models (LLMs [eg, GPT-4]) have the potential to transform these notes into patient-friendly language and format. OBJECTIVE To determine whether an LLM can transform discharge summaries into a format that is more readable and understandable. DESIGN, SETTING, AND PARTICIPANTS This cross-sectional study evaluated a sample of the discharge summaries of adult patients discharged from the General Internal Medicine service at NYU (New York University) Langone Health from June 1 to 30, 2023. Patients discharged as deceased were excluded. All discharge summaries were processed by the LLM between July 26 and August 5, 2023. INTERVENTIONS A secure Health Insurance Portability and Accountability Act–compliant platform, Microsoft Azure OpenAI, was used to transform these discharge summaries into a patient-friendly format between July 26 and August 5, 2023. MAIN OUTCOMES AND MEASURES Outcomes included readability as measured by Flesch-Kincaid Grade Level and understandability using Patient Education Materials Assessment Tool (PEMAT) scores. Readability and understandability of the original discharge summaries were compared with the transformed, patient-friendly discharge summaries created through the LLM. As balancing metrics, accuracy and completeness of the patient-friendly version were measured. RESULTS Discharge summaries of 50 patients (31 female [62.0%] and 19 male [38.0%]) were included. The median patient age was 65.5 (IQR, 59.0-77.5) years. Mean (SD) Flesch-Kincaid Grade Level was significantly lower in the patient-friendly discharge summaries (6.2 [0.5] vs 11.0 [1.5]; P &lt; .001). PEMAT understandability scores were significantly higher for patient-friendly discharge summaries (81% vs 13%; P &lt; .001). Two physicians reviewed each patient-friendly discharge summary for accuracy on a 6-point scale, with 54 of 100 reviews (54.0%) giving the best possible rating of 6. Summaries were rated entirely complete in 56 reviews (56.0%). Eighteen reviews noted safety concerns, mostly involving omissions, but also several inaccurate statements (termed hallucinations). CONCLUSIONS AND RELEVANCE The findings of this cross-sectional study of 50 discharge summaries suggest that LLMs can be used to translate discharge summaries into patient-friendly language and formats that are significantly more readable and understandable than discharge summaries as they appear in electronic health records. However, implementation will require improvements in accuracy, completeness, and safety. Given the safety concerns, initial implementation will require physician review. © 2024 American Medical Association. All rights reserved.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187472990&amp;doi=10.1001%2fjamanetworkopen.2024.0357&amp;partnerID=40&amp;md5=f69bdc78efc778e78319f1f817bc41fb</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: American Medical Association</dc:description>
        <bib:pages>E240357</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:25743805%20(ISSN)">
        <dc:title>JAMA Network Open</dc:title>
        <dc:identifier>DOI 10.1001/jamanetworkopen.2024.0357</dc:identifier>
        <dcterms:alternative>JAMA Netw. Open</dcterms:alternative>
        <dc:identifier>ISSN 25743805 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1664">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 25; Correspondence Address: J. Zaretsky; NYU Langone Health, New York, 550 First Ave, 10016, United States; email: jonah.zaretsky@nyulangone.org&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2296">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Não se enquadra no âmbito do tema 2: tecnologias e abordagens&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140488281&amp;doi=10.3390%2fapp122010200&amp;partnerID=40&amp;md5=9f70b7efa1f63e03c9ee7961316edb7b">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:20763417%20(ISSN)"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alcántara Francia</foaf:surname>
                        <foaf:givenName>O.A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nunez-del-Prado</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alatrista-Salas</foaf:surname>
                        <foaf:givenName>H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_1913"/>
        <dcterms:isReferencedBy rdf:resource="#item_2229"/>
        <dc:subject>natural language processing</dc:subject>
        <dc:subject>machine learning</dc:subject>
        <dc:subject>deep learning</dc:subject>
        <dc:subject>legal tech</dc:subject>
        <dc:subject>judicial prediction</dc:subject>
        <dc:subject>legal prediction</dc:subject>
        <dc:title>Survey of Text Mining Techniques Applied to Judicial Decisions Prediction</dc:title>
        <dcterms:abstract>This paper reviews the most recent literature on experiments with different Machine Learning, Deep Learning and Natural Language Processing techniques applied to predict judicial and administrative decisions. Among the most outstanding findings, we have that the most used data mining techniques are Support Vector Machine (SVM), K Nearest Neighbours (K-NN) and Random Forest (RF), and in terms of the most used deep learning techniques, we found Long-Term Memory (LSTM) and transformers such as BERT. An important finding in the papers reviewed was that the use of machine learning techniques has prevailed over those of deep learning. Regarding the place of origin of the research carried out, we found that 64% of the works belong to studies carried out in English-speaking countries, 8% in Portuguese and 28% in other languages (such as German, Chinese, Turkish, Spanish, etc.). Very few works of this type have been carried out in Spanish-speaking countries. The classification criteria of the works have been based, on the one hand, on the identification of the classifiers used to predict situations (or events with legal interference) or judicial decisions and, on the other hand, on the application of classifiers to the phenomena regulated by the different branches of law: criminal, constitutional, human rights, administrative, intellectual property, family law, tax law and others. The corpus size analyzed in the reviewed works reached 100,000 documents in 2020. Finally, another important finding lies in the accuracy of these predictive techniques, reaching predictions of over 60% in different branches of law. © 2022 by the authors.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>English</z:language>
        <z:archive>Scopus</z:archive>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140488281&amp;doi=10.3390%2fapp122010200&amp;partnerID=40&amp;md5=9f70b7efa1f63e03c9ee7961316edb7b</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>Publisher: MDPI</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:20763417%20(ISSN)">
        <prism:volume>12</prism:volume>
        <dc:title>Applied Sciences (Switzerland)</dc:title>
        <dc:identifier>DOI 10.3390/app122010200</dc:identifier>
        <prism:number>20</prism:number>
        <dcterms:alternative>Appl. Sci.</dcterms:alternative>
        <dc:identifier>ISSN 20763417 (ISSN)</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_1913">
        <rdf:value>&lt;p&gt;Export Date: 18 December 2024; Cited By: 13; Correspondence Address: O.A. Alcántara Francia; Faculty of Law, Universidad de Lima, Lima, 15023, Peru; email: oalcanta@ulima.edu.pe; H. Alatrista-Salas; Escuela de Posgrado Newman, Tacna, 23001, Peru; email: halatrista@pucp.edu.pe&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_2229">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;p&gt;Rejeitado por ser um inquérito&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Collection rdf:about="#collection_18">
        <dc:title>Rejeitados_Q2</dc:title>
        <dcterms:hasPart rdf:resource="urn:isbn:978-157735835-0%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:22128271%20(ISSN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076355086&amp;doi=10.7755%2fMFR.81.1.1&amp;partnerID=40&amp;md5=f0013419e7bf2aec8bcf52771a5664e9"/>
        <dcterms:hasPart rdf:resource="urn:isbn:03029743%20(ISSN);%20978-303032380-6%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183596144&amp;doi=10.1016%2fj.eswa.2023.122666&amp;partnerID=40&amp;md5=69e705fa3ff7e537e478ae12ba9e96d6"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185390721&amp;doi=10.1007%2fs10916-024-02045-3&amp;partnerID=40&amp;md5=73be66b342b8dd2c6648e3b7efefb9c9"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187454083&amp;partnerID=40&amp;md5=c8d81687d4e80f7949f44e46271a2a0f"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153368140&amp;doi=10.1109%2fTNNLS.2022.3227717&amp;partnerID=40&amp;md5=324ae9b15179fd9baa63c3cc4a1146da"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178660816&amp;doi=10.1016%2fj.neucom.2023.127063&amp;partnerID=40&amp;md5=ced44c7c3eb82b767e856bdac9110241"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183173863&amp;doi=10.1016%2fj.cmpb.2024.108013&amp;partnerID=40&amp;md5=1e0ec823297dd19c5125c234f2ec16a4"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184808429&amp;doi=10.1109%2fACCESS.2024.3363469&amp;partnerID=40&amp;md5=2e93755707d1b8ec5af756a0d22ad2b7"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189075847&amp;doi=10.1016%2fj.jjimei.2024.100232&amp;partnerID=40&amp;md5=09bc7bfe6af34a1a69f57acaef237b53"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182288843&amp;doi=10.2196%2f48996&amp;partnerID=40&amp;md5=bcefbc7b4792530573f06a8868a90229"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162121706&amp;doi=10.1016%2fj.inffus.2023.101861&amp;partnerID=40&amp;md5=50aafcd73742a897267f9e0ea3796df5"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148992575&amp;doi=10.2196%2f45312&amp;partnerID=40&amp;md5=a70a9b11b70f9182a6b1a9ed24dc5122"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146493821&amp;doi=10.1145%2f3530811&amp;partnerID=40&amp;md5=7656b9c032dfd94cccd4de8d7714907a"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125362171&amp;doi=10.1109%2fTPAMI.2022.3152247&amp;partnerID=40&amp;md5=e6f58af2558fe73908eed19f4a759bff"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139426381&amp;doi=10.1109%2fTPAMI.2022.3206148&amp;partnerID=40&amp;md5=390425173748f3cd12ddf88f7daabf2b"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160864044&amp;doi=10.1016%2fj.tbench.2023.100105&amp;partnerID=40&amp;md5=7ea96df397ecb98fe442739d9c87827e"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164010801&amp;partnerID=40&amp;md5=05bb5fc2067af827d1772808b404290b"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138449494&amp;doi=10.1109%2fTPAMI.2021.3095381&amp;partnerID=40&amp;md5=8fd0be18cacc69f9dde8b1485e306e22"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143257592&amp;partnerID=40&amp;md5=ef2617c31531791ec03b2a1817a283ed"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105766084&amp;doi=10.1145%2f3440755&amp;partnerID=40&amp;md5=061fb1f65175c86430f220f9f26956e8"/>
        <dcterms:hasPart rdf:resource="urn:isbn:26403498%20(ISSN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146648064&amp;doi=10.1145%2f3505244&amp;partnerID=40&amp;md5=c47712d45818aab5311b5c5e74fe07ed"/>
        <dcterms:hasPart rdf:resource="urn:isbn:03029743%20(ISSN);%20978-303108998-5%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112567461&amp;doi=10.1016%2fj.neucom.2021.05.103&amp;partnerID=40&amp;md5=3fb8d718445a6b4b81331f75cc483edb"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-166540915-5%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125517450&amp;doi=10.1038%2fs41587-022-01226-0&amp;partnerID=40&amp;md5=df48de3642b26da7f97d37f2b290d129"/>
        <dcterms:hasPart rdf:resource="urn:isbn:03029743%20(ISSN);%20978-303120085-4%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-195408546-6%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-195408552-7%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100710769&amp;doi=10.1007%2fs10462-021-09958-2&amp;partnerID=40&amp;md5=05ab3c384057f2825868bae6e14e843b"/>
        <dcterms:hasPart rdf:resource="urn:isbn:10495258%20(ISSN);%20978-171384539-3%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:02705257%20(ISSN);%20978-073811319-7%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-195591709-4%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:15505499%20(ISSN);%20978-166542812-5%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118623180&amp;doi=10.1109%2fTASLP.2021.3124365&amp;partnerID=40&amp;md5=e16f106b076431ca01cef0afc415db10"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104757563&amp;doi=10.1088%2f2632-2153%2fabc81d&amp;partnerID=40&amp;md5=bf47e8de0b59a22eba66365e9fa69ee3"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103692698&amp;doi=10.1016%2fj.csbj.2021.03.022&amp;partnerID=40&amp;md5=0c846d5dfe80765b2cc01f4a16474c5a"/>
        <dcterms:hasPart rdf:resource="urn:isbn:10495258%20(ISSN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080840963&amp;doi=10.1093%2fbioinformatics%2fbtz682&amp;partnerID=40&amp;md5=b8f287a6d1ca7da47309e2d4303e74e4"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-195214890-3%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-171382112-0%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:0736587X%20(ISSN);%20978-195214825-5%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150638039&amp;partnerID=40&amp;md5=f52cb8e8b59b88a0fb25f5e00ea3257e"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-195214862-0%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098839172&amp;doi=10.1162%2ftacl_a_00349&amp;partnerID=40&amp;md5=c14ea3cd386b168b85cd64948cf50d91"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-195073790-1%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-151088698-8%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086462845&amp;partnerID=40&amp;md5=1e2793f91021f33cad78362b32f5473d"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-195073713-0%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076566125&amp;partnerID=40&amp;md5=12ae41add678ed22ea1b3ca38ee405e8"/>
        <dcterms:hasPart rdf:resource="urn:isbn:10636919%20(ISSN);%20978-172813293-8%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-166542418-9%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073146579&amp;doi=10.1016%2fj.ijmedinf.2019.103985&amp;partnerID=40&amp;md5=b7f6466cb659b75130a1d7793d9edc82"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077809664&amp;doi=10.1109%2fACCESS.2019.2946594&amp;partnerID=40&amp;md5=64adef5d0222ffeadb825a2e0e85e4c1"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083663490&amp;partnerID=40&amp;md5=04b8908c5bdcbdaf988d79880ac86228"/>
        <dcterms:hasPart rdf:resource="urn:isbn:0736587X%20(ISSN);%20978-194808740-7%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-194808784-1%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081718759&amp;partnerID=40&amp;md5=ea3bf3752444cd102be25ee169659f72"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-194808771-1%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122024415&amp;partnerID=40&amp;md5=789d5736c80e0f9fa234e482c49cb792"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-153862156-1%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-194808781-0%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035244312&amp;doi=10.3390%2fen10030406&amp;partnerID=40&amp;md5=49d67eeb26ed1e574548abb148a2e90e"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-150905963-8%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198605397&amp;doi=10.1016%2fj.engappai.2024.108923&amp;partnerID=40&amp;md5=b132bc6cb195187c9248d199965cd04e"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188586513&amp;doi=10.1145%2f3657631&amp;partnerID=40&amp;md5=2968a96946d197508eadf753fe4c63c4"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185550710&amp;doi=10.1016%2fj.resconrec.2024.107497&amp;partnerID=40&amp;md5=dfafd31e61cbb849ff97cf0d5062e5bd"/>
        <dcterms:hasPart rdf:resource="urn:isbn:03029743%20(ISSN);%20978-303143995-7%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177494424&amp;doi=10.1088%2f1674-1056%2fad04cb&amp;partnerID=40&amp;md5=930a136c8de667c0140d4e04f64ab442"/>
        <dcterms:hasPart rdf:resource="urn:isbn:979-840070103-0%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159759354&amp;doi=10.1093%2fjamia%2focad050&amp;partnerID=40&amp;md5=761fc87a07c33adb556ba2e0e1558e64"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164667350&amp;doi=10.1016%2fj.artmed.2023.102611&amp;partnerID=40&amp;md5=971f690ffb8a71e90042cda7ba4857bf"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146170825&amp;doi=10.1002%2fwidm.1487&amp;partnerID=40&amp;md5=ad7a136881a906ae6cfcd1d877d6f840"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164418739&amp;doi=10.13998%2fj.cnki.issn1002-1248.23-0118&amp;partnerID=40&amp;md5=5feb8e8e930ad21cb423e0e646ce6b87"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148382067&amp;doi=10.1007%2fs11277-023-10199-5&amp;partnerID=40&amp;md5=ab8b6ca00b611ebff45326a8feaf894f"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153774131&amp;doi=10.3390%2fe25040639&amp;partnerID=40&amp;md5=5f3e5a6c7738a5f6c2a8437e32058469"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164381228&amp;doi=10.1109%2fACCESS.2023.3291592&amp;partnerID=40&amp;md5=ab749216c52dc9c0fefae829a07d7b0c"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123911965&amp;doi=10.1145%2f3468889&amp;partnerID=40&amp;md5=798fbcc6fbe7f19ada6178603ac81fc7"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-166548425-1%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131130897&amp;doi=10.1186%2fs12859-022-04751-6&amp;partnerID=40&amp;md5=cbee6ebe9ea6cb0d4c2d0aa62406a7dd"/>
        <dcterms:hasPart rdf:resource="urn:isbn:03029743%20(ISSN);%20978-303099738-0%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137758445&amp;doi=10.1007%2fs10115-022-01744-y&amp;partnerID=40&amp;md5=57b558f5cd815f5908bb19a6b5b1b273"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124522335&amp;doi=10.1007%2fs41666-022-00114-1&amp;partnerID=40&amp;md5=f9fbc5269b957f488746c88baa8d5ece"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-166545911-2%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123914457&amp;doi=10.1017%2fS0269888921000138&amp;partnerID=40&amp;md5=dc9fad4df0c27ca5d2b1911c2a6bf6b1"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125731549&amp;doi=10.1109%2fACCESS.2022.3155521&amp;partnerID=40&amp;md5=ec9792a9942aedcce2d3cf928699e276"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128788346&amp;doi=10.1007%2f978-3-030-99739-7_41&amp;partnerID=40&amp;md5=2f9e48c99acae3554260a344c24d84ad"/>
        <dcterms:hasPart rdf:resource="urn:isbn:19341768%20(ISSN);%20978-988156380-4%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:19449925%20(ISSN);%20978-166540507-2%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103999557&amp;doi=10.1007%2fs10489-021-02348-9&amp;partnerID=40&amp;md5=5d77f9f4aa91ead63813a5872b6fabf3"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104884252&amp;doi=10.1007%2fs40747-020-00218-4&amp;partnerID=40&amp;md5=64c5fef9f48e2bd51b2249c33eda0909"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-195408540-4%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122866794&amp;doi=10.1155%2f2021%2f5089236&amp;partnerID=40&amp;md5=156237d4adb9df296e3115c6c79a3bf8"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103703478&amp;doi=10.14569%2fIJACSA.2021.0120359&amp;partnerID=40&amp;md5=c5ea238f32d7792407c3662e592b319c"/>
        <dcterms:hasPart rdf:resource="urn:isbn:03029743%20(ISSN);%20978-303072112-1%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116194939&amp;doi=10.1016%2fj.websem.2021.100661&amp;partnerID=40&amp;md5=6eda9f8927afc10e0450b3606b1407db"/>
        <dcterms:hasPart rdf:resource="urn:isbn:10450823%20(ISSN);%20978-099924119-6%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-145036822-3%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-172815374-2%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073937450&amp;doi=10.1007%2fs00500-019-04367-8&amp;partnerID=40&amp;md5=c8aa7968b3ec58e13974e8fed347a699"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080939527&amp;doi=10.1109%2fJSTARS.2019.2948921&amp;partnerID=40&amp;md5=3a00b253333f310805af108e25fedfde"/>
        <dcterms:hasPart rdf:resource="urn:isbn:03029743%20(ISSN);%20978-303045441-8%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:03029743%20(ISSN);%20978-303049460-5%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:17426588%20(ISSN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083273058&amp;doi=10.2478%2fcait-2020-0008&amp;partnerID=40&amp;md5=7ec3ba51575723c7e47c7ed66b6b17a8"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073096103&amp;doi=10.1016%2fj.csl.2019.101023&amp;partnerID=40&amp;md5=2408364d096cf5dc9fdacd88ab4b59aa"/>
        <dcterms:hasPart rdf:resource="urn:isbn:18770509%20(ISSN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-145036674-8%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:15206149%20(ISSN);%20978-147998131-1%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066883874&amp;doi=10.1145%2f3308558.3314124&amp;partnerID=40&amp;md5=a649092c9247b659b9530fb4c14d0f89"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062591784&amp;doi=10.1145%2f3309706&amp;partnerID=40&amp;md5=dc4c008b9e1661602d5bfc9eed8bd2f8"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073109629&amp;doi=10.1016%2fj.procs.2019.08.179&amp;partnerID=40&amp;md5=ce4334a42c6373cd5152d7392d818446"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-172811985-4%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069962310&amp;doi=10.1109%2fTASLP.2019.2926125&amp;partnerID=40&amp;md5=847142e342248c27afe38d0df2da1aa7"/>
        <dcterms:hasPart rdf:resource="urn:isbn:18650929%20(ISSN);%20978-981139186-6%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062959942&amp;doi=10.7544%2fissn1000-1239.2018.20180623&amp;partnerID=40&amp;md5=3709f0bfed83f7f3b1ae371eece64fa5"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047547966&amp;doi=10.1016%2fj.ipm.2018.05.001&amp;partnerID=40&amp;md5=a582ae56711263aa7eff71669f3abe80"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026525620&amp;doi=10.1016%2fj.cogsys.2017.07.002&amp;partnerID=40&amp;md5=9d75b9df4b05758867cca261ae1f79cd"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-145036012-8%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049120836&amp;doi=10.1016%2fj.procs.2018.05.090&amp;partnerID=40&amp;md5=1138498159b4bc2211b80395e8c874d1"/>
        <dcterms:hasPart rdf:resource="urn:isbn:03029743%20(ISSN);%20978-303003839-7%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:03029743%20(ISSN);%20978-303000670-9%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053131809&amp;doi=10.1016%2fj.procs.2018.08.221&amp;partnerID=40&amp;md5=2ccf5954618e5c6a8d32354d0cd1b9f9"/>
        <dcterms:hasPart rdf:resource="urn:isbn:10450823%20(ISSN);%20978-099924112-7%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021799133&amp;doi=10.1016%2fj.knosys.2017.06.030&amp;partnerID=40&amp;md5=860d0eaa7f8c40c9c09b5ed65f5d2cfb"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015285840&amp;partnerID=40&amp;md5=0d2117c10fd37754d441df686d1dec0d"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-194562653-1%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019964925&amp;doi=10.1016%2fj.specom.2017.05.001&amp;partnerID=40&amp;md5=76110d9c502fc41ff5a1f1daa1ed513e"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038030204&amp;doi=10.1017%2fS1351324917000304&amp;partnerID=40&amp;md5=72938518ba7c5b521e78bff2539569ab"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-145034655-9%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168887254&amp;doi=10.1007%2fs00146-023-01756-4&amp;partnerID=40&amp;md5=b8862d84a40c54ca88126cffec89259e"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171432053&amp;doi=10.1007%2fs11934-023-01184-3&amp;partnerID=40&amp;md5=800d9fc4add0957dab0c0fe5bafdd32e"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178203067&amp;doi=10.1002%2fadma.202306733&amp;partnerID=40&amp;md5=496aa6ff8f35a8d27c127750a652da32"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170633110&amp;partnerID=40&amp;md5=cb3bdc7cb66f22fc9e9f7b951d347fd9"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164593928&amp;partnerID=40&amp;md5=fef886c8c7e3dd05e778fa4ef24dcc51"/>
        <dcterms:hasPart rdf:resource="urn:isbn:10816011%20(ISSN);%20978-166549336-9%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117364329&amp;doi=10.1007%2fs11227-021-04097-5&amp;partnerID=40&amp;md5=69ec1e9813b5cc2a2339c11d8c4f1c44"/>
        <dcterms:hasPart rdf:resource="urn:isbn:0736587X%20(ISSN);%20978-195591721-6%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134360349&amp;doi=10.1021%2fjacs.2c04278&amp;partnerID=40&amp;md5=80789726e4b31cc3d3b0227a66797e2e"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121749243&amp;doi=10.1016%2fj.petrol.2021.110027&amp;partnerID=40&amp;md5=b3710465c63617ddad4bda86b855d1ce"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150309440&amp;partnerID=40&amp;md5=b5a9fbb2129ec8d6a35e0d911e90d0fd"/>
        <dcterms:hasPart rdf:resource="urn:isbn:03029743%20(ISSN);%20978-303079941-0%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099723497&amp;doi=10.1109%2fACCESS.2021.3052566&amp;partnerID=40&amp;md5=ac1d068e2600ec8172edb89071c70d7f"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084041788&amp;partnerID=40&amp;md5=9842addd1346caf3cfbcb42bc5c091d7"/>
        <dcterms:hasPart rdf:resource="urn:isbn:0736587X%20(ISSN);%20979-889176094-3%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:16130073%20(ISSN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:18650929%20(ISSN);%20978-303135923-1%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:23673370%20(ISSN);%20978-303137962-8%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:03029743%20(ISSN);%20978-303136189-0%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:21945357%20(ISSN);%20978-981153382-2%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:03029743%20(ISSN);%20978-303060275-8%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182668818&amp;doi=10.1007%2fs00500-023-09536-4&amp;partnerID=40&amp;md5=1d795064a7af66fb101218e30ea19633"/>
        <dcterms:hasPart rdf:resource="urn:isbn:979-840070877-0%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197849548&amp;doi=10.11591%2fijeecs.v35.i3.pp1751-1764&amp;partnerID=40&amp;md5=9bad82202a24650da58e58ce4958cf1e"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189747049&amp;doi=10.1093%2fjamia%2focae015&amp;partnerID=40&amp;md5=e04fc88e6b58a4aa402e8555dc031061"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192194727&amp;doi=10.1007%2fs44163-024-00126-3&amp;partnerID=40&amp;md5=bfd9ba44246d420e414d74efadf63495"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183372327&amp;doi=10.3390%2fpr12010058&amp;partnerID=40&amp;md5=2314937759e1e24bdb618fe331e0b75e"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162027719&amp;doi=10.3389%2ffeduc.2023.1183162&amp;partnerID=40&amp;md5=8d14b971e2143e5afa9c432ade0dc94a"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187472990&amp;doi=10.1001%2fjamanetworkopen.2024.0357&amp;partnerID=40&amp;md5=f69bdc78efc778e78319f1f817bc41fb"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140488281&amp;doi=10.3390%2fapp122010200&amp;partnerID=40&amp;md5=9f70b7efa1f63e03c9ee7961316edb7b"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_19">
        <dc:title>Rever_Q2</dc:title>
        <dcterms:hasPart rdf:resource="urn:isbn:978-172816926-2%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-195073761-1%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048523859&amp;doi=10.1016%2fj.landusepol.2018.05.002&amp;partnerID=40&amp;md5=de03f09b1dd5f5f967aabde959d56b02"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153204653&amp;doi=10.5114%2fBIOLSPORT.2023.125623&amp;partnerID=40&amp;md5=72f951f36288364372691a144dec3114"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148608599&amp;doi=10.37074%2fjalt.2023.6.1.9&amp;partnerID=40&amp;md5=5aed8395fdc3cde275b039226634df1f"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163727625&amp;doi=10.3390%2ffi15060192&amp;partnerID=40&amp;md5=983680a889ad15454028f841da828a9f"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097333449&amp;partnerID=40&amp;md5=45a008708bb4033f27b1b4f818cc9724"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092733644&amp;partnerID=40&amp;md5=df1a6dc84cf71b099f2476907f7c8e17"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-194562675-3%20(ISBN)"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027923512&amp;doi=10.1016%2fj.ipm.2016.06.006&amp;partnerID=40&amp;md5=0bc9131b66bff800ad04a151931fd850"/>
        <dcterms:hasPart rdf:resource="https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189943350&amp;doi=10.14569%2fIJACSA.2024.0150379&amp;partnerID=40&amp;md5=b3c38533a1a335a606e07d42a7489644"/>
    </z:Collection>
</rdf:RDF>
